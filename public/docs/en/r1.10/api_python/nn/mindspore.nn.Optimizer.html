

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>mindspore.nn.Optimizer &mdash; MindSpore master documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/training.css" type="text/css" />
   
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/js/training.js"></script>
        
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="mindspore.nn.CellList" href="mindspore.nn.CellList.html" />
    <link rel="prev" title="mindspore.nn.LossBase" href="mindspore.nn.LossBase.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../design/auto_gradient.html">Functional Differential Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/distributed_training_design.html">Distributed Training Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/mindir.html">MindSpore IR (MindIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/thor.html">Second Order Optimizer</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.10/training_visual_design.html">Design of Visualization↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Specification</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../note/benchmark.html">Benchmarks</a></li>
<li class="toctree-l1"><a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.10/README.md#table-of-contents">Network List↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/operator_list.html">API List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/syntax_list.html">Syntax Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.audio.html">mindspore.dataset.audio</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.config.html">mindspore.dataset.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.text.html">mindspore.dataset.text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.vision.html">mindspore.dataset.vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../mindspore.nn.html">mindspore.nn</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../mindspore.nn.html#basic-block">Basic Block</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.Cell.html">mindspore.nn.Cell</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.GraphCell.html">mindspore.nn.GraphCell</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.LossBase.html">mindspore.nn.LossBase</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">mindspore.nn.Optimizer</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#container">Container</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#wrapper-layer">Wrapper Layer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#convolutional-neural-network-layer">Convolutional Neural Network Layer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#recurrent-neural-network-layer">Recurrent Neural Network Layer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#embedding-layer">Embedding Layer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#nonlinear-activation-function-layer">Nonlinear Activation Function Layer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#linear-layer">Linear Layer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#dropout-layer">Dropout Layer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#normalization-layer">Normalization Layer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#pooling-layer">Pooling Layer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#padding-layer">Padding Layer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#loss-function">Loss Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#optimizer">Optimizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#evaluation-metrics">Evaluation Metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#dynamic-learning-rate">Dynamic Learning Rate</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#image-processing-layer">Image Processing Layer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#matrix-processing">Matrix Processing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#tools">Tools</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#mathematical-operations">Mathematical Operations</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.nn.transformer.html">mindspore.nn.transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.ops.function.html">mindspore.ops.function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.10/api_cpp/mindspore.html">C++ API↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Mapping</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../note/api_mapping/pytorch_api_mapping.html">PyTorch and MindSpore API Mapping Table</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/api_mapping/tensorflow_api_mapping.html">TensorFlow and MindSpore API Mapping Table</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Migration Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/enveriment_preparation.html">Environment Preparation and Information Acquisition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/analysis_and_preparation.html">Model Analysis and Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/model_development/model_development.html">Constructing MindSpore Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/debug_and_tune.html">Debugging and Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/sample_code.html">Network Migration Debugging Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/faq.html">FAQs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/typical_api_comparision.html">Differences Between MindSpore and PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/use_third_party_op.html">Using Third-party Operator Libraries Based on Customized Interfaces</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../faq/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/data_processing.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/implement_problem.html">Implement Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/network_compilation.html">Network Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/operators_compile.html">Operators Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/usage_migrate_3rd.html">Migration from a Third-party Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/precision_tuning.html">Precision Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/distributed_configure.html">Distributed Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/feature_advice.html">Feature Advice</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../RELEASE.html">Release Notes</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../mindspore.nn.html">mindspore.nn</a> &raquo;</li>
      <li>mindspore.nn.Optimizer</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/api_python/nn/mindspore.nn.Optimizer.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="mindspore-nn-optimizer">
<h1>mindspore.nn.Optimizer<a class="headerlink" href="#mindspore-nn-optimizer" title="Permalink to this headline"></a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.Optimizer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">Optimizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parameters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_decay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/mindspore/nn/optim/optimizer.html#Optimizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Optimizer" title="Permalink to this definition"></a></dt>
<dd><p>Base class for updating parameters. Never use this class directly, but instantiate one of its subclasses instead.</p>
<p>Grouping parameters is supported. If parameters are grouped, different strategy of <cite>learning_rate</cite>, <cite>weight_decay</cite>
and <cite>grad_centralization</cite> can be applied to each group.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If parameters are not grouped, the <cite>weight_decay</cite> in optimizer will be applied on the network parameters without
‘beta’ or ‘gamma’ in their names. Users can group parameters to change the strategy of decaying weight. When
parameters are grouped, each group can set <cite>weight_decay</cite>. If not, the <cite>weight_decay</cite> in optimizer will be
applied.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>learning_rate</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference internal" href="../mindspore/mindspore.Tensor.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><em>Iterable</em><em>, </em><em>LearningRateSchedule</em><em>]</em>) – <ul>
<li><p>float: The fixed learning rate value. Must be equal to or greater than 0.</p></li>
<li><p>int: The fixed learning rate value. Must be equal to or greater than 0. It will be converted to float.</p></li>
<li><p>Tensor: Its value should be a scalar or a 1-D vector. For scalar, fixed learning rate will be applied.
For vector, learning rate is dynamic, then the i-th step will take the i-th value as the learning rate.</p></li>
<li><p>Iterable: Learning rate is dynamic. The i-th step will take the i-th value as the learning rate.</p></li>
<li><p>LearningRateSchedule: Learning rate is dynamic. During training, the optimizer calls the instance of
LearningRateSchedule with step as the input to get the learning rate of current step.</p></li>
</ul>
</p></li>
<li><p><strong>parameters</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference internal" href="../mindspore/mindspore.Parameter.html#mindspore.Parameter" title="mindspore.Parameter"><em>Parameter</em></a><em>]</em><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a><em>]</em><em>]</em>) – <p>Must be list of <cite>Parameter</cite> or list of <cite>dict</cite>. When the
<cite>parameters</cite> is a list of <cite>dict</cite>, the string “params”, “lr”, “weight_decay”, “grad_centralization” and
“order_params” are the keys can be parsed.</p>
<ul>
<li><p>params: Required. Parameters in current group. The value must be a list of <cite>Parameter</cite>.</p></li>
<li><p>lr: Optional. If “lr” in the keys, the value of corresponding learning rate will be used.
If not, the <cite>learning_rate</cite> in optimizer will be used. Fixed and dynamic learning rate are supported.</p></li>
<li><p>weight_decay: Optional. If “weight_decay” in the keys, the value of corresponding weight decay
will be used. If not, the <cite>weight_decay</cite> in the optimizer will be used.</p></li>
<li><p>grad_centralization: Optional. Must be Boolean. If “grad_centralization” is in the keys, the set value
will be used. If not, the <cite>grad_centralization</cite> is False by default. This configuration only works on the
convolution layer.</p></li>
<li><p>order_params: Optional. When parameters is grouped, this usually is used to maintain the order of
parameters that appeared in the network to improve performance. The value should be parameters whose
order will be followed in optimizer.
If <cite>order_params</cite> in the keys, other keys will be ignored and the element of ‘order_params’ must be in
one group of <cite>params</cite>.</p></li>
</ul>
</p></li>
<li><p><strong>weight_decay</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em>) – An int or a floating point value for the weight decay.
It must be equal to or greater than 0.
If the type of <cite>weight_decay</cite> input is int, it will be converted to float. Default: 0.0.</p></li>
<li><p><strong>loss_scale</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A floating point value for the loss scale. It must be greater than 0. If the
type of <cite>loss_scale</cite> input is int, it will be converted to float. In general, use the default value. Only
when <cite>FixedLossScaleManager</cite> is used for training and the <cite>drop_overflow_update</cite> in
<cite>FixedLossScaleManager</cite> is set to False, this value needs to be the same as the <cite>loss_scale</cite> in
<cite>FixedLossScaleManager</cite>. Refer to class <a class="reference internal" href="../amp/mindspore.amp.FixedLossScaleManager.html#mindspore.amp.FixedLossScaleManager" title="mindspore.amp.FixedLossScaleManager"><code class="xref py py-class docutils literal notranslate"><span class="pre">mindspore.amp.FixedLossScaleManager</span></code></a> for more details.
Default: 1.0.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>learning_rate</cite> is not one of int, float, Tensor, Iterable, LearningRateSchedule.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If element of <cite>parameters</cite> is neither Parameter nor dict.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>loss_scale</cite> is not a float.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>weight_decay</cite> is neither float nor int.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If <cite>loss_scale</cite> is less than or equal to 0.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If <cite>weight_decay</cite> is less than 0.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If <cite>learning_rate</cite> is a Tensor, but the dimension of tensor is greater than 1.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Optimizer.broadcast_params">
<span class="sig-name descname"><span class="pre">broadcast_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optim_result</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/mindspore/nn/optim/optimizer.html#Optimizer.broadcast_params"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Optimizer.broadcast_params" title="Permalink to this definition"></a></dt>
<dd><p>Apply Broadcast operations in the sequential order of parameter groups.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>optim_result</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – The results of updating parameters. This input is used to ensure that the parameters are
updated before they are broadcast.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>bool, the status flag.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Optimizer.decay_weight">
<span class="sig-name descname"><span class="pre">decay_weight</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gradients</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/mindspore/nn/optim/optimizer.html#Optimizer.decay_weight"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Optimizer.decay_weight" title="Permalink to this definition"></a></dt>
<dd><p>Weight decay.</p>
<p>An approach to reduce the overfitting of a deep learning neural network model. User-defined optimizers based
on <a class="reference internal" href="#mindspore.nn.Optimizer" title="mindspore.nn.Optimizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">mindspore.nn.Optimizer</span></code></a> can also call this interface to apply weight decay.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>gradients</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference internal" href="../mindspore/mindspore.Tensor.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>]</em>) – The gradients of network parameters, and have the same shape as the parameters.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>tuple[Tensor], The gradients after weight decay.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Optimizer.flatten_gradients">
<span class="sig-name descname"><span class="pre">flatten_gradients</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gradients</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/mindspore/nn/optim/optimizer.html#Optimizer.flatten_gradients"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Optimizer.flatten_gradients" title="Permalink to this definition"></a></dt>
<dd><p>Flatten gradients into several chunk tensors grouped by data type if network parameters are flattened.</p>
<p>A method to enable performance improvement by using contiguous memory for parameters and gradients.
User-defined optimizers based on <a class="reference internal" href="#mindspore.nn.Optimizer" title="mindspore.nn.Optimizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">mindspore.nn.Optimizer</span></code></a> should call this interface to support
contiguous memory for network parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>gradients</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference internal" href="../mindspore/mindspore.Tensor.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>]</em>) – The gradients of network parameters.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>tuple[Tensor], The gradients after flattened, or the original gradients if parameters are not flattened.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Optimizer.get_lr">
<span class="sig-name descname"><span class="pre">get_lr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/mindspore/nn/optim/optimizer.html#Optimizer.get_lr"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Optimizer.get_lr" title="Permalink to this definition"></a></dt>
<dd><p>The optimizer calls this interface to get the learning rate for the current step. User-defined optimizers based
on <a class="reference internal" href="#mindspore.nn.Optimizer" title="mindspore.nn.Optimizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">mindspore.nn.Optimizer</span></code></a> can also call this interface before updating the parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>float, the learning rate of current step.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Optimizer.get_lr_parameter">
<span class="sig-name descname"><span class="pre">get_lr_parameter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">param</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/mindspore/nn/optim/optimizer.html#Optimizer.get_lr_parameter"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Optimizer.get_lr_parameter" title="Permalink to this definition"></a></dt>
<dd><p>When parameters is grouped and learning rate is different for each group, get the learning rate of the specified
<cite>param</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>param</strong> (<em>Union</em><em>[</em><a class="reference internal" href="../mindspore/mindspore.Parameter.html#mindspore.Parameter" title="mindspore.Parameter"><em>Parameter</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference internal" href="../mindspore/mindspore.Parameter.html#mindspore.Parameter" title="mindspore.Parameter"><em>Parameter</em></a><em>]</em><em>]</em>) – The <cite>Parameter</cite> or list of <cite>Parameter</cite>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Parameter, single <cite>Parameter</cite> or <cite>list[Parameter]</cite> according to the input type. If learning rate is dynamic,
<cite>LearningRateSchedule</cite> or <cite>list[LearningRateSchedule]</cite> that used to calculate the learning rate will be
returned.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">conv_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">no_conv_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">group_params</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">conv_params</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">0.05</span><span class="p">},</span>
<span class="gp">... </span>                <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">no_conv_params</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">}]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">group_params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">conv_lr</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">get_lr_parameter</span><span class="p">(</span><span class="n">conv_params</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">conv_lr</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
<span class="go">0.05</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Optimizer.get_weight_decay">
<span class="sig-name descname"><span class="pre">get_weight_decay</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/mindspore/nn/optim/optimizer.html#Optimizer.get_weight_decay"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Optimizer.get_weight_decay" title="Permalink to this definition"></a></dt>
<dd><p>The optimizer calls this interface to get the weight decay value for the current step.
User-defined optimizers based on <a class="reference internal" href="#mindspore.nn.Optimizer" title="mindspore.nn.Optimizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">mindspore.nn.Optimizer</span></code></a> can also call this interface
before updating the parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>float, the weight decay value of current step.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Optimizer.gradients_centralization">
<span class="sig-name descname"><span class="pre">gradients_centralization</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gradients</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/mindspore/nn/optim/optimizer.html#Optimizer.gradients_centralization"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Optimizer.gradients_centralization" title="Permalink to this definition"></a></dt>
<dd><p>Gradients centralization.</p>
<p>A method for optimizing convolutional layer parameters to improve the training speed of a deep learning neural
network model. User-defined optimizers based on <a class="reference internal" href="#mindspore.nn.Optimizer" title="mindspore.nn.Optimizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">mindspore.nn.Optimizer</span></code></a> can also call this interface to
centralize gradients.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>gradients</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference internal" href="../mindspore/mindspore.Tensor.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>]</em>) – The gradients of network parameters, and have the same shape as the parameters.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>tuple[Tensor], The gradients after gradients centralization.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Optimizer.scale_grad">
<span class="sig-name descname"><span class="pre">scale_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gradients</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/mindspore/nn/optim/optimizer.html#Optimizer.scale_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Optimizer.scale_grad" title="Permalink to this definition"></a></dt>
<dd><p>Restore gradients for mixed precision.</p>
<p>User-defined optimizers based on <a class="reference internal" href="#mindspore.nn.Optimizer" title="mindspore.nn.Optimizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">mindspore.nn.Optimizer</span></code></a> can also call this interface to restore
gradients.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>gradients</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference internal" href="../mindspore/mindspore.Tensor.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>]</em>) – The gradients of network parameters, and have the same shape as the parameters.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>tuple[Tensor], The gradients after loss scale.</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="mindspore.nn.Optimizer.target">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">target</span></span><a class="headerlink" href="#mindspore.nn.Optimizer.target" title="Permalink to this definition"></a></dt>
<dd><p>The property is used to determine whether the parameter is updated on host or device. The input type is str
and can only be ‘CPU’, ‘Ascend’ or ‘GPU’.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="mindspore.nn.Optimizer.unique">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">unique</span></span><a class="headerlink" href="#mindspore.nn.Optimizer.unique" title="Permalink to this definition"></a></dt>
<dd><p>Whether to make the gradients unique in optimizer. Generally, it is used in sparse networks. Set to True if the
gradients of the optimizer are sparse, while set to False if the forward network has made the parameters unique,
that is, the gradients of the optimizer is no longer sparse.
The default value is True when it is not set.</p>
</dd></dl>

</dd></dl>

</section>


           </div>
           
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="mindspore.nn.LossBase.html" class="btn btn-neutral float-left" title="mindspore.nn.LossBase" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="mindspore.nn.CellList.html" class="btn btn-neutral float-right" title="mindspore.nn.CellList" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
</body>
</html>