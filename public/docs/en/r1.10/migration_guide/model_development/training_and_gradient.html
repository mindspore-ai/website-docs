<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Network Training and Gadient Derivation &mdash; MindSpore master documentation</title>
      <link rel="stylesheet" href="../../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/training.css" type="text/css" /><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/js/training.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Inference and Training Process" href="training_and_evaluation_procession.html" />
    <link rel="prev" title="Learning Rate and Optimizer" href="learning_rate_and_optimizer.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../design/auto_gradient.html">Functional Differential Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/distributed_training_design.html">Distributed Training Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/mindir.html">MindSpore IR (MindIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/thor.html">Second Order Optimizer</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.10/training_visual_design.html">Design of Visualization↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Specification</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../note/benchmark.html">Benchmarks</a></li>
<li class="toctree-l1"><a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.10/README.md#table-of-contents">Network List↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/operator_list.html">API List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/syntax_list.html">Syntax Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.dataset.audio.html">mindspore.dataset.audio</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.dataset.config.html">mindspore.dataset.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.dataset.text.html">mindspore.dataset.text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.dataset.vision.html">mindspore.dataset.vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.nn.transformer.html">mindspore.nn.transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.ops.function.html">mindspore.ops.function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.10/api_cpp/mindspore.html">C++ API↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Mapping</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../note/api_mapping/pytorch_api_mapping.html">PyTorch and MindSpore API Mapping Table</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/api_mapping/tensorflow_api_mapping.html">TensorFlow and MindSpore API Mapping Table</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Migration Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../enveriment_preparation.html">Environment Preparation and Information Acquisition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../analysis_and_preparation.html">Model Analysis and Preparation</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="model_development.html">Constructing MindSpore Network</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="dataset.html">Constructing Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_and_loss.html">Network Entity and Loss Construction</a></li>
<li class="toctree-l2"><a class="reference internal" href="learning_rate_and_optimizer.html">Learning Rate and Optimizer</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Network Training and Gadient Derivation</a></li>
<li class="toctree-l2"><a class="reference internal" href="training_and_evaluation_procession.html">Inference and Training Process</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../debug_and_tune.html">Debugging and Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sample_code.html">Network Migration Debugging Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">FAQs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../typical_api_comparision.html">Differences Between MindSpore and PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use_third_party_op.html">Using Third-party Operator Libraries Based on Customized Interfaces</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../faq/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/data_processing.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/implement_problem.html">Implement Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/network_compilation.html">Network Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/operators_compile.html">Operators Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/usage_migrate_3rd.html">Migration from a Third-party Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/precision_tuning.html">Precision Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/distributed_configure.html">Distributed Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/feature_advice.html">Feature Advice</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="model_development.html">Constructing MindSpore Network</a> &raquo;</li>
      <li>Network Training and Gadient Derivation</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/migration_guide/model_development/training_and_gradient.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="network-training-and-gadient-derivation">
<h1>Network Training and Gadient Derivation<a class="headerlink" href="#network-training-and-gadient-derivation" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.10/docs/mindspore/source_en/migration_guide/model_development/training_and_gradient.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.10/resource/_static/logo_source_en.png" /></a></p>
<section id="automatic-differentiation">
<h2>Automatic Differentiation<a class="headerlink" href="#automatic-differentiation" title="Permalink to this headline"></a></h2>
<p>After the forward network is constructed, MindSpore provides an interface to <a class="reference external" href="https://mindspore.cn/tutorials/en/r1.10/beginner/autograd.html">automatic differentiation</a> to calculate the gradient results of the model.
In the tutorial of <a class="reference external" href="https://mindspore.cn/tutorials/en/r1.10/advanced/derivation.html">automatic derivation</a>, some descriptions of various gradient calculation scenarios are given.</p>
</section>
<section id="network-training">
<h2>Network Training<a class="headerlink" href="#network-training" title="Permalink to this headline"></a></h2>
<p>The entire training network consists of the forward network (network and loss function), automatic gradient derivation and optimizer update. MindSpore provides three ways to implement this process.</p>
<ol class="arabic simple">
<li><p>Encapsulate <code class="docutils literal notranslate"><span class="pre">model</span></code> and perform network training by using <code class="docutils literal notranslate"><span class="pre">model.train</span></code> or <code class="docutils literal notranslate"><span class="pre">model.fit</span></code>, such as <a class="reference external" href="https://mindspore.cn/tutorials/en/r1.10/beginner/train.html">model training</a>.</p></li>
<li><p>Apply the encapsulated <a class="reference external" href="https://www.mindspore.cn/docs/en/r1.10/api_python/nn/mindspore.nn.TrainOneStepCell.html">TrainOneStepCell</a> and <a class="reference external" href="https://www.mindspore.cn/docs/en/r1.10/api_python/nn/mindspore.nn.TrainOneStepWithLossScaleCell.html">TrainOneStepWithLossScaleCell</a> separately to common training process and training process with <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r1.10/others/mixed_precision.html">loss_scale</a>, such as <a class="reference external" href="https://mindspore.cn/tutorials/en/r1.10/beginner/quick_start.html">Quick Start: Linear Fitting</a>.</p></li>
<li><p>Customize training Cell.</p></li>
</ol>
<section id="customizing-training-cell">
<h3>Customizing training Cell<a class="headerlink" href="#customizing-training-cell" title="Permalink to this headline"></a></h3>
<p>The first two methods are illustrated with two examples from the official website. For the customized training Cell, we first review what is done in the TrainOneStepCell:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span><span class="p">,</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">mindspore.parallel._utils</span> <span class="kn">import</span> <span class="p">(</span><span class="n">_get_device_num</span><span class="p">,</span> <span class="n">_get_gradients_mean</span><span class="p">,</span>
                                       <span class="n">_get_parallel_mode</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">TrainOneStepCell</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TrainOneStepCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">auto_prefix</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">network</span> <span class="o">=</span> <span class="n">network</span>  <span class="c1"># Network structure with loss</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">set_grad</span><span class="p">()</span>   <span class="c1"># Required for PYNATIVE mode. If True, the inverse network requiring the computation of gradients will be generated when the forward network is executed.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>   <span class="c1"># Optimizer for parameter updates</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">parameters</span>    <span class="c1"># Get the parameters of the optimizer</span>

        <span class="c1"># Related logic of parallel computation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reducer_flag</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad_reducer</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">identity</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">parallel_mode</span> <span class="o">=</span> <span class="n">_get_parallel_mode</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reducer_flag</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parallel_mode</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">DATA_PARALLEL</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">HYBRID_PARALLEL</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reducer_flag</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">_get_gradients_mean</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">degree</span> <span class="o">=</span> <span class="n">_get_device_num</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">grad_reducer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">DistributedGradReducer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">degree</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
        <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">,</span> <span class="n">grad_position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">)</span>   <span class="c1"># obtain the loss and get the gradient of all Parameter free variables</span>
        <span class="c1"># grads = grad_op(grads)    # Some computing logic for the gradient can be added here, such as gradient clipping</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_reducer</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>  <span class="c1"># Gradient aggregation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>    <span class="c1"># Optimizer updates parameters</span>
        <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<p>The whole training process can be encapsulated into a Cell, in which the forward computation, backward gradient derivation and parameter update of the network can be realized, and we can do some special processing on the gradient after obtaining it.</p>
<section id="gradient-clipping">
<h4>Gradient Clipping<a class="headerlink" href="#gradient-clipping" title="Permalink to this headline"></a></h4>
<p>When there is gradient explosion or particularly large gradient during the training process, in a case that training is not stable, you can consider adding gradient clipping. Here is an example of the common use of global_norm for gradient clipping scenarios.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">ops</span>

<span class="n">_grad_scale</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MultitypeFuncGraph</span><span class="p">(</span><span class="s2">&quot;grad_scale&quot;</span><span class="p">)</span>

<span class="nd">@_grad_scale</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">tensor_grad_scale</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">grad</span> <span class="o">*</span> <span class="n">ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">Reciprocal</span><span class="p">()(</span><span class="n">scale</span><span class="p">),</span> <span class="n">ops</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">grad</span><span class="p">))</span>

<span class="k">class</span> <span class="nc">MyTrainOneStepCell</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">TrainOneStepWithLossScaleCell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Network training package class with gradient clip.</span>

<span class="sd">    Append an optimizer to the training network after that the construct function</span>
<span class="sd">    can be called to create the backward graph.</span>

<span class="sd">    Args:</span>
<span class="sd">        network (Cell): The training network.</span>
<span class="sd">        optimizer (Cell): Optimizer for updating the weights.</span>
<span class="sd">        sens (Number): The adjust parameter. Default value is 1.0.</span>
<span class="sd">        grad_clip (bool): Whether clip gradients. Default value is False.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scale_sense</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">grad_clip</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scale_sense</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)):</span>
            <span class="n">scale_sense</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">FixedLossScaleUpdateCell</span><span class="p">(</span><span class="n">scale_sense</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MyTrainOneStepCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scale_sense</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad_clip</span> <span class="o">=</span> <span class="n">grad_clip</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">img_shape</span><span class="p">,</span> <span class="n">gt_bboxe</span><span class="p">,</span> <span class="n">gt_label</span><span class="p">,</span> <span class="n">gt_num</span><span class="p">):</span>
        <span class="c1"># Most are attributes and methods of base class, and refer to the corresponding base class API for details</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">img_shape</span><span class="p">,</span> <span class="n">gt_bboxe</span><span class="p">,</span> <span class="n">gt_label</span><span class="p">,</span> <span class="n">gt_num</span><span class="p">)</span>
        <span class="n">scaling_sens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_sense</span>
        <span class="c1"># Start floating point overflow detection. Create and clear overflow detection status</span>
        <span class="n">status</span><span class="p">,</span> <span class="n">scaling_sens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_overflow_check</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">scaling_sens</span><span class="p">)</span>
        <span class="c1"># Multiply the gradient by a scale to prevent the gradient from overflowing</span>
        <span class="n">scaling_sens_filled</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span> <span class="o">*</span> <span class="n">ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">scaling_sens</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">loss</span><span class="p">))</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">,</span> <span class="n">weights</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">img_shape</span><span class="p">,</span> <span class="n">gt_bboxe</span><span class="p">,</span> <span class="n">gt_label</span><span class="p">,</span> <span class="n">gt_num</span><span class="p">,</span> <span class="n">scaling_sens_filled</span><span class="p">)</span>
        <span class="c1"># Calculate the true gradient value by dividing the obtained gradient by the scale</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hyper_map</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_grad_scale</span><span class="p">,</span> <span class="n">scaling_sens</span><span class="p">),</span> <span class="n">grads</span><span class="p">)</span>
        <span class="c1"># Gradient clipping</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_clip</span><span class="p">:</span>
            <span class="n">grads</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">clip_by_global_norm</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
        <span class="c1"># Gradient aggregation</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_reducer</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>

        <span class="c1"># Get floating point overflow status</span>
        <span class="n">cond</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_overflow_status</span><span class="p">(</span><span class="n">status</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span>
        <span class="c1"># Calculate loss scaling factor based on overflow state during dynamic loss scale</span>
        <span class="n">overflow</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">process_loss_scale</span><span class="p">(</span><span class="n">cond</span><span class="p">)</span>
        <span class="c1"># If there is no overflow, execute the optimizer to update the parameters</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">overflow</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">cond</span><span class="p">,</span> <span class="n">scaling_sens</span><span class="o">.</span><span class="n">value</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="gradient-accumulation">
<h4>Gradient Accumulation<a class="headerlink" href="#gradient-accumulation" title="Permalink to this headline"></a></h4>
<p>Gradient accumulation is a method that data samples of the training neural network are split into several small Batch  by Batch, and are calculated in order to solve the problem that Batch size is too large due to lack of memory, or the OOM, that is, the neural network can not be trained or the network model is too large to load.</p>
<p>Please refer to <a class="reference external" href="https://mindspore.cn/tutorials/experts/en/r1.10/others/gradient_accumulation.html">Gradient Accumulation</a> for details.</p>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="learning_rate_and_optimizer.html" class="btn btn-neutral float-left" title="Learning Rate and Optimizer" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="training_and_evaluation_procession.html" class="btn btn-neutral float-right" title="Inference and Training Process" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>