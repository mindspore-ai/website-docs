

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>mindspore.ops.ApplyPowerSign &mdash; MindSpore master documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/training.css" type="text/css" />
   
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/js/training.js"></script>
        
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="mindspore.ops.ApplyProximalAdagrad" href="mindspore.ops.ApplyProximalAdagrad.html" />
    <link rel="prev" title="mindspore.ops.ApplyMomentum" href="mindspore.ops.ApplyMomentum.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../design/auto_gradient.html">Functional Differential Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/distributed_training_design.html">Distributed Training Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/mindir.html">MindSpore IR (MindIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/thor.html">Second Order Optimizer</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.8/training_visual_design.html">Design of Visualization↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Specification</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../note/benchmark.html">Benchmarks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/network_list.html">Network List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/operator_list.html">Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/syntax_list.html">Syntax Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.audio.html">mindspore.dataset.audio</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.config.html">mindspore.dataset.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.text.html">mindspore.dataset.text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.vision.html">mindspore.dataset.vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.nn.transformer.html">mindspore.nn.transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../mindspore.ops.html">mindspore.ops</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#operator-primitives">Operator Primitives</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#decorators">Decorators</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../mindspore.ops.html#neural-network-layer-operators">Neural Network Layer Operators</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../mindspore.ops.html#neural-network">Neural Network</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mindspore.ops.html#loss-function">Loss Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mindspore.ops.html#activation-function">Activation Function</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="../mindspore.ops.html#optimizer">Optimizer</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.Adam.html">mindspore.ops.Adam</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.AdamNoUpdateParam.html">mindspore.ops.AdamNoUpdateParam</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.AdamWeightDecay.html">mindspore.ops.AdamWeightDecay</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.AdaptiveAvgPool2D.html">mindspore.ops.AdaptiveAvgPool2D</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyAdadelta.html">mindspore.ops.ApplyAdadelta</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyAdagrad.html">mindspore.ops.ApplyAdagrad</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyAdagradDA.html">mindspore.ops.ApplyAdagradDA</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyAdagradV2.html">mindspore.ops.ApplyAdagradV2</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyAdaMax.html">mindspore.ops.ApplyAdaMax</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyAddSign.html">mindspore.ops.ApplyAddSign</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyCenteredRMSProp.html">mindspore.ops.ApplyCenteredRMSProp</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyFtrl.html">mindspore.ops.ApplyFtrl</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyGradientDescent.html">mindspore.ops.ApplyGradientDescent</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyMomentum.html">mindspore.ops.ApplyMomentum</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">mindspore.ops.ApplyPowerSign</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyProximalAdagrad.html">mindspore.ops.ApplyProximalAdagrad</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyProximalGradientDescent.html">mindspore.ops.ApplyProximalGradientDescent</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyRMSProp.html">mindspore.ops.ApplyRMSProp</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.FusedSparseAdam.html">mindspore.ops.FusedSparseAdam</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.FusedSparseFtrl.html">mindspore.ops.FusedSparseFtrl</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.FusedSparseLazyAdam.html">mindspore.ops.FusedSparseLazyAdam</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.FusedSparseProximalAdagrad.html">mindspore.ops.FusedSparseProximalAdagrad</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.LARSUpdate.html">mindspore.ops.LARSUpdate</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.SparseApplyAdagrad.html">mindspore.ops.SparseApplyAdagrad</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.SparseApplyAdagradV2.html">mindspore.ops.SparseApplyAdagradV2</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.SparseApplyProximalAdagrad.html">mindspore.ops.SparseApplyProximalAdagrad</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.SGD.html">mindspore.ops.SGD</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.SparseApplyFtrl.html">mindspore.ops.SparseApplyFtrl</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.SparseApplyFtrlV2.html">mindspore.ops.SparseApplyFtrlV2</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../mindspore.ops.html#distance-function">Distance Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mindspore.ops.html#sampling-operator">Sampling Operator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mindspore.ops.html#image-processing">Image Processing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mindspore.ops.html#text-processing">Text Processing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#mathematical-operators">Mathematical Operators</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#tensor-operation-operator">Tensor Operation Operator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#parameter-operation-operator">Parameter Operation Operator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#data-operation-operator">Data Operation Operator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#communication-operator">Communication Operator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#debugging-operator">Debugging Operator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#sparse-operator">Sparse Operator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#other-operators">Other Operators</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#operator-information-registration">Operator Information Registration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#customizing-operator">Customizing Operator</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.ops.functional.html">mindspore.ops.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.parallel.nn.html">mindspore.parallel.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.8/api_cpp/mindspore.html">C++ API↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Mapping</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../note/api_mapping/pytorch_api_mapping.html">PyTorch and MindSpore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/api_mapping/tensorflow_api_mapping.html">TensorFlow and MindSpore</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Migration Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/preparation.html">Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/script_analysis.html">Network Script Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/script_development.html">Network Script Development</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/neural_network_debug.html">Network Debugging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/performance_optimization.html">Performance Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/inference.html">Inference Execution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/sample_code.html">Network Migration Debugging Example</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../faq/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/data_processing.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/implement_problem.html">Implement Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/network_compilation.html">Network Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/operators_compile.html">Operators Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/usage_migrate_3rd.html">Migration from a Third-party Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/precision_tuning.html">Precision Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/distributed_configure.html">Distributed Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/feature_advice.html">Feature Advice</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../RELEASE.html">Release Notes</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../mindspore.ops.html">mindspore.ops</a> &raquo;</li>
      <li>mindspore.ops.ApplyPowerSign</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/api_python/ops/mindspore.ops.ApplyPowerSign.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="mindspore-ops-applypowersign">
<h1>mindspore.ops.ApplyPowerSign<a class="headerlink" href="#mindspore-ops-applypowersign" title="Permalink to this headline"></a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ApplyPowerSign">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ApplyPowerSign</span></span><a class="reference internal" href="../../_modules/mindspore/ops/operations/nn_ops.html#ApplyPowerSign"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ApplyPowerSign" title="Permalink to this definition"></a></dt>
<dd><p>Updates relevant entries according to the AddSign algorithm.</p>
<p>The AddSign algorithm was proposed in <a class="reference external" href="https://arxiv.org/abs/1709.07417">Neural Optimizer Search with Reinforcement Learning</a>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ll} \\
    m_{t+1} = \beta * m_{t} + (1 - \beta) * g \\
    \text{update} = \exp(\text{logbase} * \text{sign_decay} * sign(g) * sign(m)) * g \\
    var = var - lr_{t+1} * \text{update}
\end{array}\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(t\)</span> represents updating step while <span class="math notranslate nohighlight">\(m\)</span> represents the 1st moment vector, <span class="math notranslate nohighlight">\(m_{t}\)</span>
is the last moment of <span class="math notranslate nohighlight">\(m_{t+1}\)</span>, <span class="math notranslate nohighlight">\(lr\)</span> represents scaling factor <cite>lr</cite>, <span class="math notranslate nohighlight">\(g\)</span> represents <cite>grad</cite>,
<span class="math notranslate nohighlight">\(\beta\)</span> represents <cite>beta</cite>.</p>
<p>All of inputs comply with the implicit type conversion rules to make the data types consistent.
If <cite>lr</cite>, <cite>logbase</cite>, <cite>sign_decay</cite> or <cite>beta</cite> is a number, the number is automatically converted to Tensor,
and the data type is consistent with the Tensor data type involved in the operation.
If inputs are tensors and have different data types, the lower priority data type will be converted to
the relatively highest priority data type.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>On Ascend, input data type of float64 is currently not supported.</p>
</div>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>var</strong> (Parameter) - Variable tensor to be updated. With float64, float32 or float16 data type.
If data type of <cite>var</cite> is float16, all inputs must have the same data type as <cite>var</cite>.
The shape is <span class="math notranslate nohighlight">\((N, *)\)</span> where <span class="math notranslate nohighlight">\(*\)</span> means, any number of additional dimensions.</p></li>
<li><p><strong>m</strong> (Parameter) - Variable tensor to be updated, has the same shape and data type as <cite>var</cite>.</p></li>
<li><p><strong>lr</strong> (Union[Number, Tensor]) - The learning rate value, should be a scalar or Tensor
with float64, float32 or float16 data type.</p></li>
<li><p><strong>logbase</strong> (Union[Number, Tensor]) - Should be a scalar or Tensor with float64, float32 or float16 data type.</p></li>
<li><p><strong>sign_decay</strong> (Union[Number, Tensor]) - Should be a scalar or Tensor with float64, float32 or
float16 data type.</p></li>
<li><p><strong>beta</strong> (Union[Number, Tensor]) - The exponential decay rate, should be a scalar or Tensor
with float64, float32 or float16 data type.</p></li>
<li><p><strong>grad</strong> (Tensor) - A tensor of the same shape and data type as <cite>var</cite>, for the gradient.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tuple of 2 Tensors, the updated parameters.</p>
<ul class="simple">
<li><p><strong>var</strong> (Tensor) - The same shape and data type as <cite>var</cite>.</p></li>
<li><p><strong>m</strong> (Tensor) - The same shape and data type as <cite>m</cite>.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If dtype of <cite>var</cite>, <cite>lr</cite>, <cite>logbase</cite>, <cite>sign_decay</cite>, <cite>beta</cite> or <cite>grad</cite> is not one of float16,</p></li>
<li><p><strong>float32</strong><strong> or </strong><strong>float64.</strong> – </p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>lr</cite>, <cite>logbase</cite>, <cite>sign_decay</cite> or <cite>beta</cite> is neither a Number nor a Tensor.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>grad</cite> is not a Tensor.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#RuntimeError" title="(in Python v3.8)"><strong>RuntimeError</strong></a> – If the data type of <cite>lr</cite>, <cite>logbase</cite>, <cite>sign_decay</cite> and <cite>grad</cite> conversion of Parameter
    is not supported.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">apply_power_sign</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ApplyPowerSign</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">var</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">],</span>
<span class="gp">... </span>                                              <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;var&quot;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
<span class="gp">... </span>                                            <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;m&quot;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">logbase</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">e</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">sign_decay</span> <span class="o">=</span> <span class="mf">0.99</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
<span class="gp">... </span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_power_sign</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">var</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">logbase</span><span class="p">,</span>
<span class="gp">... </span>                                       <span class="bp">self</span><span class="o">.</span><span class="n">sign_decay</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">out</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grad</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="go">(Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="go">[[ 5.95575690e-01,  3.89676481e-01],</span>
<span class="go"> [ 9.85252112e-02,  4.88201708e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="go">[[ 5.70000052e-01,  5.19999981e-01],</span>
<span class="go"> [ 1.89999998e-01,  6.20000064e-01]]))</span>
</pre></div>
</div>
</dd></dl>

</section>


           </div>
           
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="mindspore.ops.ApplyMomentum.html" class="btn btn-neutral float-left" title="mindspore.ops.ApplyMomentum" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="mindspore.ops.ApplyProximalAdagrad.html" class="btn btn-neutral float-right" title="mindspore.ops.ApplyProximalAdagrad" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>