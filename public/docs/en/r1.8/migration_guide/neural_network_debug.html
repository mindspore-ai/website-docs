<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Network Debugging &mdash; MindSpore master documentation</title>
      <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/training.css" type="text/css" /><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/js/training.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Performance Profiling" href="performance_optimization.html" />
    <link rel="prev" title="Call Third-Party Operators by Customized Operators" href="use_third_party_op.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../design/auto_gradient.html">Functional Differential Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/distributed_training_design.html">Distributed Training Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/mindir.html">MindSpore IR (MindIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/thor.html">Second Order Optimizer</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.8/training_visual_design.html">Design of Visualization↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Specification</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/benchmark.html">Benchmarks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/network_list.html">Network List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/operator_list.html">Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/syntax_list.html">Syntax Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.audio.html">mindspore.dataset.audio</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.config.html">mindspore.dataset.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.text.html">mindspore.dataset.text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.vision.html">mindspore.dataset.vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.transformer.html">mindspore.nn.transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.functional.html">mindspore.ops.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.parallel.nn.html">mindspore.parallel.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.8/api_cpp/mindspore.html">C++ API↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Mapping</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/pytorch_api_mapping.html">PyTorch and MindSpore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/tensorflow_api_mapping.html">TensorFlow and MindSpore</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Migration Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="preparation.html">Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="script_analysis.html">Network Script Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="script_development.html">Network Script Development</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Network Debugging</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#the-basic-process-of-network-debugging">The Basic Process of Network Debugging</a></li>
<li class="toctree-l2"><a class="reference internal" href="#common-methods-used-in-network-debugging">Common Methods Used in Network Debugging</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#process-debugging">Process Debugging</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#process-debugging-with-pynative-mode">Process Debugging with PyNative Mode</a></li>
<li class="toctree-l4"><a class="reference internal" href="#getting-more-error-messages">Getting More Error Messages</a></li>
<li class="toctree-l4"><a class="reference internal" href="#common-errors">Common Errors</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#loss-value-comparison">Loss Value Comparison</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#main-steps">Main Steps</a></li>
<li class="toctree-l4"><a class="reference internal" href="#related-issues-locating">Related Issues Locating</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#precision-debugging-tools">Precision Debugging Tools</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#customized-debugging-information">Customized Debugging Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="#hyper-parameter-optimization-with-mindoptimizer">Hyper-Parameter Optimization with MindOptimizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="#loss-value-anomaly-locating">Loss Value Anomaly Locating</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="performance_optimization.html">Performance Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="inference.html">Inference Execution</a></li>
<li class="toctree-l1"><a class="reference internal" href="sample_code.html">Network Migration Debugging Example</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/data_processing.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/implement_problem.html">Implement Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/network_compilation.html">Network Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/operators_compile.html">Operators Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/usage_migrate_3rd.html">Migration from a Third-party Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/precision_tuning.html">Precision Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/distributed_configure.html">Distributed Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/feature_advice.html">Feature Advice</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Network Debugging</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/migration_guide/neural_network_debug.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="network-debugging">
<h1>Network Debugging<a class="headerlink" href="#network-debugging" title="Permalink to this headline"></a></h1>
<p>Translator: <a class="reference external" href="https://gitee.com/deng-zhihua">Soleil</a></p>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.8/docs/mindspore/source_en/migration_guide/neural_network_debug.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.8/resource/_static/logo_source_en.png"></a></p>
<p>This chapter introduces the basic principles and common tools of Network Debugging, as well as some solutions to some common problems.</p>
<section id="the-basic-process-of-network-debugging">
<h2>The Basic Process of Network Debugging<a class="headerlink" href="#the-basic-process-of-network-debugging" title="Permalink to this headline"></a></h2>
<p>The process of Network Debugging is divided into the following steps：</p>
<ol class="arabic">
<li><p>The network process is successfully debugged with no error during the whole process of network execution, proper output of loss value, and normal completion of parameter update.</p>
<p>In general, if you use the <code class="docutils literal notranslate"><span class="pre">model.train</span></code> interface to execute a step completely without receiving an error, it means that it is executed normally and completed the parameter update; if you need to confirm accurately, you can save the checkpoint file for two consecutive steps by using the parameter <code class="docutils literal notranslate"><span class="pre">save_checkpoint_</span> <span class="pre">steps=1</span></code> in <code class="docutils literal notranslate"><span class="pre">mindspore.CheckpointConfig</span></code>, or use the <code class="docutils literal notranslate"><span class="pre">save_checkpoint</span></code> interface to save the Checkpoint file directly, and then print the weight values in the Checkpoint file with the following code to see if the weights of the two steps have changed. Finally, the update is completed.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">ckpt</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">(</span><span class="n">ckpt_path</span><span class="p">)</span>
<span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">ckpt</span><span class="p">:</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">ckpt</span><span class="p">[</span><span class="n">param</span><span class="p">]</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Multiple iterations of the network are executed to output the loss values, and the loss values have a basic convergence trend.</p></li>
<li><p>Network accuracy debugging and hyper-parameter optimization.</p></li>
</ol>
</section>
<section id="common-methods-used-in-network-debugging">
<h2>Common Methods Used in Network Debugging<a class="headerlink" href="#common-methods-used-in-network-debugging" title="Permalink to this headline"></a></h2>
<section id="process-debugging">
<h3>Process Debugging<a class="headerlink" href="#process-debugging" title="Permalink to this headline"></a></h3>
<p>This section introduces the problems and solutions during Network Debugging process after the script development is generally completed.</p>
<section id="process-debugging-with-pynative-mode">
<h4>Process Debugging with PyNative Mode<a class="headerlink" href="#process-debugging-with-pynative-mode" title="Permalink to this headline"></a></h4>
<p>For script development and network process debugging, we recommend using the PyNative mode for debugging. The PyNative mode supports executing single operators, normal functions and networks, as well as separate operations for computing gradients. In PyNative mode, you can easily set breakpoints and get intermediate results of network execution, and you can also debug the network by means of pdb.</p>
<p>By default, MindSpore is in Graph mode, which can be set as PyNative mode via <code class="docutils literal notranslate"><span class="pre">set_context(mode=PYNATIVE_MODE)</span></code>. Related examples can be found in <a class="reference external" href="https://www.mindspore.cn/tutorials/zh-CN/r1.8/advanced/pynative_graph/pynative.html">Debugging With PyNative Mode</a>.</p>
</section>
<section id="getting-more-error-messages">
<h4>Getting More Error Messages<a class="headerlink" href="#getting-more-error-messages" title="Permalink to this headline"></a></h4>
<p>During the network process debugging, if you need to get more information about error messages, you can get it by the following ways:</p>
<ul class="simple">
<li><p>Using pdb for debugging in PyNative mode, and using pdb to print relevant stack and contextual information to help locate problems.</p></li>
<li><p>Using Print operator to print more contextual information. Related examples can be found in <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r1.8/debug/custom_debug.html#mindspore-print-operator-introduction">Print Operator Features</a>.</p></li>
<li><p>Adjusting the log level to get more error information. MindSpore can easily adjust the log level through environment variables. Related examples can be found in <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r1.8/debug/custom_debug.html#log-related-environment-variables-and-configurations">Logging-related Environment Variables And Configurations</a>.</p></li>
</ul>
</section>
<section id="common-errors">
<h4>Common Errors<a class="headerlink" href="#common-errors" title="Permalink to this headline"></a></h4>
<p>During network process debugging, the common errors are as follows:</p>
<ul>
<li><p>The operator execution error.</p>
<p>During the network process debugging, operator execution errors are often reported such as shape mismatch and unsupported dtype. Then, according to the error message, you should check whether the operator is used correctly and whether the shape of the input data is consistent with the expectation and make corresponding modifications.</p>
<p>Supports for related operators and API introductions can be found in <a class="reference external" href="https://www.mindspore.cn/docs/en/r1.8/note/operator_list.html">Operator Support List</a> and <a class="reference external" href="https://www.mindspore.cn/docs/en/r1.8/index.html">Operators Python API</a>.</p>
</li>
<li><p>The same script works in PyNative mode, but reports bugs in Graph mode.</p>
<p>In MindSpore’s Graph mode, the code in the <code class="docutils literal notranslate"><span class="pre">construct</span></code> function is parsed by the MindSpore framework, and there is some Python syntax that is not yet supported which results in errors. In this case, you should follow <a class="reference external" href="https://www.mindspore.cn/docs/en/r1.8/note/static_graph_syntax_support.html">MindSpore’s Syntax Description</a> according to the error message.</p>
</li>
<li><p>Distributed parallel training script is misconfigured.</p>
<p>Distributed parallel training scripts and environment configuration can be found in <a class="reference external" href="https://www.mindspore.cn/docs/en/r1.8/design/distributed_training_design.html">Distributed Parallel Training Tutorial</a>.</p>
</li>
</ul>
</section>
</section>
<section id="loss-value-comparison">
<h3>Loss Value Comparison<a class="headerlink" href="#loss-value-comparison" title="Permalink to this headline"></a></h3>
<p>With a benchmark script, the loss values run by the benchmark script can be compared with those run by the MindSpore script, which can be used to verify the correctness of the overall network structure and the accuracy of the operator.</p>
<section id="main-steps">
<h4>Main Steps<a class="headerlink" href="#main-steps" title="Permalink to this headline"></a></h4>
<ol class="arabic">
<li><p>Guaranteeting Identical Input</p>
<p>It is necessary to ensure that the inputs are the same in both networks, so that they can have the same network output in the same network structure. The same inputs can be guaranteed using following ways:</p>
<ul>
<li><p>Using numpy to construct the input data to ensure the same inputs to the networks. MindSpore supports free conversion of Tensor and numpy. The following script can be used to construct the input data.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">input</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
</pre></div>
</div>
</li>
<li><p>Using the same dataset for computation. MindSpore supports the use of the TFRecord dataset, which can be read by using the <code class="docutils literal notranslate"><span class="pre">mindspore.dataset.TFRecordDataset</span></code> interface.</p></li>
</ul>
</li>
<li><p>Removing the Influence of Randomness in the Network</p>
<p>The main methods to remove the effect of randomness in the network are to set the same randomness seed, turn off the data shuffle, modify the code to remove the effect of random operators in the network such as dropout and initializer, etc.</p>
</li>
<li><p>Ensuring the Same Settings for the Relevant Hyperparameters</p>
<p>It is necessary to ensure the same settings for the hyperparameters in the network in order to guarantee the same input and the same output of the operator.</p>
</li>
<li><p>Running the network and comparing the output loss values. Generally, the error of the loss value is about 1‰. Because the operator itself has a certain accuracy error. As the number of steps increases, the error will have a certain accumulation.</p></li>
</ol>
</section>
<section id="related-issues-locating">
<h4>Related Issues Locating<a class="headerlink" href="#related-issues-locating" title="Permalink to this headline"></a></h4>
<p>If the loss errors are large, the problem locating can be done by using following ways:</p>
<ul>
<li><p>Checking whether the input and hyperparameter settings are the same, and whether the randomness effect is completely removed.</p>
<p>if the loss value differs significantly after multiple executions of the same script, it means that the effect of randomness in the network is not completely removed.</p>
</li>
<li><p>Overall judgment.</p>
<p>If there is a large error in the first iteration of loss values, it means that there is a problem with the forward calculation of the network.</p>
<p>If the loss value in the first iteration is within the error range but the second iteration starts with a large error in the loss value, it means that there should be no problem in the forward calculation of the network and there may be problems in the reverse gradient and weight update calculation.</p>
</li>
<li><p>After having the overall judgment, compare the accuracy of input and output values from rough to fine.</p>
<p>First, compare the input and output values layer by layer for each subnet starting from the input, and identify the subnets that initially have problems.</p>
<p>Then, compare the network structure in the subnet and the input and output of the operator, find the network structure or operator that occurs problems, and modify it.</p>
<p>If you find any operator accuracy problems during the process, you can raise an issue on the <a class="reference external" href="https://gitee.com/mindspore/mindspore">MindSpore Code Hosting Platform</a>, and the relevant personnel will follow up on the problem.</p>
</li>
<li><p>MindSpore provides various tools for acquiring intermediate network data, which can be used according to the actual situation.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r1.8/debug/dump.html#dump-introduction">Data Dump function</a></p></li>
<li><p><a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r1.8/debug/custom_debug.html#mindspore-print-operator-introduction">Use Print Operator To Print Related Information</a></p></li>
<li><p><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.8/index.html">Using The Visualization Component MindInsight</a></p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="precision-debugging-tools">
<h3>Precision Debugging Tools<a class="headerlink" href="#precision-debugging-tools" title="Permalink to this headline"></a></h3>
<section id="customized-debugging-information">
<h4>Customized Debugging Information<a class="headerlink" href="#customized-debugging-information" title="Permalink to this headline"></a></h4>
<ul>
<li><p><a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r1.8/debug/custom_debug.html#callback-capabilities-of-mindspore">Callback Function</a></p>
<p>MindSpore has provided ModelCheckpoint, LossMonitor, SummaryCollector and other Callback classes for saving model parameters, monitoring loss values, saving training process information, etc. Users can also customize Callback functions to implement starting and ending runs at each epoch and step, and please refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r1.8/debug/custom_debug.html#custom-callback">Custom Callback</a> for specific examples.</p>
</li>
<li><p><a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r1.8/debug/custom_debug.html#mindspore-metrics-introduction">MindSpore Metrics Function</a></p>
<p>When the training is finished, metrics can be used to evaluate the training results. MindSpore provides various metrics for evaluation, such as: <code class="docutils literal notranslate"><span class="pre">accuracy</span></code>, <code class="docutils literal notranslate"><span class="pre">loss</span></code>, <code class="docutils literal notranslate"><span class="pre">precision</span></code>, <code class="docutils literal notranslate"><span class="pre">recall</span></code>, <code class="docutils literal notranslate"><span class="pre">F1</span></code>, etc.</p>
</li>
<li><p>Customized Learning Rate</p>
<p>MindSpore provides some common implementations of dynamic learning rate and some common optimizers with adaptive learning rate adjustment functions, and <a class="reference external" href="https://www.mindspore.cn/docs/en/r1.8/api_python/mindspore.nn.html#dynamic-learning-rate">Dynamic Learning Rate</a> and <a class="reference external" href="https://www.mindspore.cn/docs/en/r1.8/api_python/mindspore.nn.html#optimizer">Optimizer Functions</a> in the API documentation can be found.</p>
<p>At the same time, the user can implement a customized dynamic learning rate, as exemplified by WarmUpLR:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">WarmUpLR</span><span class="p">(</span><span class="n">LearningRateSchedule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">WarmUpLR</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1">## check the input</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;learning_rate must be float.&quot;</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="s2">&quot;learning_rate&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">warmup_steps</span><span class="p">,</span> <span class="s1">&#39;warmup_steps&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="c1">## define the operators</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span> <span class="o">=</span> <span class="n">warmup_steps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Minimum</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cast</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">global_step</span><span class="p">):</span>
        <span class="c1">## calculate the lr</span>
        <span class="n">warmup_percent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">global_step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">warmup_percent</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="hyper-parameter-optimization-with-mindoptimizer">
<h4>Hyper-Parameter Optimization with MindOptimizer<a class="headerlink" href="#hyper-parameter-optimization-with-mindoptimizer" title="Permalink to this headline"></a></h4>
<p>MindSpore provides MindOptimizer tools to help users perform hyper-parameter optimization conveniently, and the detailed examples and usage methods can be found in <a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.8/hyper_parameters_auto_tuning.html">Hyper-Parameter Optimization With MindOptimizer</a>.</p>
</section>
<section id="loss-value-anomaly-locating">
<h4>Loss Value Anomaly Locating<a class="headerlink" href="#loss-value-anomaly-locating" title="Permalink to this headline"></a></h4>
<p>For cases where the loss value is INF, NAN, or the loss value does not converge, you can investigate the following scenarios:</p>
<ol class="arabic">
<li><p>Checking for loss_scale overflow.</p>
<p>In the scenario of using loss_scale with mixed precision, the situation that the loss value is INF and NAN may be caused by the scale value being too large. If it is dynamic loss_scale, the scale value will be adjusted automatically; if it is static loss_scale, the scale value needs to be reduced.</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">scale=1</span></code> case still has a loss value of INF, NAN, there should be an overflow of operators in the network and further investigation for locating the problem is needed.</p>
</li>
<li><p>The causes of abnormal loss values may be caused by abnormal input data, operator overflow, gradient disappearance, gradient explosion, etc.</p>
<p>To check the intermediate value of the network such as operator overflow, gradient of 0, abnormal weight, gradient disappearance and gradient explosion, it is recommended to use <a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.8/debugger.html">MindInsight Debugger</a> to set the corresponding detection points for detection and debugging, which can locate the problem in a more comprehensive way with the strong debuggability.</p>
<p>The following are a few simple initial troubleshooting methods:</p>
<ul>
<li><p>Observing whether the weight values appear or loading the saved Checkpoint file to print the weight values can make a preliminary judgment. Printing the weight values can refer to the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">ckpt</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">(</span><span class="n">ckpt_path</span><span class="p">)</span>
<span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">ckpt</span><span class="p">:</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">ckpt</span><span class="p">[</span><span class="n">param</span><span class="p">]</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Checking whether the gradient is 0 or comparing whether the weight values of Checkpoint files saved in different steps have changed can make a preliminary judgment. The comparison of the weight values of Checkpoint files can be referred to the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">ckpt1</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">(</span><span class="n">ckpt1_path</span><span class="p">)</span>
<span class="n">ckpt2</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">(</span><span class="n">ckpt2_path</span><span class="p">)</span>
<span class="nb">sum</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">same</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">param1</span><span class="p">,</span><span class="n">param2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">ckpt1</span><span class="p">,</span><span class="n">ckpt2</span><span class="p">):</span>
    <span class="nb">sum</span> <span class="o">=</span> <span class="nb">sum</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">value1</span> <span class="o">=</span> <span class="n">ckpt</span><span class="p">[</span><span class="n">param1</span><span class="p">]</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
    <span class="n">value2</span> <span class="o">=</span> <span class="n">ckpt</span><span class="p">[</span><span class="n">param2</span><span class="p">]</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">value1</span> <span class="o">==</span> <span class="n">value2</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;same value: &#39;</span><span class="p">,</span> <span class="n">param1</span><span class="p">,</span> <span class="n">value1</span><span class="p">)</span>
        <span class="n">same</span> <span class="o">=</span> <span class="n">same</span> <span class="o">+</span> <span class="mi">1</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;All params num: &#39;</span><span class="p">,</span> <span class="nb">sum</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;same params num: &#39;</span><span class="p">,</span> <span class="n">same</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Checking whether there is NAN, INF abnormal data in the weight value, you can also load the Checkpoint file for a brief judgment. In general, if there is NAN, INF in the weight value, there is also NAN, INF in the gradient calculation, and there may be an overflow situation. The relevant code reference is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">ckpt</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">(</span><span class="n">ckpt_path</span><span class="p">)</span>
<span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">ckpt</span><span class="p">:</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">ckpt</span><span class="p">[</span><span class="n">param</span><span class="p">]</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;NAN value:&#39;</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;INF value:&#39;</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</li>
</ol>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="use_third_party_op.html" class="btn btn-neutral float-left" title="Call Third-Party Operators by Customized Operators" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="performance_optimization.html" class="btn btn-neutral float-right" title="Performance Profiling" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>