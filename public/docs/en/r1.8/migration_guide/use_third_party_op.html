

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Call Third-Party Operators by Customized Operators &mdash; MindSpore master documentation</title>
  

  
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/training.css" type="text/css" />
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/js/training.js"></script>
        
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Network Debugging" href="neural_network_debug.html" />
    <link rel="prev" title="Migration Script" href="migration_script.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../design/auto_gradient.html">Functional Differential Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/distributed_training_design.html">Distributed Training Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/mindir.html">MindSpore IR (MindIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/thor.html">Second Order Optimizer</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.8/training_visual_design.html">Design of Visualization↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Specification</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/benchmark.html">Benchmarks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/network_list.html">Network List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/operator_list.html">Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/syntax_list.html">Syntax Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.audio.html">mindspore.dataset.audio</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.config.html">mindspore.dataset.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.text.html">mindspore.dataset.text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.vision.html">mindspore.dataset.vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.transformer.html">mindspore.nn.transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.functional.html">mindspore.ops.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.parallel.nn.html">mindspore.parallel.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.8/api_cpp/mindspore.html">C++ API↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Mapping</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/pytorch_api_mapping.html">PyTorch and MindSpore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/tensorflow_api_mapping.html">TensorFlow and MindSpore</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Migration Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="preparation.html">Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="script_analysis.html">Network Script Analysis</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="script_development.html">Network Script Development</a><ul class="current">
<li class="toctree-l2"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.8/migrate_3rd_scripts_mindconverter.html">Using Mindcoverter to Perform Migration</a></li>
<li class="toctree-l2"><a class="reference internal" href="migration_script.html">Migration Script</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Call Third-Party Operators by Customized Operators</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#using-pytorch-aten-operators-for-docking">Using PyTorch Aten operators for Docking</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#1-downloading-the-project-files">1. Downloading the Project files</a></li>
<li class="toctree-l4"><a class="reference internal" href="#2-writing-and-calling-the-source-code-file-of-pytorch-aten-operators">2. Writing and calling the Source Code File of PyTorch Aten Operators</a></li>
<li class="toctree-l4"><a class="reference internal" href="#3-using-the-compilation-script-setup-py-to-generate-so">3. Using the compilation script <code class="docutils literal notranslate"><span class="pre">setup.py</span></code> to generate so</a></li>
<li class="toctree-l4"><a class="reference internal" href="#4-using-the-customized-operator">4. Using the Customized Operator</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="neural_network_debug.html">Network Debugging</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance_optimization.html">Performance Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="inference.html">Inference Execution</a></li>
<li class="toctree-l1"><a class="reference internal" href="sample_code.html">Network Migration Debugging Example</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/data_processing.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/implement_problem.html">Implement Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/network_compilation.html">Network Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/operators_compile.html">Operators Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/usage_migrate_3rd.html">Migration from a Third-party Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/precision_tuning.html">Precision Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/distributed_configure.html">Distributed Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/feature_advice.html">Feature Advice</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="script_development.html">Network Script Development</a> &raquo;</li>
      <li>Call Third-Party Operators by Customized Operators</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/migration_guide/use_third_party_op.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section class="tex2jax_ignore mathjax_ignore" id="call-third-party-operators-by-customized-operators">
<h1>Call Third-Party Operators by Customized Operators<a class="headerlink" href="#call-third-party-operators-by-customized-operators" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.8/docs/mindspore/source_en/migration_guide/use_third_party_op.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.8/resource/_static/logo_source_en.png"></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>When built-in operators cannot meet requirements during network development, you can call the Python API <a class="reference external" href="https://www.mindspore.cn/docs/en/r1.8/api_python/ops/mindspore.ops.Custom.html#mindspore-ops-custom">Custom</a> primitive defined in MindSpore to quickly create different types of customized operators for use.</p>
<p>You can choose different customized operator developing methods base on needs.
See: <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r1.8/operation/op_custom.html">custom_operator_custom</a>.</p>
<p>There is a defining method called <code class="docutils literal notranslate"><span class="pre">aot</span></code> which has a special use. The <code class="docutils literal notranslate"><span class="pre">aot</span></code> mode can call the corresponding <code class="docutils literal notranslate"><span class="pre">cpp</span></code>/<code class="docutils literal notranslate"><span class="pre">cuda</span></code> function by loading the precompiled <code class="docutils literal notranslate"><span class="pre">so</span></code>. Therefore, when a third-party library provides the <code class="docutils literal notranslate"><span class="pre">cpp</span></code>/<code class="docutils literal notranslate"><span class="pre">cuda</span></code> function <code class="docutils literal notranslate"><span class="pre">API</span></code>, you can try to call its function interface in <code class="docutils literal notranslate"><span class="pre">so</span></code>.</p>
<p>Here is an example of how to use <code class="docutils literal notranslate"><span class="pre">Aten</span></code> library of PyTorch Aten.</p>
</section>
<section id="using-pytorch-aten-operators-for-docking">
<h2>Using PyTorch Aten operators for Docking<a class="headerlink" href="#using-pytorch-aten-operators-for-docking" title="Permalink to this headline"></a></h2>
<p>When migrating a network using the PyTorch Aten operator encounters a shortage of built-in operators, we can use the <code class="docutils literal notranslate"><span class="pre">aot</span></code> development method of the <code class="docutils literal notranslate"><span class="pre">Custom</span></code> operator to call PyTorch Aten’s operator for fast verification.</p>
<p>PyTorch provides a way to support the introduction of PyTorch’s header files to write <code class="docutils literal notranslate"><span class="pre">cpp/cuda</span></code> code by using its associated data structures and compile it into <code class="docutils literal notranslate"><span class="pre">so</span></code>. See:<a class="reference external" href="https://pytorch.org/docs/stable/_modules/torch/utils/cpp_extension.html#CppExtension">https://pytorch.org/docs/stable/_modules/torch/utils/cpp_extension.html#CppExtension</a>.</p>
<p>Using a combination of the two methods, the customized operator can call the PyTorch Aten operator as follows:</p>
<section id="1-downloading-the-project-files">
<h3>1. Downloading the Project files<a class="headerlink" href="#1-downloading-the-project-files" title="Permalink to this headline"></a></h3>
<p>User can download the project files from <a class="reference external" href="https://obs.dualstack.cn-north-4.myhuaweicloud.com/mindspore-website/notebook/migration_guide/test_custom_pytorch.tar">here</a>.</p>
<p>Use the following command to extract files and find the folder <code class="docutils literal notranslate"><span class="pre">test_custom_pytorch</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tar<span class="w"> </span>xvf<span class="w"> </span>test_custom_pytorch.tar
</pre></div>
</div>
<p>The folder include the following files:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>test_custom_pytorch
├── env.sh                           # set PyTorch/lib into LD_LIBRARY_PATH
├── leaky_relu.cpp                   # an example of use Aten CPU operator
├── leaky_relu.cu                    # an example of use Aten GPU operator
├── ms_ext.cpp                       # convert Tensors between MindSpore and PyTorch
├── ms_ext.h                         # convert API
├── README.md
├── run_cpu.sh                       # a script to run cpu case
├── run_gpu.sh                       # a script to run gpu case
├── setup.py                         # a script to compile cpp/cu into so
├── test_cpu_op_in_gpu_device.py     # a test file to run Aten CPU operator on GPU device
├── test_cpu_op.py                   # a test file to run Aten CPU operator on CPU device
└── test_gpu_op.py                   # a test file to run Aten GPU operator on GPU device
</pre></div>
</div>
<p>Using the PyTorch Aten operator focuses mainly on env.sh, setup.py, leaky_relu.cpp/cu, test_*, .py.</p>
<p>Among them, env.sh is used to set environment variables, setup.py is used to compile so, leaky_relu.cpp/cu is used to reference the source code that calls the PyTorch Aten operator, and test_*.py is used to refer to the call Custom operator.</p>
</section>
<section id="2-writing-and-calling-the-source-code-file-of-pytorch-aten-operators">
<h3>2. Writing and calling the Source Code File of PyTorch Aten Operators<a class="headerlink" href="#2-writing-and-calling-the-source-code-file-of-pytorch-aten-operators" title="Permalink to this headline"></a></h3>
<p>Refer to leaky_relu.cpp/cu to write a source code file that calls the PyTorch Aten operator.</p>
<p>The customized operator of <code class="docutils literal notranslate"><span class="pre">aot</span></code> type adopts the <code class="docutils literal notranslate"><span class="pre">AOT</span></code> compilation method, which requires network developers to hand-write the source code file of the operator implementation based on a specific interface, and compile the source code file into a dynamic link library in advance, and then the framework will automatically call the function defined in the dynamic link library. In terms of the development language of the operator implementation, the <code class="docutils literal notranslate"><span class="pre">GPU</span></code> platform supports <code class="docutils literal notranslate"><span class="pre">CUDA</span></code>, and the <code class="docutils literal notranslate"><span class="pre">CPU</span></code> platform supports <code class="docutils literal notranslate"><span class="pre">C</span></code> and <code class="docutils literal notranslate"><span class="pre">C++</span></code>. The interface specification of the operator implemented by the operators in the source code file is as follows:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">extern</span><span class="w"> </span><span class="s">&quot;C&quot;</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">func_name</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">nparam</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">**</span><span class="n">params</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="o">*</span><span class="n">ndims</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="o">**</span><span class="n">shapes</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">**</span><span class="n">dtypes</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">stream</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">extra</span><span class="p">);</span>
</pre></div>
</div>
<p>If the <code class="docutils literal notranslate"><span class="pre">cpu</span></code> operator is called, taking <code class="docutils literal notranslate"><span class="pre">leaky_relu.cpp</span></code> as an example, the file provides the function <code class="docutils literal notranslate"><span class="pre">LeakyRelu</span></code> required by <code class="docutils literal notranslate"><span class="pre">AOT</span></code>, which calls <code class="docutils literal notranslate"><span class="pre">torch::leaky_relu_out</span></code>  function of PyTorch Aten:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;string.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/extension.h&gt;</span><span class="c1"> // Header file reference section</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;ms_ext.h&quot;</span>

<span class="k">extern</span><span class="w"> </span><span class="s">&quot;C&quot;</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">LeakyRelu</span><span class="p">(</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">nparam</span><span class="p">,</span>
<span class="w">    </span><span class="kt">void</span><span class="o">**</span><span class="w"> </span><span class="n">params</span><span class="p">,</span>
<span class="w">    </span><span class="kt">int</span><span class="o">*</span><span class="w"> </span><span class="n">ndims</span><span class="p">,</span>
<span class="w">    </span><span class="kt">int64_t</span><span class="o">**</span><span class="w"> </span><span class="n">shapes</span><span class="p">,</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="o">**</span><span class="w"> </span><span class="n">dtypes</span><span class="p">,</span>
<span class="w">    </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">stream</span><span class="p">,</span>
<span class="w">    </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">extra</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">tensors</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">get_torch_tensors</span><span class="p">(</span><span class="n">nparam</span><span class="p">,</span><span class="w"> </span><span class="n">params</span><span class="p">,</span><span class="w"> </span><span class="n">ndims</span><span class="p">,</span><span class="w"> </span><span class="n">shapes</span><span class="p">,</span><span class="w"> </span><span class="n">dtypes</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">kCPU</span><span class="p">);</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">at_input</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">at_output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tensors</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">leaky_relu_out</span><span class="p">(</span><span class="n">at_output</span><span class="p">,</span><span class="w"> </span><span class="n">at_input</span><span class="p">);</span>
<span class="w">    </span><span class="c1">// If you are using a version without output, the code is as follows:</span>
<span class="w">    </span><span class="c1">// torch::Tensor output = torch::leaky_relu(at_input);</span>
<span class="w">    </span><span class="c1">// at_output.copy_(output);</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>If the <code class="docutils literal notranslate"><span class="pre">gpu</span></code> operator is called, take <code class="docutils literal notranslate"><span class="pre">leaky_relu.cu</span></code> as an example:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;string.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/extension.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;ms_ext.h&quot;</span>

<span class="k">extern</span><span class="w"> </span><span class="s">&quot;C&quot;</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">LeakyRelu</span><span class="p">(</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">nparam</span><span class="p">,</span>
<span class="w">    </span><span class="kt">void</span><span class="o">**</span><span class="w"> </span><span class="n">params</span><span class="p">,</span>
<span class="w">    </span><span class="kt">int</span><span class="o">*</span><span class="w"> </span><span class="n">ndims</span><span class="p">,</span>
<span class="w">    </span><span class="kt">int64_t</span><span class="o">**</span><span class="w"> </span><span class="n">shapes</span><span class="p">,</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="o">**</span><span class="w"> </span><span class="n">dtypes</span><span class="p">,</span>
<span class="w">    </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">stream</span><span class="p">,</span>
<span class="w">    </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">extra</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">custream</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">cudaStream_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">stream</span><span class="p">);</span>
<span class="w">    </span><span class="n">cudaStreamSynchronize</span><span class="p">(</span><span class="n">custream</span><span class="p">);</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">tensors</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">get_torch_tensors</span><span class="p">(</span><span class="n">nparam</span><span class="p">,</span><span class="w"> </span><span class="n">params</span><span class="p">,</span><span class="w"> </span><span class="n">ndims</span><span class="p">,</span><span class="w"> </span><span class="n">shapes</span><span class="p">,</span><span class="w"> </span><span class="n">dtypes</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">kCUDA</span><span class="p">);</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">at_input</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">at_output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tensors</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">leaky_relu_out</span><span class="p">(</span><span class="n">at_output</span><span class="p">,</span><span class="w"> </span><span class="n">at_input</span><span class="p">);</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>PyTorch Aten provides operator functions versions with output and operator functions versions without output. Operator functions with output have the ‘_out’ suffix, and PyTorch Aten provides 300+ <code class="docutils literal notranslate"><span class="pre">apis</span></code> of common operators.</p>
<p>When <code class="docutils literal notranslate"><span class="pre">torch::*_out</span></code> is called, <code class="docutils literal notranslate"><span class="pre">output</span></code> copy is not needed. When the versions without <code class="docutils literal notranslate"><span class="pre">_out</span></code>suffix is called, API <code class="docutils literal notranslate"><span class="pre">torch.Tensor.copy_</span></code> is needed to called to result copy.</p>
<p>To see which functions are supported for calling PyTorch Aten, the <code class="docutils literal notranslate"><span class="pre">CPU</span></code> version refers to the PyTorch installation path: <code class="docutils literal notranslate"><span class="pre">python*/site-packages/torch/include/ATen/CPUFunctions_inl.h</span></code> , and for the corresponding <code class="docutils literal notranslate"><span class="pre">GPU</span></code> version, refers to<code class="docutils literal notranslate"><span class="pre">python*/site-packages/torch/include/ATen/CUDAFunctions_inl.h</span></code>。</p>
<p>The apis provided by ms_ext.h are used in the above use case, which are briefly described here:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Convert MindSpore kernel&#39;s inputs/outputs to PyTorch Aten&#39;s Tensor</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">get_torch_tensors</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">nparam</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="o">**</span><span class="w"> </span><span class="n">params</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="o">*</span><span class="w"> </span><span class="n">ndims</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="o">**</span><span class="w"> </span><span class="n">shapes</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="o">**</span><span class="w"> </span><span class="n">dtypes</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">Device</span><span class="w"> </span><span class="n">device</span><span class="p">)</span><span class="w"> </span><span class="p">;</span>
</pre></div>
</div>
</section>
<section id="3-using-the-compilation-script-setup-py-to-generate-so">
<h3>3. Using the compilation script <code class="docutils literal notranslate"><span class="pre">setup.py</span></code> to generate so<a class="headerlink" href="#3-using-the-compilation-script-setup-py-to-generate-so" title="Permalink to this headline"></a></h3>
<p>setup.py uses the <code class="docutils literal notranslate"><span class="pre">cppextension</span></code> provided by PyTorch Aten to compile the above <code class="docutils literal notranslate"><span class="pre">c++/cuda</span></code> source code into an <code class="docutils literal notranslate"><span class="pre">so</span></code> file.</p>
<p>Before execution, you need to make sure that PyTorch is installed.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>torch
</pre></div>
</div>
<p>Then add PyTorch’s <code class="docutils literal notranslate"><span class="pre">lib</span></code> into <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code>。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="k">$(</span>python3<span class="w"> </span>-c<span class="w"> </span><span class="s1">&#39;import torch, os; print(os.path.dirname(torch.__file__))&#39;</span><span class="k">)</span>/lib:<span class="nv">$LD_LIBRARY_PATH</span>
</pre></div>
</div>
<p>Run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cpu:<span class="w"> </span>python<span class="w"> </span>setup.py<span class="w"> </span>leaky_relu.cpp<span class="w"> </span>leaky_relu_cpu.so
gpu:<span class="w"> </span>python<span class="w"> </span>setup.py<span class="w"> </span>leaky_relu.cu<span class="w"> </span>leaky_relu_gpu.so
</pre></div>
</div>
<p>Then the so files that we need may be obtained.</p>
</section>
<section id="4-using-the-customized-operator">
<h3>4. Using the Customized Operator<a class="headerlink" href="#4-using-the-customized-operator" title="Permalink to this headline"></a></h3>
<p>Taking CPU as an example, use the Custom operator to call the above PyTorch Aten operator, see the code test_cpu_op.py:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">Cell</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">LeakyRelu</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="s2">&quot;./leaky_relu_cpu.so:LeakyRelu&quot;</span><span class="p">,</span> <span class="n">out_shape</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">func_type</span><span class="o">=</span><span class="s2">&quot;aot&quot;</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">leaky_relu</span> <span class="o">=</span> <span class="n">LeakyRelu</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">leaky_relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x0</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<p>Run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>test_cpu_op.py
</pre></div>
</div>
<p>Result:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[ 0.    -0.001]
 [-0.002  1.   ]]
</pre></div>
</div>
<p>Attention:</p>
<p>When using a PyTorch Aten <code class="docutils literal notranslate"><span class="pre">GPU</span></code> operator，set <code class="docutils literal notranslate"><span class="pre">device_target</span></code>to <code class="docutils literal notranslate"><span class="pre">&quot;GPU&quot;</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">)</span>
<span class="n">op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="s2">&quot;./leaky_relu_gpu.so:LeakyRelu&quot;</span><span class="p">,</span> <span class="n">out_shape</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">func_type</span><span class="o">=</span><span class="s2">&quot;aot&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>When using a PyTorch Aten <code class="docutils literal notranslate"><span class="pre">CPU</span></code> operator and <code class="docutils literal notranslate"><span class="pre">device_target</span></code> is <code class="docutils literal notranslate"><span class="pre">&quot;GPU&quot;</span></code>, the settings that need to be added are as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">)</span>
<span class="n">op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="s2">&quot;./leaky_relu_cpu.so:LeakyRelu&quot;</span><span class="p">,</span> <span class="n">out_shape</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">func_type</span><span class="o">=</span><span class="s2">&quot;aot&quot;</span><span class="p">)</span>
<span class="n">op</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;primitive_target&quot;</span><span class="p">,</span> <span class="s2">&quot;CPU&quot;</span><span class="p">)</span>
</pre></div>
</div>
<blockquote>
<div><ol class="arabic simple">
<li><p>Compile so with cppextension requires a compiler version that meets the tool’s needs, and check for the presence of gcc/clang/nvcc.</p></li>
<li><p>Compile so with cppextension will generate a build folder in the script path, which stores so. The script will copy so to outside of build, but cppextension will skip compilation if it finds that there is already so in build, so if it is a newly compiled so, remember to empty the so under the build.</p></li>
<li><p>The following tests is based on PyTorch 1.9.1，cuda11.1，python3.7. The download link:<a class="reference external" href="https://download.pytorch.org/whl/cu111/torch-1.9.1%2Bcu111-cp37-cp37m-linux_x86_64.whl">https://download.pytorch.org/whl/cu111/torch-1.9.1%2Bcu111-cp37-cp37m-linux_x86_64.whl</a>. The cuda version supported by PyTorch Aten needs to be consistent with the local cuda version, and whether other versions are supported needs to be explored by the user.</p></li>
</ol>
</div></blockquote>
</section>
</section>
</section>


           </div>
           
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="migration_script.html" class="btn btn-neutral float-left" title="Migration Script" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="neural_network_debug.html" class="btn btn-neutral float-right" title="Network Debugging" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
</body>
</html>