

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>mindspore.common.tensor &mdash; MindSpore master documentation</title>
  

  
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/training.css" type="text/css" />
   
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/js/training.js"></script>
        
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../design/auto_gradient.html">Functional Differential Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/distributed_training_design.html">Distributed Training Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/mindir.html">MindSpore IR (MindIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/thor.html">Second Order Optimizer</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.9/training_visual_design.html">Design of Visualization↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Specification</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../note/benchmark.html">Benchmarks</a></li>
<li class="toctree-l1"><a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.9/README.md#table-of-contents">Network List↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../note/operator_list.html">API List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../note/syntax_list.html">Syntax Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.dataset.audio.html">mindspore.dataset.audio</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.dataset.config.html">mindspore.dataset.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.dataset.text.html">mindspore.dataset.text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.dataset.vision.html">mindspore.dataset.vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.nn.transformer.html">mindspore.nn.transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.ops.function.html">mindspore.ops.function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.9/api_cpp/mindspore.html">C++ API↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Mapping</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../note/api_mapping/pytorch_api_mapping.html">PyTorch and MindSpore API Mapping Table</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../note/api_mapping/tensorflow_api_mapping.html">TensorFlow and MindSpore API Mapping Table</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Migration Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../migration_guide/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../migration_guide/enveriment_preparation.html">Environment Preparation and Information Acquisition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../migration_guide/analysis_and_preparation.html">Model Analysis and Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../migration_guide/model_development/model_development.html">Constructing MindSpore Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../migration_guide/debug_and_tune.html">Debugging and Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../migration_guide/sample_code.html">Network Migration Debugging Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../migration_guide/faq.html">FAQs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../migration_guide/typical_api_comparision.html">Differences Between MindSpore and PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../migration_guide/use_third_party_op.html">Using Third-party Operator Libraries Based on Customized Interfaces</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/data_processing.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/implement_problem.html">Implement Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/network_compilation.html">Network Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/operators_compile.html">Operators Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/usage_migrate_3rd.html">Migration from a Third-party Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/precision_tuning.html">Precision Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/distributed_configure.html">Distributed Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/feature_advice.html">Feature Advice</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../RELEASE.html">Release Notes</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../index.html">Module code</a> &raquo;</li>
      <li>mindspore.common.tensor</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for mindspore.common.tensor</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2020-2022 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>
<span class="sd">&quot;&quot;&quot;Tensor implementation.&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Tensor&#39;</span><span class="p">,</span> <span class="s1">&#39;RowTensor&#39;</span><span class="p">,</span> <span class="s1">&#39;SparseTensor&#39;</span><span class="p">,</span> <span class="s1">&#39;COOTensor&#39;</span><span class="p">,</span> <span class="s1">&#39;CSRTensor&#39;</span><span class="p">]</span>

<span class="kn">import</span> <span class="nn">numbers</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">mindspore.communication.management</span> <span class="kn">import</span> <span class="n">get_rank</span><span class="p">,</span> <span class="n">get_group_size</span>
<span class="kn">from</span> <span class="nn">mindspore.common._utils</span> <span class="kn">import</span> <span class="n">is_shape_unknown</span>
<span class="kn">from</span> <span class="nn">mindspore.common.seed</span> <span class="kn">import</span> <span class="n">get_seed</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">log</span> <span class="k">as</span> <span class="n">logger</span>
<span class="kn">from</span> <span class="nn">mindspore.common</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="kn">from</span> <span class="nn">mindspore.common._register_for_tensor</span> <span class="kn">import</span> <span class="n">tensor_operator_registry</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">COOTensor</span> <span class="k">as</span> <span class="n">COOTensor_</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">CSRTensor</span> <span class="k">as</span> <span class="n">CSRTensor_</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">RowTensor</span> <span class="k">as</span> <span class="n">RowTensor_</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">Tensor</span> <span class="k">as</span> <span class="n">Tensor_</span>
<span class="kn">from</span> <span class="nn">mindspore._checkparam</span> <span class="kn">import</span> <span class="n">Rel</span>
<span class="kn">from</span> <span class="nn">mindspore._checkparam</span> <span class="kn">import</span> <span class="n">Validator</span> <span class="k">as</span> <span class="n">validator</span>

<span class="n">np_types</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">int16</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span>
            <span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">uint16</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">uint32</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">uint64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
            <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">bool_</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">complex64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">complex128</span><span class="p">)</span>
<span class="n">tensor_dynamic_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;set_default_dtype&quot;</span><span class="p">,</span> <span class="s2">&quot;_set_default_dtype&quot;</span><span class="p">,</span> <span class="s2">&quot;shape&quot;</span><span class="p">,</span> <span class="s2">&quot;_shape&quot;</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="s2">&quot;_dtype&quot;</span><span class="p">,</span>
                       <span class="s2">&quot;dim&quot;</span><span class="p">,</span> <span class="s2">&quot;ndim&quot;</span><span class="p">,</span> <span class="s2">&quot;virtual_flag&quot;</span><span class="p">,</span> <span class="s2">&quot;init_finished&quot;</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">_check_input_data_type</span><span class="p">(</span><span class="n">input_data</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check the type of input_data for Tensor&quot;&quot;&quot;</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;input_data&#39;</span><span class="p">,</span> <span class="n">input_data</span><span class="p">,</span>
                               <span class="p">(</span><span class="n">Tensor_</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">str_</span><span class="p">,</span> <span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="nb">complex</span><span class="p">),</span>
                               <span class="s1">&#39;Tensor&#39;</span><span class="p">)</span>
    <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">int16</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">uint16</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">uint32</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">uint64</span><span class="p">,</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">bool_</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">str_</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">complex64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">complex128</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="ow">and</span> <span class="n">input_data</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">valid_dtypes</span> <span class="ow">and</span> \
            <span class="n">input_data</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">kind</span> <span class="o">!=</span> <span class="s1">&#39;U&#39;</span> <span class="ow">and</span> <span class="n">input_data</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">kind</span> <span class="o">!=</span> <span class="s1">&#39;S&#39;</span><span class="p">:</span>  <span class="c1"># Support dtype np.str_</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For Tensor, the input_data is a numpy array, &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;but it&#39;s data type: </span><span class="si">{</span><span class="n">input_data</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2"> is not in supported list: &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">i</span><span class="o">.</span><span class="vm">__name__</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">valid_dtypes</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="ow">and</span> <span class="n">input_data</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">kind</span> <span class="o">==</span> <span class="s2">&quot;S&quot;</span> <span class="ow">and</span> \
            <span class="n">input_data</span><span class="o">.</span><span class="n">shape</span> <span class="ow">and</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;enable_ge&quot;</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For binary string input in GE mode, the shape of the data must be ()&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span> <span class="ow">and</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">valid_dtypes</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;For Tensor, the input_data is </span><span class="si">{</span><span class="n">input_data</span><span class="si">}</span><span class="s2"> that contain unsupported element.&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="Tensor"><a class="viewcode-back" href="../../../api_python/mindspore/mindspore.Tensor.html#mindspore.Tensor">[docs]</a><span class="k">class</span> <span class="nc">Tensor</span><span class="p">(</span><span class="n">Tensor_</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Tensor is a data structure that stores an n-dimensional array.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_data (Union[Tensor, float, int, bool, tuple, list, numpy.ndarray]): The data to be stored. It can be</span>
<span class="sd">            another Tensor, Python number or NumPy ndarray. Default: None.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`): Used to indicate the data type of the output Tensor. The argument should</span>
<span class="sd">            be defined in `mindspore.dtype`. If it is None, the data type of the output Tensor will be the same</span>
<span class="sd">            as the `input_data`. Default: None.</span>
<span class="sd">        shape (Union[tuple, list, int]): Used to indicate the shape of the output Tensor. The argument should be</span>
<span class="sd">            a list of integers, a tuple of integers or an integer. If `input_data` is available,</span>
<span class="sd">            `shape` doesn&#39;t need to be set. Default: None.</span>
<span class="sd">        init (Initializer): The information of init data.</span>
<span class="sd">            &#39;init&#39; is used for delayed initialization in parallel mode. Usually, it is not recommended to use</span>
<span class="sd">            &#39;init&#39; interface to initialize Tensor in the other conditions. If &#39;init&#39; interface is used to initialize</span>
<span class="sd">            Tensor, the `Tensor.init_data` API needs to be called to convert `Tensor` to the actual data.</span>
<span class="sd">            Default: None.</span>
<span class="sd">        internal (bool): Whether it is created by the framework.</span>
<span class="sd">            &#39;True&#39; means that the tensor is created by framework.</span>
<span class="sd">            &#39;False&#39; means that the tensor is created by user.</span>
<span class="sd">            Default: False</span>
<span class="sd">        const_arg (bool): Whether the tensor is a constant when it is used for the argument of a network.</span>
<span class="sd">            Default: False.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.common.initializer import One</span>
<span class="sd">        &gt;&gt;&gt; # initialize a tensor with numpy.ndarray</span>
<span class="sd">        &gt;&gt;&gt; t1 = Tensor(np.zeros([1, 2, 3]), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; print(t1)</span>
<span class="sd">        [[[0. 0. 0.]</span>
<span class="sd">        [0. 0. 0.]]]</span>
<span class="sd">        &gt;&gt;&gt; print(type(t1))</span>
<span class="sd">        &lt;class &#39;mindspore.common.tensor.Tensor&#39;&gt;</span>
<span class="sd">        &gt;&gt;&gt; print(t1.shape)</span>
<span class="sd">        (1, 2, 3)</span>
<span class="sd">        &gt;&gt;&gt; print(t1.dtype)</span>
<span class="sd">        Float32</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # initialize a tensor with a float scalar</span>
<span class="sd">        &gt;&gt;&gt; t2 = Tensor(0.1)</span>
<span class="sd">        &gt;&gt;&gt; print(t2)</span>
<span class="sd">        0.1</span>
<span class="sd">        &gt;&gt;&gt; print(type(t2))</span>
<span class="sd">        &lt;class &#39;mindspore.common.tensor.Tensor&#39;&gt;</span>
<span class="sd">        &gt;&gt;&gt; print(t2.shape)</span>
<span class="sd">        ()</span>
<span class="sd">        &gt;&gt;&gt; print(t2.dtype)</span>
<span class="sd">        Float32</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # initialize a tensor with a tuple</span>
<span class="sd">        &gt;&gt;&gt; t3 = Tensor((1, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(t3)</span>
<span class="sd">        [1 2]</span>
<span class="sd">        &gt;&gt;&gt; print(type(t3))</span>
<span class="sd">        &lt;class &#39;mindspore.common.tensor.Tensor&#39;&gt;</span>
<span class="sd">        &gt;&gt;&gt; print(t3.shape)</span>
<span class="sd">        (2,)</span>
<span class="sd">        &gt;&gt;&gt; print(t3.dtype)</span>
<span class="sd">        Int64</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; # initialize a tensor with init</span>
<span class="sd">        &gt;&gt;&gt; t4 = Tensor(shape = (1, 3), dtype=ms.float32, init=One())</span>
<span class="sd">        &gt;&gt;&gt; print(t4)</span>
<span class="sd">        [[1. 1. 1.]]</span>
<span class="sd">        &gt;&gt;&gt; print(type(t4))</span>
<span class="sd">        &lt;class &#39;mindspore.common.tensor.Tensor&#39;&gt;</span>
<span class="sd">        &gt;&gt;&gt; print(t4.shape)</span>
<span class="sd">        (1, 3)</span>
<span class="sd">        &gt;&gt;&gt; print(t4.dtype)</span>
<span class="sd">        Float32</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">delta_seed</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">internal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">const_arg</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_finished</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">internal</span><span class="p">:</span>
            <span class="n">Tensor_</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># If input data is numpy number, convert it to np array</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">np_types</span><span class="p">):</span>
                <span class="n">input_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>

            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">):</span>
                <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">shape</span><span class="p">,)</span>

            <span class="n">_check_tensor_input</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">init</span><span class="p">)</span>

            <span class="c1"># If input_data is tuple/list/numpy.ndarray, it&#39;s support in check_type method.</span>
            <span class="k">if</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span> <span class="ow">and</span> <span class="kc">None</span> <span class="ow">in</span> <span class="n">shape</span><span class="p">)</span> <span class="ow">or</span> <span class="n">init</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">shape</span> <span class="o">=</span> <span class="n">_check_tensor_dynamic_shape</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">init</span><span class="p">)</span>
                <span class="n">Tensor_</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">_check_input_data_type</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">validator</span><span class="o">.</span><span class="n">check_type_name</span><span class="p">(</span>
                        <span class="s1">&#39;dtype&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span> <span class="o">+</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">string</span><span class="p">),</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_default_dtype</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>

                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="n">input_data</span><span class="o">.</span><span class="n">flags</span><span class="p">[</span><span class="s1">&#39;FORC&#39;</span><span class="p">]):</span>
                    <span class="n">input_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">Tensor_</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">Tensor_</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_data</span><span class="p">)</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;const_arg&#39;</span><span class="p">,</span> <span class="n">const_arg</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="s1">&#39;Tensor&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">const_arg</span> <span class="o">=</span> <span class="n">const_arg</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">virtual_flag</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init</span> <span class="o">=</span> <span class="n">init</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_finished</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># if cur Tensor is a index value of another Tensor,</span>
        <span class="c1"># parent_tensor_ set to another Tensor</span>
        <span class="c1"># index_of_parent_ will set to the index</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">parent_tensor_</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">index_of_parent_</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">slice_num_of_persistent_data_</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">slice_shape_of_persistent_data_</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="fm">__getattribute__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">item</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tensor_dynamic_list</span><span class="p">:</span>
            <span class="k">if</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For tensor of dynamic shape, methods and attributes that require value &quot;</span>
                                     <span class="sa">f</span><span class="s2">&quot;are not supported, but got </span><span class="si">{</span><span class="n">item</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">object</span><span class="o">.</span><span class="fm">__getattribute__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_set_default_dtype</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Set tensor default dtype&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span>
        <span class="k">return</span> <span class="n">dtype</span>

    <span class="k">def</span> <span class="nf">__deepcopy__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memodict</span><span class="p">):</span>
        <span class="n">new_obj</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">new_obj</span><span class="o">.</span><span class="n">init</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init</span>
        <span class="n">new_obj</span><span class="o">.</span><span class="n">virtual_flag</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">virtual_flag</span>
        <span class="n">new_obj</span><span class="o">.</span><span class="n">const_arg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">const_arg</span>
        <span class="k">return</span> <span class="n">new_obj</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_finished</span><span class="p">:</span>
            <span class="n">Tensor_</span><span class="o">.</span><span class="n">data_sync</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">Tensor_</span><span class="o">.</span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">return</span> <span class="s1">&#39;&#39;</span>

    <span class="k">def</span> <span class="fm">__eq__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)):</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="c1"># bool type is not supported for `Equal` operator in backend.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span> <span class="ow">or</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">other</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()))</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span> <span class="o">==</span> <span class="n">other</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;__eq__&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__ne__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)):</span>
            <span class="k">return</span> <span class="kc">True</span>
        <span class="c1">#  bool type is not supported for `NotEqual` operator in backend.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span> <span class="ow">or</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">other</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span> <span class="o">!=</span> <span class="n">other</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()))</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;__ne__&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__hash__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">hash</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">))</span>

    <span class="k">def</span> <span class="fm">__neg__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;__neg__&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="fm">__invert__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;__logical_not__&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="fm">__round__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;round&#39;</span><span class="p">)()(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="fm">__bool__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">():</span>
            <span class="k">return</span> <span class="nb">bool</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,):</span>
            <span class="k">return</span> <span class="nb">bool</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The truth value of an array with several elements is ambiguous.&quot;</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_convert_scalar_</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">message</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">():</span>
            <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,):</span>
            <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__int__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_scalar_</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="s2">&quot;Only one element tensors can be converted to Python scalars&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__float__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_scalar_</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="s2">&quot;Only one element tensors can be converted to Python scalars&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__index__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="s2">&quot;int8&quot;</span>
                <span class="ow">or</span> <span class="n">data</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="s2">&quot;int16&quot;</span>
                <span class="ow">or</span> <span class="n">data</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="s2">&quot;int32&quot;</span>
                <span class="ow">or</span> <span class="n">data</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="s2">&quot;int64&quot;</span>
                <span class="ow">or</span> <span class="n">data</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="s2">&quot;bool&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Only integer tensors of a single element can be converted to an index.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_scalar_</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span>
                                     <span class="s2">&quot;Only integer tensors of a single element can be converted to an index.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__pos__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="fm">__abs__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">number</span><span class="p">):</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__add__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;__add__&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__and__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)):</span>
            <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;bitwise_and&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Unsupported operand type(s) for &amp;: &#39;Tensor&#39; and &#39;</span><span class="si">{}</span><span class="s2">&#39;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">other</span><span class="p">)))</span>

    <span class="k">def</span> <span class="fm">__xor__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)):</span>
            <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;bitwise_xor&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Unsupported operand type(s) for ^: &#39;Tensor&#39; and &#39;</span><span class="si">{}</span><span class="s2">&#39;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">other</span><span class="p">)))</span>

    <span class="k">def</span> <span class="fm">__or__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)):</span>
            <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;bitwise_or&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Unsupported operand type(s) for |: &#39;Tensor&#39; and &#39;</span><span class="si">{}</span><span class="s2">&#39;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">other</span><span class="p">)))</span>

    <span class="k">def</span> <span class="fm">__radd__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__add__</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__iadd__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__add__</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__sub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;__sub__&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__rsub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;__sub__&#39;</span><span class="p">)(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__isub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__sub__</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__mul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;__mul__&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__rmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__mul__</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__imul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__mul__</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__truediv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;__truediv__&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__rtruediv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;__truediv__&#39;</span><span class="p">)(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__mod__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;__mod__&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__rmod__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;__mod__&#39;</span><span class="p">)(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__imod__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__mod__</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__pow__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;__pow__&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__rpow__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;__rpow__&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__floordiv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;__floordiv__&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__rfloordiv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;__floordiv__&#39;</span><span class="p">)(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__ifloordiv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__floordiv__</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__lt__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;__lt__&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="fm">__le__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;__le__&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;__getitem__&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span><span class="p">:</span>
            <span class="n">out</span><span class="o">.</span><span class="n">parent_tensor_</span> <span class="o">=</span> <span class="bp">self</span>
            <span class="n">out</span><span class="o">.</span><span class="n">index_of_parent_</span> <span class="o">=</span> <span class="n">index</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="fm">__setitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;__setitem__&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">assign_value</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent_tensor_</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">index_of_parent_</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">parent_tensor_</span><span class="o">.</span><span class="fm">__setitem__</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">index_of_parent_</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="fm">__gt__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;__gt__&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="fm">__ge__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;__ge__&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;shape&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">out</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Not support len of a 0-D tensor&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">type_none</span><span class="p">:</span>
            <span class="k">return</span> <span class="s2">&quot;Unknown Tensor type!&quot;</span>
        <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the shape of the tensor as a tuple.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the dtype of the tensor (:class:`mindspore.dtype`).&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the total number of elements in tensor.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_size</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">ndim</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the number of tensor dimensions.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">has_init</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Whether tensor is initialized.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">init</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">itemsize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the length of one tensor element in bytes.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_itemsize</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">strides</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the tuple of bytes to step in each dimension when traversing a tensor.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_strides</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">nbytes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the total number of bytes taken by the tensor.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nbytes</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">T</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the transposed tensor.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>

<div class="viewcode-block" id="Tensor.from_numpy"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.from_numpy.html#mindspore.Tensor.from_numpy">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">from_numpy</span><span class="p">(</span><span class="n">array</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Convert numpy array to Tensor.</span>
<span class="sd">        If the data is not C contiguous, the data will be copied to C contiguous to construct the tensor.</span>
<span class="sd">        Otherwise, The tensor will be constructed using this numpy array without copy.</span>

<span class="sd">        Args:</span>
<span class="sd">            array (numpy.array): The input array.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same data type as input array.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; x = np.array([1, 2])</span>
<span class="sd">            &gt;&gt;&gt; output = Tensor.from_numpy(x)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [1 2]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">array</span><span class="o">.</span><span class="n">flags</span><span class="p">[</span><span class="s1">&#39;C_CONTIGUOUS&#39;</span><span class="p">]:</span>
            <span class="n">array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">array</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">Tensor_</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">array</span><span class="p">))</span></div>

<div class="viewcode-block" id="Tensor.set_const_arg"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.set_const_arg.html#mindspore.Tensor.set_const_arg">[docs]</a>    <span class="k">def</span> <span class="nf">set_const_arg</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">const_arg</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Specify whether the tensor is a constant when it is used for the argument of a network.</span>

<span class="sd">        Args:</span>
<span class="sd">            const_arg (bool): Whether the tensor is a constant when it is used for the argument of a network.</span>
<span class="sd">                Default: True.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has been specified whether to be a const network argument.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `const_arg` is not a bool.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([[1,2,3],[4,5,6]], dtype=np.float32))</span>
<span class="sd">            &gt;&gt;&gt; x.set_const_arg(True)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;const_arg&#39;</span><span class="p">,</span> <span class="n">const_arg</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="s1">&#39;set_const_arg&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">const_arg</span> <span class="o">=</span> <span class="n">const_arg</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="Tensor.assign_value"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.assign_value.html#mindspore.Tensor.assign_value">[docs]</a>    <span class="k">def</span> <span class="nf">assign_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Assign another tensor value to this tensor.</span>

<span class="sd">        Args:</span>
<span class="sd">            value (Tensor): Tensor for assignment.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, Tensor that&#39;s been assigned.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">assign_value_cpp</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="Tensor.item"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.item.html#mindspore.Tensor.item">[docs]</a>    <span class="k">def</span> <span class="nf">item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the item at the specified index of the tensor.</span>

<span class="sd">        Note:</span>
<span class="sd">            Tensor.item returns a Tensor scalar instead of a Python scalar.</span>

<span class="sd">        Args:</span>
<span class="sd">            index (Union[None, int, tuple(int)]): The index in Tensor. Default: None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A Tensor scalar, dtype is the same with the original Tensor.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the length of the `index` is not equal to self.ndim.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([[1,2,3],[4,5,6]], dtype=np.float32))</span>
<span class="sd">            &gt;&gt;&gt; x = x.item((0,1))</span>
<span class="sd">            &gt;&gt;&gt; print(x)</span>
<span class="sd">            2.0</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;item&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span></div>

<div class="viewcode-block" id="Tensor.itemset"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.itemset.html#mindspore.Tensor.itemset">[docs]</a>    <span class="k">def</span> <span class="nf">itemset</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Insert scalar into a tensor (scalar is cast to tensor&#39;s dtype, if possible).</span>

<span class="sd">        There must be at least 1 argument, and define the last argument as item.</span>
<span class="sd">        Then, tensor.itemset(\*args) is equivalent to :math:`tensor[args] = item`.</span>

<span class="sd">        Args:</span>
<span class="sd">            args (Union[(numbers.Number), (int/tuple(int), numbers.Number)]): The arguments that</span>
<span class="sd">                specify the index and value. If `args` contain one argument (a scalar),</span>
<span class="sd">                it is only used in case tensor is of size 1. If `args` contain two</span>
<span class="sd">                arguments, the last argument is the value to be set and must be a</span>
<span class="sd">                scalar, the first argument specifies a single tensor element location.</span>
<span class="sd">                It is either an int or a tuple.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A new tensor that doesn&#39;t affect the original tensor, with value set by :math:`tensor[args] = item`.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the length of the first argument is not equal to self.ndim.</span>
<span class="sd">            IndexError: If only one argument is provided, and the original Tensor is not scalar.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([[1,2,3],[4,5,6]], dtype=np.float32))</span>
<span class="sd">            &gt;&gt;&gt; print(x.itemset((0,1), 4))</span>
<span class="sd">            [[1. 4. 3.]</span>
<span class="sd">            [4. 5. 6.]]</span>
<span class="sd">            &gt;&gt;&gt; print(x)</span>
<span class="sd">            [[1. 2. 3.]</span>
<span class="sd">            [4. 5. 6.]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;itemset&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span></div>

<div class="viewcode-block" id="Tensor.asnumpy"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.asnumpy.html#mindspore.Tensor.asnumpy">[docs]</a>    <span class="k">def</span> <span class="nf">asnumpy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Convert tensor to numpy array. Returns self tensor as a NumPy ndarray. This tensor and the returned ndarray</span>
<span class="sd">        share the same underlying storage. Changes to self tensor will be reflected in the ndarray.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A numpy ndarray which shares the same underlying storage with the tensor.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([1, 2], dtype=np.float32))</span>
<span class="sd">            &gt;&gt;&gt; y = x.asnumpy()</span>
<span class="sd">            &gt;&gt;&gt; y[0] = 11</span>
<span class="sd">            &gt;&gt;&gt; print(x)</span>
<span class="sd">            [11.  2.]</span>
<span class="sd">            &gt;&gt;&gt; print(y)</span>
<span class="sd">            [11.  2.]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">Tensor_</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">value</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the value of the tensor or the parameter.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The value of the tensor or the parameter.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([1, 2], dtype=np.float32))</span>
<span class="sd">            &gt;&gt;&gt; x_value = x.value()</span>
<span class="sd">            &gt;&gt;&gt; print(x_value)</span>
<span class="sd">            [1.  2.]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span>

<div class="viewcode-block" id="Tensor.flush_from_cache"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.flush_from_cache.html#mindspore.Tensor.flush_from_cache">[docs]</a>    <span class="k">def</span> <span class="nf">flush_from_cache</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Flush cache data to host if tensor is cache enable.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([1, 2], dtype=np.float32))</span>
<span class="sd">            &gt;&gt;&gt; y = x.flush_from_cache()</span>
<span class="sd">            &gt;&gt;&gt; print(y)</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="n">Tensor_</span><span class="o">.</span><span class="n">_flush_from_cache</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.addcdiv"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.addcdiv.html#mindspore.Tensor.addcdiv">[docs]</a>    <span class="k">def</span> <span class="nf">addcdiv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Performs the element-wise division of tensor x1 by tensor x2,</span>
<span class="sd">        multiply the result by the scalar value and add it to input_data.</span>

<span class="sd">        .. math::</span>
<span class="sd">            y[i] = input\_data[i] + value[i] * (x1[i] / x2[i])</span>

<span class="sd">        Args:</span>
<span class="sd">            x1 (Tensor): The numerator tensor.</span>
<span class="sd">            x2 (Tensor): The denominator tensor.</span>
<span class="sd">            value (Tensor): The multiplier for tensor x1/x2.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same shape and dtype as x1/x2.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([1, 1, 1, 1]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; x1 = Tensor(np.array([1, 2, 3, 4]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; x2 = Tensor(np.array([4, 3, 2, 1]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; value = Tensor([1], mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; y = x.addcdiv(x1, x2, value)</span>
<span class="sd">            &gt;&gt;&gt; print(y)</span>
<span class="sd">            [1.25      1.6666667 2.5       5.       ]</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;addcdiv&#39;</span><span class="p">)()(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.addcmul"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.addcmul.html#mindspore.Tensor.addcmul">[docs]</a>    <span class="k">def</span> <span class="nf">addcmul</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Performs the element-wise product of tensor x1 and tensor x2,</span>
<span class="sd">        multiply the result by the scalar value and add it to input_data.</span>

<span class="sd">        .. math::</span>
<span class="sd">            y[i] = input\_data[i] + value[i] * (x1[i] * x2[i])</span>

<span class="sd">        Args:</span>
<span class="sd">            x1 (Tensor): The tensor to be multiplied.</span>
<span class="sd">            x2 (Tensor): The tensor to be multiplied.</span>
<span class="sd">            value (Tensor): The multiplier for tensor x1*x2.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same shape and dtype as x1*x2.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([1, 1, 1]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; x1 = Tensor(np.array([[1], [2], [3]]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; x2 = Tensor(np.array([[1, 2, 3]]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; value = Tensor([1], mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; y = x.addcmul(x1, x2, value)</span>
<span class="sd">            &gt;&gt;&gt; print(y)</span>
<span class="sd">            [[ 2.  3.  4.]</span>
<span class="sd">            [ 3.  5.  7.]</span>
<span class="sd">            [ 4.  7. 10.]]</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;addcmul&#39;</span><span class="p">)()(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Performs the element-wise addition of input tensors.</span>

<span class="sd">        .. math::</span>
<span class="sd">            output[i] = x[i] + y[i]</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Tensor): The tensor to be added.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same shape and dtype as input tensors.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; y = Tensor(np.array([4, 5, 6]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; z = x.add(y)</span>
<span class="sd">            &gt;&gt;&gt; print(z)</span>
<span class="sd">            [[ 5.  7.  9.]]</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;add&#39;</span><span class="p">)()(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">addr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vec1</span><span class="p">,</span> <span class="n">vec2</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Executes the outer-product of `vec1` and `vec2` and adds it to the input tensor.</span>

<span class="sd">        If `vec1` is a vector of size :math:`N` and `vec2` is a vector of size :math:`M`, then `x` must be</span>
<span class="sd">        broadcastable with a matrix of size :math:`(N, M)` and `out` will be a matrix of size :math:`(N, M)`.</span>

<span class="sd">        The optional values `beta` and `alpha` are the scale factors on the outer product between `vec1` and `vec2`</span>
<span class="sd">        and the added matrix `x` respectively. If `beta` is 0, then `x` will be ignored.</span>

<span class="sd">        .. math::</span>
<span class="sd">            output = β x + α (vec1 ⊗ vec2)</span>

<span class="sd">        Args:</span>
<span class="sd">            vec1 (Tensor): The first tensor to be multiplied. The shape of the tensor is :math:`(N,)`.</span>
<span class="sd">            vec2 (Tensor): The second tensor to be multiplied. The shape of the tensor is :math:`(M,)`.</span>
<span class="sd">            beta (scalar[int, float, bool], optional): Multiplier for `x` (β). The `beta` must be int or</span>
<span class="sd">                float or bool, Default: 1.</span>
<span class="sd">            alpha (scalar[int, float, bool], optional): Multiplier for `vec1` @ `vec2` (α). The `alpha` must</span>
<span class="sd">                be int or float or bool, Default: 1.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, the shape of the output tensor is :math:`(N, M)`, has the same dtype as `x`.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `x`, `vec1`, `vec2` is not a Tensor.</span>
<span class="sd">            TypeError: If inputs `x`, `vec1`, &#39;vec2&#39; are not the same dtype.</span>
<span class="sd">            ValueError: If `x` is not a 2-D Tensor.</span>
<span class="sd">                If `vec1`, `vec2` is not a 1-D Tensor.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([[2., 2.], [3., 2.], [3., 4.]], np.float32))</span>
<span class="sd">            &gt;&gt;&gt; vec1 = Tensor(np.array([2., 3., 2.], np.float32))</span>
<span class="sd">            &gt;&gt;&gt; vec2 = Tensor(np.array([3, 4], np.float32))</span>
<span class="sd">            &gt;&gt;&gt; output = x.addr(vec1, vec2)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [[ 8. 10.]</span>
<span class="sd">            [12. 14.]</span>
<span class="sd">            [ 9. 12.]]</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;addr&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vec1</span><span class="p">,</span> <span class="n">vec2</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<div class="viewcode-block" id="Tensor.all"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.all.html#mindspore.Tensor.all">[docs]</a>    <span class="k">def</span> <span class="nf">all</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(),</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Check all tensor elements along a given axis evaluate to True.</span>

<span class="sd">        Args:</span>
<span class="sd">            axis (Union[None, int, tuple(int)]): Dimensions of reduction.</span>
<span class="sd">                When the axis is None or empty tuple, reduce all dimensions. When the axis is int or</span>
<span class="sd">                tuple(int), if the dimension of Tensor is dim, the value range is [-dim, dim). Default: ().</span>
<span class="sd">            keep_dims (bool): Whether to keep the reduced dimensions. Default: False.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, if all tensor elements along the given axis evaluate to True, its value is True,</span>
<span class="sd">            otherwise its value is False. If the axis is None or empty tuple, reduce all dimensions.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        See also:</span>
<span class="sd">            :func:`mindspore.Tensor.any`: Check any tensor element along a given axis evaluate to True.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; a = Tensor([True, True, False])</span>
<span class="sd">            &gt;&gt;&gt; output = a.all()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            False</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">axis</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;all&#39;</span><span class="p">)(</span><span class="n">keep_dims</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.any"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.any.html#mindspore.Tensor.any">[docs]</a>    <span class="k">def</span> <span class="nf">any</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(),</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Check any tensor element along a given axis evaluate to True.</span>

<span class="sd">        Args:</span>
<span class="sd">            axis (Union[None, int, tuple(int)]): Dimensions of reduction.</span>
<span class="sd">                When the axis is None or empty tuple, reduce all dimensions. When the axis is int or</span>
<span class="sd">                tuple(int), if the dimension of Tensor is dim, the value range is [-dim, dim). Default: ().</span>
<span class="sd">            keep_dims (bool): Whether to keep the reduced dimensions. Default: False.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, if any tensor element along the given axis evaluates to True, its value is True,</span>
<span class="sd">            otherwise its value is False. If the axis is None or empty tuple, reduce all dimensions.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        See also:</span>
<span class="sd">            :func:`mindspore.Tensor.all`: Check all tensor elements along a given axis evaluate to True.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; a = Tensor([True, True, False])</span>
<span class="sd">            &gt;&gt;&gt; output = a.any()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            True</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">axis</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;any&#39;</span><span class="p">)(</span><span class="n">keep_dims</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.atan2"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.atan2.html#mindspore.Tensor.atan2">[docs]</a>    <span class="k">def</span> <span class="nf">atan2</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns arctangent of x/y element-wise.</span>

<span class="sd">        `x` refers to self tensor.</span>

<span class="sd">        It returns :math:`\theta\ \in\ [-\pi, \pi]`</span>
<span class="sd">        such that :math:`x = r*\sin(\theta), y = r*\cos(\theta)`, where :math:`r = \sqrt{x^2 + y^2}`.</span>

<span class="sd">        Args of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">        If they have different data types, the lower precision data type will be converted to</span>
<span class="sd">        the relatively highest precision data type.</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Tensor): The input tensor. It has the same shape with `x` after broadcasting,</span>
<span class="sd">                or the shape of `x` is the same as `y` after broadcasting.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, the shape is the same as the one after broadcasting, and the data type is same as `x`.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `x` or `y` is not a Tensor.</span>
<span class="sd">            RuntimeError: If the data type of `x` and `y` conversion of Parameter is required</span>
<span class="sd">                          when data type conversion of Parameter is not supported.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([0, 1]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; y = Tensor(np.array([1, 1]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = x.atan2(y)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [0.        0.7853982]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;atan2&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.view"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.view.html#mindspore.Tensor.view">[docs]</a>    <span class="k">def</span> <span class="nf">view</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">shape</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reshape the tensor according to the input shape. It&#39;s the same as :func:`mindspore.Tensor.reshape`,</span>
<span class="sd">        implemented by the underlying reshape operator.</span>

<span class="sd">        Args:</span>
<span class="sd">            shape (Union[tuple(int), int]): Dimension of the output tensor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, which dimension is the input shape&#39;s value.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; a = Tensor(np.array([[1, 2, 3], [2, 3, 4]], dtype=np.float32))</span>
<span class="sd">            &gt;&gt;&gt; output = a.view((3, 2))</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [[1. 2.]</span>
<span class="sd">            [3. 2.]</span>
<span class="sd">            [3. 4.]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">shape</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The shape variable should not be empty&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Only one tuple is needed, but got </span><span class="si">{</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;reshape&#39;</span><span class="p">)()(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.bitwise_and"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.bitwise_and.html#mindspore.Tensor.bitwise_and">[docs]</a>    <span class="k">def</span> <span class="nf">bitwise_and</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns bitwise `and` of two tensors element-wise.</span>

<span class="sd">        Refer to :func:`mindspore.ops.bitwise_and` for more detail.</span>

<span class="sd">        Args:</span>
<span class="sd">            x (Tensor): The input tensor with int16, int32 or uint16 data type.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same type as the `x`.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; a = Tensor(np.array([0, 0, 1, -1, 1, 1, 1]), mindspore.int16)</span>
<span class="sd">            &gt;&gt;&gt; b = Tensor(np.array([0, 1, 1, -1, -1, 2, 3]), mindspore.int16)</span>
<span class="sd">            &gt;&gt;&gt; output = a.bitwise_and(b)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [ 0  0  1 -1  1  0  1]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;bitwise_and&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.bitwise_or"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.bitwise_or.html#mindspore.Tensor.bitwise_or">[docs]</a>    <span class="k">def</span> <span class="nf">bitwise_or</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns bitwise `or` of two tensors element-wise.</span>

<span class="sd">        Refer to :func:`mindspore.ops.bitwise_or` for more detail.</span>

<span class="sd">        Args:</span>
<span class="sd">            x (Tensor): The input tensor with int16, int32 or uint16 data type.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same type as the `x`.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; a = Tensor(np.array([0, 0, 1, -1, 1, 1, 1]), mindspore.int16)</span>
<span class="sd">            &gt;&gt;&gt; b = Tensor(np.array([0, 1, 1, -1, -1, 2, 3]), mindspore.int16)</span>
<span class="sd">            &gt;&gt;&gt; output = a.bitwise_or(b)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [ 0  1  1 -1 -1  3  3]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;bitwise_or&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.bitwise_xor"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.bitwise_xor.html#mindspore.Tensor.bitwise_xor">[docs]</a>    <span class="k">def</span> <span class="nf">bitwise_xor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns bitwise `xor` of two tensors element-wise.</span>

<span class="sd">        Refer to :func:`mindspore.ops.bitwise_xor` for more detail.</span>

<span class="sd">        Args:</span>
<span class="sd">            x (Tensor): The input tensor with int16, int32 or uint16 data type.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same type as the `x`.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; a = Tensor(np.array([0, 0, 1, -1, 1, 1, 1]), mindspore.int16)</span>
<span class="sd">            &gt;&gt;&gt; b = Tensor(np.array([0, 1, 1, -1, -1, 2, 3]), mindspore.int16)</span>
<span class="sd">            &gt;&gt;&gt; output = a.bitwise_xor(b)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [ 0  1  0  0 -2  3  2]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;bitwise_xor&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.scatter_mul"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.scatter_mul.html#mindspore.Tensor.scatter_mul">[docs]</a>    <span class="k">def</span> <span class="nf">scatter_mul</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Creates a new tensor by multiplying the values from the positions in self tensor indicated by</span>
<span class="sd">        `indices`, with values from `updates`. When divided values are provided for the same</span>
<span class="sd">        index, the result of the update will be to divided these values respectively. Except that</span>
<span class="sd">        the updates are applied on output `Tensor` instead of input `Parameter`.</span>
<span class="sd">        The variable `input_x` refers to self tensor.</span>

<span class="sd">        The last axis of `indices` is the depth of each index vectors. For each index vector,</span>
<span class="sd">        there must be a corresponding value in `updates`. The shape of `updates` should be</span>
<span class="sd">        equal to the shape of `input_x[indices]`. For more details, see use cases.</span>

<span class="sd">        Note:</span>
<span class="sd">            - If some values of the `indices` are out of bound, instead of raising an index error,</span>
<span class="sd">              the corresponding `updates` will not be updated to `input_x`.</span>

<span class="sd">        Args:</span>
<span class="sd">            indices (Tensor): The index of input tensor whose data type is int32 or int64. The rank must be at least 2.</span>
<span class="sd">            updates (Tensor): The tensor to update the input tensor, has the same type as input,</span>
<span class="sd">                and updates shape should be equal to indices.shape[:-1] + input_x.shape[indices.shape[-1]:].</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same shape and type as self tensor.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If dtype of `indices` is neither int32 nor int64.</span>
<span class="sd">            ValueError: If length of shape of self tensor is less than the last dimension of shape of `indices`.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; input_x = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; indices = Tensor(np.array([[0, 0], [0, 0]]), mindspore.int32)</span>
<span class="sd">            &gt;&gt;&gt; updates = Tensor(np.array([1.0, 2.2]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; # Next, demonstrate the approximate operation process of this operator:</span>
<span class="sd">            &gt;&gt;&gt; # 1, indices[0] = [0, 0], indices[1] = [0, 0]</span>
<span class="sd">            &gt;&gt;&gt; # 2, And input_x[0, 0] = -0.1</span>
<span class="sd">            &gt;&gt;&gt; # 3, So input_x[indices] = [-0.1, -0.1]</span>
<span class="sd">            &gt;&gt;&gt; # 4, Satisfy the above formula: input_x[indices].shape=(2) == updates.shape=(2)</span>
<span class="sd">            &gt;&gt;&gt; # 5, Perform the multiply operation for the first time:</span>
<span class="sd">            &gt;&gt;&gt; #      first_input_x = input_x[0][0] * updates[0] = [[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]</span>
<span class="sd">            &gt;&gt;&gt; # 6, Perform the multiply operation for the second time:</span>
<span class="sd">            &gt;&gt;&gt; #      second_input_x = input_x[0][0] * updates[1] = [[-0.22, 0.3, 3.6], [0.4, 0.5, -3.2]]</span>
<span class="sd">            &gt;&gt;&gt; output = input_x.scatter_mul(indices, updates)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [[-0.22  0.3   3.6  ]</span>
<span class="sd">             [ 0.4   0.5   -3.2 ]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;tensor_scatter_mul&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.scatter_div"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.scatter_div.html#mindspore.Tensor.scatter_div">[docs]</a>    <span class="k">def</span> <span class="nf">scatter_div</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Creates a new tensor by dividing the values from the positions in self tensor indicated by</span>
<span class="sd">        `indices`, with values from `updates`. When divided values are provided for the same</span>
<span class="sd">        index, the result of the update will be to divided these values respectively. Except that</span>
<span class="sd">        the updates are applied on output `Tensor` instead of input `Parameter`.</span>

<span class="sd">        The last axis of `indices` is the depth of each index vectors. For each index vector,</span>
<span class="sd">        there must be a corresponding value in `updates`. The shape of `updates` should be</span>
<span class="sd">        equal to the shape of `input_x[indices]`, the variable `input_x` refers to self tensor.</span>
<span class="sd">        For more details, see use cases.</span>

<span class="sd">        Note:</span>
<span class="sd">            - If some values of the `indices` are out of bound, instead of raising an index error,</span>
<span class="sd">              the corresponding `updates` will not be updated to `input_x`.</span>
<span class="sd">            - The operator can&#39;t handle division by 0 exceptions, so the user needs to make sure</span>
<span class="sd">              there is no 0 value in `updates`.</span>

<span class="sd">        Args:</span>
<span class="sd">            indices (Tensor): The index of input tensor whose data type is int32 or int64.</span>
<span class="sd">                The rank must be at least 2.</span>
<span class="sd">            updates (Tensor): The tensor to update the input tensor, has the same type as input,</span>
<span class="sd">                and updates.shape should be equal to indices.shape[:-1] + input_x.shape[indices.shape[-1]:].</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same shape and type as self tensor.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If dtype of `indices` is neither int32 nor int64.</span>
<span class="sd">            ValueError: If length of shape of self tensor is less than the last dimension of shape of `indices`.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; input_x = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]).astype(&#39;float32&#39;))</span>
<span class="sd">            &gt;&gt;&gt; indices = Tensor(np.array([[0, 0], [0, 0]]).astype(&#39;int32&#39;))</span>
<span class="sd">            &gt;&gt;&gt; updates = Tensor(np.array([1.0, 2.0]).astype(&#39;float32&#39;))</span>
<span class="sd">            &gt;&gt;&gt; output = input_x.scatter_div(indices, updates)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [[-0.05  0.3  3.6  ]</span>
<span class="sd">             [ 0.4   0.5  -3.2 ]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;tensor_scatter_div&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.ger"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.ger.html#mindspore.Tensor.ger">[docs]</a>    <span class="k">def</span> <span class="nf">ger</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Ger product of `self` and `x`. Calculate the outer product of two arrays. If `self` is a 1D</span>
<span class="sd">        Tensor of shape :math:`(m,)` and `x` is a 1D Tensor of shape :math:`(n,)`, then `output` must be a Tensor of</span>
<span class="sd">        shape :math:`(m, n)`.</span>

<span class="sd">        Note:</span>
<span class="sd">            Currently Ascend does not support float64 data input.</span>

<span class="sd">        Refer to :func:`mindspore.ops.ger` for more detail.</span>

<span class="sd">        Args:</span>
<span class="sd">            x (Tensor): input Tensor, with dtype of float16, float32 or float64.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, output matrix with the same dtype as inputs.With `self` shape :math:`(m,)` and</span>
<span class="sd">            `x` shape of :math:`(n,)`, the `output` has shape :math:`(m, n)`.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; x1 = Tensor([1., 2., 3., 4.], mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; x2 = Tensor([1., 2., 3.], mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = x1.ger(x2)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [[ 1.  2.  3.]</span>
<span class="sd">             [ 2.  4.  6.]</span>
<span class="sd">             [ 3.  6.  9.]</span>
<span class="sd">             [ 4.  8. 12.]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;ger&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.broadcast_to"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.broadcast_to.html#mindspore.Tensor.broadcast_to">[docs]</a>    <span class="k">def</span> <span class="nf">broadcast_to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Broadcasts input tensor to a given shape.</span>

<span class="sd">        Refer to :func:`mindspore.ops.broadcast_to` for more detail.</span>

<span class="sd">        Args:</span>
<span class="sd">            shape (tuple): The target shape to broadcast. Can be fully specified, or have -1 in one position</span>
<span class="sd">                           where it will be substituted by the input tensor&#39;s shape in that position.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, with the given `shape` and the same data type as `self`.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `shape` is not a tuple.</span>
<span class="sd">            ValueError: If the target and input shapes are incompatible, or if a - 1</span>
<span class="sd">                        in the target shape is in an invalid location.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">            &gt;&gt;&gt; shape = (2, 3)</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]).astype(np.float32))</span>
<span class="sd">            &gt;&gt;&gt; output = x.broadcast_to(shape)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [[1. 2. 3.]</span>
<span class="sd">             [1. 2. 3.]]</span>
<span class="sd">            &gt;&gt;&gt; shape = (-1, 2)</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([[1], [2]]).astype(np.float32))</span>
<span class="sd">            &gt;&gt;&gt; output = x.broadcast_to(shape)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [[1. 1.]</span>
<span class="sd">             [2. 2.]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;broadcast_to&#39;</span><span class="p">)(</span><span class="n">shape</span><span class="p">)(</span><span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.expand_as"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.expand_as.html#mindspore.Tensor.expand_as">[docs]</a>    <span class="k">def</span> <span class="nf">expand_as</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Expand the dimension of target tensor to the dimension of input tensor.</span>

<span class="sd">        Args:</span>
<span class="sd">            x (Tensor): The input tensor. The shape of the input tensor must obey</span>
<span class="sd">                the broadcasting rule.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same dimension as input tensor.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor([1, 2, 3], dtype=mstype.float32)</span>
<span class="sd">            &gt;&gt;&gt; y = Tensor(np.ones((2, 3)), dtype=mstype.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = x.expand_as(y)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [[1. 2. 3.]</span>
<span class="sd">            [1. 2. 3.]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;broadcast_to&#39;</span><span class="p">)(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)(</span><span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.exp"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.exp.html#mindspore.Tensor.exp">[docs]</a>    <span class="k">def</span> <span class="nf">exp</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns exponential of a tensor element-wise.</span>

<span class="sd">        .. math::</span>

<span class="sd">            out_i = e^{x_i}</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = x.exp()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [ 2.718282  7.389056 54.598152]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;exp&#39;</span><span class="p">)()(</span><span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.sqrt"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.sqrt.html#mindspore.Tensor.sqrt">[docs]</a>    <span class="k">def</span> <span class="nf">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns sqrt of a tensor element-wise.</span>

<span class="sd">        .. math::</span>

<span class="sd">            out_{i} = \\sqrt{x_{i}}</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If output is not a Tensor.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([1.0, 4.0, 9.0]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = x.sqrt()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [1. 2. 3.]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;sqrt&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.square"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.square.html#mindspore.Tensor.square">[docs]</a>    <span class="k">def</span> <span class="nf">square</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns square of a tensor element-wise.</span>

<span class="sd">        .. math::</span>

<span class="sd">            out_{i} = (x_{i})^2</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 3.0]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = x.square()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [1. 4. 9.]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;square&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.sub"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.sub.html#mindspore.Tensor.sub">[docs]</a>    <span class="k">def</span> <span class="nf">sub</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Refer to :func:`mindspore.ops.sub` for more detail.</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Union[Tensor, number.Number, bool]): The input, when the first input is a Tensor,</span>
<span class="sd">                the second input should be a number.Number or bool value, or a Tensor</span>
<span class="sd">                whose data type is number or bool\_.</span>
<span class="sd">                When the first input is Scalar, the second input must be a Tensor whose data type is number or bool\_.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">            and the data type is the one with higher precision or higher digits among the inputs.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `y` is not a number.Number or a bool or a Tensor.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">            &gt;&gt;&gt; y = Tensor(np.array([4, 5, 6]), mindspore.int32)</span>
<span class="sd">            &gt;&gt;&gt; output = x.sub(y)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [-3 -3 -3]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;sub&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.tan"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.tan.html#mindspore.Tensor.tan">[docs]</a>    <span class="k">def</span> <span class="nf">tan</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes tangent of `x` element-wise.</span>

<span class="sd">        .. math::</span>

<span class="sd">            out_i = tan(x_i)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same shape as self.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If self is not a Tensor.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; a = Tensor([-1.0, 0.0, 1.0]).astype(&quot;float32&quot;)</span>
<span class="sd">            &gt;&gt;&gt; output = a.tan()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [-1.5574081 0. 1.5574081]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;tan&#39;</span><span class="p">)()(</span><span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.tanh"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.tanh.html#mindspore.Tensor.tanh">[docs]</a>    <span class="k">def</span> <span class="nf">tanh</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Tanh activation function.</span>

<span class="sd">        Computes hyperbolic tangent of self Tensor element-wise. The Tanh function is defined as:</span>

<span class="sd">        .. math::</span>

<span class="sd">            tanh(x_i) = \frac{\exp(x_i) - \exp(-x_i)}{\exp(x_i) + \exp(-x_i)} = \frac{\exp(2x_i) - 1}{\exp(2x_i) + 1},</span>

<span class="sd">        where :math:`x_i` is an element of the self Tensor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, with the same dtype and shape as self.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If dtype of self tensor is not float16, float32, float64, complex64 or complex128.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU``  ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3, 4, 5]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = input_x.tanh()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [0.7615941 0.9640276 0.9950547 0.9993293 0.9999092]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;tanh&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.cosh"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.cosh.html#mindspore.Tensor.cosh">[docs]</a>    <span class="k">def</span> <span class="nf">cosh</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes hyperbolic cosine of `x` element-wise.</span>

<span class="sd">        .. math::</span>

<span class="sd">            out_i = \cosh(x_i)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same shape as `x`.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; a = Tensor(np.array([0.24, 0.83, 0.31, 0.09]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = a.cosh()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [1.0289385 1.364684 1.048436 1.0040528]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;cosh&#39;</span><span class="p">)()(</span><span class="bp">self</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">acos</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes arccosine of input tensors element-wise.</span>

<span class="sd">        .. math::</span>

<span class="sd">            out_i = cos^{-1}(x_i)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same shape as `x`.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; a = Tensor(np.array([0.74, 0.04, 0.30, 0.56]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = a.acos()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [0.737726  1.5307857 1.2661036 0.9764105]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;acos&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">cos</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes cosine of input element-wise.</span>

<span class="sd">        .. math::</span>
<span class="sd">            out_i = cos(x_i)</span>

<span class="sd">        .. warning::</span>
<span class="sd">            Currently support Float16, Float32 data type. If use Float64, there may</span>
<span class="sd">            be a problem of missing precision.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same shape as `x`.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; a = Tensor(np.array([0.24, 0.83, 0.31, 0.09]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = a.cos()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [0.971338 0.6748758 0.95233357 0.9959527]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;cos&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">acosh</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes inverse hyperbolic cosine of the inputs element-wise.</span>

<span class="sd">        .. math::</span>

<span class="sd">            out_i = \cosh^{-1}(input_i)</span>

<span class="sd">        .. warning::</span>
<span class="sd">            Given an input tensor x, the function computes inverse hyperbolic cosine of every element.</span>
<span class="sd">            Input range is [1, inf].</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same shape as `x`.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; a = Tensor(np.array([1.0, 1.5, 3.0, 100.0]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = a.acosh()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [0.        0.9624237 1.7627472 5.298292]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;acosh&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">asin</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes arcsine of input tensors element-wise.</span>

<span class="sd">        .. math::</span>

<span class="sd">        out_i = sin^{-1}(x_i)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same shape and dtype as `x`.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([0.74, 0.04, 0.30, 0.56]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = x.asin()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [0.8330704  0.04001067 0.30469266 0.5943858]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;asin&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">)</span>

<div class="viewcode-block" id="Tensor.abs"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.abs.html#mindspore.Tensor.abs">[docs]</a>    <span class="k">def</span> <span class="nf">abs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return absolute value element-wisely.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; a = Tensor([1.1, -2.1]).astype(&quot;float32&quot;)</span>
<span class="sd">            &gt;&gt;&gt; output = a.abs()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [1.1 2.1]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;abs&#39;</span><span class="p">)()(</span><span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.ceil"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.ceil.html#mindspore.Tensor.ceil">[docs]</a>    <span class="k">def</span> <span class="nf">ceil</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Rounds a tensor up to the closest integer element-wise.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If dtype of self tensor is not float16 or float32.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; a = Tensor([1.1, 2.5, -1.5]).astype(&quot;float32&quot;)</span>
<span class="sd">            &gt;&gt;&gt; output = a.ceil()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [ 2.  3. -1.]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;ceil&#39;</span><span class="p">)()(</span><span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.lerp"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.lerp.html#mindspore.Tensor.lerp">[docs]</a>    <span class="k">def</span> <span class="nf">lerp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Does a linear interpolation of two tensors start and end based on a float or tensor weight.</span>

<span class="sd">        If `weight` is a tensor, the shapes of two inputs need to be broadcast.</span>
<span class="sd">        If `weight` is a float, the shapes of `end` need to be broadcast.</span>

<span class="sd">        Args:</span>
<span class="sd">            end (Tensor): The tensor with the ending points. Data type must be float16 or float32.</span>
<span class="sd">            weight (Union[float, Tensor]): The weight for the interpolation formula. Must be a float</span>
<span class="sd">                or a scalar tensor with float16 or float32 data type.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same type and shape as self tensor.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `end` is not a tensor.</span>
<span class="sd">            TypeError: If `weight` is neither scalar(float) nor tensor.</span>
<span class="sd">            TypeError: If dtype of `end` is neither float16 nor float32.</span>
<span class="sd">            TypeError: If dtype of `weight` is neither float16 nor float32 when it is a tensor.</span>
<span class="sd">            TypeError: If self tensor and `end` have different data types.</span>
<span class="sd">            TypeError: If self tensor, `end` and `weight` have different data types when `weight` is a tensor.</span>
<span class="sd">            ValueError: If `end` could not be broadcast to tensor with shape of self tensor.</span>
<span class="sd">            ValueError: If `weight` could not be broadcast to tensor with shapes of `end` when it is a tensor.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; start = Tensor(np.array([1., 2., 3., 4.]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; end = Tensor(np.array([10., 10., 10., 10.]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = start.lerp( end, 0.5)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [5.5 6. 6.5 7. ]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;lerp&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.norm"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.norm.html#mindspore.Tensor.norm">[docs]</a>    <span class="k">def</span> <span class="nf">norm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the matrix norm or vector norm of a given tensor.</span>

<span class="sd">        Args:</span>
<span class="sd">            axis (Union[int, list, tuple]): Specifies which dimension or dimensions of input to</span>
<span class="sd">                calculate the norm across.</span>
<span class="sd">            p (int): The order of norm. Default: 2. `p` is greater than or equal to 0.</span>
<span class="sd">            keep_dims (bool): Whether the output tensors have dim retained or not. Default: False.</span>
<span class="sd">            epsilon (float): A value added to the denominator for numerical stability. Default: 1e-12.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same dtype as self tensor, which shape depends on the args axis.</span>
<span class="sd">            For example, if the size of input is (2, 3, 4), axis is [0, 1], Outputs&#39; shape will be (4,).</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If dtype of self tensor is not one of: float16, float32.</span>
<span class="sd">            TypeError: If `p` is not an int.</span>
<span class="sd">            TypeError: If `axis` is not an int, a tuple or a list.</span>
<span class="sd">            TypeError: If `axis` is a tuple or a list, but the element of `axis` is not an int.</span>
<span class="sd">            TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">            TypeError: If `epsilon` is not a float.</span>
<span class="sd">            ValueError: If the element of `axis` is out of the range (-len(input_x.shape), len(input_x.shape)).</span>
<span class="sd">                input_x refers to self tensor.</span>
<span class="sd">            ValueError: If the length of shape of `axis` is bigger than the length of shape of self tensor.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; input_x = Tensor(np.array([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]]).astype(np.float32))</span>
<span class="sd">            &gt;&gt;&gt; output = input_x.norm([0, 1], p=2)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [ 9.165152 10.954452]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;norm&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.renorm"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.renorm.html#mindspore.Tensor.renorm">[docs]</a>    <span class="k">def</span> <span class="nf">renorm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">maxnorm</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Renormalizes the sub-tensors along dimension `dim`, and each sub-tensor&#39;s p-norm should not exceed the</span>
<span class="sd">        &#39;maxnorm&#39;. The values of current sub-tensor don&#39;t need change if the p-norm of the sub-tensor is less than</span>
<span class="sd">        `maxnorm`. Otherwise the sub-tensor needs to be modified to the original value of the corresponding position</span>
<span class="sd">        divided by the p-norm of the substensor and then multiplied by `maxnorm`.</span>

<span class="sd">        Args:</span>
<span class="sd">            p (int): Power of norm calculation.</span>
<span class="sd">            dim (int): The dimension that expected to get the slice-tensor.</span>
<span class="sd">            maxnorm (float): Max norm.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same dtype and shape as itself.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If dtype of `p` is not int.</span>
<span class="sd">            TypeError: If dtype of `dim` is not int.</span>
<span class="sd">            TypeError: If dtype of `maxnorm` is not float32.</span>
<span class="sd">            ValueError: If the value of `p` less than 1.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; y = x.renorm(p=1, dim=0, maxnorm=5.)</span>
<span class="sd">            &gt;&gt;&gt; print(y)</span>
<span class="sd">            [[1.       1.        1.        ]</span>
<span class="sd">            [1.6666666 1.6666666 1.6666666 ]</span>
<span class="sd">            [1.6666667 1.6666667 1.6666667 ]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;renorm&quot;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">maxnorm</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.approximate_equal"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.approximate_equal.html#mindspore.Tensor.approximate_equal">[docs]</a>    <span class="k">def</span> <span class="nf">approximate_equal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">tolerance</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns True if abs(x-y) is smaller than tolerance element-wise, otherwise False.</span>

<span class="sd">        .. math::</span>

<span class="sd">            out_i = \begin{cases}</span>
<span class="sd">            &amp; \text{ if } \left | x_{i} - y_{i} \right | &lt; \text{tolerance},\ \ True  \\</span>
<span class="sd">            &amp; \text{ if } \left | x_{i} - y_{i} \right | \ge \text{tolerance},\ \  False</span>
<span class="sd">            \end{cases}</span>

<span class="sd">        where `tolerance` indicates Acceptable maximum tolerance.</span>

<span class="sd">        Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">        If they have different data types, the lower precision data type will be converted to</span>
<span class="sd">        the relatively highest precision data type.</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Tensor): Second tensor to compare, with data type belongs to float32, float16.</span>
<span class="sd">            tolerance (float): The maximum deviation that two elements can be considered equal. Default: 1e-05.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same shape as self tensor, and the data type is bool.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `tolerance` is not a float.</span>
<span class="sd">            RuntimeError: If the data type of `x`, `y` conversion of Parameter is given</span>
<span class="sd">                        but data type conversion of Parameter is not supported.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from mindspore.common import dtype as mstype</span>
<span class="sd">            &gt;&gt;&gt; tol = 2.</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mstype.float32)</span>
<span class="sd">            &gt;&gt;&gt; y = Tensor(np.array([2, 4, 6]), mstype.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = Tensor(x).approximate_equal(Tensor(y), tol)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [ True  False  False]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_isinstance</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_isinstance</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_isinstance</span><span class="p">(</span><span class="s2">&quot;tolerance&quot;</span><span class="p">,</span> <span class="n">tolerance</span><span class="p">,</span> <span class="nb">float</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="n">input_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
        <span class="n">input_y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span> <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span> <span class="k">else</span> <span class="n">y</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;__lt__&#39;</span><span class="p">)(</span><span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;abs&#39;</span><span class="p">)()(</span>
            <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;__sub__&#39;</span><span class="p">)(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
        <span class="p">),</span> <span class="n">tolerance</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.matrix_determinant"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.matrix_determinant.html#mindspore.Tensor.matrix_determinant">[docs]</a>    <span class="k">def</span> <span class="nf">matrix_determinant</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes the determinant of one or more square matrices.</span>

<span class="sd">        `x` refers to self tensor.</span>

<span class="sd">        Returns:</span>

<span class="sd">            Tensor, The shape is :math:`x\_shape[:-2]`, the dtype is same as &#39;x&#39;.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If self tensor is not a Tensor.</span>
<span class="sd">            TypeError: If dtype of self tensor not float32, float64, complex64 or complex128.</span>
<span class="sd">            ValueError: If the last two dimensions of self tensor is not same size.</span>
<span class="sd">            ValueError: If the dimension of self tensor is less than 2.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; input_x = Tensor(np.array([[[-4.5, -1.5], [7.0, 6.0]], [[2.5, 0.5], [3.0, 9.0]]]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = input_x.matrix_determinant()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [-16.5 21. ]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;matrix_determinant&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.log1p"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.log1p.html#mindspore.Tensor.log1p">[docs]</a>    <span class="k">def</span> <span class="nf">log1p</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the natural logarithm of one plus the input tensor element-wise.</span>

<span class="sd">        `x` refers to self tensor.</span>

<span class="sd">        .. math::</span>
<span class="sd">            out_i = {log_e}(x_i + 1)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same shape as the `x`.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `x` is not a Tensor.</span>
<span class="sd">            TypeError: If dtype of `x` is neither float16 nor float32.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = x.log1p()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [0.6931472 1.0986123 1.609438 ]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;log1p&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.logit"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.logit.html#mindspore.Tensor.logit">[docs]</a>    <span class="k">def</span> <span class="nf">logit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculate the logit of a tensor element-wise. When eps is not None, element in &#39;x&#39; is clamped to [eps, 1-eps].</span>
<span class="sd">        When eps is None, input &#39;x&#39; is not clamped.</span>

<span class="sd">        `x` refers to self tensor.</span>

<span class="sd">        .. math::</span>
<span class="sd">            \begin{align}</span>
<span class="sd">            y_{i} &amp; = \ln(\frac{z_{i}}{1 - z_{i}}) \\</span>
<span class="sd">            z_{i} &amp; = \begin{cases}</span>
<span class="sd">            x_{i} &amp; \text{if eps is None} \\</span>
<span class="sd">            \text{eps} &amp; \text{if } x_{i} \lt \text{eps} \\</span>
<span class="sd">            x_{i} &amp; \text{if } \text{eps} \leq x_{i} \leq 1 - \text{eps} \\</span>
<span class="sd">            1 - \text{eps} &amp; \text{if } x_{i} \gt 1 - \text{eps}</span>
<span class="sd">            \end{cases}</span>
<span class="sd">            \end{align}</span>

<span class="sd">        Args:</span>
<span class="sd">            eps (float, optional): The epsilon. The input clamp bound is defined as [eps, 1-eps]. Default: None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, with the same shape as the `x`.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `eps` is not a float.</span>
<span class="sd">            TypeError: If `x` is not a Tensor.</span>
<span class="sd">            TypeError: If dtype of `x` is not float16, float32 or float64.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``GPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([0.1, 0.2, 0.3]).astype(np.float32))</span>
<span class="sd">            &gt;&gt;&gt; output = x.logit(eps=1e-5)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [-2.1972246 -1.3862944 -0.8472978]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">eps</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">eps</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.0</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;eps&#39;</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="p">(</span><span class="nb">float</span><span class="p">,),</span> <span class="s1">&#39;Tensor.logit&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;logit&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.log_matrix_determinant"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.log_matrix_determinant.html#mindspore.Tensor.log_matrix_determinant">[docs]</a>    <span class="k">def</span> <span class="nf">log_matrix_determinant</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes the sign and the log of the absolute value of the determinant of one or more square matrices.</span>

<span class="sd">        `x` refers to self tensor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, The signs of the log determinants. The shape is :math:`x\_shape[:-2]`, the dtype is same as `x`.</span>

<span class="sd">            Tensor, The absolute values of the log determinants. The shape is :math:`x\_shape[:-2]`,</span>
<span class="sd">            the dtype is same as `x`.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If self tensor is not a Tensor.</span>
<span class="sd">            TypeError: If dtype of self tensor not float32, float64, complex64 or complex128.</span>
<span class="sd">            ValueError: If the last two dimensions of self tensor is not same size.</span>
<span class="sd">            ValueError: If the dimension of self tensor is less than 2.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; input_x = Tensor(np.array([[[-4.5, -1.5], [7.0, 6.0]], [[2.5, 0.5], [3.0, 9.0]]]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; sign, output  = input_x.log_matrix_determinant()</span>
<span class="sd">            &gt;&gt;&gt; print(sign)</span>
<span class="sd">            [-1.   1.]</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [2.80336046e+00    3.04452229e+00]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;log_matrix_determinant&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.isclose"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.isclose.html#mindspore.Tensor.isclose">[docs]</a>    <span class="k">def</span> <span class="nf">isclose</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">,</span> <span class="n">equal_nan</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a boolean Tensor where two Tensors are element-wise equal within a tolerance.</span>

<span class="sd">        Args:</span>
<span class="sd">            x2 (Tensor): Second Tensor to compare, with data type belongs to float32, float16, int32.</span>
<span class="sd">            rtol (float, optional): Relative tolerance. Default: 1e-05.</span>
<span class="sd">            atol (float, optional): Absolute tolerance. Default: 1e-08.</span>
<span class="sd">            equal_nan (bool, optional): If True, then two NaNs will be considered equal. Default: False.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A bool Tensor, with the shape as broadcasted result of the input Tensor and `x2`.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If either of self Tensor and `x2` is not Tensor.</span>
<span class="sd">            TypeError: If either of self Tensor and `x2` is not float16, float32 or int32.</span>
<span class="sd">            TypeError: If either of `atol` and `rtol` is not float.</span>
<span class="sd">            TypeError: If `equal_nan` is not bool.</span>
<span class="sd">            TypeError: If the dtype of self Tensor is not same as the `x2`.</span>
<span class="sd">            ValueError: If self Tensor and `x2` can not be broadcast.</span>
<span class="sd">            ValueError: If either of `atol` and `rtol` is less than zero.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; input = Tensor(np.array([1.3, 2.1, 3.2, 4.1, 5.1]), mindspore.float16)</span>
<span class="sd">            &gt;&gt;&gt; other = Tensor(np.array([1.3, 3.3, 2.3, 3.1, 5.1]), mindspore.float16)</span>
<span class="sd">            &gt;&gt;&gt; output = input.isclose(other)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [ True False False False  True]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;isclose&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">rtol</span><span class="p">,</span> <span class="n">atol</span><span class="p">,</span> <span class="n">equal_nan</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.isfinite"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.isfinite.html#mindspore.Tensor.isfinite">[docs]</a>    <span class="k">def</span> <span class="nf">isfinite</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Determines which elements are finite for each position.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same shape of input, and the dtype is bool.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If self Tensor is not Tensor.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; a = Tensor(np.array([np.log(-1), 1, np.log(0)]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = a.isfinite()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [False True False]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;isfinite&#39;</span><span class="p">)()(</span><span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.inv"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.inv.html#mindspore.Tensor.inv">[docs]</a>    <span class="k">def</span> <span class="nf">inv</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes Reciprocal of this Tensor element-wise.</span>

<span class="sd">        .. math::</span>
<span class="sd">            out_i = \frac{1}{x_{i} }</span>

<span class="sd">        where `x` refers to self Tensor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same type and shape as self Tensor.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If dtype of this Tensor is not one of float16, float32, int32.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([0.25, 0.4, 0.31, 0.52]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = x.inv()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [4.        2.5       3.2258065 1.923077 ]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;inv&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.invert"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.invert.html#mindspore.Tensor.invert">[docs]</a>    <span class="k">def</span> <span class="nf">invert</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Flips all bits of this Tensor element-wise.</span>

<span class="sd">        .. math::</span>
<span class="sd">            out_i = \sim x_{i}</span>

<span class="sd">        where `x` refers to self Tensor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same shape as as self Tensor.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If dtype of this Tensor is neither int16 nor uint16.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([25, 4, 13, 9]), mindspore.int16)</span>
<span class="sd">            &gt;&gt;&gt; output = x.invert()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [-26 -5 -14 -10]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;invert&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.pow"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.pow.html#mindspore.Tensor.pow">[docs]</a>    <span class="k">def</span> <span class="nf">pow</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">power</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculate the power of Tensor.</span>

<span class="sd">        .. math::</span>

<span class="sd">            out_{i} = x_{i} ^{ y_{i}}</span>

<span class="sd">        Note:</span>
<span class="sd">            - The current Tensor and `power` comply with the implicit type conversion rules to make the data</span>
<span class="sd">              types consistent.</span>
<span class="sd">            - Dtypes of the current Tensor and power cannot be bool at the same time, and the shapes of them</span>
<span class="sd">              can be broadcast.</span>

<span class="sd">        Args:</span>
<span class="sd">            power (Union[Tensor, number.Number, bool]): The power value, should be a number.Number or bool value,</span>
<span class="sd">                or a Tensor whose data type is number or bool\_.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">            and the data type is the one with higher precision or higher digits among `Tensor` and `power`.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `power` is not one of the following: Tensor, number.Number or bool.</span>
<span class="sd">            ValueError: If the shape of the current Tensor and `power` are different.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; y = 3.0</span>
<span class="sd">            &gt;&gt;&gt; output = x.pow(y)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [ 1.  8. 64.]</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; y = Tensor(np.array([2.0, 4.0, 3.0]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = x.pow(y)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [ 1. 16. 64.]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;pow&#39;</span><span class="p">)()(</span><span class="bp">self</span><span class="p">,</span> <span class="n">power</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.log"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.log.html#mindspore.Tensor.log">[docs]</a>    <span class="k">def</span> <span class="nf">log</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the natural logarithm of self Tensor element-wise.</span>

<span class="sd">        .. math::</span>
<span class="sd">            y_i = log_e(x_i)</span>

<span class="sd">        .. note::</span>
<span class="sd">            The dimension of the self Tensor on Ascend should be less than or equal to 8, and the dimension of the</span>
<span class="sd">            self Tensor on the CPU should be less than 8.</span>

<span class="sd">        .. warning::</span>
<span class="sd">            If the input value of operator Log is within the range (0, 0.01] or [0.95, 1.05], the output accuracy may</span>
<span class="sd">            be inaccurate.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same shape and dtype as self.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If dtype of self Tensor is not float16, float32, float64, complex64 or complex128 on GPU and CPU.</span>
<span class="sd">            TypeError: If dtype of self Tensor is not float16 or float32 on Ascend.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = x.log()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [0.        0.6931472 1.3862944]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.mean"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.mean.html#mindspore.Tensor.mean">[docs]</a>    <span class="k">def</span> <span class="nf">mean</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(),</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reduces a dimension of a tensor by averaging all elements in the dimension, by default. And also can</span>
<span class="sd">        reduce a dimension of `x` along the axis. Determine whether the dimensions of the output and input are the</span>
<span class="sd">        same by controlling `keep_dims`.</span>

<span class="sd">        Args:</span>
<span class="sd">            axis (Union[None, int, tuple(int), list(int)]): Dimensions of reduction.</span>
<span class="sd">                When the axis is None or empty tuple, reduce all dimensions. When the axis is int, tuple(int) or</span>
<span class="sd">                list(int), if the dimension of Tensor is dim, the value range is [-dim, dim). Default: ().</span>
<span class="sd">            keep_dims (bool): Whether to keep the reduced dimensions. Default: False.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same data type as input tensor.</span>

<span class="sd">            - If `axis` is (), and `keep_dims` is False,</span>
<span class="sd">              the output is a 0-D tensor representing the product of all elements in the input tensor.</span>
<span class="sd">            - If `axis` is int, set as 1, and `keep_dims` is False,</span>
<span class="sd">              the shape of output is :math:`(x_0, x_2, ..., x_R)`.</span>
<span class="sd">            - If `axis` is tuple(int), set as (1, 2), and `keep_dims` is False,</span>
<span class="sd">              the shape of output is :math:`(x_0, x_3, ..., x_R)`.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `axis` is not one of the following: int, tuple or list.</span>
<span class="sd">            TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">            ValueError: If `axis` is out of range.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        See also:</span>
<span class="sd">            :func:`mindspore.Tensor.std`: Compute the standard deviation along the specified axis.</span>

<span class="sd">            :func:`mindspore.Tensor.var`: Compute the variance along the specified axis.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3], dtype=np.float32))</span>
<span class="sd">            &gt;&gt;&gt; output = input_x.mean()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            2.0</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">axis</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;mean&#39;</span><span class="p">)(</span><span class="n">keep_dims</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.amin"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.amin.html#mindspore.Tensor.amin">[docs]</a>    <span class="k">def</span> <span class="nf">amin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(),</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reduces a dimension of a tensor by the minimum value in the dimension, by default. And also can</span>
<span class="sd">        reduce a dimension of `x` along the axis. Determine whether the dimensions of the output and input are the</span>
<span class="sd">        same by controlling `keep_dims`.</span>

<span class="sd">        Args:</span>
<span class="sd">            axis (Union[None, int, tuple(int), list(int)]): Dimensions of reduction.</span>
<span class="sd">                When the axis is None or empty tuple, reduce all dimensions. When the axis is int, tuple(int) or</span>
<span class="sd">                list(int), if the dimension of Tensor is dim, the value range is [-dim, dim). Default: ().</span>
<span class="sd">            keep_dims (bool): Whether to keep the reduced dimensions. Default: False.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same data type as input tensor.</span>

<span class="sd">            - If `axis` is (), and `keep_dims` is False,</span>
<span class="sd">              the output is a 0-D tensor representing the product of all elements in the input tensor.</span>
<span class="sd">            - If `axis` is int, set as 1, and `keep_dims` is False,</span>
<span class="sd">              the shape of output is :math:`(x_0, x_2, ..., x_R)`.</span>
<span class="sd">            - If `axis` is tuple(int), set as (1, 2), and `keep_dims` is False,</span>
<span class="sd">              the shape of output is :math:`(x_0, x_3, ..., x_R)`.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `axis` is not one of the following: int, tuple or list.</span>
<span class="sd">            TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">            ValueError: If `axis` is out of range.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3], dtype=np.float32))</span>
<span class="sd">            &gt;&gt;&gt; output = input_x.amin()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            1.0</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;amin&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.amax"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.amax.html#mindspore.Tensor.amax">[docs]</a>    <span class="k">def</span> <span class="nf">amax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(),</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reduces a dimension of a tensor by the maximum value in the dimension, by default. And also can</span>
<span class="sd">        reduce a dimension of `x` along the axis. Determine whether the dimensions of the output and input are the</span>
<span class="sd">        same by controlling `keep_dims`.</span>

<span class="sd">        Args:</span>
<span class="sd">            axis (Union[None, int, tuple(int), list(int)]): Dimensions of reduction.</span>
<span class="sd">                When the axis is None or empty tuple, reduce all dimensions. When the axis is int, tuple(int) or</span>
<span class="sd">                list(int), if the dimension of Tensor is dim, the value range is [-dim, dim). Default: ().</span>
<span class="sd">            keep_dims (bool): Whether to keep the reduced dimensions. Default: False.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same data type as input tensor.</span>

<span class="sd">            - If `axis` is (), and `keep_dims` is False,</span>
<span class="sd">              the output is a 0-D tensor representing the product of all elements in the input tensor.</span>
<span class="sd">            - If `axis` is int, set as 1, and `keep_dims` is False,</span>
<span class="sd">              the shape of output is :math:`(x_0, x_2, ..., x_R)`.</span>
<span class="sd">            - If `axis` is tuple(int), set as (1, 2), and `keep_dims` is False,</span>
<span class="sd">              the shape of output is :math:`(x_0, x_3, ..., x_R)`.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `axis` is not one of the following: int, tuple or list.</span>
<span class="sd">            TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">            ValueError: If `axis` is out of range.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3], dtype=np.float32))</span>
<span class="sd">            &gt;&gt;&gt; output = input_x.amax()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            3.0</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;amax&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.prod"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.prod.html#mindspore.Tensor.prod">[docs]</a>    <span class="k">def</span> <span class="nf">prod</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(),</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reduces a dimension of a tensor by product all elements in the dimension, by default. And also can</span>
<span class="sd">        reduce a dimension of `x` along the axis. Determine whether the dimensions of the output and input are the</span>
<span class="sd">        same by controlling `keep_dims`.</span>

<span class="sd">        Args:</span>
<span class="sd">            axis (Union[None, int, tuple(int), list(int)]): Dimensions of reduction.</span>
<span class="sd">                When the axis is None or empty tuple, reduce all dimensions. When the axis is int, tuple(int) or</span>
<span class="sd">                list(int), if the dimension of Tensor is dim, the value range is [-dim, dim). Default: ().</span>
<span class="sd">            keep_dims (bool): Whether to keep the reduced dimensions. Default: False.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same data type as input tensor.</span>

<span class="sd">            - If `axis` is (), and `keep_dims` is False,</span>
<span class="sd">              the output is a 0-D tensor representing the product of all elements in the input tensor.</span>
<span class="sd">            - If `axis` is int, set as 1, and `keep_dims` is False,</span>
<span class="sd">              the shape of output is :math:`(x_0, x_2, ..., x_R)`.</span>
<span class="sd">            - If `axis` is tuple(int), set as (1, 2), and `keep_dims` is False,</span>
<span class="sd">              the shape of output is :math:`(x_0, x_3, ..., x_R)`.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `axis` is not one of the following: int, tuple or list.</span>
<span class="sd">            TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">            ValueError: If `axis` is out of range.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3], dtype=np.float32))</span>
<span class="sd">            &gt;&gt;&gt; output = input_x.prod()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            6.0</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;prod&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.select"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.select.html#mindspore.Tensor.select">[docs]</a>    <span class="k">def</span> <span class="nf">select</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">condition</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The conditional tensor determines whether the corresponding element in the output must be</span>
<span class="sd">        selected from the current Tensor (if true) or :math:`y` (if false) based on the value of each element.</span>

<span class="sd">        It can be defined as:</span>

<span class="sd">        .. math::</span>
<span class="sd">            out_i = \begin{cases}</span>
<span class="sd">            tensor_i, &amp; \text{if } condition_i \\</span>
<span class="sd">            y_i, &amp; \text{otherwise}</span>
<span class="sd">            \end{cases}</span>

<span class="sd">        Args:</span>
<span class="sd">            condition (Tensor[bool]): The condition tensor, decides which element is chosen.</span>
<span class="sd">              The shape is the same as the current Tensor.</span>
<span class="sd">            y (Union[Tensor, int, float]): If y is Tensor, the shape is the same as the current Tensor.</span>
<span class="sd">              If y is an int or a float, it will be cast to the type of int32 or float32, and broadcast to the same</span>
<span class="sd">              shape as the Tensor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same shape as the current Tensor.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `y` is not a Tensor, an int or a float.</span>
<span class="sd">            ValueError: The shapes of inputs are different.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; # 1) y is Tensor</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; cond = Tensor([True, False])</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor([2,3], mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; y = Tensor([1,2], mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = x.select(cond, y)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [2. 2.]</span>
<span class="sd">            &gt;&gt;&gt; # 2) y is a float</span>
<span class="sd">            &gt;&gt;&gt; cond = Tensor([True, False])</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor([2,3], mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; y = 2.0</span>
<span class="sd">            &gt;&gt;&gt; output = x.select(cond, y)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [2. 2.]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;Tensor.select&#39;, the argument &#39;condition&#39; should be Tensor,&quot;</span>
                            <span class="sa">f</span><span class="s2">&quot; but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">condition</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;Tensor.select&#39;, the argument &#39;y&#39; should be Tensor, int or float,&quot;</span>
                            <span class="sa">f</span><span class="s2">&quot; but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;Tensor.select&#39;, if the argument &#39;y&#39; is int,&quot;</span>
                            <span class="sa">f</span><span class="s2">&quot; then the tensor type should be int32 but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="nb">float</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;Tensor.select&#39;, if the argument &#39;y&#39; is float,&quot;</span>
                            <span class="sa">f</span><span class="s2">&quot; then the tensor type should be float32 but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">input_y</span> <span class="o">=</span> <span class="n">y</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)):</span>
            <span class="n">input_y</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;zeros_like&#39;</span><span class="p">)()(</span><span class="bp">self</span><span class="p">)</span> <span class="o">+</span> <span class="n">y</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="n">input_y</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;cast&#39;</span><span class="p">)(</span><span class="n">input_y</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">input_y</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;cast&#39;</span><span class="p">)(</span><span class="n">input_y</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;select&#39;</span><span class="p">)(</span><span class="n">condition</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.transpose"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.transpose.html#mindspore.Tensor.transpose">[docs]</a>    <span class="k">def</span> <span class="nf">transpose</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">axes</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return a tensor with axes transposed.</span>

<span class="sd">        - For a 1-D tensor, this has no effect, as a transposed vector is simply the same vector.</span>
<span class="sd">        - For a 2-D tensor, this is a standard matrix transpose.</span>
<span class="sd">        - For an n-D tensor, if axes are given, their order indicates how the axes are permuted.</span>

<span class="sd">        If axes are not provided and ``tensor.shape = (i[0], i[1],...i[n-2], i[n-1])``,</span>
<span class="sd">        then ``tensor.transpose().shape = (i[n-1], i[n-2], ... i[1], i[0])``.</span>

<span class="sd">        Args:</span>
<span class="sd">            axes(Union[None, tuple(int), list(int), int], optional): If axes is None or</span>
<span class="sd">                blank, the method will reverse the order of the axes. If axes is tuple(int)</span>
<span class="sd">                or list(int), tensor.transpose() will transpose the tensor to the new axes order.</span>
<span class="sd">                If axes is int, this form is simply intended as a convenience alternative to the</span>
<span class="sd">                tuple/list form.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same dimension as input tensor, with axes suitably permuted.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If input arguments have types not specified above.</span>
<span class="sd">            ValueError: If the number of `axes` is not equal to Tensor&#39;s ndim.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.ones((1,2,3), dtype=np.float32))</span>
<span class="sd">            &gt;&gt;&gt; x = x.transpose()</span>
<span class="sd">            &gt;&gt;&gt; print(x.shape)</span>
<span class="sd">            (3, 2, 1)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="n">perm</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_transpose_axis</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;transpose&#39;</span><span class="p">)()(</span><span class="bp">self</span><span class="p">,</span> <span class="n">perm</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.col2im"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.col2im.html#mindspore.Tensor.col2im">[docs]</a>    <span class="k">def</span> <span class="nf">col2im</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">padding_value</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Combines an array of sliding local blocks into a large containing tensor.</span>

<span class="sd">        Args:</span>
<span class="sd">            output_size (Tensor): 1D tensor with 2 elements of data type int.</span>
<span class="sd">            kernel_size (Union[int, tuple[int], list[int]]): The size of the kernel, should be two int</span>
<span class="sd">                for height and width. If type is int, it means that height equal with width. Must be specified.</span>
<span class="sd">            dilation (Union[int, tuple[int], list[int]]): The size of the dilation, should be two int</span>
<span class="sd">                for height and width. If type is int, it means that height equal with width. Default: 1.</span>
<span class="sd">            padding_value (Union[int, tuple[int], list[int]]): The size of the padding, should be two int</span>
<span class="sd">                for height and width. If type is int, it means that height equal with width. Default: 1.</span>
<span class="sd">            stride (Union[int, tuple[int], list[int]]): The size of the stride, should be two int</span>
<span class="sd">                for height and width. If type is int, it means that height equal with width. Default: 0.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A 4D Tensor, with same type as input &#39;x&#39;.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If :attr:`kernel_size`, `dilation`, `padding_value`, `stride` data type is not in</span>
<span class="sd">                Union[int, tuple[int], list[int]].</span>
<span class="sd">            ValueError: If :attr:`kernel_size`, `dilation`, `stride` value is less than zero or elements</span>
<span class="sd">                number more than 2.</span>
<span class="sd">            ValueError: If :attr:`padding_value` value is not greater than zero or elements number more than 2.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``GPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(input_data=np.random.rand(16, 16, 4, 25), dtype=mstype.float32)</span>
<span class="sd">            &gt;&gt;&gt; output_size = Tensor(input_data=[8, 8], dtype=mstype.int32)</span>
<span class="sd">            &gt;&gt;&gt; y = x.col2im(output_size, kernel_size=[2, 2], dilation=[2, 2], padding_value=[2, 2], stride=[2, 2])</span>
<span class="sd">            &gt;&gt;&gt; print(y.shape)</span>
<span class="sd">            (16, 16, 8, 8)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;col2im&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">padding_value</span><span class="p">,</span> <span class="n">stride</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.reshape"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.reshape.html#mindspore.Tensor.reshape">[docs]</a>    <span class="k">def</span> <span class="nf">reshape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">shape</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Give a new shape to a tensor without changing its data.</span>

<span class="sd">        Args:</span>
<span class="sd">            shape(Union[int, tuple(int), list(int)]): The new shape should be compatible</span>
<span class="sd">                with the original shape. If an integer, then the result will be a 1-D</span>
<span class="sd">                tensor of that length. One shape dimension can be -1. In this case, the</span>
<span class="sd">                value is inferred from the length of the tensor and remaining dimensions.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, with new specified shape.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If new shape is not integer, list or tuple.</span>
<span class="sd">            ValueError: If new shape is not compatible with the original shape.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]], dtype=mstype.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = x.reshape((3, 2))</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [[-0.1  0.3]</span>
<span class="sd">            [ 3.6  0.4]</span>
<span class="sd">            [ 0.5 -3.2]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="n">new_shape</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_reshape_shp</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;reshape&#39;</span><span class="p">)()(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_shape</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.ravel"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.ravel.html#mindspore.Tensor.ravel">[docs]</a>    <span class="k">def</span> <span class="nf">ravel</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return a contiguous flattened tensor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, a 1-D tensor, containing the same elements of the input.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        See also:</span>
<span class="sd">            :func:`mindspore.Tensor.reshape`: Give a new shape to a tensor without changing its data.</span>

<span class="sd">            :func:`mindspore.Tensor.flatten`: Return a copy of the tensor collapsed into one dimension.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.ones((2,3,4), dtype=np.float32))</span>
<span class="sd">            &gt;&gt;&gt; output = x.ravel()</span>
<span class="sd">            &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">            (24,)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="n">reshape_op</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;reshape&#39;</span><span class="p">)()</span>
        <span class="k">return</span> <span class="n">reshape_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,))</span></div>

<div class="viewcode-block" id="Tensor.round"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.round.html#mindspore.Tensor.round">[docs]</a>    <span class="k">def</span> <span class="nf">round</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns half to even of the tensor element-wise.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same shape and type as the Tensor.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([0.8, 1.5, 2.3, 2.5, -4.5]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = x.round()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [ 1.  2.  2.  2. -4.]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;round&#39;</span><span class="p">)()(</span><span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.flatten"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.flatten.html#mindspore.Tensor.flatten">[docs]</a>    <span class="k">def</span> <span class="nf">flatten</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s1">&#39;C&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return a copy of the tensor collapsed into one dimension.</span>

<span class="sd">        Args:</span>
<span class="sd">            order (str, optional): Can choose between &#39;C&#39; and &#39;F&#39;. &#39;C&#39; means to</span>
<span class="sd">                flatten in row-major (C-style) order. &#39;F&#39; means to flatten in column-major</span>
<span class="sd">                (Fortran-style) order. Default: &#39;C&#39;.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same data type as input.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `order` is not string type.</span>
<span class="sd">            ValueError: If `order` is string type, but not &#39;C&#39; or &#39;F&#39;.</span>

<span class="sd">        See also:</span>
<span class="sd">            :func:`mindspore.Tensor.reshape`: Give a new shape to a tensor without changing its data.</span>

<span class="sd">            :func:`mindspore.Tensor.ravel`: Return a contiguous flattened tensor.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.ones((2,3,4), dtype=np.float32))</span>
<span class="sd">            &gt;&gt;&gt; output = x.flatten()</span>
<span class="sd">            &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">            (24,)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="n">reshape_op</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;reshape&#39;</span><span class="p">)()</span>
        <span class="n">trans_op</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;transpose&#39;</span><span class="p">)()</span>

        <span class="n">order</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_flatten_order</span><span class="p">(</span><span class="n">order</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">order</span> <span class="o">==</span> <span class="s1">&#39;C&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">reshape_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,))</span>

        <span class="n">perm</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">reshape_op</span><span class="p">(</span><span class="n">trans_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">perm</span><span class="p">),</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,))</span></div>

<div class="viewcode-block" id="Tensor.narrow"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.narrow.html#mindspore.Tensor.narrow">[docs]</a>    <span class="k">def</span> <span class="nf">narrow</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">length</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a narrowed tensor from input tensor.</span>
<span class="sd">        The dimension axis is input from start to start + length.</span>

<span class="sd">        Args:</span>
<span class="sd">            axis (int): the axis along which to narrow.</span>
<span class="sd">            start (int): the starting dimension.</span>
<span class="sd">            length (int): the distance to the ending dimension.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor.</span>

<span class="sd">            - output (Tensors) - The narrowed tensor.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: axis is not integer.</span>
<span class="sd">            TypeError: start is not integer.</span>
<span class="sd">            TypeError: length is not integer.</span>
<span class="sd">            ValueError: axis is not in the range of [0, ndim-1].</span>
<span class="sd">            ValueError: start is not in the range of [0, shape[axis]-1].</span>
<span class="sd">            ValueError: start+length is greater than shape[axis]-1.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import mindspore</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], mindspore.int32)</span>
<span class="sd">            &gt;&gt;&gt; output = x.narrow(0, 0, 2)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [[ 1 2 3]</span>
<span class="sd">             [ 4 5 6]]</span>
<span class="sd">            &gt;&gt;&gt; output = x.narrow(1, 1, 2)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [[ 2 3]</span>
<span class="sd">             [ 5 6]</span>
<span class="sd">             [ 8 9]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;narrow&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">length</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.swapaxes"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.swapaxes.html#mindspore.Tensor.swapaxes">[docs]</a>    <span class="k">def</span> <span class="nf">swapaxes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis1</span><span class="p">,</span> <span class="n">axis2</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Interchange two axes of a tensor.</span>

<span class="sd">        Args:</span>
<span class="sd">            axis1 (int): First axis.</span>
<span class="sd">            axis2 (int): Second axis.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Transposed tensor, has the same data type as the input.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `axis1` or `axis2` is not integer.</span>
<span class="sd">            ValueError: If `axis1` or `axis2` is not in the range of :math:`[-ndim, ndim-1]`.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.ones((2,3,4), dtype=np.float32))</span>
<span class="sd">            &gt;&gt;&gt; output = x.swapaxes(0, 2)</span>
<span class="sd">            &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">            (4,3,2)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="n">axis1</span><span class="p">,</span> <span class="n">axis2</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_swapaxes_axis</span><span class="p">((</span><span class="n">axis1</span><span class="p">,</span> <span class="n">axis2</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">axis1</span> <span class="o">==</span> <span class="n">axis2</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span>
        <span class="k">if</span> <span class="n">axis1</span> <span class="o">&gt;</span> <span class="n">axis2</span><span class="p">:</span>
            <span class="n">axis1</span><span class="p">,</span> <span class="n">axis2</span> <span class="o">=</span> <span class="n">axis2</span><span class="p">,</span> <span class="n">axis1</span>

        <span class="n">perm</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">axis2</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">:</span>
            <span class="n">new_perm</span> <span class="o">=</span> <span class="n">perm</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">axis1</span><span class="p">]</span> <span class="o">+</span> <span class="n">perm</span><span class="p">[</span><span class="n">axis2</span><span class="p">:</span><span class="n">axis2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> \
                       <span class="n">perm</span><span class="p">[</span><span class="n">axis1</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span><span class="n">axis2</span><span class="p">]</span> <span class="o">+</span> <span class="n">perm</span><span class="p">[</span><span class="n">axis1</span><span class="p">:</span><span class="n">axis1</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">perm</span><span class="p">[</span><span class="n">axis2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">new_perm</span> <span class="o">=</span> <span class="n">perm</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">axis1</span><span class="p">]</span> <span class="o">+</span> <span class="n">perm</span><span class="p">[</span><span class="n">axis2</span><span class="p">:</span><span class="n">axis2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> \
                       <span class="n">perm</span><span class="p">[</span><span class="n">axis1</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span><span class="n">axis2</span><span class="p">]</span> <span class="o">+</span> <span class="n">perm</span><span class="p">[</span><span class="n">axis1</span><span class="p">:</span><span class="n">axis1</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;transpose&#39;</span><span class="p">)()(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_perm</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.squeeze"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.squeeze.html#mindspore.Tensor.squeeze">[docs]</a>    <span class="k">def</span> <span class="nf">squeeze</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Remove the dimension of shape 1 from the Tensor</span>

<span class="sd">        Args:</span>
<span class="sd">            axis (Union[None, int, list(int), tuple(int)], optional): Selects a subset of the entries of</span>
<span class="sd">                length one in the shape. If an axis is selected with shape entry greater than one,</span>
<span class="sd">                an error is raised. Default is None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, with all or a subset of the dimensions of length 1 removed.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If input arguments have types not specified above.</span>
<span class="sd">            ValueError: If axis is greater than one.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        See also:</span>
<span class="sd">            :func:`mindspore.Tensor.expand_as`: Expand the dimension of target tensor to the dimension of input tensor.</span>

<span class="sd">            :func:`mindspore.Tensor.reshape`: Give a new shape to a tensor without changing its data.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.ones((1,2,2), dtype=np.float32))</span>
<span class="sd">            &gt;&gt;&gt; print(x)</span>
<span class="sd">            [[[1. 1.]</span>
<span class="sd">            [1. 1.]]]</span>
<span class="sd">            &gt;&gt;&gt; print(x.shape)</span>
<span class="sd">            (1, 2, 2)</span>
<span class="sd">            &gt;&gt;&gt; y = x.squeeze()</span>
<span class="sd">            &gt;&gt;&gt; print(y)</span>
<span class="sd">            [[1. 1.]</span>
<span class="sd">            [1. 1.]]</span>
<span class="sd">            &gt;&gt;&gt; print(y.shape)</span>
<span class="sd">            (2, 2)</span>
<span class="sd">            &gt;&gt;&gt; y = x.squeeze(axis=0)</span>
<span class="sd">            &gt;&gt;&gt; print(y)</span>
<span class="sd">            [[1. 1.]</span>
<span class="sd">            [1. 1.]]</span>
<span class="sd">            &gt;&gt;&gt; print(y.shape)</span>
<span class="sd">            (2, 2)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;squeeze&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">new_shape</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">prepare_shape_for_squeeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;reshape&#39;</span><span class="p">)()(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_shape</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.expand_dims"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.expand_dims.html#mindspore.Tensor.expand_dims">[docs]</a>    <span class="k">def</span> <span class="nf">expand_dims</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Insert a dimension of shape 1 at the specified axis of Tensor</span>

<span class="sd">        Args:</span>
<span class="sd">            axis (int): the axis at which to insert the singleton dimension.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, with inserted dimension of length 1.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If axis is not an int.</span>
<span class="sd">            ValueError: If axis is not in range [-self.ndim - 1, self.ndim + 1).</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.ones((2,2), dtype=np.float32))</span>
<span class="sd">            &gt;&gt;&gt; print(x)</span>
<span class="sd">            [[1. 1.]</span>
<span class="sd">            [1. 1.]]</span>
<span class="sd">            &gt;&gt;&gt; print(x.shape)</span>
<span class="sd">            (2, 2)</span>
<span class="sd">            &gt;&gt;&gt; y = x.expand_dims(axis=0)</span>
<span class="sd">            &gt;&gt;&gt; print(y)</span>
<span class="sd">            [[[1. 1.]</span>
<span class="sd">            [1. 1.]]]</span>
<span class="sd">            &gt;&gt;&gt; print(y.shape)</span>
<span class="sd">            (1, 2, 2)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_is_int</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="s1">&#39;axis&#39;</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span> <span class="s1">&#39;axis&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;expand_dims&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.astype"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.astype.html#mindspore.Tensor.astype">[docs]</a>    <span class="k">def</span> <span class="nf">astype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return a copy of the tensor, cast to a specified type.</span>

<span class="sd">        Args:</span>
<span class="sd">            dtype (Union[:class:`mindspore.dtype`, numpy.dtype, str]): Designated tensor dtype, can be in</span>
<span class="sd">                format of :class:`mindspore.dtype.float32` or :class:`numpy.float32` or `float32`.</span>
<span class="sd">            copy (bool, optional): By default, astype always returns a newly allocated</span>
<span class="sd">                tensor. If this is set to false, the input tensor is returned instead</span>
<span class="sd">                of a copy. Default: True.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, with the designated dtype.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If the specified dtype cannot be understood.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.ones((1,2,2,1), dtype=np.float32))</span>
<span class="sd">            &gt;&gt;&gt; x = x.astype(&quot;int32&quot;)</span>
<span class="sd">            &gt;&gt;&gt; print(x.dtype)</span>
<span class="sd">            Int32</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">_check_astype_and_convert</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">copy</span> <span class="ow">and</span> <span class="n">dtype</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;cast&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.argmax"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.argmax.html#mindspore.Tensor.argmax">[docs]</a>    <span class="k">def</span> <span class="nf">argmax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the indices of the maximum values along an axis.</span>

<span class="sd">        Args:</span>
<span class="sd">            axis (int, optional): By default, the index is into</span>
<span class="sd">                the flattened tensor, otherwise along the specified axis. Default: None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, indices into the input tensor. It has the same</span>
<span class="sd">            shape as self.shape with the dimension along axis removed.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the axis is out of range.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        See also:</span>
<span class="sd">            :func:`mindspore.Tensor.argmin`: Return the indices of the minimum values along an axis.</span>

<span class="sd">            :func:`mindspore.Tensor.min`: Return the minimum of a tensor or minimum along an axis.</span>

<span class="sd">            :func:`mindspore.Tensor.max`: Return the maximum of a tensor or maximum along an axis.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; a = Tensor(np.arange(10, 16).reshape(2, 3).astype(&quot;float32&quot;))</span>
<span class="sd">            &gt;&gt;&gt; print(a.argmax())</span>
<span class="sd">            5</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># P.Argmax only supports float</span>
        <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
            <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">axis</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_axis_in_range</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;argmax&#39;</span><span class="p">)(</span><span class="n">axis</span><span class="p">)(</span><span class="n">a</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.argmin"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.argmin.html#mindspore.Tensor.argmin">[docs]</a>    <span class="k">def</span> <span class="nf">argmin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the indices of the minimum values along an axis.</span>

<span class="sd">        Args:</span>
<span class="sd">            axis (int, optional): By default, the index is into</span>
<span class="sd">                the flattened tensor, otherwise along the specified axis. Default: None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, indices into the input tensor. It has the same</span>
<span class="sd">            shape as self.shape with the dimension along axis removed.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the axis is out of range.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        See also:</span>
<span class="sd">            :func:`mindspore.Tensor.argmax`: Return the indices of the maximum values along an axis.</span>

<span class="sd">            :func:`mindspore.Tensor.min`: Return the minimum of a tensor or minimum along an axis.</span>

<span class="sd">            :func:`mindspore.Tensor.max`: Return the maximum of a tensor or maximum along an axis.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; a = Tensor(np.arange(10, 16).reshape(2, 3).astype(&quot;float32&quot;))</span>
<span class="sd">            &gt;&gt;&gt; print(a.argmin())</span>
<span class="sd">            0</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># P.Argmin only supports float</span>
        <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
            <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">axis</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_axis_in_range</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
        <span class="c1"># P.Argmin is currently not supported</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;argmax&#39;</span><span class="p">)(</span><span class="n">axis</span><span class="p">)(</span><span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;__neg__&#39;</span><span class="p">)(</span><span class="n">a</span><span class="p">))</span></div>

<div class="viewcode-block" id="Tensor.argmax_with_value"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.argmax_with_value.html#mindspore.Tensor.argmax_with_value">[docs]</a>    <span class="k">def</span> <span class="nf">argmax_with_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the maximum value with corresponding index.</span>

<span class="sd">        Note:</span>
<span class="sd">            In auto_parallel and semi_auto_parallel mode, the first output index can not be used.</span>

<span class="sd">        .. warning::</span>
<span class="sd">            - If there are multiple maximum values, the index of the first maximum value is used.</span>
<span class="sd">            - The value range of `axis` is [-dims, dims - 1]. `dims` is the dimension length of this tensor.</span>

<span class="sd">        Args:</span>
<span class="sd">            axis (int): The dimension to reduce. Default: 0.</span>
<span class="sd">            keep_dims (bool): Whether to reduce dimension, if true the output will keep the same dimension as the input,</span>
<span class="sd">                            the output will reduce dimension if false. Default: False.</span>

<span class="sd">        Returns:</span>
<span class="sd">            tuple (Tensor), tuple of 2 tensors, containing the corresponding index and the maximum value of the input</span>
<span class="sd">            tensor.</span>

<span class="sd">            - **index** (Tensor) - The index for the maximum value of the input tensor.</span>
<span class="sd">              If `keep_dims` is true, the shape of</span>
<span class="sd">              output tensors is :math:`(x_1, x_2, ..., x_{axis-1}, 1, x_{axis+1}, ..., x_N)`. Otherwise, the shape is</span>
<span class="sd">              :math:`(x_1, x_2, ..., x_{axis-1}, x_{axis+1}, ..., x_N)` .</span>
<span class="sd">            - **value** (Tensor) - The maximum value of input tensor, with the same shape as index.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">            TypeError: If `axis` is not an int.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([0.0, 0.4, 0.6, 0.7, 0.1]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; index, output = x.argmax_with_value()</span>
<span class="sd">            &gt;&gt;&gt; print(index, output)</span>
<span class="sd">            3 0.7</span>
<span class="sd">            &gt;&gt;&gt; index, output = x.argmax_with_value(keep_dims=True)</span>
<span class="sd">            &gt;&gt;&gt; print(index, output)</span>
<span class="sd">            [3] [0.7]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;argmax_with_value&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.argmin_with_value"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.argmin_with_value.html#mindspore.Tensor.argmin_with_value">[docs]</a>    <span class="k">def</span> <span class="nf">argmin_with_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the minimum value with corresponding index.</span>

<span class="sd">        Note:</span>
<span class="sd">            In auto_parallel and semi_auto_parallel mode, the first output index can not be used.</span>

<span class="sd">        .. warning::</span>
<span class="sd">            - If there are multiple minimum values, the index of the first minimum value is used.</span>
<span class="sd">            - The value range of `axis` is [-dims, dims - 1]. `dims` is the dimension length of this tensor.</span>

<span class="sd">        Args:</span>
<span class="sd">            axis (int): The dimension to reduce. Default: 0.</span>
<span class="sd">            keep_dims (bool): Whether to reduce dimension, if true the output will keep the same dimension as the input,</span>
<span class="sd">                            the output will reduce dimension if false. Default: False.</span>

<span class="sd">        Returns:</span>
<span class="sd">            tuple (Tensor), tuple of 2 tensors, containing the corresponding index and the minimum value of the input</span>
<span class="sd">            tensor.</span>

<span class="sd">            - **index** (Tensor) - The index for the minimum value of the input tensor.</span>
<span class="sd">              If `keep_dims` is true, the shape of</span>
<span class="sd">              output tensors is :math:`(x_1, x_2, ..., x_{axis-1}, 1, x_{axis+1}, ..., x_N)`. Otherwise, the shape is</span>
<span class="sd">              :math:`(x_1, x_2, ..., x_{axis-1}, x_{axis+1}, ..., x_N)` .</span>
<span class="sd">            - **value** (Tensor) - The minimum value of input tensor, with the same shape as index.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">            TypeError: If `axis` is not an int.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([0.0, 0.4, 0.6, 0.7, 0.1]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; index, output = x.argmin_with_value()</span>
<span class="sd">            &gt;&gt;&gt; print(index, output)</span>
<span class="sd">            0 0.0</span>
<span class="sd">            &gt;&gt;&gt; index, output = x.argmin_with_value(keep_dims=True)</span>
<span class="sd">            &gt;&gt;&gt; print(index, output)</span>
<span class="sd">            [0] [0.0]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;argmin_with_value&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.cumsum"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.cumsum.html#mindspore.Tensor.cumsum">[docs]</a>    <span class="k">def</span> <span class="nf">cumsum</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the cumulative sum of the elements along a given axis.</span>

<span class="sd">        Note:</span>
<span class="sd">            If ``self.dtype`` is :class:`int8`, :class:`int16` or :class:`bool`, the result</span>
<span class="sd">            `dtype` will be elevated to :class:`int32`, :class:`int64` is not supported.</span>

<span class="sd">        Args:</span>
<span class="sd">            axis (int, optional): Axis along which the cumulative sum is computed. The</span>
<span class="sd">                default (None) is to compute the cumsum over the flattened array.</span>
<span class="sd">            dtype (:class:`mindspore.dtype`, optional): If not specified, stay the same as original</span>
<span class="sd">                tensor, unless it has an integer dtype with a precision less than :class:`float32`.</span>
<span class="sd">                In that case, :class:`float32` is used. Default: None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor.</span>

<span class="sd">        See also:</span>
<span class="sd">            :func:`mindspore.Tensor.sum`: Return sum of tensor elements over a given axis.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the axis is out of range.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; a = Tensor(np.ones((3,3)).astype(&quot;float32&quot;))</span>
<span class="sd">            &gt;&gt;&gt; output = a.cumsum(axis=0)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [[1. 1. 1.]</span>
<span class="sd">            [2. 2. 2.]</span>
<span class="sd">            [3. 3. 3.]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span>
        <span class="n">original_dtype</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span>
        <span class="c1"># If original tensor is int, and has precision less then int32, convert to int32</span>
        <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int16</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
            <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_axis_in_range</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">original_dtype</span> <span class="o">!=</span> <span class="n">dtype</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;cumsum&#39;</span><span class="p">)()(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;cumsum&#39;</span><span class="p">)()(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.cummin"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.cummin.html#mindspore.Tensor.cummin">[docs]</a>    <span class="k">def</span> <span class="nf">cummin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a tuple (values,indices) where &#39;values&#39; is the cumulative minimum value of self Tensor</span>
<span class="sd">        along the dimension `axis`, and `indices` is the index location of each minimum value.</span>

<span class="sd">        .. math::</span>
<span class="sd">            \begin{array}{ll} \\</span>
<span class="sd">                y{i} = min(x{1}, x{2}, ... , x{i})</span>
<span class="sd">            \end{array}</span>

<span class="sd">        Args:</span>
<span class="sd">            axis (int): The dimension to do the operation over. The value of `axis` must be in the range</span>
<span class="sd">                `[-x.ndim, x.ndim - 1]`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            tuple [Tensor], tuple of 2 Tensors, containing the cumulative minimum of elements and the index,</span>
<span class="sd">            The shape of each output tensor is the same as self Tensor.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `axis` is not an int.</span>
<span class="sd">            ValueError: If `axis` is out the range of `[-x.ndim, x.ndim - 1]`.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">            &gt;&gt;&gt; import mindspore</span>
<span class="sd">            &gt;&gt;&gt; a = Tensor([-0.2284, -0.6628,  0.0975,  0.2680, -1.3298, -0.4220], mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = a.cummin(axis=0)</span>
<span class="sd">            &gt;&gt;&gt; print(output[0])</span>
<span class="sd">            [-0.2284 -0.6628 -0.6628 -0.6628 -1.3298 -1.3298]</span>
<span class="sd">            &gt;&gt;&gt; print(output[1])</span>
<span class="sd">            [0 1 1 1 4 4]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;cummin&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.cummax"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.cummax.html#mindspore.Tensor.cummax">[docs]</a>    <span class="k">def</span> <span class="nf">cummax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a tuple (values,indices) where &#39;values&#39; is the cumulative maximum value of self Tensor</span>
<span class="sd">        along the dimension `axis`, and `indices` is the index location of each maximum value.</span>

<span class="sd">        .. math::</span>
<span class="sd">            \begin{array}{ll} \\</span>
<span class="sd">                y{i} = max(x{1}, x{2}, ... , x{i})</span>
<span class="sd">            \end{array}</span>

<span class="sd">        Args:</span>
<span class="sd">            axis (int): The dimension to do the operation over. The value of `axis` must be in the range</span>
<span class="sd">                `[-x.ndim, x.ndim - 1]`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            tuple [Tensor], tuple of 2 Tensors, containing the cumulative maximum of elements and the index,</span>
<span class="sd">            The shape of each output tensor is the same as self Tensor.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `axis` is not an int.</span>
<span class="sd">            ValueError: If `axis` is out the range of `[-x.ndim, x.ndim - 1]`.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import mindspore</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([[3, 4, 6, 10], [1, 6, 7, 9], [4, 3, 8, 7], [1, 3, 7, 9]]).astype(np.float32))</span>
<span class="sd">            &gt;&gt;&gt; output = x.cummax(axis=0)</span>
<span class="sd">            &gt;&gt;&gt; print(output[0])</span>
<span class="sd">            [[ 3.  4.  6. 10.]</span>
<span class="sd">             [ 3.  6.  7. 10.]</span>
<span class="sd">             [ 4.  6.  8. 10.]</span>
<span class="sd">             [ 4.  6.  8. 10.]]</span>
<span class="sd">            &gt;&gt;&gt; print(output[1])</span>
<span class="sd">            [[0 0 0 0]</span>
<span class="sd">             [0 1 1 0]</span>
<span class="sd">             [2 1 2 0]</span>
<span class="sd">             [2 1 2 0]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;cummax&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.index_fill"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.index_fill.html#mindspore.Tensor.index_fill">[docs]</a>    <span class="k">def</span> <span class="nf">index_fill</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fills the elements under the `dim` dimension of the self Tensor with the input `value`</span>
<span class="sd">        by selecting the indices in the order given in `index`.</span>

<span class="sd">        Args:</span>
<span class="sd">            dim (Union[int, Tensor]): Dimension along which to fill the input Tensor. Only supports</span>
<span class="sd">                an int number or a 0-dimensional Tensor, whose data type is int32 or int64.</span>
<span class="sd">            index (Tensor): Indices of the input Tensor to fill in. The dtype must be int32.</span>
<span class="sd">            value (Union[bool, int, float, Tensor]): Value to fill the returned Tensor. If `value` is</span>
<span class="sd">                a Tensor, it must be a 0-dimensional Tensor and has the same dtype as self Tensor. Otherwise,</span>
<span class="sd">                the `value` will be cast to a 0-dimensional Tensor with the same data type as self Tensor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same dtype and shape as self Tensor.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `dim` is neither int number nor Tensor.</span>
<span class="sd">            TypeError: When `dim` is a Tensor and its dtype is not int32 or int64.</span>
<span class="sd">            TypeError: If `index` is not a Tensor.</span>
<span class="sd">            TypeError: If dtype of `index` is not int32.</span>
<span class="sd">            TypeError: If `value` is not a bool, int, float, or Tensor.</span>
<span class="sd">            TypeError: If dtype of self Tensor and `value` are not the same.</span>
<span class="sd">            ValueError: When `dim` is a Tensor and its rank is not equal to 0.</span>
<span class="sd">            ValueError: If the rank of `index` is greater than 1D.</span>
<span class="sd">            ValueError: When `value` is a Tensor and its rank is not equal to 0.</span>
<span class="sd">            RuntimeError: If the value of `dim` is out the range of `[-self.ndim, self.ndim - 1]`.</span>
<span class="sd">            RuntimeError: If the values of `index` are out the range of `[-self.shape[dim], self.shape[dim] - 1]`.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``GPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import mindspore</span>
<span class="sd">            &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]).astype(np.float32))</span>
<span class="sd">            &gt;&gt;&gt; index = Tensor([0, 2], mindspore.int32)</span>
<span class="sd">            &gt;&gt;&gt; value = Tensor(-2.0, mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; y = x.index_fill(1, index, value)</span>
<span class="sd">            &gt;&gt;&gt; print(y)</span>
<span class="sd">            [[-2. 2. -2.]</span>
<span class="sd">             [-2. 5. -2.]</span>
<span class="sd">             [-2. 8. -2.]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;index_fill&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.inplace_update"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.inplace_update.html#mindspore.Tensor.inplace_update">[docs]</a>    <span class="k">def</span> <span class="nf">inplace_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Update some rows of a tensor with values of v according to the specified indices.</span>

<span class="sd">        Note:</span>
<span class="sd">            `indices` refers to the left-most dimension.</span>

<span class="sd">        Args:</span>
<span class="sd">            v (Tensor): A tensor with the same type and same dimension size except the first dimension, which must be</span>
<span class="sd">              the same as the size of indices.</span>
<span class="sd">            indices (Union[int, tuple]): Indices into the left-most dimension determining which rows to be updated.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, with updated values.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: if indices is not int or tuple.</span>
<span class="sd">            TypeError: if indices is tuple but any of its element is not int.</span>
<span class="sd">            ValueError: the Tensor shape is different from that of v.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; import mindspore</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([[1, 2], [3, 4], [5, 6]]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; v = Tensor(np.array([[0.1, 0.2], [0.3, 0.4]]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; indices = (0, 1)</span>
<span class="sd">            &gt;&gt;&gt; output = x.inplace_update(v, indices)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [[0.1 0.2]</span>
<span class="sd">             [0.3 0.4]</span>
<span class="sd">             [5.  6. ]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;inplace_update&#39;</span><span class="p">)(</span><span class="n">indices</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.copy"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.copy.html#mindspore.Tensor.copy">[docs]</a>    <span class="k">def</span> <span class="nf">copy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return a copy of the tensor.</span>

<span class="sd">        Note:</span>
<span class="sd">            The current implementation does not support `order` argument.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Copied tensor.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; a = Tensor(np.ones((3,3)).astype(&quot;float32&quot;))</span>
<span class="sd">            &gt;&gt;&gt; output = a.copy()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [[1. 1. 1.]</span>
<span class="sd">            [1. 1. 1.]</span>
<span class="sd">            [1. 1. 1.]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span>
        <span class="n">origin_dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span>
        <span class="n">logical_not_op</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;logical_not&#39;</span><span class="p">)()</span>
        <span class="k">if</span> <span class="n">origin_dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">logical_not_op</span><span class="p">(</span><span class="n">logical_not_op</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">origin_dtype</span> <span class="o">!=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="mf">1.0</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">origin_dtype</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span></div>

<div class="viewcode-block" id="Tensor.max"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.max.html#mindspore.Tensor.max">[docs]</a>    <span class="k">def</span> <span class="nf">max</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">initial</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the maximum of a tensor or maximum along an axis.</span>

<span class="sd">        Args:</span>
<span class="sd">            axis (Union[None, int, list, tuple of ints], optional): Axis or</span>
<span class="sd">                axes along which to operate. By default, flattened input is used. If</span>
<span class="sd">                this is a tuple of ints, the maximum is selected over multiple axes,</span>
<span class="sd">                instead of a single axis or all the axes as before. Default: None.</span>
<span class="sd">            keepdims (bool, optional):</span>
<span class="sd">                If this is set to True, the axes which are reduced are left in the</span>
<span class="sd">                result as dimensions with size one. With this option, the result will</span>
<span class="sd">                broadcast correctly against the input array. Default: False.</span>
<span class="sd">            initial (scalar, optional):</span>
<span class="sd">                The minimum value of an output element. Must be present to allow</span>
<span class="sd">                computation on empty slice. Default: None.</span>
<span class="sd">            where (bool Tensor, optional):</span>
<span class="sd">                A boolean tensor which is broadcasted to match the dimensions of array,</span>
<span class="sd">                and selects elements to include in the reduction. If non-default value</span>
<span class="sd">                is passed, initial must also be provided. Default: True.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor or scalar, maximum of input tensor. If `axis` is None, the result is a scalar</span>
<span class="sd">            value. If `axis` is given, the result is a tensor of dimension ``self.ndim - 1``.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If arguments have types not specified above.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        See also:</span>
<span class="sd">            :func:`mindspore.Tensor.argmin`: Return the indices of the minimum values along an axis.</span>

<span class="sd">            :func:`mindspore.Tensor.argmax`: Return the indices of the maximum values along an axis.</span>

<span class="sd">            :func:`mindspore.Tensor.min`: Return the minimum of a tensor or minimum along an axis.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; a = Tensor(np.arange(4).reshape((2, 2)).astype(&#39;float32&#39;))</span>
<span class="sd">            &gt;&gt;&gt; output = a.max()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            3.0</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">reduce_</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;reduce&quot;</span><span class="p">)</span>
        <span class="n">reduce_max</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;reduce_max&quot;</span><span class="p">)</span>
        <span class="n">maximum</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;maximum&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">reduce_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduce_max</span><span class="p">(</span><span class="n">keepdims</span><span class="p">),</span> <span class="n">cmp_fn</span><span class="o">=</span><span class="n">maximum</span><span class="p">(),</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">,</span>
                       <span class="n">initial</span><span class="o">=</span><span class="n">initial</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="n">where</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.min"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.min.html#mindspore.Tensor.min">[docs]</a>    <span class="k">def</span> <span class="nf">min</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">initial</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the minimum of a tensor or minimum along an axis.</span>

<span class="sd">        Args:</span>
<span class="sd">            axis (Union[None, int, list, tuple of ints], optional): Axis or</span>
<span class="sd">                axes along which to operate. By default, flattened input is used. If</span>
<span class="sd">                this is a tuple of ints, the minimum is selected over multiple axes,</span>
<span class="sd">                instead of a single axis or all the axes as before. Default: None.</span>
<span class="sd">            keepdims (bool, optional):</span>
<span class="sd">                If this is set to True, the axes which are reduced are left in the</span>
<span class="sd">                result as dimensions with size one. With this option, the result will</span>
<span class="sd">                broadcast correctly against the input tensor. Default: False.</span>
<span class="sd">            initial (scalar, optional):</span>
<span class="sd">                The maximum value of an output element. Must be present to allow</span>
<span class="sd">                computation on empty slice. Default: None.</span>
<span class="sd">            where (bool Tensor, optional):</span>
<span class="sd">                A boolean tensor which is broadcasted to match the dimensions of tensor,</span>
<span class="sd">                and selects elements to include in the reduction. If non-default value</span>
<span class="sd">                is passed, initial must also be provided. Default: True.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor or scalar, minimum of input tensor. If the axis is None, the result is a scalar</span>
<span class="sd">            value. If `axis` is given, the result is a tensor of dimension ``self.ndim - 1``.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If arguments have types not specified above.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        See also:</span>
<span class="sd">            :func:`mindspore.Tensor.argmin`: Return the indices of the minimum values along an axis.</span>

<span class="sd">            :func:`mindspore.Tensor.argmax`: Return the indices of the maximum values along an axis.</span>

<span class="sd">            :func:`mindspore.Tensor.max`: Return the maximum of a tensor or maximum along an axis.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">            &gt;&gt;&gt; a = Tensor(np.arange(4).reshape((2,2)).astype(&#39;float32&#39;))</span>
<span class="sd">            &gt;&gt;&gt; output = a.min()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            0.0</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">reduce_</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;reduce&quot;</span><span class="p">)</span>
        <span class="n">reduce_min</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;reduce_min&quot;</span><span class="p">)</span>
        <span class="n">minimum</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;minimum&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">reduce_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduce_min</span><span class="p">(</span><span class="n">keepdims</span><span class="p">),</span> <span class="n">cmp_fn</span><span class="o">=</span><span class="n">minimum</span><span class="p">(),</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">,</span>
                       <span class="n">initial</span><span class="o">=</span><span class="n">initial</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="n">where</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.scatter_add"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.scatter_add.html#mindspore.Tensor.scatter_add">[docs]</a>    <span class="k">def</span> <span class="nf">scatter_add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Creates a new tensor by adding the values from the positions in self tensor indicated by</span>
<span class="sd">        `indices`, with values from `updates`. When multiple values are given for the same</span>
<span class="sd">        index, the updated result will be the sum of all values. This operation is almost</span>
<span class="sd">        equivalent to using ScatterNdAdd, except that the updates are applied on output `Tensor`</span>
<span class="sd">        instead of input `Parameter`.</span>

<span class="sd">        The last axis of `indices` is the depth of each index vectors. For each index vector,</span>
<span class="sd">        there must be a corresponding value in `updates`. The shape of `updates` should be</span>
<span class="sd">        equal to the shape of `self[indices]`. For more details, see use cases.</span>

<span class="sd">        Note:</span>
<span class="sd">            On GPU, if some values of the `indices` are out of bound, instead of raising an index error,</span>
<span class="sd">            the corresponding `updates` will not be updated to self tensor. On CPU, if some values of</span>
<span class="sd">            the `indices` are out of bound, raising an index error. On Ascend, out of bound checking is</span>
<span class="sd">            not supported, if some values of the `indices` are out of bound, unknown errors may be caused.</span>

<span class="sd">        Args:</span>
<span class="sd">            indices (Tensor): The index of input tensor whose data type is int32 or int64.</span>
<span class="sd">                The rank must be at least 2.</span>
<span class="sd">            updates (Tensor): The tensor to update the input tensor, has the same type as self tensor,</span>
<span class="sd">                and updates. Shape should be equal to indices.shape[:-1] + self.shape[indices.shape[-1]:].</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same shape and type as self tensor.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If dtype of `indices` is neither int32 nor int64.</span>
<span class="sd">            ValueError: If length of shape of self tensor is less than the last dimension of shape of `indices`.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]).astype(&#39;float32&#39;))</span>
<span class="sd">            &gt;&gt;&gt; indices = Tensor(np.array([[0, 0], [0, 0]]).astype(&#39;int32&#39;))</span>
<span class="sd">            &gt;&gt;&gt; updates = Tensor(np.array([1.0, 2.2]).astype(&#39;float32&#39;))</span>
<span class="sd">            &gt;&gt;&gt; output = x.scatter_add(indices, updates)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [[ 3.1  0.3  3.6]</span>
<span class="sd">            [ 0.4  0.5 -3.2]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;tensor_scatter_add&quot;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.scatter_sub"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.scatter_sub.html#mindspore.Tensor.scatter_sub">[docs]</a>    <span class="k">def</span> <span class="nf">scatter_sub</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Creates a new tensor by subtracting the values from the positions in self tensor indicated by</span>
<span class="sd">        `indices`, with values from `updates`. When multiple values are provided for the same</span>
<span class="sd">        index, the result of the update will be to subtract these values respectively. This operation is almost</span>
<span class="sd">        equivalent to using :class:`mindspore.ops.ScatterNdSub` , except that the updates are applied on output `Tensor`</span>
<span class="sd">        instead of input `Parameter`.</span>

<span class="sd">        The last axis of `indices` is the depth of each index vectors. For each index vector,</span>
<span class="sd">        there must be a corresponding value in `updates`. The shape of `updates` should be</span>
<span class="sd">        equal to the shape of `self[indices]`. For more details, see use cases.</span>

<span class="sd">        Note:</span>
<span class="sd">            On GPU, if some values of the `indices` are out of bound, instead of raising an index error,</span>
<span class="sd">            the corresponding `updates` will not be updated to self tensor. On CPU, if some values of</span>
<span class="sd">            the `indices` are out of bound, raising an index error. On Ascend, out of bound checking is</span>
<span class="sd">            not supported, if some values of the `indices` are out of bound, unknown errors may be caused.</span>

<span class="sd">        Args:</span>
<span class="sd">            indices (Tensor): The index of input tensor whose data type is int32 or int64.</span>
<span class="sd">                The rank must be at least 2.</span>
<span class="sd">            updates (Tensor): The tensor to update the input tensor, has the same type as input,</span>
<span class="sd">                and updates.shape should be equal to indices.shape[:-1] + self.shape[indices.shape[-1]:].</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same shape and type as self tensor.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If dtype of `indices` is neither int32 nor int64.</span>
<span class="sd">            ValueError: If length of shape of self tensor is less than the last dimension of shape of `indices`.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]).astype(&#39;float32&#39;))</span>
<span class="sd">            &gt;&gt;&gt; indices = Tensor(np.array([[0, 0], [0, 0]]).astype(&#39;int32&#39;))</span>
<span class="sd">            &gt;&gt;&gt; updates = Tensor(np.array([1.0, 2.2]).astype(&#39;float32&#39;))</span>
<span class="sd">            &gt;&gt;&gt; output = x.scatter_sub(indices, updates)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [[-3.3000002  0.3        3.6      ]</span>
<span class="sd">            [ 0.4        0.5       -3.2      ]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;tensor_scatter_sub&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.scatter_min"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.scatter_min.html#mindspore.Tensor.scatter_min">[docs]</a>    <span class="k">def</span> <span class="nf">scatter_min</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        By comparing the value at the position indicated by `indices` in self tensor with the value in the `updates`,</span>
<span class="sd">        the value at the index will eventually be equal to the smallest one to create a new tensor.</span>

<span class="sd">        The last axis of the index is the depth of each index vector. For each index vector,</span>
<span class="sd">        there must be a corresponding value in `updates`. The shape of `updates` should be</span>
<span class="sd">        equal to the shape of `input_x[indices]`. For more details, see case below.</span>

<span class="sd">        Note:</span>
<span class="sd">            If some values of the `indices` are out of range, instead of raising an index error,</span>
<span class="sd">            the corresponding `updates` will not be updated to `input_x`.</span>

<span class="sd">        Args:</span>
<span class="sd">            indices (Tensor): The index of input tensor whose data type is int32 or int64.</span>
<span class="sd">                The rank must be at least 2.</span>
<span class="sd">            updates (Tensor): The tensor to update the input tensor, has the same type as input,</span>
<span class="sd">                and updates.shape should be equal to indices.shape[:-1] + input_x.shape[indices.shape[-1]:].</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same shape and type as `input_x`.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If dtype of `indices` is neither int32 nor int64.</span>
<span class="sd">            ValueError: If length of shape of `input_x` is less than the last dimension of shape of `indices`.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]).astype(&#39;float32&#39;))</span>
<span class="sd">            &gt;&gt;&gt; indices = Tensor(np.array([[0, 0], [0, 0]]).astype(&#39;int32&#39;))</span>
<span class="sd">            &gt;&gt;&gt; updates = Tensor(np.array([1.0, 2.2]).astype(&#39;float32&#39;))</span>
<span class="sd">            &gt;&gt;&gt; output = x.scatter_min(indices, updates)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [[ -0.1  0.3  3.6]</span>
<span class="sd">            [ 0.4  0.5 -3.2]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;tensor_scatter_min&#39;</span><span class="p">)()(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.scatter_max"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.scatter_max.html#mindspore.Tensor.scatter_max">[docs]</a>    <span class="k">def</span> <span class="nf">scatter_max</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        By comparing the value at the position indicated by `indices` in `x` with the value in the `updates`,</span>
<span class="sd">        the value at the index will eventually be equal to the largest one to create a new tensor.</span>

<span class="sd">        The last axis of the index is the depth of each index vector. For each index vector,</span>
<span class="sd">        there must be a corresponding value in `updates`. The shape of `updates` should be</span>
<span class="sd">        equal to the shape of `input_x[indices]`. For more details, see case below.</span>

<span class="sd">        Note:</span>
<span class="sd">            If some values of the `indices` are out of bound, instead of raising an index error,</span>
<span class="sd">            the corresponding `updates` will not be updated to `input_x`.</span>

<span class="sd">        Args:</span>
<span class="sd">            indices (Tensor): The index of input tensor whose data type is int32 or int64.</span>
<span class="sd">                The rank must be at least 2.</span>
<span class="sd">            updates (Tensor): The tensor to update the input tensor, has the same type as input,</span>
<span class="sd">                and updates.shape should be equal to indices.shape[:-1] + input_x.shape[indices.shape[-1]:].</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same shape and type as `input_x`.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If dtype of `indices` is neither int32 nor int64.</span>
<span class="sd">            ValueError: If length of shape of `input_x` is less than the last dimension of shape of `indices`.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; input_x = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; indices = Tensor(np.array([[0, 0], [0, 0]]), mindspore.int32)</span>
<span class="sd">            &gt;&gt;&gt; updates = Tensor(np.array([1.0, 2.2]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; # Next, demonstrate the approximate operation process of this operator:</span>
<span class="sd">            &gt;&gt;&gt; # 1, indices[0] = [0, 0], indices[1] = [0, 0]</span>
<span class="sd">            &gt;&gt;&gt; # 2, And input_x[0, 0] = -0.1</span>
<span class="sd">            &gt;&gt;&gt; # 3, So input_x[indices] = [-0.1, -0.1]</span>
<span class="sd">            &gt;&gt;&gt; # 4, Satisfy the above formula: input_x[indices].shape=(2) == updates.shape=(2)</span>
<span class="sd">            &gt;&gt;&gt; op = ops.TensorScatterMax()</span>
<span class="sd">            &gt;&gt;&gt; # 5, Perform the max operation for the first time:</span>
<span class="sd">            &gt;&gt;&gt; #      first_input_x = Max(input_x[0][0], updates[0]) = [[1.0, 0.3, 3.6], [0.4, 0.5, -3.2]]</span>
<span class="sd">            &gt;&gt;&gt; # 6, Perform the max operation for the second time:</span>
<span class="sd">            &gt;&gt;&gt; #      second_input_x = Max(input_x[0][0], updates[1]) = [[2.2, 0.3, 3.6], [0.4, 0.5, -3.2]]</span>
<span class="sd">            &gt;&gt;&gt; output = op(input_x, indices, updates)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [[ 2.2  0.3  3.6]</span>
<span class="sd">            [ 0.4  0.5 -3.2]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;tensor_scatter_max&#39;</span><span class="p">)()(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.fill"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.fill.html#mindspore.Tensor.fill">[docs]</a>    <span class="k">def</span> <span class="nf">fill</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fill the tensor with a scalar value.</span>

<span class="sd">        Note:</span>
<span class="sd">            Unlike Numpy, tensor.fill() will always return a new tensor, instead of</span>
<span class="sd">            filling the original tensor.</span>

<span class="sd">        Args:</span>
<span class="sd">            value (Union[None, int, float, bool]): All elements of a will be assigned this value.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, with the original dtype and shape.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If input arguments have types not specified above.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; a = Tensor(np.arange(4).reshape((2,2)).astype(&#39;float32&#39;))</span>
<span class="sd">            &gt;&gt;&gt; print(a.fill(1.0))</span>
<span class="sd">            [[1. 1.]</span>
<span class="sd">            [1. 1.]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For &#39;Tensor.fill&#39;, if the argument &#39;value&#39; is None, the type of the original &quot;</span>
                                <span class="s2">&quot;tensor must be float, but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="s1">&#39;nan&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;tile&quot;</span><span class="p">)()(</span><span class="n">value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For &#39;Tensor.fill&#39;, the type of the argument &#39;value&#39; must be int, float or bool, &quot;</span>
                            <span class="s2">&quot;but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">value</span><span class="p">)))</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;fill&quot;</span><span class="p">)(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.fills"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.fills.html#mindspore.Tensor.fills">[docs]</a>    <span class="k">def</span> <span class="nf">fills</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create a tensor of the same shape and type as the input tensor and fill it with specified value.</span>

<span class="sd">        Note:</span>
<span class="sd">            Unlike Numpy, tensor.fills() will always returns a new tensor, instead of</span>
<span class="sd">            filling the original tensor.</span>

<span class="sd">        Args:</span>
<span class="sd">            value (Union[int, float, Tensor]): All elements of the output tensor will be assigned this value. The</span>
<span class="sd">                type should be int, float or 0-dimensional tensor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, with the same shape and type as input tensor.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `value` has types not specified above.</span>
<span class="sd">            RuntimeError: If `value` cannot be converted to the same type as `x`.</span>
<span class="sd">            ValueError: If `value` is a tensor and the length of dimension is not 0.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``GPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.arange(4).reshape((2, 2)).astype(&#39;float32&#39;))</span>
<span class="sd">            &gt;&gt;&gt; print(x.fills(1.0))</span>
<span class="sd">            [[1. 1.]</span>
<span class="sd">            [1. 1.]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;fills&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.masked_fill"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.masked_fill.html#mindspore.Tensor.masked_fill">[docs]</a>    <span class="k">def</span> <span class="nf">masked_fill</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fills elements of self tensor with value where mask is True.</span>
<span class="sd">        The shapes of self tensor and `mask` need to be the same or broadcastable.</span>

<span class="sd">        Args:</span>
<span class="sd">            mask (Tensor[bool]): The boolean mask.</span>
<span class="sd">            value (Union[float, Tensor]): The value to fill in with, which dtype is the same as self.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same type and shape as self.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `mask` is not a Tensor.</span>
<span class="sd">            TypeError: If dtype of `mask` is not bool.</span>
<span class="sd">            ValueError: If the shapes of self tensor and `mask` could not be broadcast.</span>
<span class="sd">            TypeError: If dtype of self tensor or `value` is not one of float16, float32, int8, int32.</span>
<span class="sd">            TypeError: If dtype of `value` is different from that of self.</span>
<span class="sd">            TypeError: If `value` is neither float number nor Tensor.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; a = Tensor(np.arange(4)).astype(&#39;float32&#39;)</span>
<span class="sd">            &gt;&gt;&gt; print(a)</span>
<span class="sd">            [0. 1. 2. 3.]</span>
<span class="sd">            &gt;&gt;&gt; mask = Tensor([False, False, True, True])</span>
<span class="sd">            &gt;&gt;&gt; print(a.masked_fill(mask, 0.0))</span>
<span class="sd">            [0. 1. 0. 0.]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">)):</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;scalar_to_tensor&quot;</span><span class="p">)(</span><span class="n">value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For &#39;Tensor.masked_fill&#39;, the type of the argument &#39;mask&#39; must be Tensor, but &quot;</span>
                            <span class="s2">&quot;got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">mask</span><span class="p">)))</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_name</span><span class="p">(</span><span class="s1">&#39;mask&#39;</span><span class="p">,</span> <span class="n">mask</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">],</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;masked_fill&quot;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.ptp"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.ptp.html#mindspore.Tensor.ptp">[docs]</a>    <span class="k">def</span> <span class="nf">ptp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The name of the function comes from the acronym for &quot;peak to peak&quot;. Calculate the difference between the</span>
<span class="sd">        maximum value and the minimum value along the axis.</span>

<span class="sd">        Note:</span>
<span class="sd">            Numpy argument `out` is not supported.</span>

<span class="sd">        Args:</span>
<span class="sd">            axis (Union[None, int, tuple(int)]): Axis or axes along which the range is computed.</span>
<span class="sd">                The default is to compute the variance of the flattened tensor. Default: None.</span>
<span class="sd">            keepdims (bool): If this is set to True, the axes which are reduced are left in the result as</span>
<span class="sd">                dimensions with size one. With this option, the result will broadcast correctly against the tensor.</span>
<span class="sd">                Default is False.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `self` is not a tensor, or `axis` and `keepdims` have types not specified above.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor([[4.0, 9.0, 2.0, 10.0], [6.0, 9.0, 7.0, 12.0]]).astype(&quot;float32&quot;)</span>
<span class="sd">            &gt;&gt;&gt; print(x.ptp(axis=1))</span>
<span class="sd">            [8. 6.]</span>
<span class="sd">            &gt;&gt;&gt; print(x.ptp(axis=0))</span>
<span class="sd">            [2. 0. 5. 2.]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">keepdims</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For &#39;Tensor.ptp&#39;, the type of the argument &#39;keepdims&#39; must be bool, &quot;</span>
                            <span class="s2">&quot;but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">keepdims</span><span class="p">)))</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">axis</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_axis_type</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
            <span class="n">axis</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_axis_valid</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.minimum"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.minimum.html#mindspore.Tensor.minimum">[docs]</a>    <span class="k">def</span> <span class="nf">minimum</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes the minimum of self and input tensor element-wise.</span>

<span class="sd">        Note:</span>
<span class="sd">            - `self` and `other` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">            - `other` must be tensor or scalar.</span>
<span class="sd">            - When `other` is also tensor, dtypes of `self` and `other` cannot be bool at the same time.</span>
<span class="sd">            - When `other` is scalar, the scalar could only be a constant.</span>
<span class="sd">            - Shapes of `self` and `other` are supposed to be broadcast.</span>
<span class="sd">            - If one of the elements being compared is a NaN, then that element is returned.</span>

<span class="sd">        .. math::</span>
<span class="sd">            output_i = min(x_i, y_i)</span>

<span class="sd">        Args:</span>
<span class="sd">            other (Union[Tensor, Number, bool]): a number or bool or tensor whose data type is number or bool.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">            and the data type is the one with higher precision or higher digits among `self` and `other`.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `other` is not one of the following: Tensor, Number, bool.</span>
<span class="sd">            ValueError: If `self` and `other` are not the same shape after broadcast.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import mindspore</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([1.0, 5.0, 3.0]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; y = Tensor(np.array([4.0, 2.0, 6.0]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; out = x.minimum(y)</span>
<span class="sd">            &gt;&gt;&gt; print(out)</span>
<span class="sd">            [1. 2. 3.]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;minimum&#39;</span><span class="p">)()(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.clip"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.clip.html#mindspore.Tensor.clip">[docs]</a>    <span class="k">def</span> <span class="nf">clip</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Clips (limits) the values in a Tensor.</span>

<span class="sd">        Given an interval, values outside the interval are clipped to the interval edges.</span>
<span class="sd">        For example, if an interval of :math:`[0, 1]` is specified, values smaller than 0 become 0,</span>
<span class="sd">        and values larger than 1 become 1.</span>

<span class="sd">        Note:</span>
<span class="sd">            Currently, clip with `xmin=nan` or `xmax=nan` is not supported.</span>

<span class="sd">        Args:</span>
<span class="sd">            xmin (Tensor, scalar, None): Minimum value. If None, clipping is not performed</span>
<span class="sd">                on the lower interval edge. Not more than one of `xmin` and `xmax` may be None.</span>
<span class="sd">            xmax (Tensor, scalar, None): Maximum value. If None, clipping is not performed</span>
<span class="sd">                on the upper interval edge. Not more than one of `xmin` and `xmax` may be None.</span>
<span class="sd">                If `xmin` or `xmax` are tensors, then `xmin`, `xmax` and the given tensor</span>
<span class="sd">                will be broadcasted to match their shapes.</span>
<span class="sd">            dtype (:class:`mindspore.dtype`, optional): Overrides the dtype of the</span>
<span class="sd">                output Tensor. Default is None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, a tensor with the elements of the input tensor, but where values</span>
<span class="sd">            &lt; `xmin` are replaced with `xmin`, and those &gt; `xmax` with `xmax`.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If inputs have types not specified above.</span>
<span class="sd">            ValueError: If the shapes of `x1` and `x2` cannot broadcast, or both `xmin` and `xmax` are `None`.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor([1, 2, 3, -4, 0, 3, 2, 0]).astype(&quot;float32&quot;)</span>
<span class="sd">            &gt;&gt;&gt; y = x.clip(0, 2)</span>
<span class="sd">            &gt;&gt;&gt; print(y)</span>
<span class="sd">            [1. 2. 2. 0. 0. 2. 2. 0.]</span>
<span class="sd">            &gt;&gt;&gt; t = Tensor([1, 1, 1, 1, 1, 1, 1, 1])</span>
<span class="sd">            &gt;&gt;&gt; y = x.clip(t, 2)</span>
<span class="sd">            &gt;&gt;&gt; print(y)</span>
<span class="sd">            [1. 2. 2. 1. 1. 2. 2. 1.]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">xmin</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">xmax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;Tensor.clip&#39;, the argument &#39;xmin&#39; and &#39;xman&#39; cannot all be None.&quot;</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span>
        <span class="c1"># F.maximum/minimum does not support when both operands are scalar</span>
        <span class="k">if</span> <span class="n">xmin</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">xmin</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">xmin</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">xmin</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;maximum&quot;</span><span class="p">)()(</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,)),</span> <span class="n">xmin</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;maximum&quot;</span><span class="p">)()(</span><span class="n">x</span><span class="p">,</span> <span class="n">xmin</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">xmax</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">xmax</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">xmax</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">xmax</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;minimum&quot;</span><span class="p">)()(</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,)),</span> <span class="n">xmax</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;minimum&quot;</span><span class="p">)()(</span><span class="n">x</span><span class="p">,</span> <span class="n">xmax</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">dtype</span> <span class="o">!=</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span></div>

    <span class="k">def</span> <span class="nf">_init_check</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_init</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_data</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span>

<div class="viewcode-block" id="Tensor.init_data"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.init_data.html#mindspore.Tensor.init_data">[docs]</a>    <span class="k">def</span> <span class="nf">init_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">slice_index</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">opt_shard_group</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the tensor format data of this Tensor.</span>

<span class="sd">        Note:</span>
<span class="sd">            The init_data function can be called once for the same tensor.</span>

<span class="sd">        Args:</span>
<span class="sd">            slice_index (int): Slice index of a parameter&#39;s slices.</span>
<span class="sd">                It is used when initialize a slice of a parameter, it guarantees that devices</span>
<span class="sd">                using the same slice can generate the same tensor. Default: None.</span>
<span class="sd">            shape (list[int]): Shape of the slice, it is used when initialize a slice of the parameter. Default: None.</span>
<span class="sd">            opt_shard_group(str): Optimizer shard group which is used in auto or semi auto parallel mode</span>
<span class="sd">                to get one shard of a parameter&#39;s slice. Default: None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Initialized Tensor.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">            &gt;&gt;&gt; from mindspore.common.initializer import initializer, Constant</span>
<span class="sd">            &gt;&gt;&gt; x = initializer(Constant(1), [2, 2], ms.float32)</span>
<span class="sd">            &gt;&gt;&gt; out = x.init_data()</span>
<span class="sd">            &gt;&gt;&gt; print(out)</span>
<span class="sd">            [[1. 1.]</span>
<span class="sd">             [1. 1.]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;init_data must be set Tensor.init, init can&#39;t be None&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">dtype_to_nptype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
        <span class="k">except</span> <span class="ne">ValueError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="s2">&quot;Error shape=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">critical</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>

        <span class="k">class</span> <span class="nc">seed_context</span><span class="p">:</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;Set and restore seed.&quot;&quot;&quot;</span>

            <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">init</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">init</span> <span class="o">=</span> <span class="n">init</span>
                <span class="n">global_seed</span> <span class="o">=</span> <span class="n">get_seed</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_np_seed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">get_state</span><span class="p">()[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">need_set_seed</span> <span class="o">=</span> <span class="p">(</span><span class="n">slice_index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_global_seed</span> <span class="o">=</span> <span class="n">global_seed</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_device_num</span> <span class="o">=</span> <span class="mi">1</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">need_set_seed</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_device_num</span> <span class="o">=</span> <span class="n">get_group_size</span><span class="p">()</span>

            <span class="k">def</span> <span class="fm">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">need_set_seed</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">seed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">seed</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">slice_index</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_seed</span><span class="p">)</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">seed</span> <span class="o">=</span> <span class="n">slice_index</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_seed</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">slice_index</span> <span class="o">+</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">delta_seed</span><span class="p">)</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">seed</span> <span class="o">=</span> <span class="n">slice_index</span> <span class="o">+</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">delta_seed</span>
                        <span class="n">Tensor</span><span class="o">.</span><span class="n">delta_seed</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_num</span>

            <span class="k">def</span> <span class="fm">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ptype</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">trace</span><span class="p">):</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">need_set_seed</span><span class="p">:</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_np_seed</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">seed</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">seed</span>

        <span class="k">with</span> <span class="n">seed_context</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">opt_shard_group</span><span class="p">:</span>
            <span class="n">rank</span> <span class="o">=</span> <span class="n">get_rank</span><span class="p">(</span><span class="n">opt_shard_group</span><span class="p">)</span>
            <span class="n">size</span> <span class="o">=</span> <span class="n">get_group_size</span><span class="p">(</span><span class="n">opt_shard_group</span><span class="p">)</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">size</span><span class="p">)[</span><span class="n">rank</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">assign_value</span><span class="p">(</span><span class="n">Tensor_</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="Tensor.to_tensor"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.to_tensor.html#mindspore.Tensor.to_tensor">[docs]</a>    <span class="k">def</span> <span class="nf">to_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">slice_index</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">opt_shard_group</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return init_data() and get the tensor format data of this Tensor.</span>

<span class="sd">        Note:</span>
<span class="sd">            The usage of `to_tensor` is deprecated. Please use `init_data`.</span>

<span class="sd">        Args:</span>
<span class="sd">            slice_index (int): Slice index of a parameter&#39;s slices.</span>
<span class="sd">                It is used when initialize a slice of a parameter, it guarantees that devices</span>
<span class="sd">                using the same slice can generate the same tensor. Default: None.</span>
<span class="sd">            shape (list[int]): Shape of the slice, it is used when initialize a slice of the parameter. Default: None.</span>
<span class="sd">            opt_shard_group(str): Optimizer shard group which is used in auto or semi auto parallel mode</span>
<span class="sd">                to get one shard of a parameter&#39;s slice. Default: None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Initialized Tensor.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: `indices` is neither int32 nor int64.</span>
<span class="sd">            ValueError: The length of the shape of the tensor is less than the last dimension of `indices`.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">            &gt;&gt;&gt; from mindspore.common.initializer import initializer, Constant</span>
<span class="sd">            &gt;&gt;&gt; x = initializer(Constant(1), [2, 2], ms.float32)</span>
<span class="sd">            &gt;&gt;&gt; out = x.to_tensor()</span>
<span class="sd">            &gt;&gt;&gt; print(out)</span>
<span class="sd">            [[1. 1.]</span>
<span class="sd">             [1. 1.]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;WARN_DEPRECATED: The usage of to_tensor is deprecated.&quot;</span>
                       <span class="s2">&quot; Please use init_data&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_data</span><span class="p">(</span><span class="n">slice_index</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">opt_shard_group</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.resize"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.resize.html#mindspore.Tensor.resize">[docs]</a>    <span class="k">def</span> <span class="nf">resize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">new_shape</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Changes shape and size of tensor in-place.</span>

<span class="sd">        If the shape of the new tensor is larger than the shape of the original tensor, the new tensor will be filled</span>
<span class="sd">        with 0. And if the shape of the new tensor is smaller than the shape of the original tensor, the new tensor is</span>
<span class="sd">        filled with the elements of the original tensor in order.</span>

<span class="sd">        Note:</span>
<span class="sd">            Instead of changing the size of the input tensor and returns nothing as in numpy,</span>
<span class="sd">            this method returns a new Tensor with the input size.</span>
<span class="sd">            Numpy argument `refcheck` is not supported.</span>

<span class="sd">        Args:</span>
<span class="sd">            new_shape (Union[ints, tuple of ints]): Shape of resized tensor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        See also:</span>
<span class="sd">            :func:`mindspore.Tensor.reshape`: Give a new shape to a tensor without changing its data.</span>

<span class="sd">            :func:`mindspore.Tensor.repeat`: Repeat elements of a tensor.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32))</span>
<span class="sd">            &gt;&gt;&gt; y = x.resize(3, 3)</span>
<span class="sd">            &gt;&gt;&gt; print(y)</span>
<span class="sd">            [[1. 2. 3.]</span>
<span class="sd">            [4. 5. 6.]</span>
<span class="sd">            [0. 0. 0.]]</span>
<span class="sd">            &gt;&gt;&gt; y = x.resize(2, 2)</span>
<span class="sd">            &gt;&gt;&gt; print(y)</span>
<span class="sd">            [[1. 2.]</span>
<span class="sd">            [3. 4.]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">new_shape</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">new_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">tuple</span><span class="p">):</span>
                <span class="n">new_shape</span> <span class="o">=</span> <span class="n">new_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">flattened</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="n">cur_size</span> <span class="o">=</span> <span class="n">flattened</span><span class="o">.</span><span class="n">size</span>
        <span class="n">new_size</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;shape_mul&#39;</span><span class="p">)(</span><span class="n">new_shape</span><span class="p">)</span>
        <span class="n">diff_size</span> <span class="o">=</span> <span class="n">new_size</span> <span class="o">-</span> <span class="n">cur_size</span>
        <span class="k">if</span> <span class="n">diff_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">pad_val</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;fill&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="p">(</span><span class="n">diff_size</span><span class="p">,),</span> <span class="mi">0</span><span class="p">)</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;concatenate&#39;</span><span class="p">)(</span><span class="mi">0</span><span class="p">)((</span><span class="n">flattened</span><span class="p">,</span> <span class="n">pad_val</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">flattened</span><span class="p">[:</span><span class="n">new_size</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">res</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">new_shape</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.diagonal"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.diagonal.html#mindspore.Tensor.diagonal">[docs]</a>    <span class="k">def</span> <span class="nf">diagonal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">axis1</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">axis2</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return specified diagonals.</span>

<span class="sd">        Args:</span>
<span class="sd">            offset (int, optional): Offset of the diagonal from the main diagonal.</span>
<span class="sd">                Can be positive or negative. Defaults to main diagonal.</span>
<span class="sd">            axis1 (int, optional): Axis to be used as the first axis of the 2-D</span>
<span class="sd">                sub-arrays from which the diagonals should be taken. Defaults to</span>
<span class="sd">                first axis (0).</span>
<span class="sd">            axis2 (int, optional): Axis to be used as the second axis of the 2-D</span>
<span class="sd">                sub-arrays from which the diagonals should be taken. Defaults to</span>
<span class="sd">                second axis.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, if Tensor is 2-D, return a 1-D Tensor containing the diagonal.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the input tensor has less than two dimensions.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        See also:</span>
<span class="sd">            :func:`mindspore.Tensor.trace`: Return the sum along diagonals of the tensor.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; a = Tensor(np.arange(4).reshape(2, 2))</span>
<span class="sd">            &gt;&gt;&gt; print(a)</span>
<span class="sd">            [[0 1]</span>
<span class="sd">            [2 3]]</span>
<span class="sd">            &gt;&gt;&gt; output = a.diagonal()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [0 3]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">ndim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span>
        <span class="k">if</span> <span class="n">ndim</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;Tensor.diagonal&#39;, the original tensor requires at least two dimensions, &quot;</span>
                             <span class="s2">&quot;but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ndim</span><span class="p">))</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span>

        <span class="n">axes</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_axis_valid</span><span class="p">((</span><span class="n">axis1</span><span class="p">,</span> <span class="n">axis2</span><span class="p">),</span> <span class="n">ndim</span><span class="p">)</span>
        <span class="n">perm</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndim</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">:</span>
                <span class="n">perm</span> <span class="o">+=</span> <span class="p">(</span><span class="n">i</span><span class="p">,)</span>
        <span class="n">perm</span> <span class="o">+=</span> <span class="n">axes</span>
        <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">perm</span><span class="p">)</span>

        <span class="n">shape</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">n</span><span class="p">,</span> <span class="n">m</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>

        <span class="n">e</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;eye&#39;</span><span class="p">)(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">offset</span> <span class="o">&gt;=</span> <span class="n">m</span> <span class="ow">or</span> <span class="n">offset</span> <span class="o">&lt;=</span> <span class="o">-</span><span class="n">n</span><span class="p">:</span>
            <span class="n">e</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;fill&#39;</span><span class="p">)(</span><span class="n">dtype</span><span class="p">,</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">offset</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">e</span> <span class="o">=</span> <span class="n">e</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">offset</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">e_left</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;fill&#39;</span><span class="p">)(</span><span class="n">dtype</span><span class="p">,</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">offset</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
                <span class="n">e_right</span> <span class="o">=</span> <span class="n">e</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="n">m</span> <span class="o">-</span> <span class="n">offset</span><span class="p">:</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">e</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;concatenate&#39;</span><span class="p">)(</span><span class="mi">1</span><span class="p">)((</span><span class="n">e_left</span><span class="p">,</span> <span class="n">e_right</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">offset</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">e_upper</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;fill&#39;</span><span class="p">)(</span><span class="n">dtype</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="n">offset</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
                <span class="n">e_lower</span> <span class="o">=</span> <span class="n">e</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">n</span> <span class="o">+</span> <span class="n">offset</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
                <span class="n">e</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;concatenate&#39;</span><span class="p">)(</span><span class="mi">0</span><span class="p">)((</span><span class="n">e_upper</span><span class="p">,</span> <span class="n">e_lower</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">e</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;broadcast_to&#39;</span><span class="p">)(</span><span class="n">shape</span><span class="p">)(</span><span class="n">e</span><span class="p">)</span>

        <span class="n">prod</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;__mul__&#39;</span><span class="p">)(</span><span class="n">a</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;reduce_sum&#39;</span><span class="p">)(</span><span class="n">prod</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">begin</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
            <span class="n">begin</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span>
        <span class="n">last_dim_begin</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="n">offset</span><span class="p">)</span>
        <span class="n">begin</span> <span class="o">+=</span> <span class="p">(</span><span class="n">last_dim_begin</span><span class="p">,)</span>
        <span class="n">size</span> <span class="o">=</span> <span class="n">res</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">last_dim_end</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span>
            <span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">offset</span><span class="p">))</span> <span class="o">-</span> <span class="n">last_dim_begin</span>
        <span class="k">if</span> <span class="n">last_dim_end</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">([])</span>
        <span class="n">size</span> <span class="o">+=</span> <span class="p">(</span><span class="n">last_dim_end</span><span class="p">,)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;tensor_slice&#39;</span><span class="p">)(</span><span class="n">res</span><span class="p">,</span> <span class="n">begin</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">res</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.trace"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.trace.html#mindspore.Tensor.trace">[docs]</a>    <span class="k">def</span> <span class="nf">trace</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">axis1</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">axis2</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the sum along diagonals of the tensor.</span>

<span class="sd">        Args:</span>
<span class="sd">            offset (int, optional): Offset of the diagonal from the main diagonal.</span>
<span class="sd">                Can be positive or negative. Defaults to main diagonal.</span>
<span class="sd">            axis1 (int, optional): Axis to be used as the first axis of the 2-D</span>
<span class="sd">                sub-arrays from which the diagonals should be taken. Defaults to</span>
<span class="sd">                first axis (0).</span>
<span class="sd">            axis2 (int, optional): Axis to be used as the second axis of the 2-D</span>
<span class="sd">                sub-arrays from which the diagonals should be taken. Defaults to</span>
<span class="sd">                second axis.</span>
<span class="sd">            dtype (:class:`mindspore.dtype`, optional): defaults to None. Overrides the dtype of the</span>
<span class="sd">                output Tensor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, the sum along diagonals.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the input tensor has less than two dimensions.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        See also:</span>
<span class="sd">            :func:`mindspore.Tensor.diagonal`: Return specified diagonals.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.eye(3, dtype=np.float32))</span>
<span class="sd">            &gt;&gt;&gt; print(x.trace())</span>
<span class="sd">            3.0</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">d</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">diagonal</span><span class="p">(</span><span class="n">offset</span><span class="p">,</span> <span class="n">axis1</span><span class="o">=</span><span class="n">axis1</span><span class="p">,</span> <span class="n">axis2</span><span class="o">=</span><span class="n">axis2</span><span class="p">)</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">d</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">dtype</span> <span class="o">=</span> <span class="n">d</span><span class="o">.</span><span class="n">dtype</span>
        <span class="k">if</span> <span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;fill&#39;</span><span class="p">)(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;reduce_sum&#39;</span><span class="p">)(</span><span class="n">d</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">res</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.take"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.take.html#mindspore.Tensor.take">[docs]</a>    <span class="k">def</span> <span class="nf">take</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;clip&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Takes elements from a tensor along an axis.</span>

<span class="sd">        Args:</span>
<span class="sd">            indices (Tensor): The indices with shape `(Nj...)` of the values to extract.</span>
<span class="sd">            axis (int, optional): The axis over which to select values. By default,</span>
<span class="sd">                the flattened input tensor is used. Default: `None`.</span>
<span class="sd">            mode (&#39;raise&#39;, &#39;wrap&#39;, &#39;clip&#39;, optional):</span>

<span class="sd">                - raise: Raises an error;</span>

<span class="sd">                - wrap: Wraps around;</span>

<span class="sd">                - clip: Clips to the range. &#39;clip&#39; mode means that all indices that are</span>
<span class="sd">                  too large are replaced by the index that addresses the last element</span>
<span class="sd">                  along that axis. Note that this disables indexing with negative numbers.</span>

<span class="sd">                Default: &#39;clip&#39;.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, the indexed result.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If `axis` is out of range, or `mode` has values other than (&#39;raise&#39;, &#39;wrap&#39;, &#39;clip&#39;)</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; a = Tensor(np.array([4, 3, 5, 7, 6, 8]))</span>
<span class="sd">            &gt;&gt;&gt; indices = Tensor(np.array([0, 1, 4]))</span>
<span class="sd">            &gt;&gt;&gt; output = a.take(indices)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [4 3 6]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;raise&#39;</span><span class="p">,</span> <span class="s1">&#39;wrap&#39;</span><span class="p">,</span> <span class="s1">&#39;clip&#39;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;Tensor.take&#39;, the argument &#39;mode&#39; should be one of in [&#39;raise&#39;, &#39;wrap&#39;, &#39;clip&#39;],&quot;</span>
                             <span class="sa">f</span><span class="s2">&quot; but got </span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
            <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span>
        <span class="n">ndim</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">ndim</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_axis_in_range</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span> <span class="o">+</span> <span class="n">ndim</span> <span class="k">if</span> <span class="n">axis</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">axis</span>

        <span class="n">shape_a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">shape_indices</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">size_indices</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">size</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;check_indices&#39;</span><span class="p">)(</span><span class="n">shape_a</span><span class="p">[</span><span class="n">axis</span><span class="p">],</span> <span class="n">indices</span><span class="p">,</span> <span class="n">mode</span><span class="p">)</span>

        <span class="c1"># reshapes indices to shape (Ni..., Nj..., Nk)</span>
        <span class="n">shape_ni</span> <span class="o">=</span> <span class="n">shape_a</span><span class="p">[:</span><span class="n">axis</span><span class="p">]</span>
        <span class="n">shape_nk</span> <span class="o">=</span> <span class="n">shape_a</span><span class="p">[</span><span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:]</span>
        <span class="n">shape_out</span> <span class="o">=</span> <span class="n">shape_ni</span> <span class="o">+</span> <span class="n">shape_indices</span> <span class="o">+</span> <span class="n">shape_nk</span>
        <span class="n">shape_indices</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">size_indices</span> <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">axis</span> <span class="k">else</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndim</span><span class="p">))</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">shape_indices</span><span class="p">)</span>
        <span class="n">shape_indices</span> <span class="o">=</span> <span class="n">shape_ni</span> <span class="o">+</span> <span class="p">(</span><span class="n">indices</span><span class="o">.</span><span class="n">size</span><span class="p">,)</span> <span class="o">+</span> <span class="n">shape_nk</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;broadcast_to&#39;</span><span class="p">)(</span><span class="n">shape_indices</span><span class="p">)(</span><span class="n">indices</span><span class="p">)</span>

        <span class="n">res</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;gather_d&#39;</span><span class="p">)(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">res</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">shape_out</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.choose"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.choose.html#mindspore.Tensor.choose">[docs]</a>    <span class="k">def</span> <span class="nf">choose</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">choices</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;clip&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Construct a tensor from an index tensor and a list of tensors to choose from.</span>

<span class="sd">        Args:</span>
<span class="sd">            choices (Union[tuple, list, Tensor]): Choice tensors. The input tensor and all of the</span>
<span class="sd">                `choices` must be broadcasted to the same shape. If `choices` is itself a tensor,</span>
<span class="sd">                then its outermost dimension (i.e., the one corresponding to ``choices.shape[0]``)</span>
<span class="sd">                is taken as defining the &quot;sequence&quot;.</span>
<span class="sd">            mode (&#39;raise&#39;, &#39;wrap&#39;, &#39;clip&#39;, optional): Specifies how indices outside</span>
<span class="sd">                ``[0, n-1]`` will be treated:</span>

<span class="sd">                - raise: Raises an error;</span>

<span class="sd">                - wrap: Wraps around;</span>

<span class="sd">                - clip: Clips to the range. &#39;clip&#39; mode means that values greater than n-1 are mapped to n-1.</span>
<span class="sd">                  Note that this disables indexing with negative numbers.</span>

<span class="sd">                Default: &#39;clip&#39;.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, the merged result.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the input tensor and any of the `choices` cannot be broadcast.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; choices = [[0, 1, 2, 3], [10, 11, 12, 13], [20, 21, 22, 23], [30, 31, 32, 33]]</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([2, 3, 1, 0]))</span>
<span class="sd">            &gt;&gt;&gt; print(x.choose(choices))</span>
<span class="sd">            [20 31 12  3]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">choices</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="n">shape_choice</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">infer_out_shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">choices</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
            <span class="n">choices</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;broadcast_to&#39;</span><span class="p">)((</span><span class="n">choices</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],)</span> <span class="o">+</span> <span class="n">shape_choice</span><span class="p">)(</span><span class="n">choices</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># broadcasts choices to the same shape if choices is a sequence</span>
            <span class="n">choicelist</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">shapes</span> <span class="o">=</span> <span class="p">()</span>
            <span class="k">for</span> <span class="n">choice</span> <span class="ow">in</span> <span class="n">choices</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">choice</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
                    <span class="n">choice</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;make_tensor&#39;</span><span class="p">)(</span><span class="n">choice</span><span class="p">)</span>
                <span class="n">shapes</span> <span class="o">+=</span> <span class="p">(</span><span class="n">choice</span><span class="o">.</span><span class="n">shape</span><span class="p">,)</span>
                <span class="n">choicelist</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">choice</span><span class="p">)</span>
            <span class="n">shape_choice</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">infer_out_shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="o">*</span><span class="n">shapes</span><span class="p">)</span>
            <span class="n">tmp</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">choice</span> <span class="ow">in</span> <span class="n">choicelist</span><span class="p">:</span>
                <span class="n">tmp</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;broadcast_to&#39;</span><span class="p">)(</span><span class="n">shape_choice</span><span class="p">)(</span><span class="n">choice</span><span class="p">))</span>
            <span class="n">choices</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;stack&#39;</span><span class="p">)(</span><span class="n">tmp</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">choices</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;Tensor.choose&#39;, the original tensor and the argument &#39;choices&#39; cannot be scalars.&quot;</span>
                             <span class="sa">f</span><span class="s2">&quot; Their dimensions should all be &gt; 0, but got the original tensor&#39;s dimension &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">, &#39;choices&#39; dimension </span><span class="si">{</span><span class="n">choices</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;broadcast_to&#39;</span><span class="p">)(</span><span class="n">shape_choice</span><span class="p">)(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">choices</span><span class="o">.</span><span class="n">dtype</span>
        <span class="c1"># adjusts dtype for F.tensor_mul and F.gather_nd</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">choices</span> <span class="o">=</span> <span class="n">choices</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;check_indices&#39;</span><span class="p">)(</span><span class="n">choices</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">a</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">allow_negative_index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="n">grids</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">ndim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndim</span><span class="p">):</span>
            <span class="n">dim_grid</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">])),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
            <span class="n">dim_shape</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">expanded_shape</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">i</span><span class="p">)</span>
            <span class="n">dim_grid</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;broadcast_to&#39;</span><span class="p">)(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">)(</span><span class="n">dim_grid</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">dim_shape</span><span class="p">))</span>
            <span class="n">grids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dim_grid</span><span class="p">)</span>
        <span class="n">grid</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;stack&#39;</span><span class="p">)(</span><span class="n">grids</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;concatenate&#39;</span><span class="p">)(</span><span class="o">-</span><span class="mi">1</span><span class="p">)((</span><span class="n">a</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)),</span> <span class="n">grid</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;gather_nd&#39;</span><span class="p">)(</span><span class="n">choices</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.searchsorted"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.searchsorted.html#mindspore.Tensor.searchsorted">[docs]</a>    <span class="k">def</span> <span class="nf">searchsorted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">side</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="n">sorter</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Finds indices where elements should be inserted to maintain order.</span>

<span class="sd">        Args:</span>
<span class="sd">            v (Union[int, float, bool, list, tuple, Tensor]): Values to insert into the tensor.</span>
<span class="sd">            side (&#39;left&#39;, &#39;right&#39;, optional): If &#39;left&#39;, the index of the first suitable</span>
<span class="sd">                location found is given. If &#39;right&#39;, return the last such index. If there is</span>
<span class="sd">                no suitable index, return either 0 or N (where N is the length of the tensor).</span>
<span class="sd">                Default: &#39;left&#39;.</span>
<span class="sd">            sorter (Union[int, float, bool, list, tuple, Tensor]): 1-D optional tensor of</span>
<span class="sd">                integer indices that sort the tensor into ascending order. They are typically</span>
<span class="sd">                the result of argsort. Default: None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, array of insertion points with the same shape as `v`.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If argument for `side` or `sorter` is invalid.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3, 4, 5]))</span>
<span class="sd">            &gt;&gt;&gt; print(x.searchsorted(3))</span>
<span class="sd">            2</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">side</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="s1">&#39;right&#39;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;Tensor.searchsorted&#39;, the argument &#39;side&#39; should be one of in &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;[&#39;left&#39;, &#39;right&#39;], but got </span><span class="si">{</span><span class="n">side</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="n">v</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;make_tensor&#39;</span><span class="p">)(</span><span class="n">v</span><span class="p">)</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">if</span> <span class="n">sorter</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sorter</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For Tensor.searchsorted, the type of the argument &#39;sorter&#39; must be one of &#39;int&#39;, &quot;</span>
                                <span class="s2">&quot;&#39;float&#39;, &#39;bool&#39;, &#39;list&#39;, &#39;tuple&#39;, &#39;Tensor&#39;, but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">sorter</span><span class="p">)))</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sorter</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
                <span class="n">sorter</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;make_tensor&#39;</span><span class="p">)(</span><span class="n">sorter</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">sorter</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">sorter</span><span class="o">.</span><span class="n">size</span> <span class="o">!=</span> <span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;sorter must be 1-D array with the same size as the Tensor&#39;</span><span class="p">)</span>
            <span class="n">sorter</span> <span class="o">=</span> <span class="n">sorter</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">sorter</span><span class="o">.</span><span class="n">shape</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;gather_nd&#39;</span><span class="p">)(</span><span class="n">a</span><span class="p">,</span> <span class="n">sorter</span><span class="p">)</span>
        <span class="n">less_op</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;__le__&#39;</span><span class="p">)</span> <span class="k">if</span> <span class="n">side</span> <span class="o">==</span> <span class="s1">&#39;left&#39;</span> <span class="k">else</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;__lt__&#39;</span><span class="p">)</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;fill&#39;</span><span class="p">)(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">j</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;fill&#39;</span><span class="p">)(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>

        <span class="n">sort_range</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">validator</span><span class="o">.</span><span class="n">get_log2_size</span><span class="p">(</span><span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;shape_mul&#39;</span><span class="p">)(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">sort_range</span><span class="p">:</span>
            <span class="n">mid</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="o">-</span><span class="n">j</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">less_op</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;gather_nd&#39;</span><span class="p">)(</span><span class="n">a</span><span class="p">,</span> <span class="n">mid</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">mid</span><span class="o">.</span><span class="n">shape</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))))</span>
            <span class="n">i</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;select&#39;</span><span class="p">)(</span><span class="n">mask</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">mid</span><span class="p">)</span>
            <span class="n">j</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;select&#39;</span><span class="p">)(</span><span class="n">mask</span><span class="p">,</span> <span class="n">mid</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">j</span></div>

<div class="viewcode-block" id="Tensor.gather_nd"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.gather_nd.html#mindspore.Tensor.gather_nd">[docs]</a>    <span class="k">def</span> <span class="nf">gather_nd</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gathers slices from a input tensor by indices.</span>
<span class="sd">        Using given indices to gather slices from a input tensor with a specified shape.</span>
<span class="sd">        input tensor&#39;s shape is :math:`(N,*)` where :math:`*` means any number of additional dimensions. For convenience</span>
<span class="sd">        define it as `input_x`, the variable `input_x` refers to input tensor.</span>
<span class="sd">        `indices` is an K-dimensional integer tensor. Suppose that it is a (K-1)-dimensional tensor and each element</span>
<span class="sd">        of it defines a slice of input tensor:</span>

<span class="sd">        .. math::</span>
<span class="sd">            output[(i_0, ..., i_{K-2})] = input\_x[indices[(i_0, ..., i_{K-2})]]</span>

<span class="sd">        The last dimension of `indices` can not more than the rank of input tensor:</span>
<span class="sd">        :math:`indices.shape[-1] &lt;= input\_x.rank`.</span>

<span class="sd">        Args:</span>
<span class="sd">            indices (Tensor): The index tensor that gets the collected elements, with int32 or int64 data type.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same type as input tensor and the shape is</span>
<span class="sd">            :math:`indices\_shape[:-1] + input\_x\_shape[indices\_shape[-1]:]`.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If length of shape of input tensor is less than the last dimension of `indices`.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; input_x = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; indices = Tensor(np.array([[0, 0], [1, 1]]), mindspore.int32)</span>
<span class="sd">            &gt;&gt;&gt; output = input_x.gather_nd(indices)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [-0.1  0.5]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor_</span><span class="p">,),</span> <span class="s1">&#39;Tensor.gather_nd&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;gather_nd&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.gather"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.gather.html#mindspore.Tensor.gather">[docs]</a>    <span class="k">def</span> <span class="nf">gather</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_indices</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the slice of the input tensor corresponding to the elements of `input_indices` on the specified `axis`.</span>
<span class="sd">        The shape of input tensor is :math:`(x_1, x_2, ..., x_R)`. For convenience, define it as `input_params`,</span>
<span class="sd">        the variable `input_params` refers to input tensor.</span>

<span class="sd">        Note:</span>
<span class="sd">            1. The value of `input_indices` must be in the range of `[0, input_param.shape[axis])`, the result</span>
<span class="sd">               is undefined out of range.</span>
<span class="sd">            2. The data type of `input_params` cannot be</span>
<span class="sd">               `bool_ &lt;https://www.mindspore.cn/docs/en/r1.9/api_python/mindspore.html#mindspore.dtype&gt;`_ on Ascend</span>
<span class="sd">               platform currently.</span>

<span class="sd">        Args:</span>
<span class="sd">            input_indices (Tensor): Index tensor to be sliced, the shape of tensor is :math:`(y_1, y_2, ..., y_S)`.</span>
<span class="sd">                Specifies the indices of elements of the original Tensor. The data type can be int32 or int64.</span>
<span class="sd">            axis (int): Specifies the dimension index to gather indices.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, the shape of tensor is</span>
<span class="sd">            :math:`input\_params.shape[:axis] + input\_indices.shape + input\_params.shape[axis + 1:]`.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `axis` is not an int.</span>
<span class="sd">            TypeError: If `input_indices` is not a tensor of type int.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; # case1: input_indices is a Tensor with shape (5, ).</span>
<span class="sd">            &gt;&gt;&gt; input_params = Tensor(np.array([1, 2, 3, 4, 5, 6, 7]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; input_indices = Tensor(np.array([0, 2, 4, 2, 6]), mindspore.int32)</span>
<span class="sd">            &gt;&gt;&gt; axis = 0</span>
<span class="sd">            &gt;&gt;&gt; output = input_params.gather(input_indices, axis)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [1. 3. 5. 3. 7.]</span>
<span class="sd">            &gt;&gt;&gt; # case2: input_indices is a Tensor with shape (2, 2). When the input_params has one dimension,</span>
<span class="sd">            &gt;&gt;&gt; # the output shape is equal to the input_indices shape.</span>
<span class="sd">            &gt;&gt;&gt; input_indices = Tensor(np.array([[0, 2], [2, 6]]), mindspore.int32)</span>
<span class="sd">            &gt;&gt;&gt; axis = 0</span>
<span class="sd">            &gt;&gt;&gt; output = input_params.gather(input_indices, axis)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [[ 1. 3.]</span>
<span class="sd">             [ 3. 7.]]</span>
<span class="sd">            &gt;&gt;&gt; # case3: input_indices is a Tensor with shape (2, ) and</span>
<span class="sd">            &gt;&gt;&gt; # input_params is a Tensor with shape (3, 4) and axis is 0.</span>
<span class="sd">            &gt;&gt;&gt; input_params = Tensor(np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; input_indices = Tensor(np.array([0, 2]), mindspore.int32)</span>
<span class="sd">            &gt;&gt;&gt; axis = 0</span>
<span class="sd">            &gt;&gt;&gt; output = input_params.gather(input_indices, axis)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [[1.  2.  3.  4.]</span>
<span class="sd">             [9. 10. 11. 12.]]</span>
<span class="sd">            &gt;&gt;&gt; # case4: input_indices is a Tensor with shape (2, ) and</span>
<span class="sd">            &gt;&gt;&gt; # input_params is a Tensor with shape (3, 4) and axis is 1.</span>
<span class="sd">            &gt;&gt;&gt; input_params = Tensor(np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; input_indices = Tensor(np.array([0, 2]), mindspore.int32)</span>
<span class="sd">            &gt;&gt;&gt; axis = 1</span>
<span class="sd">            &gt;&gt;&gt; output = input_params.gather(input_indices, axis)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [[1.  3.]</span>
<span class="sd">             [5.  7.]</span>
<span class="sd">             [9. 11.]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_is_int</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="s1">&#39;axis&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;gather&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_indices</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.var"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.var.html#mindspore.Tensor.var">[docs]</a>    <span class="k">def</span> <span class="nf">var</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute the variance along the specified axis.</span>

<span class="sd">        The variance is the average of the squared deviations from the mean, i.e.,</span>
<span class="sd">        :math:`var = mean(abs(x - x.mean())**2)`.</span>

<span class="sd">        Return the variance, which is computed for the flattened array by default,</span>
<span class="sd">        otherwise over the specified axis.</span>

<span class="sd">        Note:</span>
<span class="sd">            Numpy arguments `dtype`, `out` and `where` are not supported.</span>

<span class="sd">        Args:</span>
<span class="sd">            axis (Union[None, int, tuple(int)]): Axis or axes along which the variance is computed.</span>
<span class="sd">                The default is to compute the variance of the flattened array. Default: `None`.</span>
<span class="sd">            ddof (int): Means Delta Degrees of Freedom. Default: 0.</span>
<span class="sd">                The divisor used in calculations is :math:`N - ddof`, where :math:`N` represents the number of elements.</span>
<span class="sd">            keepdims (bool): Default: `False`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Variance tensor.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        See also:</span>
<span class="sd">            :func:`mindspore.Tensor.mean`: Reduce a dimension of a tensor by averaging all elements in the dimension.</span>

<span class="sd">            :func:`mindspore.Tensor.std`: Compute the standard deviation along the specified axis.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; input_x = Tensor(np.array([1., 2., 3., 4.], np.float32))</span>
<span class="sd">            &gt;&gt;&gt; output = input_x.var()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            1.25</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="mi">0</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="s1">&#39;nan&#39;</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ddof</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For &#39;Tensor.var&#39;, the type of the argument &#39;ddof&#39; must be int, but got &quot;</span>
                            <span class="s2">&quot;</span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">ddof</span><span class="p">)))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">keepdims</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For &#39;Tensor.var&#39;, the type of the argument &#39;keepdims&#39; must be bool, but &quot;</span>
                            <span class="s2">&quot;got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">keepdims</span><span class="p">)))</span>

        <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">axis</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">axis</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_and_canonicalize_axes</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
        <span class="n">x_mean</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;mean&#39;</span><span class="p">)(</span><span class="kc">True</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
        <span class="n">x_sub</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;__sub__&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_mean</span><span class="p">)</span>
        <span class="n">x_pow</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;__pow__&#39;</span><span class="p">)(</span><span class="n">x_sub</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">x_sum</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;sum&#39;</span><span class="p">)(</span><span class="nb">bool</span><span class="p">(</span><span class="n">keepdims</span><span class="p">))(</span><span class="n">x_pow</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
        <span class="n">nums</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="o">==</span> <span class="p">():</span>
            <span class="n">nums</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">axis</span><span class="p">:</span>
                <span class="n">nums</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">ax</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;__truediv__&#39;</span><span class="p">)(</span><span class="n">x_sum</span><span class="p">,</span> <span class="n">nums</span> <span class="o">-</span> <span class="n">ddof</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.std"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.std.html#mindspore.Tensor.std">[docs]</a>    <span class="k">def</span> <span class="nf">std</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute the standard deviation along the specified axis.</span>

<span class="sd">        The standard deviation is the square root of the average of the squared deviations</span>
<span class="sd">        from the mean, i.e., :math:`std = sqrt(mean(abs(x - x.mean())**2))`.</span>

<span class="sd">        Return the standard deviation, which is computed for the flattened array by default,</span>
<span class="sd">        otherwise over the specified axis.</span>

<span class="sd">        Note:</span>
<span class="sd">            Numpy arguments `dtype`, `out` and `where` are not supported.</span>

<span class="sd">        Args:</span>
<span class="sd">            axis (Union[None, int, tuple(int)]): Axis or axes along which the standard</span>
<span class="sd">                deviation is computed. Default: `None`.</span>

<span class="sd">                If `None`, compute the standard deviation of the flattened array.</span>
<span class="sd">            ddof (int): Means Delta Degrees of Freedom. The divisor used in calculations is :math:`N - ddof`,</span>
<span class="sd">                where :math:`N` represents the number of elements. Default: 0.</span>
<span class="sd">            keepdims: Default: `False`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Standard deviation tensor.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        See also:</span>
<span class="sd">            :func:`mindspore.Tensor.mean`: Reduce a dimension of a tensor by averaging all elements in the dimension.</span>

<span class="sd">            :func:`mindspore.Tensor.var`: Compute the variance along the specified axis.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3, 4], dtype=np.float32))</span>
<span class="sd">            &gt;&gt;&gt; output = input_x.std()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            1.118034</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x_var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">ddof</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;__pow__&#39;</span><span class="p">)(</span><span class="n">x_var</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.sum"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.sum.html#mindspore.Tensor.sum">[docs]</a>    <span class="k">def</span> <span class="nf">sum</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">initial</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return sum of tensor elements over a given axis.</span>

<span class="sd">        Note:</span>
<span class="sd">            Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and</span>
<span class="sd">            `extobj` are not supported.</span>

<span class="sd">        Args:</span>
<span class="sd">            axis (Union[None, int, tuple(int)]): Axis or axes along which a sum is performed. Default: None.</span>
<span class="sd">                If None, sum all the elements of the input tensor.</span>
<span class="sd">                If the axis is negative, it counts from the last to the first axis.</span>
<span class="sd">                If the axis is a tuple of ints, a sum is performed on all the axes specified in the tuple</span>
<span class="sd">                instead of a single axis or all the axes as before.</span>
<span class="sd">            dtype (:class:`mindspore.dtype`, optional): defaults to None. Overrides the dtype of the</span>
<span class="sd">                output Tensor.</span>
<span class="sd">            keepdims (bool): If this is set to True, the axes which are reduced are left in the result as</span>
<span class="sd">                dimensions with size one. With this option, the result will broadcast correctly against the input array.</span>
<span class="sd">                If the default value is passed, then keepdims will not be passed through to the sum method of</span>
<span class="sd">                sub-classes of ndarray, however any non-default value will be. If the sub-class method does not</span>
<span class="sd">                implement keepdims any exceptions will be raised. Default: `False`.</span>
<span class="sd">            initial (scalar): Starting value for the sum. Default: `None`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor. A tensor with the same shape as input, with the specified axis removed.</span>
<span class="sd">            If the input tensor is a 0-d array, or if the axis is None, a scalar is returned.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If input is not array_like, or `axis` is not int or tuple of ints,</span>
<span class="sd">                or `keepdims` is not integer, or `initial` is not scalar.</span>
<span class="sd">            ValueError: If any axis is out of range or duplicate axes exist.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        See also:</span>
<span class="sd">            :func:`mindspore.Tensor.cumsum`: Return the cumulative sum of the elements along a given axis.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; input_x = Tensor(np.array([-1, 0, 1]).astype(np.float32))</span>
<span class="sd">            &gt;&gt;&gt; print(input_x.sum())</span>
<span class="sd">            0.0</span>
<span class="sd">            &gt;&gt;&gt; input_x = Tensor(np.arange(10).reshape(2, 5).astype(np.float32))</span>
<span class="sd">            &gt;&gt;&gt; print(input_x.sum(axis=1))</span>
<span class="sd">            [10. 35.]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">input_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span> <span class="k">else</span> <span class="bp">self</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">input_x</span><span class="o">.</span><span class="n">dtype</span> <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">dtype</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">keepdims</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For &#39;Tensor.sum&#39;, the type of the argument &#39;keepdims&#39; must be int, but &quot;</span>
                            <span class="s2">&quot;got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">keepdims</span><span class="p">)))</span>
        <span class="k">if</span> <span class="n">initial</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initial</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For &#39;Tensor.sum&#39;, when the argument &#39;initial&#39; is not None, it must be int, &quot;</span>
                            <span class="s2">&quot;float or bool, but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">initial</span><span class="p">)))</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">axis</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">axis</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_and_canonicalize_axes</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_type_support</span><span class="p">(</span><span class="n">input_x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="s1">&#39;GPU&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)):</span>
            <span class="n">input_x</span> <span class="o">=</span> <span class="n">input_x</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="k">if</span> <span class="mi">0</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="n">input_x</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;make_tensor&#39;</span><span class="p">)([</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;sum&#39;</span><span class="p">)(</span><span class="nb">bool</span><span class="p">(</span><span class="n">keepdims</span><span class="p">))(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">initial</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">+=</span> <span class="n">initial</span>
        <span class="k">return</span> <span class="n">res</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.repeat"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.repeat.html#mindspore.Tensor.repeat">[docs]</a>    <span class="k">def</span> <span class="nf">repeat</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">repeats</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Repeat elements of a tensor.</span>

<span class="sd">        Args:</span>
<span class="sd">            repeats (Union[int, tuple, list]): The number of repetitions for each element.</span>
<span class="sd">                `repeats` is broadcasted to fit the shape of the given axis.</span>
<span class="sd">            axis (int, optional): The axis along which to repeat values. By default,</span>
<span class="sd">                use the flattened input tensor, and return a flat output tensor. Default: None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same shape as input tensor except along the given axis.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the axis is out of range.</span>
<span class="sd">            TypeError: If arguments have types not specified above.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        See also:</span>
<span class="sd">            :func:`mindspore.Tensor.reshape`: Give a new shape to a tensor without changing its data.</span>

<span class="sd">            :func:`mindspore.Tensor.resize`: Changes shape and size of tensor in-place.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array(3))</span>
<span class="sd">            &gt;&gt;&gt; print(x.repeat(4))</span>
<span class="sd">            [3 3 3 3]</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([[1, 2],[3, 4]]))</span>
<span class="sd">            &gt;&gt;&gt; print(x.repeat(2))</span>
<span class="sd">            [1 1 2 2 3 3 4 4]</span>
<span class="sd">            &gt;&gt;&gt; print(x.repeat(3, axis=1))</span>
<span class="sd">            [[1 1 1 2 2 2]</span>
<span class="sd">            [3 3 3 4 4 4]]</span>
<span class="sd">            &gt;&gt;&gt; print(x.repeat([1,2], axis=0))</span>
<span class="sd">            [[1 2]</span>
<span class="sd">            [3 4]</span>
<span class="sd">            [3 4]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">repeats</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
            <span class="n">repeats</span> <span class="o">=</span> <span class="p">(</span><span class="n">repeats</span><span class="p">,)</span>
        <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">element</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">repeats</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">element</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;Tensor.repeat&#39;, each element in </span><span class="si">{</span><span class="n">repeats</span><span class="si">}</span><span class="s2"> should be int, but got &quot;</span>
                                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">element</span><span class="p">)</span><span class="si">}</span><span class="s2"> at index </span><span class="si">{</span><span class="n">index</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">input_x</span> <span class="o">=</span> <span class="bp">self</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">input_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
            <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;Tensor.repeat&#39;, the argument &#39;axis&#39; should be int, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_axis_in_range</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">input_x</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span> <span class="o">+</span> <span class="n">input_x</span><span class="o">.</span><span class="n">ndim</span> <span class="k">if</span> <span class="n">axis</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">axis</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">repeats</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">repeats</span> <span class="o">=</span> <span class="n">repeats</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">repeats</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">Tensor_</span><span class="p">(</span><span class="n">input_x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
            <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;repeat_elements&#39;</span><span class="p">)(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">repeats</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
        <span class="n">size</span> <span class="o">=</span> <span class="n">input_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">repeats</span><span class="p">)</span> <span class="o">!=</span> <span class="n">size</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;Tensor.repeat&#39;, the length of &#39;repeats&#39; must be the same as the shape of the &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;original tensor in the &#39;axis&#39; dimension, but got the length of &#39;repeats&#39; &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">repeats</span><span class="p">)</span><span class="si">}</span><span class="s2">, the shape of the original tensor in the &#39;axis&#39; dimension </span><span class="si">{</span><span class="n">size</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">subs</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;split&#39;</span><span class="p">)(</span><span class="n">axis</span><span class="p">,</span> <span class="n">size</span><span class="p">)(</span><span class="n">input_x</span><span class="p">)</span>
        <span class="n">repeated_subs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">sub</span><span class="p">,</span> <span class="n">rep</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">subs</span><span class="p">,</span> <span class="n">repeats</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">rep</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">repeated_subs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;repeat_elements&#39;</span><span class="p">)(</span><span class="n">sub</span><span class="p">,</span> <span class="n">rep</span><span class="p">,</span> <span class="n">axis</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;concatenate&#39;</span><span class="p">)(</span><span class="n">axis</span><span class="p">)(</span><span class="n">repeated_subs</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.bernoulli"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.bernoulli.html#mindspore.Tensor.bernoulli">[docs]</a>    <span class="k">def</span> <span class="nf">bernoulli</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">seed</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Randomly set the elements of output to 0 or 1 with the probability of P which follows the Bernoulli</span>
<span class="sd">        distribution.</span>

<span class="sd">        .. math::</span>

<span class="sd">            out_{i} \sim Bernoulli(p_{i})</span>

<span class="sd">        Args:</span>
<span class="sd">            p (Union[Tensor, float], optional): The shape of p need to be broadcast. Data type must be float32 or</span>
<span class="sd">                                                float64. The elements of p represent the probability of setting 1 for</span>
<span class="sd">                                                the corresponding broadcast position of the current Tensor. The value</span>
<span class="sd">                                                of `p` must be in the range `[0, 1]`. Default: 0.5.</span>
<span class="sd">            seed (int, optional): The seed value for random generating. The value of `seed` must be -1 or a positive</span>
<span class="sd">                                  integer. Default: -1, which means using the current timestamp.</span>

<span class="sd">        Returns:</span>
<span class="sd">            output (Tensor), with the same shape and type as x.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If dtype of Tensor is not one of: int8, uint8, int16, int32, int64, bool, float32, float64.</span>
<span class="sd">            TypeError: If dtype of `p` is not one of: float32, float64.</span>
<span class="sd">            TypeError: If dtype of `seed` is not int.</span>
<span class="sd">            ValueError: If `p` is not in range [0, 1].</span>
<span class="sd">            ValueError: If `seed` is less than 0 and not -1.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``GPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import mindspore</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3]), mindspore.int8)</span>
<span class="sd">            &gt;&gt;&gt; output = input_x.bernoulli(p=1.0)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [1 1 1]</span>
<span class="sd">            &gt;&gt;&gt; input_p = Tensor(np.array([0.0, 1.0, 1.0]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = input_x.bernoulli(input_p)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [0 1 1]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_is_int</span><span class="p">(</span><span class="n">seed</span><span class="p">,</span> <span class="s1">&#39;seed&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;bernoulli&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.random_categorical"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.random_categorical.html#mindspore.Tensor.random_categorical">[docs]</a>    <span class="k">def</span> <span class="nf">random_categorical</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_sample</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generates random samples from a given categorical distribution tensor.</span>

<span class="sd">        Args:</span>
<span class="sd">            num_sample (int): Number of sample to be drawn. Only constant values is allowed.</span>
<span class="sd">            seed (int): Random seed.Only constant values is allowed. Default: 0</span>
<span class="sd">            dtype (mindspore.dtype): The type of output. Its value must be one of mindspore.int16,</span>
<span class="sd">                mindspore.int32 and mindspore.int64. Default: mindspore.int64.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, the output Tensor with shape :math:`(batch\_size, num\_samples)`.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `dtype` is not one of the following: mindspore.int16, mindspore.int32, mindspore.int64.</span>
<span class="sd">            TypeError: If `logits` is not a Tensor.</span>
<span class="sd">            TypeError: If neither `num_sample` nor `seed` is an int.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; import mindspore.common.dtype as mstype</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.random.random((10, 5)).astype(np.float32), mstype.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = x.random_categorical(8)</span>
<span class="sd">            &gt;&gt;&gt; result = output.shape</span>
<span class="sd">            &gt;&gt;&gt; print(result)</span>
<span class="sd">            (10, 8)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_is_int</span><span class="p">(</span><span class="n">num_sample</span><span class="p">,</span> <span class="s1">&#39;num_sample&#39;</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_is_int</span><span class="p">(</span><span class="n">seed</span><span class="p">,</span> <span class="s1">&#39;seed&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;random_categorical&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_sample</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.masked_select"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.masked_select.html#mindspore.Tensor.masked_select">[docs]</a>    <span class="k">def</span> <span class="nf">masked_select</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a new 1-D Tensor which indexes the self tensor according to the boolean `mask`.</span>
<span class="sd">        The shapes of the `mask` tensor and the self tensor don&#39;t need to match, but they must be broadcastable.</span>

<span class="sd">        Args:</span>
<span class="sd">            mask (Tensor[bool]): The boolean Tensor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A 1-D Tensor, with the same type as self.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `mask` is not a bool Tensor.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3, 4]), mindspore.int64)</span>
<span class="sd">            &gt;&gt;&gt; mask = Tensor(np.array([1, 0, 1, 0]), mindspore.bool_)</span>
<span class="sd">            &gt;&gt;&gt; output = x.masked_select(mask)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [1 3]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;masked_select&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.gather_elements"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.gather_elements.html#mindspore.Tensor.gather_elements">[docs]</a>    <span class="k">def</span> <span class="nf">gather_elements</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gathers elements along an axis specified by dim.</span>

<span class="sd">        For a 3-D tensor, the output is:</span>

<span class="sd">        .. code-block::</span>

<span class="sd">            output[i][j][k] = x[index[i][j][k]][j][k]  # if dim == 0</span>

<span class="sd">            output[i][j][k] = x[i][index[i][j][k]][k]  # if dim == 1</span>

<span class="sd">            output[i][j][k] = x[i][j][index[i][j][k]]  # if dim == 2</span>

<span class="sd">        `x` and `index` have the same length of dimensions, and all dimensions except `dim` have the same size.</span>
<span class="sd">        If `dim` = i, `x` is an n-D tensor with shape :math:`(z_0, z_1, ..., z_i, ..., z_{n-1})`,</span>
<span class="sd">        the `index` must be an n-D tensor with shape :math:`(z_0, z_1, ..., y, ..., z_{n-1})`</span>
<span class="sd">        where `y`&gt;=1 and the output will have the same shape with `index`.</span>

<span class="sd">        Args:</span>
<span class="sd">            dim (int): The axis along which to index. It must be int32 or int64.</span>
<span class="sd">                The value range is [-self.ndim, self.ndim).</span>
<span class="sd">            index (Tensor): The indices of elements to gather. It can be one of the following data types:</span>
<span class="sd">                int32, int64. The value range of each index element is [-self.shape(dim), self.shape(dim)).</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same shape as index tensor, the shape of tensor is :math:`(z_1, z_2, ..., z_{n-1})`,</span>
<span class="sd">            and has the same data type with `self.dtype`.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If dtype of `dim` or `index` is neither int32 nor int64.</span>
<span class="sd">            ValueError: If length of shape of `self` is not equal to length of shape of `index`.</span>
<span class="sd">            ValueError: If the size of the dimension except `dim` is not equal between `self` and `index`.</span>
<span class="sd">            ValueError: If the value of `dim` is not in the expected range.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; import mindspore</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([[1, 2], [3, 4]]), mindspore.int32)</span>
<span class="sd">            &gt;&gt;&gt; index = Tensor(np.array([[0, 0], [1, 0]]), mindspore.int32)</span>
<span class="sd">            &gt;&gt;&gt; dim = 1</span>
<span class="sd">            &gt;&gt;&gt; output = x.gather_elements(dim, index)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [[1 1]</span>
<span class="sd">             [4 3]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;index&#39;</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor_</span><span class="p">,),</span> <span class="s1">&#39;Tensor.gather_elements&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;gather_elements&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.nonzero"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.nonzero.html#mindspore.Tensor.nonzero">[docs]</a>    <span class="k">def</span> <span class="nf">nonzero</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return a Tensor of the positions of all non-zero values.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, a 2-D Tensor whose data type is int64, containing the positions of all non-zero values of the input.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``GPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([[[1,  0], [-5, 0]]]), mindspore.int32)</span>
<span class="sd">            &gt;&gt;&gt; output = x.nonzero()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [[0 0 0]</span>
<span class="sd">             [0 1 0]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;nonzero&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.svd"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.svd.html#mindspore.Tensor.svd">[docs]</a>    <span class="k">def</span> <span class="nf">svd</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">compute_uv</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes the singular value decompositions of one or more matrices.</span>

<span class="sd">        Refer to :func:`mindspore.ops.svd` for more detail.</span>

<span class="sd">        Args:</span>
<span class="sd">            full_matrices (bool, optional): If true, compute full-sized :math:`U` and :math:`V`. If false, compute</span>
<span class="sd">                                            only the leading P singular vectors. P is the minimum of M and N.</span>
<span class="sd">                                            M, N is the row, col of the input matrix. Default: False.</span>
<span class="sd">            compute_uv (bool, optional): If true, compute the left and right singular vectors.</span>
<span class="sd">                                         If false, compute only the singular values. Default: True.</span>

<span class="sd">        Returns:</span>
<span class="sd">            - **s**  (Tensor) - Singular values. The shape is :math:`(*, P)`.</span>
<span class="sd">            - **u**  (Tensor) - Left singular vectors. If compute_uv is False, u will not be returned.</span>
<span class="sd">              The shape is :math:`(*, M, P)`. If full_matrices is True, the shape will be :math:`(*, M, M)`.</span>
<span class="sd">            - **v**  (Tensor) - Right singular vectors. If compute_uv is False, v will not be returned.</span>
<span class="sd">              The shape is :math:`(*, P, N)`. If full_matrices is True, the shape will be :math:`(*, N, N)`.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If full_matrices or compute_uv is not the type of bool.</span>
<span class="sd">            TypeError: If the rank of input less than 2.</span>
<span class="sd">            TypeError: If the type of input is not one of the following dtype: mstype.float32, mstype.float64.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor, set_context</span>
<span class="sd">            &gt;&gt;&gt; set_context(device_target=&quot;CPU&quot;)</span>
<span class="sd">            &gt;&gt;&gt; a = Tensor(np.array([[1, 2], [-4, -5], [2, 1]]).astype(np.float32))</span>
<span class="sd">            &gt;&gt;&gt; s, u, v = a.svd(full_matrices=True, compute_uv=True)</span>
<span class="sd">            &gt;&gt;&gt; print(s)</span>
<span class="sd">            [7.0652843 1.040081 ]</span>
<span class="sd">            &gt;&gt;&gt; print(u)</span>
<span class="sd">            [[ 0.30821905 -0.48819482 0.81649697]</span>
<span class="sd">             [-0.90613353  0.11070572 0.40824813]</span>
<span class="sd">             [ 0.2896955   0.8656849  0.4082479 ]]</span>
<span class="sd">            &gt;&gt;&gt; print(v)</span>
<span class="sd">            [[ 0.63863593 0.769509  ]</span>
<span class="sd">             [ 0.769509  -0.63863593]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">svd_op</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;svd&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">compute_uv</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">svd_op</span><span class="p">(</span><span class="n">full_matrices</span><span class="p">,</span> <span class="n">compute_uv</span><span class="p">)(</span><span class="bp">self</span><span class="p">)</span>

        <span class="n">s</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">svd_op</span><span class="p">(</span><span class="n">full_matrices</span><span class="p">,</span> <span class="n">compute_uv</span><span class="p">)(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">s</span></div>

<div class="viewcode-block" id="Tensor.hardshrink"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.hardshrink.html#mindspore.Tensor.hardshrink">[docs]</a>    <span class="k">def</span> <span class="nf">hardshrink</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Apply the Hard Shrink function for tensor. Calculates the output according to the input elements.</span>

<span class="sd">        The formula is defined as follows:</span>

<span class="sd">        .. math::</span>
<span class="sd">            \text{HardShrink}(x) =</span>
<span class="sd">            \begin{cases}</span>
<span class="sd">            x, &amp; \text{ if } x &gt; \lambda \\</span>
<span class="sd">            x, &amp; \text{ if } x &lt; -\lambda \\</span>
<span class="sd">            0, &amp; \text{ otherwise }</span>
<span class="sd">            \end{cases}</span>

<span class="sd">        Args:</span>
<span class="sd">            lambd (float): The threshold :math:`\lambda` defined by the Hard Shrink formula. Default: 0.5.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same shape and data type as self.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `lambd` is not a float.</span>
<span class="sd">            TypeError: If dtype of the input tensor is neither float16 nor float32.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([[0.5,  1,  2.0], [0.0533, 0.0776, -2.1233]]), ms.float32)</span>
<span class="sd">            &gt;&gt;&gt; print(x.hardshrink())</span>
<span class="sd">            [[ 0.      1.      2.    ]</span>
<span class="sd">            [ 0.      0.     -2.1233]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;hardshrink&#39;</span><span class="p">)(</span><span class="n">lambd</span><span class="p">)(</span><span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.soft_shrink"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.soft_shrink.html#mindspore.Tensor.soft_shrink">[docs]</a>    <span class="k">def</span> <span class="nf">soft_shrink</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Apply the soft shrink function for a tensor. Calculates the output according to the input elements.</span>

<span class="sd">        The formula is defined as follows:</span>

<span class="sd">        .. math::</span>
<span class="sd">            \text{SoftShrink}(x) =</span>
<span class="sd">            \begin{cases}</span>
<span class="sd">            x - \lambda, &amp; \text{ if } x &gt; \lambda \\</span>
<span class="sd">            x + \lambda, &amp; \text{ if } x &lt; -\lambda \\</span>
<span class="sd">            0, &amp; \text{ otherwise }</span>
<span class="sd">            \end{cases}</span>

<span class="sd">        Args:</span>
<span class="sd">            lambd(float): the :math:`\lambda` must be no less than zero. Default: 0.5.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same shape and data type as self.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If lambd is not a float.</span>
<span class="sd">            TypeError: If input_x is not a Tensor.</span>
<span class="sd">            TypeError: If dtype of input_x is neither float16 nor float32.</span>
<span class="sd">            ValueError: If lambd is less than 0.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; a = Tensor([[ 0.5297,  0.7871,  1.1754], [ 0.7836,  0.6218, -1.1542]]).astype(&quot;float32&quot;)</span>
<span class="sd">            &gt;&gt;&gt; output = a.soft_shrink()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [[ 0.02979  0.287    0.676  ]</span>
<span class="sd">             [ 0.2837   0.1216  -0.6543 ]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;soft_shrink&#39;</span><span class="p">)(</span><span class="n">lambd</span><span class="p">)(</span><span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.to_coo"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.to_coo.html#mindspore.Tensor.to_coo">[docs]</a>    <span class="k">def</span> <span class="nf">to_coo</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Convert a Tensor to COOTensor.</span>

<span class="sd">        Note:</span>
<span class="sd">            Only 2-D tensor is supported for now.</span>

<span class="sd">        Returns:</span>
<span class="sd">            COOTensor, a sparse representation of the original dense tensor, containing the following parts.</span>

<span class="sd">            - indices (Tensor): 2-D integer tensor, indicates the positions of `values` of the dense tensor.</span>
<span class="sd">            - values (Tensor): 1-D tensor, indicates the non-zero values of the dense tensor.</span>
<span class="sd">            - shape (tuple(int)): the shape of the COOTensor, is the same as the original dense tensor.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If input tensor is not 2-D.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``GPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; import mindspore</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([[1,  0], [-5, 0]]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = x.to_coo()</span>
<span class="sd">            &gt;&gt;&gt; print(output.indices, output.values, output.shape)</span>
<span class="sd">            [[0 0]</span>
<span class="sd">             [1 0]] [ 1. -5.] (2, 2)</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;dense_to_sparse_coo&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.to_csr"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.to_csr.html#mindspore.Tensor.to_csr">[docs]</a>    <span class="k">def</span> <span class="nf">to_csr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Convert a Tensor to CSRTensor.</span>

<span class="sd">        Note:</span>
<span class="sd">            Only 2-D tensor is supported for now.</span>

<span class="sd">        Returns:</span>
<span class="sd">            CSRTensor, a sparse representation of the original dense tensor, containing the following parts.</span>

<span class="sd">            - indptr (Tensor): 1-D integer tensor, indicates the start and end point for `values` in each row.</span>
<span class="sd">            - indices (Tensor): 1-D integer tensor, indicates the column positions of all non-zero values of the input.</span>
<span class="sd">            - values (Tensor): 1-D tensor, indicates the non-zero values of the dense tensor.</span>
<span class="sd">            - shape (tuple(int)): the shape of the CSRTensor, is the same as the original dense tensor.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If input tensor is not 2-D.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``GPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; import mindspore</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([[1,  0], [-5, 0]]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = x.to_csr()</span>
<span class="sd">            &gt;&gt;&gt; print(output.indptr, output.indices, output.values, output.shape)</span>
<span class="sd">            [0 1 2] [0 0] [ 1. -5.] (2, 2)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;dense_to_sparse_csr&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.unsorted_segment_min"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.unsorted_segment_min.html#mindspore.Tensor.unsorted_segment_min">[docs]</a>    <span class="k">def</span> <span class="nf">unsorted_segment_min</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes the minimum of a tensor along segments.</span>

<span class="sd">        Note:</span>
<span class="sd">            - If the segment_id i is absent in the segment_ids, then output[i] will be filled with</span>
<span class="sd">              the maximum value of the type of self.</span>
<span class="sd">            - The `segment_ids` must be non-negative tensor.</span>

<span class="sd">        Args:</span>
<span class="sd">            segment_ids (Tensor): A `1-D` tensor whose shape is :math:`(x_1)`,</span>
<span class="sd">                                  the value must be non-negative tensor. The data type must be int32.</span>
<span class="sd">            num_segments (int): The value specifies the number of distinct `segment_ids`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, set the number of `num_segments` as `N`, the shape is :math:`(N, x_2, ..., x_R)`.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `num_segments` is not an int.</span>
<span class="sd">            ValueError: If length of shape of `segment_ids` is not equal to 1.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [4, 2, 1]]).astype(np.float32))</span>
<span class="sd">            &gt;&gt;&gt; segment_ids = Tensor(np.array([0, 1, 1]).astype(np.int32))</span>
<span class="sd">            &gt;&gt;&gt; num_segments = 2</span>
<span class="sd">            &gt;&gt;&gt; output = x.unsorted_segment_min(segment_ids, num_segments)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [[1. 2. 3.]</span>
<span class="sd">             [4. 2. 1.]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;unsorted_segment_min&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.unsorted_segment_max"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.unsorted_segment_max.html#mindspore.Tensor.unsorted_segment_max">[docs]</a>    <span class="k">def</span> <span class="nf">unsorted_segment_max</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes the maximum along segments of a tensor.</span>

<span class="sd">        Note:</span>
<span class="sd">            - If the segment_id i is absent in the segment_ids, then output[i] will be filled with</span>
<span class="sd">              the minimum value of the type of self.</span>
<span class="sd">            - The `segment_ids` must be non-negative tensor.</span>

<span class="sd">        Args:</span>
<span class="sd">            segment_ids (Tensor): A `1-D` tensor whose shape is :math:`(x_1)`,</span>
<span class="sd">                                  the value must be non-negative tensor. The data type must be int32.</span>
<span class="sd">            num_segments (int): The value specifies the number of distinct `segment_ids`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, set the number of `num_segments` as `N`, the shape is :math:`(N, x_2, ..., x_R)`.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `num_segments` is not an int.</span>
<span class="sd">            ValueError: If length of shape of `segment_ids` is not equal to 1.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [4, 2, 1]]).astype(np.float32))</span>
<span class="sd">            &gt;&gt;&gt; segment_ids = Tensor(np.array([0, 1, 1]).astype(np.int32))</span>
<span class="sd">            &gt;&gt;&gt; num_segments = 2</span>
<span class="sd">            &gt;&gt;&gt; output = x.unsorted_segment_max(segment_ids, num_segments)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [[1. 2. 3.]</span>
<span class="sd">             [4. 5. 6.]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;unsorted_segment_max&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.unsorted_segment_prod"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.unsorted_segment_prod.html#mindspore.Tensor.unsorted_segment_prod">[docs]</a>    <span class="k">def</span> <span class="nf">unsorted_segment_prod</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes the product of a tensor along segments.</span>

<span class="sd">        Note:</span>
<span class="sd">            - If the segment_id i is absent in the segment_ids, then output[i] will be filled with 1.</span>
<span class="sd">            - The `segment_ids` must be non-negative tensor.</span>

<span class="sd">        Args:</span>
<span class="sd">            segment_ids (Tensor): A `1-D` tensor whose shape is :math:`(x_1)`,</span>
<span class="sd">                                  the value must be non-negative tensor. The data type must be int32.</span>
<span class="sd">            num_segments (int): The value specifies the number of distinct `segment_ids`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, set the number of `num_segments` as `N`, the shape is :math:`(N, x_2, ..., x_R)`.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `num_segments` is not an int.</span>
<span class="sd">            ValueError: If length of shape of `segment_ids` is not equal to 1.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [4, 2, 1]]).astype(np.float32))</span>
<span class="sd">            &gt;&gt;&gt; segment_ids = Tensor(np.array([0, 1, 0]).astype(np.int32))</span>
<span class="sd">            &gt;&gt;&gt; num_segments = 2</span>
<span class="sd">            &gt;&gt;&gt; output = x.unsorted_segment_prod(segment_ids, num_segments)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [[4. 4. 3.]</span>
<span class="sd">             [4. 5. 6.]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;unsorted_segment_prod&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.unique_consecutive"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.unique_consecutive.html#mindspore.Tensor.unique_consecutive">[docs]</a>    <span class="k">def</span> <span class="nf">unique_consecutive</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">return_idx</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the elements that are unique in each consecutive group of equivalent elements in the input tensor.</span>

<span class="sd">        Args:</span>
<span class="sd">            return_idx (bool, optional): Whether to return the indices of the end position of each element in the</span>
<span class="sd">                original input list in the returned unique list. Default: False.</span>
<span class="sd">            return_counts (bool, optional): Whether to return the counts of each unique element. Default: False.</span>
<span class="sd">            axis (int, optional): The dimension to apply unique. If None, the unique of the flattened input is</span>
<span class="sd">                returned. If specified, it must be int32 or int64. Default: None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A tensor or a tuple of tensors containing tensor objects (`output`, `idx`, `counts`). `output` has the</span>
<span class="sd">            same type as the input tensor and is used to represent the output list of unique scalar elements. If</span>
<span class="sd">            `return_idx` is True, there will be an additional returned tensor, `idx`, which has the same shape as</span>
<span class="sd">            the inupt tensor and represents the index of where the element in the original input maps to the position</span>
<span class="sd">            in the output. If `return_counts` is True, there will be an additional returned tensor, `counts`, which</span>
<span class="sd">            represents the number of occurrences for each unique value or tensor.</span>

<span class="sd">        Raises:</span>
<span class="sd">            RuntimeError: If `axis` is not in the range of :math:`[-ndim, ndim-1]`.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([1, 1, 2, 2, 3, 1, 1, 2]), mstype.int32)</span>
<span class="sd">            &gt;&gt;&gt; output, idx, counts = x.unique_consecutive(return_idx=True, return_counts=True, axis=None)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [1 2 3 1 2]</span>
<span class="sd">            &gt;&gt;&gt; print(idx)</span>
<span class="sd">            [0 0 1 1 2 3 3 4]</span>
<span class="sd">            &gt;&gt;&gt; print(counts)</span>
<span class="sd">            [2 2 1 2 1]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="n">output</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;unique_consecutive&quot;</span><span class="p">)(</span><span class="n">return_idx</span><span class="p">,</span> <span class="n">return_counts</span><span class="p">,</span> <span class="n">axis</span><span class="p">)(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">return_idx</span> <span class="ow">and</span> <span class="n">return_counts</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">counts</span>
        <span class="k">if</span> <span class="n">return_idx</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">idx</span>
        <span class="k">if</span> <span class="n">return_counts</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">counts</span>
        <span class="k">return</span> <span class="n">output</span></div>

<div class="viewcode-block" id="Tensor.unique_with_pad"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.unique_with_pad.html#mindspore.Tensor.unique_with_pad">[docs]</a>    <span class="k">def</span> <span class="nf">unique_with_pad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pad_num</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns unique elements and relative indexes in 1-D tensor, filled with padding num.</span>

<span class="sd">        The basic function is the same as the Unique operator, but the UniqueWithPad operator adds a Pad function.</span>
<span class="sd">        The returned tuple(`y`, `idx`) after the self tensor is processed by the unique operator,</span>
<span class="sd">        in which the shapes of `y` and `idx` are mostly not equal. Therefore, in order to solve the above situation,</span>
<span class="sd">        the UniqueWithPad operator will fill the `y` Tensor with the `pad_num` specified by the user</span>
<span class="sd">        to make it have the same shape as the Tensor `idx`.</span>

<span class="sd">        Args:</span>
<span class="sd">            pad_num (int): Pad num. The data type is an int.</span>

<span class="sd">        Returns:</span>
<span class="sd">            tuple(Tensor), tuple of 2 tensors, `y` and `idx`.</span>

<span class="sd">            - y (Tensor) - The unique elements filled with pad_num, the shape and data type same as self tensor.</span>
<span class="sd">            - idx (Tensor) - The index of each value of self tensor in the unique output `y`,</span>
<span class="sd">              the shape and data type same as self tensor.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If dtype of self tensor is neither int32 nor int64.</span>
<span class="sd">            ValueError: If length of shape of self tensor is not equal to 1.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([1, 1, 2, 2, 3, 1]), mstype.int32)</span>
<span class="sd">            &gt;&gt;&gt; output, idx = x.unique_with_pad(pad_num=0)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [1 2 3 0 0 0]</span>
<span class="sd">            &gt;&gt;&gt; print(idx)</span>
<span class="sd">            [0 0 1 1 2 0]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;unique_with_pad&quot;</span><span class="p">)()(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pad_num</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.diag"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.diag.html#mindspore.Tensor.diag">[docs]</a>    <span class="k">def</span> <span class="nf">diag</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Constructs a diagonal tensor with a given diagonal values.</span>

<span class="sd">        Assume self tensor has dimensions :math:`[D_1,... D_k]`, the output is a tensor of</span>
<span class="sd">        rank 2k with dimensions :math:`[D_1,..., D_k, D_1,..., D_k]` where:</span>
<span class="sd">        :math:`output[i_1,..., i_k, i_1,..., i_k] = self[i_1,..., i_k]` and 0 everywhere else.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same dtype as self tensor.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If rank of self tensor is less than 1.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor([1, 2, 3, 4]).astype(&#39;int32&#39;)</span>
<span class="sd">            &gt;&gt;&gt; output = x.diag()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [[1 0 0 0]</span>
<span class="sd">             [0 2 0 0]</span>
<span class="sd">             [0 0 3 0]</span>
<span class="sd">             [0 0 0 4]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;diag&#39;</span><span class="p">)()(</span><span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.xdivy"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.xdivy.html#mindspore.Tensor.xdivy">[docs]</a>    <span class="k">def</span> <span class="nf">xdivy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Divides self tensor by the input tensor element-wise. Returns zero when self is zero. The dtype of</span>
<span class="sd">        original Tensor must be one of float, complex or bool. For simplicity, denote the original Tensor by x.</span>

<span class="sd">        .. math::</span>

<span class="sd">            out_i = x_{i}\y_{i}</span>

<span class="sd">        `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">        &#39;y&#39; must be tensor or scalar, when y is tensor, dtypes of x and y cannot be bool at the same time,</span>
<span class="sd">        and the shapes of them could be broadcast. When y is scalar, the scalar can only be a constant.</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Union[Tensor, number.Number, bool]): The second input y is a Number,</span>
<span class="sd">              or a bool when the first input x is a tensor, or a tensor whose data type is float16,</span>
<span class="sd">              float32, float64, complex64, complex128 or bool.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">            and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `y` is not one of the following: Tensor, Number, bool.</span>
<span class="sd">            TypeError: If dtype of self and &#39;y&#39; is not in [float16, float32, float64, complex64, complex128, bool].</span>
<span class="sd">            ValueError: If self could not be broadcast to a tensor with shape of `y`.</span>
<span class="sd">            RuntimeError: If the data type of `y` conversion of Parameter is given</span>
<span class="sd">                          but data type conversion of Parameter is not supported.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([2, 4, -1]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; y = Tensor(np.array([2, 2, 2]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = x.xdivy(y)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [ 1.   2.  -0.5]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;xdivy&quot;</span><span class="p">)()(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.split"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.split.html#mindspore.Tensor.split">[docs]</a>    <span class="k">def</span> <span class="nf">split</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">output_num</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Splits a tensor into output_num of tensors along the given axis and output numbers.</span>

<span class="sd">        The tensor will be split into equally sized sub-tensors.</span>
<span class="sd">        This requires that `self.shape(axis)` is divisible by `output_num`.</span>

<span class="sd">        Args:</span>
<span class="sd">            axis (int): Index of the split position. Default: 0.</span>
<span class="sd">            output_num (int): The number of output tensors. Must be positive int. Default: 1.</span>

<span class="sd">        Returns:</span>
<span class="sd">            tuple[Tensor], the shape of each output tensor is the same, which is</span>
<span class="sd">            :math:`(y_1, y_2, ..., y_S)`. And the data type is the same with the tensor.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `axis` or `output_num` is not an int.</span>
<span class="sd">            ValueError: If `axis` is out of the range [-len(`self.shape`), len(`self.shape`)),</span>
<span class="sd">                or if the `output_num` is less than or equal to 0.</span>
<span class="sd">            ValueError: If `self.shape(axis)` is not divisible by `output_num`.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([[1, 1, 1, 1], [2, 2, 2, 2]]), mindspore.int32)</span>
<span class="sd">            &gt;&gt;&gt; print(x)</span>
<span class="sd">            [[1 1 1 1]</span>
<span class="sd">             [2 2 2 2]]</span>
<span class="sd">            &gt;&gt;&gt; output = x.split(1, 2)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            (Tensor(shape=[2, 2], dtype=Int32, value=</span>
<span class="sd">            [[1, 1],</span>
<span class="sd">             [2, 2]]), Tensor(shape=[2, 2], dtype=Int32, value=</span>
<span class="sd">            [[1, 1],</span>
<span class="sd">             [2, 2]]))</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;split&#39;</span><span class="p">)(</span><span class="n">axis</span><span class="p">,</span> <span class="n">output_num</span><span class="p">)(</span><span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.xlogy"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.xlogy.html#mindspore.Tensor.xlogy">[docs]</a>    <span class="k">def</span> <span class="nf">xlogy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes the self tensor multiplied by the logarithm of input tensor element-wise.</span>
<span class="sd">        Returns zero when self tensor is zero. The data type of the self tensor should be</span>
<span class="sd">        `number &lt;https://www.mindspore.cn/docs/en/r1.9/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">        `bool_ &lt;https://www.mindspore.cn/docs/en/r1.9/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>
<span class="sd">        To make it clear, the following content will use `x` to represent the self tensor.</span>

<span class="sd">        .. math::</span>

<span class="sd">            out_i = x_{i}\ln{y_{i}}</span>

<span class="sd">        Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">        The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">        When the inputs are two tensors,</span>
<span class="sd">        dtypes of them cannot be bool at the same time, and the shapes of them could be broadcast.</span>
<span class="sd">        When the inputs are one tensor and one scalar,</span>
<span class="sd">        the scalar could only be a constant.</span>

<span class="sd">        .. warning::</span>
<span class="sd">            - On Ascend, the data type of `x` and `y` must be float16 or float32.</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Union[Tensor, number.Number, bool]): The `y` input is a number.Number or</span>
<span class="sd">              a bool or a tensor whose data type is number or bool.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">            and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `y` is not a number.Number or a bool or a Tensor.</span>
<span class="sd">            TypeError: If dtype of `x` and &#39;y&#39; is not in [float16, float32, float64, complex64, complex128] .</span>
<span class="sd">            ValueError: If `x` could not be broadcast to a tensor with shape of `y`.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([-5, 0, 4]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; y = Tensor(np.array([2, 2, 2]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; print(x.xlogy(y))</span>
<span class="sd">            [-3.465736   0.        2.7725887]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;xlogy&quot;</span><span class="p">)()(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.erf"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.erf.html#mindspore.Tensor.erf">[docs]</a>    <span class="k">def</span> <span class="nf">erf</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes the Gauss error function of self tensor element-wise.</span>
<span class="sd">        Refer to :func:`mindspore.ops.erf` for more details.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same shap dtype as the self Tensor.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If dtype of self tensor is not float16 or float32.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU``  ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([-1, 0, 1, 2, 3]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = x.erf()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [-0.8427168   0.          0.8427168   0.99530876  0.99997765]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;erf&quot;</span><span class="p">)()(</span><span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.erfc"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.erfc.html#mindspore.Tensor.erfc">[docs]</a>    <span class="k">def</span> <span class="nf">erfc</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes the complementary error function of self tensor element-wise.</span>
<span class="sd">        Refer to :func:`mindspore.ops.erfc` for more details.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same shap dtype as the self tensor.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If dtype of self tensor is not float16 or float32.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU``  ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([-1, 0, 1, 2, 3]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = x.erfc()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [1.8427168e+00 1.0000000e+00 1.5728319e-01 4.6912432e-03 2.2351742e-05]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;erfc&quot;</span><span class="p">)()(</span><span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.top_k"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.top_k.html#mindspore.Tensor.top_k">[docs]</a>    <span class="k">def</span> <span class="nf">top_k</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="nb">sorted</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Finds values and indices of the `k` largest entries along the last dimension.</span>

<span class="sd">        .. warning::</span>
<span class="sd">            - If sorted is set to &#39;False&#39;, it will use the aicpu operator, the performance may be reduced.</span>

<span class="sd">        `input_x` refers to self tensor.</span>

<span class="sd">        If the `input_x` is a one-dimensional Tensor, finds the `k` largest entries in the Tensor,</span>
<span class="sd">        and outputs its value and index as a Tensor. Therefore, values[`k`] is the `k` largest item in `input_x`,</span>
<span class="sd">        and its index is indices [`k`].</span>

<span class="sd">        For a multi-dimensional matrix,</span>
<span class="sd">        calculates the first `k` entries in each row (corresponding vector along the last dimension), therefore:</span>

<span class="sd">        .. math::</span>

<span class="sd">            values.shape = indices.shape = input\_x.shape[:-1] + [k].</span>

<span class="sd">        If the two compared elements are the same, the one with the smaller index value is returned first.</span>

<span class="sd">        Args:</span>
<span class="sd">            k (int): The number of top elements to be computed along the last dimension, constant input is needed.</span>
<span class="sd">            sorted (bool, optional): If true, the obtained elements will be sorted by the values in descending order.</span>
<span class="sd">                Default: True.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple of 2 tensors, the values and the indices.</span>

<span class="sd">            - values (Tensor): The `k` largest elements in each slice of the last dimension.</span>
<span class="sd">            - indices (Tensor): The indices of values within the last dimension of input.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `k` is not an int.</span>
<span class="sd">            TypeError: If `sorted` is not a bool.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU``  ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; input_x = Tensor([1, 2, 3, 4, 5], ms.float16)</span>
<span class="sd">            &gt;&gt;&gt; k = 3</span>
<span class="sd">            &gt;&gt;&gt; values, indices = input_x.top_k(k, sorted=True)</span>
<span class="sd">            &gt;&gt;&gt; print((values, indices))</span>
<span class="sd">            (Tensor(shape=[3], dtype=Float16, value= [ 5.0000e+00,  4.0000e+00,  3.0000e+00]), Tensor(shape=[3],</span>
<span class="sd">              dtype=Int32, value= [4, 3, 2]))</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_is_int</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="s1">&#39;k&#39;</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="nb">sorted</span><span class="p">,</span> <span class="s1">&#39;sorted&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;top_k&quot;</span><span class="p">)(</span><span class="nb">sorted</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.sigmoid"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.sigmoid.html#mindspore.Tensor.sigmoid">[docs]</a>    <span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sigmoid activation function.</span>

<span class="sd">        Computes Sigmoid of input element-wise. The Sigmoid function is defined as:</span>

<span class="sd">        .. math::</span>

<span class="sd">            \text{sigmoid}(x_i) = \frac{1}{1 + \exp(-x_i)}</span>

<span class="sd">        where :math:`x_i` is an element of the self tensor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, with the same type and shape as the self tensor.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If dtype of self tensor is not float16, float32, float64, complex64 or complex128.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3, 4, 5]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = input_x.sigmoid()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [0.7310586  0.880797   0.95257413 0.98201376 0.9933072 ]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">)()(</span><span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.median"><a class="viewcode-back" href="../../../api_python/mindspore/Tensor/mindspore.Tensor.median.html#mindspore.Tensor.median">[docs]</a>    <span class="k">def</span> <span class="nf">median</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">global_median</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes the median of input tensor.</span>

<span class="sd">        .. warning::</span>
<span class="sd">            When attr `global_median` is True, the second output Tensor value is meaningless.</span>

<span class="sd">        Args:</span>
<span class="sd">            global_median (bool): Whether the output tensor is the global median of all input tensor elements or not.</span>
<span class="sd">                Default: False.</span>
<span class="sd">            axis (int): The dimension need to reduce. Default: 0.</span>
<span class="sd">            keep_dims (bool): Whether the output tensor need to retain `axis` dimension or not. Default: False.</span>

<span class="sd">        Returns:</span>
<span class="sd">            y (Tensor), has the same dtype as the self Tensor. If `global_median` is true, the `y` has only one</span>
<span class="sd">            element.  If `keep_dims` is true, the `y` has the same shape as the self Tensor except the shape of</span>
<span class="sd">            `y` in dimension `axis` is size 1. Otherwise, the `y` lacks `axis` dimension than input.</span>

<span class="sd">            indices (Tensor), has the same shape as the `y`, but dtype is int64.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If dtype of self Tensor is not one of the following: int16, int32, int64, float32, float64.</span>
<span class="sd">            TypeError: If `global_median` is not a bool.</span>
<span class="sd">            TypeError: If `axis` is not a int.</span>
<span class="sd">            TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">            ValueError: If `axis` is not in range of [-len(`self.shape`), len(`self.shape`) - 1).</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; # case 1 : common median compute</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([[0.57, 0.11, 0.21],[0.38, 0.50, 0.57], [0.36, 0.16, 0.44]]).astype(np.float32))</span>
<span class="sd">            &gt;&gt;&gt; y = x.median(global_median=False, axis=0, keep_dims=False)</span>
<span class="sd">            &gt;&gt;&gt; print(y)</span>
<span class="sd">            (Tensor(shape=[3], dtype=Float32, value=[0.38, 0.16, 0.44]),</span>
<span class="sd">             Tensor(shape=[3], dtype=Int64, value=[1, 2, 2]))</span>
<span class="sd">            &gt;&gt;&gt; # case 2 : global median compute</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([1, 7, 6],[5, 1, 3],[9, 17, 1]), mindspore.int32)</span>
<span class="sd">            &gt;&gt;&gt; y = x.median(global_median=True)</span>
<span class="sd">            &gt;&gt;&gt; print(y)</span>
<span class="sd">            (Tensor(shape=[1], dtype=Int32, value=[5]), Tensor(shape=[1], dtype=Int64, value=[1]))</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_axis_in_range</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;median&#39;</span><span class="p">)(</span><span class="n">global_median</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)(</span><span class="bp">self</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">addmv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mat</span><span class="p">,</span> <span class="n">vec</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Multiplies matrix `mat` and vector `vec`. Input vector is added to the final result.</span>

<span class="sd">        If `mat` is a :math:`(N, M)` tensor, `vec` is a 1-D tensor of size :math:`M`, then `x` must be broadcastable</span>
<span class="sd">        with a 1-D tensor of size :math:`N` and `out` will be 1-D tensor of size :math:`N`.</span>

<span class="sd">        The optional values `beta` and `alpha` are the matrix-vector product between `mat` and `vec` and the scale</span>
<span class="sd">        factor for the added tensor `x` respectively. If `beta` is 0, then `x` will be ignored.</span>

<span class="sd">        .. math::</span>
<span class="sd">            output = β x + α (mat @ vec)</span>

<span class="sd">        Args:</span>
<span class="sd">            mat (Tensor): The first tensor to be multiplied. The shape of the tensor is :math:`(N, M)`.</span>
<span class="sd">            vec (Tensor): The second tensor to be multiplied. The shape of the tensor is :math:`(M,)`.</span>
<span class="sd">            beta (scalar[int, float, bool], optional): Multiplier for `x` (β). The `beta` must be int or</span>
<span class="sd">                float or bool, Default: 1.</span>
<span class="sd">            alpha (scalar[int, float, bool], optional): Multiplier for `mat` @ `vec` (α). The `alpha` must</span>
<span class="sd">                be int or float or bool, Default: 1.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, the shape of the output tensor is :math:`(N,)`, has the same dtype as `x`.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `mat`, `vec`, `x` is not a Tensor.</span>
<span class="sd">            TypeError: If input tensor and `x`, `mat`, &#39;vec&#39; are not the same dtype.</span>
<span class="sd">            ValueError: If `mat` is not a 2-D Tensor.</span>
<span class="sd">                If `x`, `vec` is not a 1-D Tensor.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([2., 3.]).astype(np.float32))</span>
<span class="sd">            &gt;&gt;&gt; mat = Tensor(np.array([[2., 5., 3.], [4., 2., 2.]]).astype(np.float32))</span>
<span class="sd">            &gt;&gt;&gt; vec = Tensor(np.array([3., 2., 4.]).astype(np.float32))</span>
<span class="sd">            &gt;&gt;&gt; output = x.addmv(mat, vec)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [30. 27.]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;addmv&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mat</span><span class="p">,</span> <span class="n">vec</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">asinh</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes inverse hyperbolic sine of the input element-wise.</span>

<span class="sd">        .. math::</span>
<span class="sd">            out_i = \sinh^{-1}(input_i)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, has the same shape and type as input.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If input is not a Tensor.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([-5.0, 1.5, 3.0, 100.0]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = x.asinh()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [-2.3124382  1.1947632  1.8184465  5.298342 ]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;asinh&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">atan</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes the trigonometric inverse tangent of the input element-wise.</span>

<span class="sd">        .. math::</span>
<span class="sd">            out_i = tan^{-1}(x_i)</span>

<span class="sd">        Returns:</span>
<span class="sd">            A Tensor, has the same type as the input.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If input is not a Tensor.</span>
<span class="sd">            TypeError: If input tensor dtype is not float16 or float32.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([1.0, 0.0]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = x.atan()</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [0.7853982 0.0]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;atan&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">atanh</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes inverse hyperbolic tangent of the input element-wise.</span>

<span class="sd">        .. math::</span>
<span class="sd">            out_i = \tanh^{-1}(x_{i})</span>

<span class="sd">        .. warning::</span>
<span class="sd">            This is an experimental prototype that is subject to change and/or deletion.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A Tensor, has the same type as the input.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If input is not a Tensor.</span>
<span class="sd">            TypeError: If input tensor dtype is not float16 or float32.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([0, -0.5]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = ops.atanh(x)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [ 0. -0.54930615]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;atanh&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">bmm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mat2</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes matrix multiplication between two tensors by batch.</span>

<span class="sd">        .. math::</span>
<span class="sd">            \text{output}[..., :, :] = \text{matrix}(input_x[..., :, :]) * \text{matrix}(mat2[..., :, :])</span>

<span class="sd">        The first input tensor must be not less than `3` and the second input must be not less than `2`.</span>

<span class="sd">        Args:</span>
<span class="sd">            mat2 (Tensor): The tensor to be multiplied. The shape of the tensor is :math:`(*B, C, M)`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, the shape of the output tensor is :math:`(*B, N, M)`.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If length of shape of `input_x` is not equal to length of shape of `mat2` or</span>
<span class="sd">                length of shape of `input_x` is less than `3`.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; input_x = Tensor(np.ones(shape=[2, 4, 1, 3]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; mat2 = Tensor(np.ones(shape=[2, 4, 3, 4]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = input_x.bmm(mat2)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [[[[3. 3. 3. 3.]]</span>
<span class="sd">              [[3. 3. 3. 3.]]</span>
<span class="sd">              [[3. 3. 3. 3.]]</span>
<span class="sd">              [[3. 3. 3. 3.]]]</span>
<span class="sd">             [[[3. 3. 3. 3.]]</span>
<span class="sd">              [[3. 3. 3. 3.]]</span>
<span class="sd">              [[3. 3. 3. 3.]]</span>
<span class="sd">              [[3. 3. 3. 3.]]]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_check</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;bmm&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mat2</span><span class="p">)</span></div>


<div class="viewcode-block" id="RowTensor"><a class="viewcode-back" href="../../../api_python/mindspore/mindspore.RowTensor.html#mindspore.RowTensor">[docs]</a><span class="k">class</span> <span class="nc">RowTensor</span><span class="p">(</span><span class="n">RowTensor_</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A sparse representation of a set of tensor slices at given indices.</span>

<span class="sd">    An RowTensor is typically used to represent a subset of a larger</span>
<span class="sd">    tensor dense of shape [L0, D1, .. , DN] where L0 &gt;&gt; D0.</span>

<span class="sd">    The values in indices are the indices in the first dimension of the slices</span>
<span class="sd">    that have been extracted from the larger tensor.</span>

<span class="sd">    The dense tensor dense represented by an RowTensor slices has</span>
<span class="sd">    `dense[slices.indices[i], :, :, :, ...] = slices.values[i, :, :, :, ...]`.</span>

<span class="sd">    For example, if indices is [0], values is [[1, 2]], shape is</span>
<span class="sd">    (3, 2), then the dense representation of the row tensor will be:</span>

<span class="sd">    .. code-block::</span>

<span class="sd">        [[1, 2],</span>
<span class="sd">         [0, 0],</span>
<span class="sd">         [0, 0]]</span>

<span class="sd">    Note:</span>
<span class="sd">        This is an experimental feature and is subjected to change.</span>

<span class="sd">    Args:</span>
<span class="sd">        indices (Tensor): A 1-D integer Tensor of shape [D0]. Default: None.</span>
<span class="sd">        values (Tensor): A Tensor of any dtype of shape [D0, D1, ..., Dn]. Default: None.</span>
<span class="sd">        shape (tuple(int)): An integer tuple which contains the shape</span>
<span class="sd">            of the corresponding dense tensor. Default: None.</span>
<span class="sd">        row_tensor (RowTensor): A RowTensor object. Default: None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        RowTensor, composed of `indices`, `values`, and `shape`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, RowTensor</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor([0])</span>
<span class="sd">        &gt;&gt;&gt; values = Tensor([[1, 2]], dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; shape = (3, 2)</span>
<span class="sd">        &gt;&gt;&gt; x = RowTensor(indices, values, shape)</span>
<span class="sd">        &gt;&gt;&gt; print(x.values)</span>
<span class="sd">        [[1. 2.]]</span>
<span class="sd">        &gt;&gt;&gt; print(x.indices)</span>
<span class="sd">        [0]</span>
<span class="sd">        &gt;&gt;&gt; print(x.dense_shape)</span>
<span class="sd">        (3, 2)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">row_tensor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Init RowTensor&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_finished</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="c1"># Directly init a RowTensor from another RowTensor</span>
        <span class="k">if</span> <span class="n">row_tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">row_tensor</span><span class="p">,</span> <span class="p">(</span><span class="n">RowTensor</span><span class="p">,</span> <span class="n">RowTensor_</span><span class="p">)):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expect input `row_tensor` to be a RowTensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">row_tensor</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">indices</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">values</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;If input `row_tensor` is provided, `indices`, `values`, `shapes` should all be `None`&quot;</span><span class="p">)</span>
            <span class="n">RowTensor_</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">row_tensor</span><span class="p">)</span>
        <span class="c1"># Init a RowTensor from indices, values and shape</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">RowTensor_</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_finished</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Avoid PyTest Segfault when RowTensor is not initialized.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_finished</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">RowTensor_</span><span class="o">.</span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">return</span> <span class="s1">&#39;&#39;</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">indices</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return RowTensor&#39;s indices.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_indices</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">values</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return RowTensor&#39;s non-zero values.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">dense_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return RowTensor&#39;s shape.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span></div>


<div class="viewcode-block" id="SparseTensor"><a class="viewcode-back" href="../../../api_python/mindspore/mindspore.SparseTensor.html#mindspore.SparseTensor">[docs]</a><span class="k">class</span> <span class="nc">SparseTensor</span><span class="p">(</span><span class="n">COOTensor_</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A sparse representation of a set of nonzero elements from a tensor at given indices.</span>

<span class="sd">    SparseTensor can only be used in the `Cell`&#39;s construct method.</span>

<span class="sd">    For a tensor dense, its SparseTensor(indices, values, dense_shape) has</span>
<span class="sd">    `dense[indices[i]] = values[i]`.</span>

<span class="sd">    For example, if indices is [[0, 1], [1, 2]], values is [1, 2], dense_shape is</span>
<span class="sd">    (3, 4), then the dense representation of the sparse tensor will be:</span>

<span class="sd">    .. code-block::</span>

<span class="sd">        [[0, 1, 0, 0],</span>
<span class="sd">         [0, 0, 2, 0],</span>
<span class="sd">         [0, 0, 0, 0]]</span>

<span class="sd">    Note:</span>
<span class="sd">        The interface is deprecated from version 1.7 and will be removed in a future version.</span>
<span class="sd">        Please use &#39;COOTensor&#39; instead.</span>

<span class="sd">    Args:</span>
<span class="sd">        indices (Tensor): A 2-D integer Tensor of shape `[N, ndims]`,</span>
<span class="sd">            where N and ndims are the number of `values` and number of dimensions in</span>
<span class="sd">            the SparseTensor, respectively.</span>
<span class="sd">        values (Tensor): A 1-D tensor of any type and shape `[N]`, which</span>
<span class="sd">            supplies the values for each element in `indices`.</span>
<span class="sd">        shape (tuple(int)): A integer tuple of size `ndims`,</span>
<span class="sd">            which specifies the shape of the sparse tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        SparseTensor, composed of `indices`, `values`, and `shape`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, SparseTensor</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor([[0, 1], [1, 2]])</span>
<span class="sd">        &gt;&gt;&gt; values = Tensor([1, 2], dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; shape = (3, 4)</span>
<span class="sd">        &gt;&gt;&gt; x = SparseTensor(indices, values, shape)</span>
<span class="sd">        &gt;&gt;&gt; print(x.values)</span>
<span class="sd">        [1. 2.]</span>
<span class="sd">        &gt;&gt;&gt; print(x.indices)</span>
<span class="sd">        [[0 1]</span>
<span class="sd">         [1 2]]</span>
<span class="sd">        &gt;&gt;&gt; print(x.shape)</span>
<span class="sd">        (3, 4)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Init COOTensor.&quot;&quot;&quot;</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;&#39;SparseTensor&#39; is deprecated from version 1.7 and will be removed in a future version. &quot;</span> <span class="o">+</span>
                       <span class="s2">&quot;Please use &#39;COOTensor&#39; instead.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Inputs must follow: COOTensor(indices, values, shape).&quot;</span><span class="p">)</span>
        <span class="n">COOTensor_</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">indices</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_indices</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">values</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span></div>


<div class="viewcode-block" id="COOTensor"><a class="viewcode-back" href="../../../api_python/mindspore/mindspore.COOTensor.html#mindspore.COOTensor">[docs]</a><span class="k">class</span> <span class="nc">COOTensor</span><span class="p">(</span><span class="n">COOTensor_</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A sparse representation of a set of nonzero elements from a tensor at given indices.</span>

<span class="sd">    For a tensor dense, its COOTensor(indices, values, shape) has</span>
<span class="sd">    `dense[indices[i]] = values[i]`.</span>

<span class="sd">    For example, if indices is [[0, 1], [1, 2]], values is [1, 2], shape is</span>
<span class="sd">    (3, 4), then the dense representation of the sparse tensor will be:</span>

<span class="sd">    .. code-block::</span>

<span class="sd">        [[0, 1, 0, 0],</span>
<span class="sd">         [0, 0, 2, 0],</span>
<span class="sd">         [0, 0, 0, 0]]</span>

<span class="sd">    Note:</span>
<span class="sd">        This is an experimental feature and is subjected to change.</span>
<span class="sd">        Currently, duplicate coordinates in the indices will not be coalesced.</span>
<span class="sd">        If the indices contain out-of-bound values, the result will be undefined.</span>

<span class="sd">    Args:</span>
<span class="sd">        indices (Tensor): A 2-D integer Tensor of shape `[N, ndims]`,</span>
<span class="sd">            where N and ndims are the number of `values` and number of dimensions in</span>
<span class="sd">            the COOTensor, respectively. Currently, `ndims` must be 2.</span>
<span class="sd">            Please make sure that the indices are in range of the given shape.</span>
<span class="sd">        values (Tensor): A 1-D tensor of any type and shape `[N]`, which</span>
<span class="sd">            supplies the values for each element in `indices`.</span>
<span class="sd">        shape (tuple(int)): A integer tuple of size `ndims`,</span>
<span class="sd">            which specifies the dense_shape of the sparse tensor.</span>
<span class="sd">        coo_tensor (COOTensor): A COOTensor object.</span>

<span class="sd">    Returns:</span>
<span class="sd">        COOTensor, composed of `indices`, `values`, and `shape`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, COOTensor</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor([[0, 1], [1, 2]], dtype=ms.int32)</span>
<span class="sd">        &gt;&gt;&gt; values = Tensor([1, 2], dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; shape = (3, 4)</span>
<span class="sd">        &gt;&gt;&gt; x = COOTensor(indices, values, shape)</span>
<span class="sd">        &gt;&gt;&gt; print(x.values)</span>
<span class="sd">        [1. 2.]</span>
<span class="sd">        &gt;&gt;&gt; print(x.indices)</span>
<span class="sd">        [[0 1]</span>
<span class="sd">         [1 2]]</span>
<span class="sd">        &gt;&gt;&gt; print(x.shape)</span>
<span class="sd">        (3, 4)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">coo_tensor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Init COOTensor&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_finished</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="c1"># Directly init a COOTensor from another COOTensor</span>
        <span class="k">if</span> <span class="n">coo_tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">coo_tensor</span><span class="p">,</span> <span class="p">(</span><span class="n">COOTensor</span><span class="p">,</span> <span class="n">COOTensor_</span><span class="p">)):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expect input `coo_tensor` to be a COOTensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">coo_tensor</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">indices</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">values</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;If input `coo_tensor` is provided, `indices`, `values`, `shapes` should all be `None`&quot;</span><span class="p">)</span>
            <span class="n">COOTensor_</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">coo_tensor</span><span class="p">)</span>
        <span class="c1"># Init a COOTensor from indices, values and shape</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_coo_tensor_input</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_coo_tensor_shape</span><span class="p">(</span><span class="n">indices</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">values</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_coo_tensor_dtype</span><span class="p">(</span><span class="n">indices</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">indices</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;stop_gradient&#39;</span><span class="p">)(</span><span class="n">indices</span><span class="p">)</span>
            <span class="n">COOTensor_</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_finished</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Avoid PyTest Segfault when COOTensor is not initialized.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_finished</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">COOTensor_</span><span class="o">.</span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">return</span> <span class="s1">&#39;&#39;</span>

    <span class="k">def</span> <span class="fm">__neg__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">COOTensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__add__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Input tensors should have the same shape.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;tensor_scatter_add&quot;</span><span class="p">)(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">COOTensor</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;coo_add&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Add with </span><span class="si">%s</span><span class="s2"> is not supported.&quot;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">other</span><span class="p">))</span>

    <span class="k">def</span> <span class="fm">__sub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Input tensors should have the same shape.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;tensor_scatter_add&quot;</span><span class="p">)(</span><span class="o">-</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">COOTensor</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;coo_add&#39;</span><span class="p">)(</span>
                <span class="bp">self</span><span class="p">,</span> <span class="o">-</span><span class="n">other</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Subtract with </span><span class="si">%s</span><span class="s2"> is not supported.&quot;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">other</span><span class="p">))</span>

    <span class="k">def</span> <span class="fm">__mul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Input tensors should have the same shape.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="n">other_values</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;gather_nd&quot;</span><span class="p">)(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">COOTensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span> <span class="o">*</span> <span class="n">other_values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Multiply with </span><span class="si">%s</span><span class="s2"> is not supported.&quot;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">other</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">__div__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Input tensors should have the same shape.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;For sparse divide, zero values in the dense tensor are ignored.&quot;</span><span class="p">)</span>
            <span class="n">other_values</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;gather_nd&quot;</span><span class="p">)(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">COOTensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span> <span class="o">/</span> <span class="n">other_values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Divide with </span><span class="si">%s</span><span class="s2"> is not supported.&quot;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">other</span><span class="p">))</span>

    <span class="k">def</span> <span class="fm">__truediv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__div__</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">indices</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return COOTensor&#39;s indices.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_indices</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">values</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return COOTensor&#39;s non-zero values.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return COOTensor&#39;s shape.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span>

<div class="viewcode-block" id="COOTensor.coalesce"><a class="viewcode-back" href="../../../api_python/mindspore/mindspore.COOTensor.html#mindspore.COOTensor.coalesce">[docs]</a>    <span class="k">def</span> <span class="nf">coalesce</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the coalesced sparse tensor of the input.</span>

<span class="sd">        Returns:</span>
<span class="sd">            COOTensor.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``GPU``</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">res_indices</span><span class="p">,</span> <span class="n">res_values</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;coalesce&quot;</span><span class="p">)(</span><span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="o">.</span><span class="n">transpose</span><span class="p">(),</span>
                                                                              <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">COOTensor</span><span class="p">(</span><span class="n">res_indices</span><span class="o">.</span><span class="n">transpose</span><span class="p">(),</span> <span class="n">res_values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span></div>

<div class="viewcode-block" id="COOTensor.to_csr"><a class="viewcode-back" href="../../../api_python/mindspore/mindspore.COOTensor.html#mindspore.COOTensor.to_csr">[docs]</a>    <span class="k">def</span> <span class="nf">to_csr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Converts COOTensor to CSRTensor.</span>

<span class="sd">        Note:</span>
<span class="sd">            Currently only supports CPU backend with LLVM 12.0.1 installed.</span>

<span class="sd">        Returns:</span>
<span class="sd">            CSRTensor.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``GPU`` ``CPU``</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">row_indices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
        <span class="n">col_indices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">idx_dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">row_indices</span><span class="p">,</span> <span class="n">sort_idx</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;sort&quot;</span><span class="p">)(</span>
            <span class="n">row_indices</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
        <span class="n">row_indices</span> <span class="o">=</span> <span class="n">row_indices</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">idx_dtype</span><span class="p">)</span>
        <span class="n">col_indices</span> <span class="o">=</span> <span class="n">col_indices</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">]</span>
        <span class="n">values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">]</span>
        <span class="n">indptr</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;coo2csr&quot;</span><span class="p">)(</span><span class="n">row_indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">CSRTensor</span><span class="p">(</span><span class="n">indptr</span><span class="p">,</span> <span class="n">col_indices</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span></div>

<div class="viewcode-block" id="COOTensor.to_dense"><a class="viewcode-back" href="../../../api_python/mindspore/mindspore.COOTensor.html#mindspore.COOTensor.to_dense">[docs]</a>    <span class="k">def</span> <span class="nf">to_dense</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Converts COOTensor to Dense Tensor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``GPU``</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">zeros_tensor</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;zeros&quot;</span><span class="p">)(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;tensor_scatter_add&quot;</span><span class="p">)(</span>
            <span class="n">zeros_tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the dtype of the values of COOTensor (:class:`mindspore.dtype`).&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the number of non-zero values.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">size</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">itemsize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the length of one tensor element in bytes.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">itemsize</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">ndim</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the number of tensor dimensions.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<div class="viewcode-block" id="COOTensor.astype"><a class="viewcode-back" href="../../../api_python/mindspore/mindspore.COOTensor.html#mindspore.COOTensor.astype">[docs]</a>    <span class="k">def</span> <span class="nf">astype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return a copy of the COOTensor, cast its values to a specified type.</span>

<span class="sd">        Args:</span>
<span class="sd">            dtype (Union[:class:`mindspore.dtype`, numpy.dtype, str]): Designated tensor dtype.</span>

<span class="sd">        Returns:</span>
<span class="sd">            COOTensor.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor, COOTensor</span>
<span class="sd">            &gt;&gt;&gt; indices = Tensor([[0, 1], [1, 2]], dtype=ms.int32)</span>
<span class="sd">            &gt;&gt;&gt; values = Tensor([1, 2], dtype=ms.float32)</span>
<span class="sd">            &gt;&gt;&gt; shape = (3, 4)</span>
<span class="sd">            &gt;&gt;&gt; coo_tensor = COOTensor(indices, values, shape)</span>
<span class="sd">            &gt;&gt;&gt; print(coo_tensor.astype(ms.float64).dtype)</span>
<span class="sd">            Float64</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">COOTensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span></div>

<div class="viewcode-block" id="COOTensor.to_tuple"><a class="viewcode-back" href="../../../api_python/mindspore/mindspore.COOTensor.html#mindspore.COOTensor.to_tuple">[docs]</a>    <span class="k">def</span> <span class="nf">to_tuple</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return indices, values and shape as a tuple.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span></div>

<div class="viewcode-block" id="COOTensor.abs"><a class="viewcode-back" href="../../../api_python/mindspore/mindspore.COOTensor.html#mindspore.COOTensor.abs">[docs]</a>    <span class="k">def</span> <span class="nf">abs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return absolute value element-wisely.</span>

<span class="sd">        Returns:</span>
<span class="sd">            COOTensor.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">COOTensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span></div>

<div class="viewcode-block" id="COOTensor.add"><a class="viewcode-back" href="../../../api_python/mindspore/mindspore.COOTensor.html#mindspore.COOTensor.add">[docs]</a>    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="n">thresh</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the sum with another COOTensor.</span>

<span class="sd">        Args:</span>
<span class="sd">            other(COOTensor): the second SparseTensor to sum.</span>
<span class="sd">            thresh(Tensor): the magnitude threshold that determines</span>
<span class="sd">                if an output value/index pair pair take space.</span>

<span class="sd">        Returns:</span>
<span class="sd">            COOTensor, representing the sum.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If any input(self/other)&#39;s indices&#39;s dim is not equal to 2.</span>
<span class="sd">            ValueError: If any input(self/other)&#39;s values&#39;s dim is not equal to 1.</span>
<span class="sd">            ValueError: If any input(self/other)&#39;s shape&#39;s dim is not equal to 1.</span>
<span class="sd">            ValueError: If thresh&#39;s dim is not equal to 0.</span>
<span class="sd">            TypeError: If any input(self/other)&#39;s indices&#39;s type is not equal to int64.</span>
<span class="sd">            TypeError: If any input(self/other)&#39;s shape&#39;s type is not equal to int64.</span>
<span class="sd">            ValueError: If any input(self/other)&#39;s indices&#39;s length is not equal to</span>
<span class="sd">                its values&#39;s length.</span>
<span class="sd">            TypeError: If any input(self/other)&#39;s values&#39;s type is not equal to anf of</span>
<span class="sd">                (int8/int16/int32/int64/float32/float64/complex64/complex128)</span>
<span class="sd">            TypeError: If thresh&#39;s type is not equal to anf of</span>
<span class="sd">                (int8/int16/int32/int64/float32/float64)</span>
<span class="sd">            TypeError: If self&#39;s indices&#39;s type is not equal to other&#39;s indices&#39;s type</span>
<span class="sd">            TypeError: If self&#39;s values&#39;s type is not equal to other&#39;s values&#39;s type</span>
<span class="sd">            TypeError: If self&#39;s shape&#39;s type is not equal to other&#39;s shape&#39;s type</span>
<span class="sd">            TypeError: If (self/other)&#39;s value&#39;s type is not matched with thresh&#39;s type</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``CPU`` ``GPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor, COOTensor</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">            &gt;&gt;&gt; indics0 = Tensor([[0, 1], [1, 2]], dtype=mstype.int64)</span>
<span class="sd">            &gt;&gt;&gt; values0 = Tensor([1, 2], dtype=mstype.int32)</span>
<span class="sd">            &gt;&gt;&gt; shape0 = (3, 4)</span>
<span class="sd">            &gt;&gt;&gt; input0 = COOTensor(indics0, values0, shape0)</span>
<span class="sd">            &gt;&gt;&gt; indics1 = Tensor([[0, 0], [1, 1]], dtype=mstype.int64)</span>
<span class="sd">            &gt;&gt;&gt; values1 = Tensor([3, 4], dtype=mstype.int32)</span>
<span class="sd">            &gt;&gt;&gt; shape1 = (3, 4)</span>
<span class="sd">            &gt;&gt;&gt; input1 = COOTensor(indics1, values1, shape1)</span>
<span class="sd">            &gt;&gt;&gt; thres = Tensor(0, dtype=mstype.int32)</span>
<span class="sd">            &gt;&gt;&gt; out = input0.add(input1, thres)</span>
<span class="sd">            &gt;&gt;&gt; print(out)</span>
<span class="sd">            COOTensor(shape = [3, 4], dtype = Int32, indices=Tensor(shape=[4, 2],</span>
<span class="sd">            dtype = Int64, value=[[0 0], [0 1], [1 1], [1 2]]),  values=Tensor(shape[4],</span>
<span class="sd">            dtype=Int32, value=[3 1 4 2]))</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;coo_add&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="n">thresh</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="CSRTensor"><a class="viewcode-back" href="../../../api_python/mindspore/mindspore.CSRTensor.html#mindspore.CSRTensor">[docs]</a><span class="k">class</span> <span class="nc">CSRTensor</span><span class="p">(</span><span class="n">CSRTensor_</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs a sparse tensor in CSR (Compressed Sparse Row) format, with specified</span>
<span class="sd">    values indicated by `values` and row and column positions indicated by `indptr`</span>
<span class="sd">    and `indices`.</span>

<span class="sd">    For example, if indptr is [0, 1, 2, 2], indices is [1, 2], values is [1., 2.], shape is</span>
<span class="sd">    (3, 4), then the dense representation of the sparse tensor will be:</span>

<span class="sd">    .. code-block::</span>

<span class="sd">        [[0., 1., 0., 0.],</span>
<span class="sd">         [0., 0., 2., 0.],</span>
<span class="sd">         [0., 0., 0., 0.]]</span>

<span class="sd">    Note:</span>
<span class="sd">        This is an experimental feature and is subjected to change.</span>
<span class="sd">        If the length of values or indices exceeds the range indicated by indptr, its behavior will be undefined.</span>

<span class="sd">    Args:</span>
<span class="sd">        indptr (Tensor): 1-D Tensor of shape `[M]`, which equals to `shape[0] + 1`, which indicates the</span>
<span class="sd">            start and end point for `values` in each row. Default: None. If provided,</span>
<span class="sd">            must be :class:`mindspore.int16`, :class:`mindspore.int32` or :class:`mindspore.int64`.</span>
<span class="sd">        indices (Tensor): 1-D Tensor of shape `[N]`, which has the same length as `values`. `indices`</span>
<span class="sd">            indicates the which column `values` should be placed. Default: None. If provided,</span>
<span class="sd">            must be :class:`mindspore.int16`, :class:`mindspore.int32` or :class:`mindspore.int64`.</span>
<span class="sd">        values (Tensor): Tensor, which has the same length as `indices` (values.shape[0] == indices.shape[0]).</span>
<span class="sd">            `values`  stores the data for CSRTensor. Default: None.</span>
<span class="sd">        shape (tuple(int)): A tuple indicates the shape of the CSRTensor, and `shape[0]` must equal to `M - 1`,</span>
<span class="sd">            which all equal to number of rows of the CSRTensor. Default: None.</span>
<span class="sd">        csr_tensor (CSRTensor): A CSRTensor object.  Values&#39; feature dimension should match with</span>
<span class="sd">            CSRTensor&#39;s feature dimension (values.shape[1:] == csr_tensor.shape[2:]). Default: None.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        CSRTensor, with shape defined by `shape`, and dtype inferred from `value`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, CSRTensor</span>
<span class="sd">        &gt;&gt;&gt; # initialize a csr_tensor with indptr, indices, values and shape</span>
<span class="sd">        &gt;&gt;&gt; indptr = Tensor([0, 1, 2], dtype=ms.int32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor([0, 1], dtype=ms.int32)</span>
<span class="sd">        &gt;&gt;&gt; values = Tensor([1, 2], dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; shape = (2, 4)</span>
<span class="sd">        &gt;&gt;&gt; csr_tensor = CSRTensor(indptr, indices, values, shape)</span>
<span class="sd">        &gt;&gt;&gt; # access a data member of CSRTensor</span>
<span class="sd">        &gt;&gt;&gt; print(indptr == csr_tensor.indptr)</span>
<span class="sd">        [ True  True  True]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indptr</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">indices</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">csr_tensor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="s2">&quot;Init CSRTensor&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_finished</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="c1"># Directly init a CSRTensor from another CSRTensor</span>
        <span class="k">if</span> <span class="n">csr_tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">csr_tensor</span><span class="p">,</span> <span class="p">(</span><span class="n">CSRTensor</span><span class="p">,</span> <span class="n">CSRTensor_</span><span class="p">)):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expect input `csr_tensor` to be a CSRTensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">csr_tensor</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">indptr</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">indices</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">values</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                    <span class="s2">&quot;If input `csr_tensor` is provided, `indptr`, `indices`, `values`, `shapes` should all be `None`&quot;</span><span class="p">)</span>
            <span class="n">CSRTensor_</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">csr_tensor</span><span class="p">)</span>
        <span class="c1"># Init a CSRTensor from indptr, indices, values and shape</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_csr_tensor_input</span><span class="p">(</span><span class="n">indptr</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_csr_tensor_shape</span><span class="p">(</span><span class="n">indptr</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">indices</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">values</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_csr_tensor_dtype</span><span class="p">(</span><span class="n">indptr</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">indices</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">indptr</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;stop_gradient&#39;</span><span class="p">)(</span><span class="n">indptr</span><span class="p">)</span>
            <span class="n">indices</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;stop_gradient&#39;</span><span class="p">)(</span><span class="n">indices</span><span class="p">)</span>
            <span class="n">CSRTensor_</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indptr</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_finished</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Avoid PyTest Segfault when CSRTensor is not initialized.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_finished</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">CSRTensor_</span><span class="o">.</span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">return</span> <span class="s1">&#39;&#39;</span>

    <span class="k">def</span> <span class="fm">__mul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;csr_mul&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">CSRTensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">indptr</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="n">res</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__div__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;For CSR divide, zero values in the dense tensor are ignored.&quot;</span><span class="p">)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;csr_div&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">CSRTensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">indptr</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="n">res</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__truediv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__div__</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__neg__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">CSRTensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">indptr</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__add__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Input tensors should have the same shape.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">CSRTensor</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;csr_add&#39;</span><span class="p">)(</span>
                <span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Add with </span><span class="si">%s</span><span class="s2"> is not supported.&quot;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">other</span><span class="p">))</span>

    <span class="k">def</span> <span class="fm">__sub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Input tensors should have the same shape.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">CSRTensor</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;csr_add&#39;</span><span class="p">)(</span>
                <span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">Tensor</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Subtract with </span><span class="si">%s</span><span class="s2"> is not supported.&quot;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">other</span><span class="p">))</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">indptr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return CSRTensor&#39;s row indices pointers.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_indptr</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">indices</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return CSRTensor&#39;s column indices.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_indices</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">values</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return CSRTensor&#39;s non-zero values.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return CSRTensor&#39;s shape.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the dtype of the values of CSRTensor (:class:`mindspore.dtype`).&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the number of non-zero values.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">size</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">itemsize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the length of one tensor element in bytes.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">itemsize</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">ndim</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the number of tensor dimensions.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<div class="viewcode-block" id="CSRTensor.to_tuple"><a class="viewcode-back" href="../../../api_python/mindspore/mindspore.CSRTensor.html#mindspore.CSRTensor.to_tuple">[docs]</a>    <span class="k">def</span> <span class="nf">to_tuple</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return indptr, indices, values and shape as a tuple.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">indptr</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span></div>

<div class="viewcode-block" id="CSRTensor.to_coo"><a class="viewcode-back" href="../../../api_python/mindspore/mindspore.CSRTensor.html#mindspore.CSRTensor.to_coo">[docs]</a>    <span class="k">def</span> <span class="nf">to_coo</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Converts CSRTensor to COOTensor.</span>

<span class="sd">        Note:</span>
<span class="sd">            Currently only supports CPU backend with LLVM 12.0.1 installed.</span>

<span class="sd">        Returns:</span>
<span class="sd">            COOTensor.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``GPU`` ``CPU``</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Currently only support 2-D CSRTensor when converting to COOTensor.&quot;</span><span class="p">)</span>
        <span class="n">row_indices</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;csr2coo&quot;</span><span class="p">)(</span><span class="bp">self</span><span class="o">.</span><span class="n">indptr</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">coo_indices</span> <span class="o">=</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;stack&quot;</span><span class="p">)((</span><span class="n">row_indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">COOTensor</span><span class="p">(</span><span class="n">coo_indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span></div>

<div class="viewcode-block" id="CSRTensor.to_dense"><a class="viewcode-back" href="../../../api_python/mindspore/mindspore.CSRTensor.html#mindspore.CSRTensor.to_dense">[docs]</a>    <span class="k">def</span> <span class="nf">to_dense</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Converts CSRTensor to Dense Tensor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``GPU``</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;csr_to_dense&quot;</span><span class="p">)(</span><span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="CSRTensor.astype"><a class="viewcode-back" href="../../../api_python/mindspore/mindspore.CSRTensor.html#mindspore.CSRTensor.astype">[docs]</a>    <span class="k">def</span> <span class="nf">astype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return a copy of the CSRTensor, cast its values to a specified type.</span>

<span class="sd">        Args:</span>
<span class="sd">            dtype (Union[:class:`mindspore.dtype`, numpy.dtype, str]): Designated tensor dtype.</span>

<span class="sd">        Returns:</span>
<span class="sd">            CSRTensor.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor, CSRTensor</span>
<span class="sd">            &gt;&gt;&gt; indptr = Tensor([0, 1, 2], dtype=ms.int32)</span>
<span class="sd">            &gt;&gt;&gt; indices = Tensor([0, 1], dtype=ms.int32)</span>
<span class="sd">            &gt;&gt;&gt; values = Tensor([1, 2], dtype=ms.float32)</span>
<span class="sd">            &gt;&gt;&gt; shape = (2, 4)</span>
<span class="sd">            &gt;&gt;&gt; csr_tensor = CSRTensor(indptr, indices, values, shape)</span>
<span class="sd">            &gt;&gt;&gt; print(csr_tensor.astype(ms.float64).dtype)</span>
<span class="sd">            Float64</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">CSRTensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">indptr</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span></div>

<div class="viewcode-block" id="CSRTensor.mv"><a class="viewcode-back" href="../../../api_python/mindspore/mindspore.CSRTensor.html#mindspore.CSRTensor.mv">[docs]</a>    <span class="k">def</span> <span class="nf">mv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dense_vector</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the matrix multiplication result of the right-multiply dense matrix of the CSRTensor.</span>
<span class="sd">        The CSRTensor with shape `[M, N]` needs to adapt the dense vector with shape `[N, 1]`</span>
<span class="sd">        to get the dense vector with result `[M, 1]`.</span>

<span class="sd">        Note:</span>
<span class="sd">            Currently only supports CPU backend with LLVM 12.0.1 installed.</span>

<span class="sd">        Args:</span>
<span class="sd">            dense_vector (Tensor): A dense Tensor, its shape must be (csr_tensor.shape[1], 1)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor, CSRTensor</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">            &gt;&gt;&gt; indptr = Tensor([0, 1, 2], dtype=mstype.int32)</span>
<span class="sd">            &gt;&gt;&gt; indices = Tensor([0, 1], dtype=mstype.int32)</span>
<span class="sd">            &gt;&gt;&gt; values = Tensor([2, 1], dtype=mstype.float32)</span>
<span class="sd">            &gt;&gt;&gt; dense_shape = (2, 4)</span>
<span class="sd">            &gt;&gt;&gt; csr_tensor = CSRTensor(indptr, indices, values, dense_shape)</span>
<span class="sd">            &gt;&gt;&gt; dense = Tensor([[1], [1], [1], [1]], dtype=mstype.float32)</span>
<span class="sd">            &gt;&gt;&gt; print(csr_tensor.mv(dense))</span>
<span class="sd">            [[2.]</span>
<span class="sd">            [1.]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;dense_vector&#39;</span><span class="p">,</span> <span class="n">dense_vector</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor_</span><span class="p">,),</span> <span class="s1">&#39;CSRTensor.mv&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;csr_mv&quot;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dense_vector</span><span class="p">)</span></div>

<div class="viewcode-block" id="CSRTensor.mm"><a class="viewcode-back" href="../../../api_python/mindspore/mindspore.CSRTensor.html#mindspore.CSRTensor.mm">[docs]</a>    <span class="k">def</span> <span class="nf">mm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dense_matrix</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the matrix multiplication result of the right-multiply dense matrix of the CSRTensor.</span>
<span class="sd">        The CSRTensor with shape `[M, N]` needs to adapt the dense matrix with shape `[N, K]`</span>
<span class="sd">        to get the dense matrix with result `[M, K]`.</span>

<span class="sd">        Note:</span>
<span class="sd">            Currently only supports CPU backend with LLVM 12.0.1 installed.</span>

<span class="sd">        Args:</span>
<span class="sd">            dense_matrix (Tensor): A dense Tensor, its shape[0] should be equal to csr_tensor.shape[1]</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor, CSRTensor</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">            &gt;&gt;&gt; indptr = Tensor([0, 1, 2], dtype=mstype.int32)</span>
<span class="sd">            &gt;&gt;&gt; indices = Tensor([0, 1], dtype=mstype.int32)</span>
<span class="sd">            &gt;&gt;&gt; values = Tensor([2, 1], dtype=mstype.float32)</span>
<span class="sd">            &gt;&gt;&gt; dense_shape = (2, 4)</span>
<span class="sd">            &gt;&gt;&gt; csr_tensor = CSRTensor(indptr, indices, values, dense_shape)</span>
<span class="sd">            &gt;&gt;&gt; dense_matrix = Tensor([[1., 2.], [1, 2.], [1, 2.], [1., 2.]], dtype=mstype.float32)</span>
<span class="sd">            &gt;&gt;&gt; print(csr_tensor.mm(dense_matrix))</span>
<span class="sd">            [[2. 4.]</span>
<span class="sd">            [1. 2.]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;dense_matrix&#39;</span><span class="p">,</span> <span class="n">dense_matrix</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor_</span><span class="p">,),</span> <span class="s1">&#39;CSRTensor.mm&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;csr_mm&quot;</span><span class="p">)()(</span><span class="bp">self</span><span class="o">.</span><span class="n">indptr</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
                                                        <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dense_matrix</span><span class="p">)</span></div>

<div class="viewcode-block" id="CSRTensor.sum"><a class="viewcode-back" href="../../../api_python/mindspore/mindspore.CSRTensor.html#mindspore.CSRTensor.sum">[docs]</a>    <span class="k">def</span> <span class="nf">sum</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reduces a dimension of a CSRTensor by summing all elements in the dimension.</span>

<span class="sd">        Note:</span>
<span class="sd">            Currently only supports CPU backend with LLVM 12.0.1 installed.</span>

<span class="sd">        Args:</span>
<span class="sd">            axis (int): The dimensions to reduce.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor, the dtype is the same as `CSRTensor.values`.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor, CSRTensor</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">            &gt;&gt;&gt; indptr = Tensor([0, 1, 2], dtype=mstype.int32)</span>
<span class="sd">            &gt;&gt;&gt; indices = Tensor([0, 1], dtype=mstype.int32)</span>
<span class="sd">            &gt;&gt;&gt; values = Tensor([2, 1], dtype=mstype.float32)</span>
<span class="sd">            &gt;&gt;&gt; dense_shape = (2, 4)</span>
<span class="sd">            &gt;&gt;&gt; csr_tensor = CSRTensor(indptr, indices, values, dense_shape)</span>
<span class="sd">            &gt;&gt;&gt; print(csr_tensor.sum(1))</span>
<span class="sd">            [[2.]</span>
<span class="sd">            [1.]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;csr_reduce_sum&quot;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>

<div class="viewcode-block" id="CSRTensor.abs"><a class="viewcode-back" href="../../../api_python/mindspore/mindspore.CSRTensor.html#mindspore.CSRTensor.abs">[docs]</a>    <span class="k">def</span> <span class="nf">abs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return absolute value element-wisely.</span>

<span class="sd">        Returns:</span>
<span class="sd">            CSRTensor, with all values being non-negative.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">CSRTensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">indptr</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span></div>

<div class="viewcode-block" id="CSRTensor.add"><a class="viewcode-back" href="../../../api_python/mindspore/mindspore.CSRTensor.html#mindspore.CSRTensor.add">[docs]</a>    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Addition of two CSR Tensors : C = alpha * A + beta * B</span>

<span class="sd">        Args:</span>
<span class="sd">            b (CSRTensor): Sparse CSR Tensor.</span>
<span class="sd">            alpha(Tensor): Dense Tensor, its shape must be able to broadcast to self.</span>
<span class="sd">            beta(Tensor): Dense Tensor, its shape must be able to broadcast to b.</span>

<span class="sd">        Returns:</span>
<span class="sd">            CSRTensor.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor, CSRTensor</span>
<span class="sd">            &gt;&gt;&gt; import mindspore.common.dtype as mstype</span>
<span class="sd">            &gt;&gt;&gt; indptr = Tensor([0, 1, 2], dtype=mstype.int32)</span>
<span class="sd">            &gt;&gt;&gt; indices = Tensor([0, 1], dtype=mstype.int32)</span>
<span class="sd">            &gt;&gt;&gt; values_a = Tensor([2, 1], dtype=mstype.float32)</span>
<span class="sd">            &gt;&gt;&gt; values_b = Tensor([1, 2], dtype=mstype.float32)</span>
<span class="sd">            &gt;&gt;&gt; dense_shape = (2, 4)</span>
<span class="sd">            &gt;&gt;&gt; alpha = Tensor(1, mstype.float32)</span>
<span class="sd">            &gt;&gt;&gt; beta = Tensor(1, mstype.float32)</span>
<span class="sd">            &gt;&gt;&gt; a = CSRTensor(indptr, indices, values_a, dense_shape)</span>
<span class="sd">            &gt;&gt;&gt; b = CSRTensor(indptr, indices, values_b, dense_shape)</span>
<span class="sd">            &gt;&gt;&gt; print(a.add(b, alpha, beta))</span>
<span class="sd">                CSRTensor(shape=[2,4], dtype=Float32,</span>
<span class="sd">                          indptr=Tensor(shape=[3], dtype=Int32, value = [0, 1, 2]),</span>
<span class="sd">                          indices=Tensor(shape=[2], dtype=Int32, value = [0, 1]),</span>
<span class="sd">                          values=Tensor(shape=[2], dtype=Float32, value = [3.0, 3.0]))</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;csr_add&#39;</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span></div></div>


<span class="k">def</span> <span class="nf">_vm_compare</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Implement `vm_compare` for tensor.&quot;&quot;&quot;</span>
    <span class="n">obj_str</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">obj_str</span> <span class="o">==</span> <span class="s2">&quot;shape&quot;</span><span class="p">:</span>
        <span class="n">fn</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(),</span> <span class="n">obj_str</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">fn</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">fn</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(),</span> <span class="n">obj_str</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">fn</span><span class="p">())</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">fn</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(),</span> <span class="n">obj_str</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">obj_str</span> <span class="o">=</span> <span class="s2">&quot;__r&quot;</span> <span class="o">+</span> <span class="n">obj_str</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
        <span class="n">fn</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(),</span> <span class="n">obj_str</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">fn</span><span class="p">(</span><span class="n">y</span><span class="p">)))</span>


<span class="k">def</span> <span class="nf">_check_tensor_input</span><span class="p">(</span><span class="n">input_data</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check the tensor input.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">input_data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;When initializing a tensor with &#39;input_data&#39;, &#39;shape&#39; should be set to None.&quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;But got shape: </span><span class="si">{</span><span class="n">shape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">init</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">shape</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;init, dtype and shape must have values at the same time.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">input_data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="ow">and</span> <span class="n">input_data</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">input_data</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;input_data can not contain zero dimension.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span> <span class="ow">and</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">1</span> \
                <span class="ow">and</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;input_data can not contain zero dimension.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">init</span><span class="p">,</span> <span class="s2">&quot;__enable_zero_dim__&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">init</span><span class="o">.</span><span class="n">__enable_zero_dim__</span><span class="p">)</span> <span class="ow">and</span> <span class="mi">0</span> <span class="ow">in</span> <span class="n">shape</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Shape can not contain zero value.&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_tensor_dynamic_shape</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check if the tensor has dynamic shape.&quot;&quot;&quot;</span>
    <span class="n">shape_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape_list</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">shape_replaced_list</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">shape_list</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">shape_replaced_list</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="n">shape_replaced_list</span>
    <span class="k">if</span> <span class="n">is_shape_unknown</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">init</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;If setting dynamic shape, dtype must not be None, init must be None&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">shape</span>


<span class="k">def</span> <span class="nf">_check_astype_and_convert</span><span class="p">(</span><span class="n">dtype</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check whether dtype is a valid input, and convert to mstype&quot;&quot;&quot;</span>
    <span class="n">all_types</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">__dtype__</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;int&quot;</span><span class="p">,</span> <span class="s2">&quot;float&quot;</span><span class="p">,</span> <span class="s2">&quot;bool&quot;</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">dtype</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">all_types</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For Tensor.astype, the string input type must be one of </span><span class="si">{</span><span class="n">all_types</span><span class="si">}</span><span class="s2">, &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;but got &#39;</span><span class="si">{</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&#39;.&quot;</span><span class="p">)</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">pytype_to_dtype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">dtype</span><span class="o">.</span><span class="n">lower</span><span class="p">()))</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="nb">type</span><span class="p">):</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">pytype_to_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span> <span class="o">+</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">,):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;For Tensor.astype, the input type must be one of </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">,)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">np_types</span><span class="p">)</span><span class="si">}</span><span class="s2">,&quot;</span>
            <span class="sa">f</span><span class="s2">&quot; but got &#39;</span><span class="si">{</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&#39;.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dtype</span>


<span class="n">tensor_operator_registry</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s1">&#39;vm_compare&#39;</span><span class="p">,</span> <span class="n">_vm_compare</span><span class="p">)</span>
</pre></div>

           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
</body>
</html>