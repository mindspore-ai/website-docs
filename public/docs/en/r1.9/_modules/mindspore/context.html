<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore.context &mdash; MindSpore master documentation</title>
      <link rel="stylesheet" href="../../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/training.css" type="text/css" /><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/js/training.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../design/auto_gradient.html">Functional Differential Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/distributed_training_design.html">Distributed Training Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/mindir.html">MindSpore IR (MindIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/thor.html">Second Order Optimizer</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.9/training_visual_design.html">Design of Visualization↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Specification</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../note/benchmark.html">Benchmarks</a></li>
<li class="toctree-l1"><a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.9/README.md#table-of-contents">Network List↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/operator_list.html">API List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/syntax_list.html">Syntax Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.dataset.audio.html">mindspore.dataset.audio</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.dataset.config.html">mindspore.dataset.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.dataset.text.html">mindspore.dataset.text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.dataset.vision.html">mindspore.dataset.vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.nn.transformer.html">mindspore.nn.transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.ops.function.html">mindspore.ops.function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.9/api_cpp/mindspore.html">C++ API↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Mapping</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../note/api_mapping/pytorch_api_mapping.html">PyTorch and MindSpore API Mapping Table</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/api_mapping/tensorflow_api_mapping.html">TensorFlow and MindSpore API Mapping Table</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Migration Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/enveriment_preparation.html">Environment Preparation and Information Acquisition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/analysis_and_preparation.html">Model Analysis and Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/model_development/model_development.html">Constructing MindSpore Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/debug_and_tune.html">Debugging and Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/sample_code.html">Network Migration Debugging Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/faq.html">FAQs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/typical_api_comparision.html">Differences Between MindSpore and PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/use_third_party_op.html">Using Third-party Operator Libraries Based on Customized Interfaces</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../faq/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/data_processing.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/implement_problem.html">Implement Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/network_compilation.html">Network Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/operators_compile.html">Operators Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/usage_migrate_3rd.html">Migration from a Third-party Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/precision_tuning.html">Precision Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/distributed_configure.html">Distributed Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/feature_advice.html">Feature Advice</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../index.html">Module code</a> &raquo;</li>
      <li>mindspore.context</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for mindspore.context</h1><div class="highlight"><pre>
<span></span> <span class="c1"># Copyright 2020-2022 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">The context of mindspore, used to configure the current execution environment,</span>
<span class="sd">includes the execution mode, execution backend and other feature switches.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span>

<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">threading</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">namedtuple</span>
<span class="kn">from</span> <span class="nn">types</span> <span class="kn">import</span> <span class="n">FunctionType</span>

<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">log</span> <span class="k">as</span> <span class="n">logger</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">MSContext</span><span class="p">,</span> <span class="n">ms_ctx_param</span>
<span class="kn">from</span> <span class="nn">mindspore._checkparam</span> <span class="kn">import</span> <span class="n">args_type_check</span><span class="p">,</span> <span class="n">Validator</span><span class="p">,</span> <span class="n">args_unreset_check</span>
<span class="kn">from</span> <span class="nn">mindspore.parallel._auto_parallel_context</span> <span class="kn">import</span> <span class="n">_set_auto_parallel_context</span><span class="p">,</span> <span class="n">_get_auto_parallel_context</span><span class="p">,</span> \
    <span class="n">_reset_auto_parallel_context</span>
<span class="kn">from</span> <span class="nn">mindspore.parallel._ps_context</span> <span class="kn">import</span> <span class="n">_set_ps_context</span><span class="p">,</span> <span class="n">_get_ps_context</span><span class="p">,</span> <span class="n">_reset_ps_context</span><span class="p">,</span> \
    <span class="n">_need_reset_device_target_for_ps</span>
<span class="kn">from</span> <span class="nn">mindspore.default_config</span> <span class="kn">import</span> <span class="n">__device_target__</span><span class="p">,</span> <span class="n">__package_name__</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;GRAPH_MODE&#39;</span><span class="p">,</span> <span class="s1">&#39;PYNATIVE_MODE&#39;</span><span class="p">,</span> <span class="s1">&#39;set_context&#39;</span><span class="p">,</span> <span class="s1">&#39;get_context&#39;</span><span class="p">,</span> <span class="s1">&#39;set_auto_parallel_context&#39;</span><span class="p">,</span>
           <span class="s1">&#39;get_auto_parallel_context&#39;</span><span class="p">,</span> <span class="s1">&#39;reset_auto_parallel_context&#39;</span><span class="p">,</span> <span class="s1">&#39;ParallelMode&#39;</span><span class="p">,</span> <span class="s1">&#39;set_ps_context&#39;</span><span class="p">,</span>
           <span class="s1">&#39;get_ps_context&#39;</span><span class="p">,</span> <span class="s1">&#39;reset_ps_context&#39;</span><span class="p">]</span>

<span class="n">GRAPH_MODE</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">PYNATIVE_MODE</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">_DEVICE_APP_MEMORY_SIZE</span> <span class="o">=</span> <span class="mi">31</span>  <span class="c1"># The max memory size of graph plus variable.</span>
<span class="n">_re_pattern</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;[1-9][0-9]*(\.)?[0-9]*GB|0\.[0-9]*GB&#39;</span>
<span class="n">_k_context</span> <span class="o">=</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">_make_directory</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Make directory.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">path</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">or</span> <span class="n">path</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;&quot;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.set_context&#39;, the &#39;save_graphs_path&#39; or the &#39;print_file_path&#39; is invalid &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;type, it should be Non-empty string, but got &#39;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">&#39;.&quot;</span><span class="p">)</span>

    <span class="n">path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">realpath</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;The absolute path is </span><span class="si">%r</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;The directory(</span><span class="si">%s</span><span class="s2">) doesn&#39;t exist, will create it&quot;</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">FileExistsError</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;The directory(</span><span class="si">%s</span><span class="s2">) already exist.&quot;</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">PermissionError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">critical</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;No write permission on the directory &#39;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">&#39;&#39;, error = </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="fm">__str__</span><span class="p">()</span> <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">No write permission on the directory &#39;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">&#39;.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">path</span>


<span class="k">def</span> <span class="nf">_get_print_file_name</span><span class="p">(</span><span class="n">file_name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Add timestamp suffix to file name. Rename the file name:  file_name + &quot;.&quot; + time(seconds).&quot;&quot;&quot;</span>
    <span class="n">time_second</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()))</span>
    <span class="n">file_name</span> <span class="o">=</span> <span class="n">file_name</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span> <span class="o">+</span> <span class="n">time_second</span>
    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">file_name</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;print_file_path&#39; </span><span class="si">{}</span><span class="s2"> already exists, &quot;</span>
                         <span class="s2">&quot;please check it&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">file_name</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">file_name</span>


<span class="k">class</span> <span class="nc">_ThreadLocalInfo</span><span class="p">(</span><span class="n">threading</span><span class="o">.</span><span class="n">local</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Thread local Info used for store thread local attributes.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">_ThreadLocalInfo</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reserve_class_name_in_scope</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">debug_runtime</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">reserve_class_name_in_scope</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get whether to save the network class name in the scope.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reserve_class_name_in_scope</span>

    <span class="nd">@reserve_class_name_in_scope</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">reserve_class_name_in_scope</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reserve_class_name_in_scope</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Set whether to save the network class name in the scope.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reserve_class_name_in_scope</span> <span class="o">=</span> <span class="n">reserve_class_name_in_scope</span>


<span class="n">_ContextRecord</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span>
    <span class="s2">&quot;_ContextRecord&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;is_pynative_mode&quot;</span><span class="p">,</span> <span class="s2">&quot;switch_context_fn&quot;</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">_ContextSwitchInfo</span><span class="p">(</span><span class="n">threading</span><span class="o">.</span><span class="n">local</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Record of context switch information.</span>

<span class="sd">    Args:</span>
<span class="sd">        is_pynative (bool): Whether to adopt the PyNative mode.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_pynative</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">_ContextSwitchInfo</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context_stack</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">is_pynative</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">push</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">push</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_pynative</span><span class="p">,</span> <span class="n">switch_context_fn</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Push a context switch record onto the stack.</span>

<span class="sd">        Args:</span>
<span class="sd">            is_pynative (bool): Whether context switch to PyNative mode.</span>
<span class="sd">            switch_context_fn (Function): A callable that executes the context switch.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">switch_context_fn</span><span class="p">,</span> <span class="n">FunctionType</span><span class="p">):</span>
            <span class="n">switch_context_fn</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context_stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">_ContextRecord</span><span class="p">(</span><span class="n">is_pynative</span><span class="p">,</span> <span class="n">switch_context_fn</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">pop</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context_stack</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">_Context</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    _Context is the environment in which operations are executed</span>

<span class="sd">    Note:</span>
<span class="sd">        Create a context through instantiating Context object is not recommended.</span>
<span class="sd">        should use context() to get the context since Context is a singleton.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_instance</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">_instance_lock</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Lock</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_instance</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_instance_lock</span><span class="o">.</span><span class="n">acquire</span><span class="p">()</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_instance</span> <span class="o">=</span> <span class="nb">object</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_instance_lock</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_instance</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_info</span> <span class="o">=</span> <span class="n">_ThreadLocalInfo</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_context_switches</span> <span class="o">=</span> <span class="n">_ContextSwitchInfo</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span> <span class="o">=</span> <span class="n">MSContext</span><span class="o">.</span><span class="n">get_instance</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_support_binary</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enable_compile_cache</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="fm">__getattribute__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr</span><span class="p">):</span>
        <span class="n">value</span> <span class="o">=</span> <span class="nb">object</span><span class="o">.</span><span class="fm">__getattribute__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attr</span> <span class="o">==</span> <span class="s2">&quot;_context_handle&quot;</span> <span class="ow">and</span> <span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Get </span><span class="si">{}</span><span class="s2"> failed, please check whether &#39;env_config_path&#39; is correct.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">attr</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">get_param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="o">.</span><span class="n">get_param</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_mode</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get current mode.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">mode</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_mode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Switch between Graph mode and PyNative mode.</span>

<span class="sd">        Args:</span>
<span class="sd">            mode (int): GRAPH_MODE or PYNATIVE_MODE.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">PYNATIVE_MODE</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_debug_runtime</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">set_backend_policy</span><span class="p">(</span><span class="s2">&quot;vm&quot;</span><span class="p">)</span>
            <span class="n">parallel_mode</span> <span class="o">=</span> <span class="n">_get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;parallel_mode&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">parallel_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">DATA_PARALLEL</span><span class="p">,</span> <span class="n">ParallelMode</span><span class="o">.</span><span class="n">STAND_ALONE</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Got </span><span class="si">{</span><span class="n">parallel_mode</span><span class="si">}</span><span class="s2">, when the user enabled SEMI_AUTO_PARALELL or AUTO_PARALLEL, &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;pynative mode dose not support, you should set &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;context.set_auto_parallel_context(parallel_mode=&#39;data_parallel&#39;) &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;or context.set_auto_parallel_context(parallel_mode=&#39;stand_alone&#39;).&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_context_switches</span><span class="o">.</span><span class="n">push</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">GRAPH_MODE</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_debug_runtime</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">set_backend_policy</span><span class="p">(</span><span class="s2">&quot;ge&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_context_switches</span><span class="o">.</span><span class="n">push</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;mode&#39; should be context.GRAPH_MODE (0) &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;or context.PYNATIVE_MODE (1), but got </span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">mode</span><span class="p">,</span> <span class="n">mode</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_backend_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">policy</span><span class="p">):</span>
        <span class="n">success</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="o">.</span><span class="n">set_backend_policy</span><span class="p">(</span><span class="n">policy</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">success</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Backend policy must be one of values in [&#39;ge&#39;, &#39;vm&#39;, &#39;ms&#39;]. &quot;</span>
                               <span class="s2">&quot;But got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">policy</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">set_save_graphs_path</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">save_graphs_path</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">save_graphs_path</span><span class="p">,</span> <span class="n">_make_directory</span><span class="p">(</span><span class="n">save_graphs_path</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">set_device_target</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The target device to run, support &quot;Ascend&quot;, &quot;GPU&quot;, and &quot;CPU&quot;.</span>

<span class="sd">        Args:</span>
<span class="sd">            target (str): &quot;Ascend&quot;, &quot;GPU&quot;, and &quot;CPU&quot;.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">valid_targets</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;CPU&quot;</span><span class="p">,</span> <span class="s2">&quot;GPU&quot;</span><span class="p">,</span> <span class="s2">&quot;Ascend&quot;</span><span class="p">,</span> <span class="s2">&quot;Davinci&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">target</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">valid_targets</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;device_target&#39; must be one of &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">valid_targets</span><span class="si">}</span><span class="s2">, but got </span><span class="si">{</span><span class="n">target</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">target</span> <span class="o">==</span> <span class="s2">&quot;Davinci&quot;</span><span class="p">:</span>
            <span class="n">target</span> <span class="o">=</span> <span class="s2">&quot;Ascend&quot;</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;The device &#39;Davinci&#39; is deprecated and will be removed in the next version. &quot;</span>
                           <span class="s2">&quot;For &#39;context.set_context&#39;, please set the argument &#39;device_target&#39; &quot;</span>
                           <span class="s2">&quot;to &#39;CPU&#39;, &#39;GPU&#39; or &#39;Ascend&#39;,if you set it to &#39;Davinci&#39;, it will be automatically &quot;</span>
                           <span class="s2">&quot;changed to &#39;Ascend&#39;.&quot;</span><span class="p">)</span>
        <span class="c1"># If in Parameter Server mode, Ascend card should not be used by server and scheduler.</span>
        <span class="k">if</span> <span class="n">_need_reset_device_target_for_ps</span><span class="p">(</span><span class="n">target</span><span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Reset device target to CPU when set_device_target.&quot;</span><span class="p">)</span>
            <span class="n">target</span> <span class="o">=</span> <span class="s2">&quot;CPU&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">device_target</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_debug_runtime</span> <span class="ow">and</span> <span class="n">target</span> <span class="o">==</span> <span class="s2">&quot;CPU&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_backend_policy</span><span class="p">(</span><span class="s2">&quot;vm&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_auto_tune_mode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tune_mode</span><span class="p">):</span>
        <span class="n">candidate</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;NO_TUNE&quot;</span><span class="p">,</span> <span class="s2">&quot;RL&quot;</span><span class="p">,</span> <span class="s2">&quot;GA&quot;</span><span class="p">,</span> <span class="s2">&quot;RL,GA&quot;</span><span class="p">,</span> <span class="s2">&quot;GA,RL&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">tune_mode</span> <span class="ow">in</span> <span class="n">candidate</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">tune_mode</span><span class="p">,</span> <span class="n">tune_mode</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;auto_tune_mode&#39; must be in &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;[&#39;NO_TUNE&#39;, &#39;RL&#39;, &#39;GA&#39;, &#39;RL,GA&#39;, &#39;GA,RL&#39;], but got </span><span class="si">{</span><span class="n">tune_mode</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_device_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device_id</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">device_id</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">device_id</span> <span class="o">&gt;</span> <span class="mi">4095</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;device_id&#39; must be in range [0, 4095], &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">device_id</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">device_id</span><span class="p">,</span> <span class="n">device_id</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_max_call_depth</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_call_depth</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">max_call_depth</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;max_call_depth&#39; must be greater than 0, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">max_call_depth</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">max_call_depth</span><span class="p">,</span> <span class="n">max_call_depth</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_profiling_options</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">option</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">option</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;profiling_option&#39; must be string, &quot;</span>
                            <span class="s2">&quot;but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">option</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">profiling_options</span><span class="p">,</span> <span class="n">option</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_variable_memory_max_size</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">variable_memory_max_size</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;set values of variable_memory_max_size and graph_memory_max_size&quot;&quot;&quot;</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;For &#39;context.set_context&#39;, the parameter &#39;variable_memory_max_size&#39; is deprecated, &quot;</span>
                       <span class="s2">&quot;and will be removed in a future &quot;</span>
                       <span class="s2">&quot;version. Please use parameter &#39;max_device_memory&#39; instead.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">Validator</span><span class="o">.</span><span class="n">check_str_by_regular</span><span class="p">(</span><span class="n">variable_memory_max_size</span><span class="p">,</span> <span class="n">_re_pattern</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;variable_memory_max_size&#39; should be in correct&quot;</span>
                             <span class="s2">&quot; format! It must be a string ending with &#39;GB&#39;, in addition to that, it must contain &quot;</span>
                             <span class="s2">&quot;only numbers or decimal points, such as </span><span class="se">\&quot;</span><span class="s2">5GB</span><span class="se">\&quot;</span><span class="s2"> or </span><span class="se">\&quot;</span><span class="s2">3.5GB</span><span class="se">\&quot;</span><span class="s2">, but got </span><span class="si">{}</span><span class="s2">.&quot;</span>
                             <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">variable_memory_max_size</span><span class="p">))</span>
        <span class="k">if</span> <span class="nb">float</span><span class="p">(</span><span class="n">variable_memory_max_size</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span> <span class="o">&gt;</span> <span class="n">_DEVICE_APP_MEMORY_SIZE</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;variable_memory_max_size&#39; should not be &quot;</span>
                             <span class="s2">&quot;greater than 31GB, but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">variable_memory_max_size</span><span class="p">))</span>
        <span class="n">variable_memory_max_size_</span> <span class="o">=</span> <span class="n">variable_memory_max_size</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot; * 1024 * 1024 * 1024&quot;</span>
        <span class="n">graph_memory_max_size</span> <span class="o">=</span> <span class="n">_DEVICE_APP_MEMORY_SIZE</span> <span class="o">-</span> <span class="nb">int</span><span class="p">(</span><span class="n">variable_memory_max_size</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>
        <span class="n">graph_memory_max_size_</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">graph_memory_max_size</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot; * 1024 * 1024 * 1024&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">variable_memory_max_size</span><span class="p">,</span> <span class="n">variable_memory_max_size_</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">_graph_memory_max_size</span><span class="p">,</span> <span class="n">graph_memory_max_size_</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_max_device_memory</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_device_memory</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">Validator</span><span class="o">.</span><span class="n">check_str_by_regular</span><span class="p">(</span><span class="n">max_device_memory</span><span class="p">,</span> <span class="n">_re_pattern</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;max_device_memory&#39; should be in correct &quot;</span>
                             <span class="s2">&quot; format! It must be a string ending with &#39;GB&#39;, in addition to that, it must contain &quot;</span>
                             <span class="s2">&quot;only numbers or decimal points, such as </span><span class="se">\&quot;</span><span class="s2">5GB</span><span class="se">\&quot;</span><span class="s2"> or </span><span class="se">\&quot;</span><span class="s2">3.5GB</span><span class="se">\&quot;</span><span class="s2">, but got </span><span class="si">{}</span><span class="s2">.&quot;</span>
                             <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">max_device_memory</span><span class="p">))</span>
        <span class="n">max_device_memory_value</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">max_device_memory</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">max_device_memory_value</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;max_device_memory&#39; should not be </span><span class="se">\&quot;</span><span class="s2">0GB</span><span class="se">\&quot;</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">max_device_memory</span><span class="p">,</span> <span class="n">max_device_memory_value</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_mempool_block_size</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mempool_block_size</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Set the block size of memory pool.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">_get_mode</span><span class="p">()</span> <span class="o">==</span> <span class="n">GRAPH_MODE</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Graph mode doesn&#39;t support to set parameter &#39;mempool_block_size&#39; of context currently, &quot;</span>
                           <span class="s2">&quot;you can use context.set_context to set pynative mode.&quot;</span><span class="p">)</span>
            <span class="k">return</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">Validator</span><span class="o">.</span><span class="n">check_str_by_regular</span><span class="p">(</span><span class="n">mempool_block_size</span><span class="p">,</span> <span class="n">_re_pattern</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;mempool_block_size&#39; should be in &quot;</span>
                             <span class="s2">&quot;correct format! Such as </span><span class="se">\&quot;</span><span class="s2">10GB</span><span class="se">\&quot;</span><span class="s2">, &quot;</span>
                             <span class="s2">&quot;but got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mempool_block_size</span><span class="p">))</span>
        <span class="n">mempool_block_size_value</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">mempool_block_size</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">mempool_block_size_value</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;context.set_context&#39;,  the argument &#39;mempool_block_size&#39; should be &quot;</span>
                             <span class="s2">&quot;greater or equal to </span><span class="se">\&quot;</span><span class="s2">1GB</span><span class="se">\&quot;</span><span class="s2">, &quot;</span>
                             <span class="s2">&quot;but got </span><span class="si">{}</span><span class="s2">GB&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">mempool_block_size</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">])))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">mempool_block_size</span><span class="p">,</span> <span class="n">mempool_block_size_value</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_print_file_path</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">file_path</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Add timestamp suffix to file name. Sets print file path.&quot;&quot;&quot;</span>
        <span class="n">print_file_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">realpath</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">print_file_path</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">IOError</span><span class="p">(</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;print_file_path&#39; should be file path, &quot;</span>
                          <span class="s2">&quot;but got directory </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">file_path</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">print_file_path</span><span class="p">):</span>
            <span class="n">_path</span><span class="p">,</span> <span class="n">_file_name</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">print_file_path</span><span class="p">)</span>
            <span class="n">path</span> <span class="o">=</span> <span class="n">_make_directory</span><span class="p">(</span><span class="n">_path</span><span class="p">)</span>
            <span class="n">file_name</span> <span class="o">=</span> <span class="n">_get_print_file_name</span><span class="p">(</span><span class="n">_file_name</span><span class="p">)</span>
            <span class="n">full_file_name</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">file_name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">full_file_name</span> <span class="o">=</span> <span class="n">print_file_path</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">print_file_path</span><span class="p">,</span> <span class="n">full_file_name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_env_config_path</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env_config_path</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check and set env_config_path.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="o">.</span><span class="n">enable_dump_ir</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;env_config_path&#39; is not supported, please &quot;</span>
                             <span class="s2">&quot;enable ENABLE_DUMP_IR with &#39;-D on&#39; and recompile source firstly.&quot;</span><span class="p">)</span>
        <span class="n">env_config_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">realpath</span><span class="p">(</span><span class="n">env_config_path</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">env_config_path</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;context.set_context&#39;, the &#39;env_config_path&#39; file </span><span class="si">%r</span><span class="s2"> is not exists, &quot;</span>
                             <span class="s2">&quot;please check whether &#39;env_config_path&#39; is correct.&quot;</span> <span class="o">%</span> <span class="n">env_config_path</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">env_config_path</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
        <span class="k">except</span> <span class="p">(</span><span class="ne">TypeError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">)</span> <span class="k">as</span> <span class="n">exo</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">exo</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">For &#39;context.set_context&#39;, open or load the &#39;env_config_path&#39; file </span><span class="si">{}</span><span class="s2"> &quot;</span>
                             <span class="s2">&quot;failed, please check whether &#39;env_config_path&#39; is json file and correct, or may not &quot;</span>
                             <span class="s2">&quot;have permission to read it.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">env_config_path</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">env_config_path</span><span class="p">,</span> <span class="n">env_config_path</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_runtime_num_threads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">runtime_num_threads</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check and set runtime_num_threads.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">runtime_num_threads</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The num of thread must bigger than 0.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">runtime_num_threads</span><span class="p">,</span> <span class="n">runtime_num_threads</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_op_timeout</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op_timeout</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Set the maximum duration of executing an operator in seconds.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">op_timeout</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The num of op exe timeout must bigger than or equal to 0.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">op_timeout</span><span class="p">,</span> <span class="n">op_timeout</span><span class="p">)</span>

    <span class="n">setters</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;mode&#39;</span><span class="p">:</span> <span class="n">set_mode</span><span class="p">,</span>
        <span class="s1">&#39;save_graphs_path&#39;</span><span class="p">:</span> <span class="n">set_save_graphs_path</span><span class="p">,</span>
        <span class="s1">&#39;device_target&#39;</span><span class="p">:</span> <span class="n">set_device_target</span><span class="p">,</span>
        <span class="s1">&#39;device_id&#39;</span><span class="p">:</span> <span class="n">set_device_id</span><span class="p">,</span>
        <span class="s1">&#39;auto_tune_mode&#39;</span><span class="p">:</span> <span class="n">set_auto_tune_mode</span><span class="p">,</span>
        <span class="s1">&#39;max_call_depth&#39;</span><span class="p">:</span> <span class="n">set_max_call_depth</span><span class="p">,</span>
        <span class="s1">&#39;profiling_options&#39;</span><span class="p">:</span> <span class="n">set_profiling_options</span><span class="p">,</span>
        <span class="s1">&#39;variable_memory_max_size&#39;</span><span class="p">:</span> <span class="n">set_variable_memory_max_size</span><span class="p">,</span>
        <span class="s1">&#39;max_device_memory&#39;</span><span class="p">:</span> <span class="n">set_max_device_memory</span><span class="p">,</span>
        <span class="s1">&#39;mempool_block_size&#39;</span><span class="p">:</span> <span class="n">set_mempool_block_size</span><span class="p">,</span>
        <span class="s1">&#39;print_file_path&#39;</span><span class="p">:</span> <span class="n">set_print_file_path</span><span class="p">,</span>
        <span class="s1">&#39;env_config_path&#39;</span><span class="p">:</span> <span class="n">set_env_config_path</span><span class="p">,</span>
        <span class="s1">&#39;runtime_num_threads&#39;</span><span class="p">:</span> <span class="n">set_runtime_num_threads</span><span class="p">,</span>
        <span class="s1">&#39;op_timeout&#39;</span><span class="p">:</span> <span class="n">set_op_timeout</span>
    <span class="p">}</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">reserve_class_name_in_scope</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get whether to save the network class name in the scope.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_info</span><span class="o">.</span><span class="n">reserve_class_name_in_scope</span>

    <span class="nd">@reserve_class_name_in_scope</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">reserve_class_name_in_scope</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reserve_class_name_in_scope</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Set whether to save the network class name in the scope.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">reserve_class_name_in_scope</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;context.set_context&#39;, the type of the property &#39;reserve_class_name_in_scope&#39; must &quot;</span>
                             <span class="s2">&quot;be bool, but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">reserve_class_name_in_scope</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_info</span><span class="o">.</span><span class="n">reserve_class_name_in_scope</span> <span class="o">=</span> <span class="n">reserve_class_name_in_scope</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">enable_ge</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="o">.</span><span class="n">get_backend_policy</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;ge&#39;</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">enable_debug_runtime</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_info</span><span class="o">.</span><span class="n">debug_runtime</span>

    <span class="nd">@enable_debug_runtime</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">enable_debug_runtime</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enable</span><span class="p">):</span>
        <span class="n">thread_info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_info</span>
        <span class="n">thread_info</span><span class="o">.</span><span class="n">debug_runtime</span> <span class="o">=</span> <span class="n">enable</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">support_binary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Whether support run .pyc or .so in graph mode.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_support_binary</span>

    <span class="nd">@support_binary</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">support_binary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">support</span><span class="p">:</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">support</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The attribute &#39;support_binary&#39; should be a bool, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">support</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_support_binary</span> <span class="o">=</span> <span class="n">support</span>



<span class="k">def</span> <span class="nf">_context</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get the global _context, if context is not created, create a new one.</span>

<span class="sd">    Returns:</span>
<span class="sd">        _Context, the global context in PyNative mode.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">_k_context</span>
    <span class="k">if</span> <span class="n">_k_context</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">default_backend</span> <span class="o">=</span> <span class="s1">&#39;debug&#39;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">default_config</span>
            <span class="n">default_backend</span> <span class="o">=</span> <span class="n">default_config</span><span class="o">.</span><span class="n">__backend__</span>
        <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;import default config fail&quot;</span><span class="p">)</span>
        <span class="n">_k_context</span> <span class="o">=</span> <span class="n">_Context</span><span class="p">()</span>
        <span class="n">_k_context</span><span class="o">.</span><span class="n">enable_debug_runtime</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">default_backend</span> <span class="o">==</span> <span class="s1">&#39;debug&#39;</span><span class="p">:</span>
            <span class="n">_k_context</span><span class="o">.</span><span class="n">enable_debug_runtime</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">default_backend</span> <span class="o">=</span> <span class="s1">&#39;vm&#39;</span>
        <span class="n">_k_context</span><span class="o">.</span><span class="n">set_backend_policy</span><span class="p">(</span><span class="n">default_backend</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_k_context</span>


<div class="viewcode-block" id="set_auto_parallel_context"><a class="viewcode-back" href="../../api_python/mindspore/mindspore.set_auto_parallel_context.html#mindspore.set_auto_parallel_context">[docs]</a><span class="nd">@args_type_check</span><span class="p">(</span><span class="n">device_num</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">global_rank</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">gradients_mean</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">gradient_fp32_sync</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">parallel_mode</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
                 <span class="n">auto_parallel_search_mode</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">search_mode</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">parameter_broadcast</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">strategy_ckpt_load_file</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
                 <span class="n">strategy_ckpt_save_file</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">full_batch</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">enable_parallel_optimizer</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">enable_alltoall</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span>
                 <span class="n">all_reduce_fusion_config</span><span class="o">=</span><span class="nb">list</span><span class="p">,</span> <span class="n">pipeline_stages</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">grad_accumulation_step</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
                 <span class="n">parallel_optimizer_config</span><span class="o">=</span><span class="nb">dict</span><span class="p">,</span> <span class="n">comm_fusion</span><span class="o">=</span><span class="nb">dict</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">set_auto_parallel_context</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Set auto parallel context, only data parallel supported on CPU.</span>

<span class="sd">    Note:</span>
<span class="sd">        Attribute name is required for setting attributes.</span>
<span class="sd">        If a program has tasks on different parallel modes, before setting a new parallel mode for the</span>
<span class="sd">        next task, interface :func:`mindspore.reset_auto_parallel_context` should be called to reset</span>
<span class="sd">        the configuration.</span>
<span class="sd">        Setting or changing parallel modes must be called before creating any Initializer, otherwise,</span>
<span class="sd">        it may have RuntimeError when compiling the network.</span>

<span class="sd">    Some configurations are parallel mode specific, see the below table for details:</span>

<span class="sd">    ===========================  ===========================</span>
<span class="sd">    Common                       AUTO_PARALLEL</span>
<span class="sd">    ===========================  ===========================</span>
<span class="sd">    device_num                   gradient_fp32_sync</span>
<span class="sd">    global_rank                  loss_repeated_mean</span>
<span class="sd">    gradients_mean               search_mode</span>
<span class="sd">    parallel_mode                strategy_ckpt_load_file</span>
<span class="sd">    all_reduce_fusion_config     strategy_ckpt_save_file</span>
<span class="sd">    enable_parallel_optimizer    dataset_strategy</span>
<span class="sd">    parallel_optimizer_config    pipeline_stages</span>
<span class="sd">    enable_alltoall              grad_accumulation_step</span>
<span class="sd">               \                 auto_parallel_search_mode</span>
<span class="sd">               \                 comm_fusion</span>
<span class="sd">    ===========================  ===========================</span>

<span class="sd">    Args:</span>
<span class="sd">        device_num (int): Available device number, the value must be in [1, 4096]. Default: 1.</span>
<span class="sd">        global_rank (int): Global rank id, the value must be in [0, 4095]. Default: 0.</span>
<span class="sd">        gradients_mean (bool): Whether to perform mean operator after allreduce of gradients.</span>
<span class="sd">                     &quot;stand_alone&quot; do not support gradients_mean. Default: False.</span>
<span class="sd">        gradient_fp32_sync (bool): Run allreduce of gradients in fp32. &quot;stand_alone&quot;, &quot;data_parallel&quot;</span>
<span class="sd">                     and &quot;hybrid_parallel&quot; do not support gradient_fp32_sync. Default: True.</span>
<span class="sd">        parallel_mode (str): There are five kinds of parallel modes, &quot;stand_alone&quot;, &quot;data_parallel&quot;,</span>
<span class="sd">                     &quot;hybrid_parallel&quot;, &quot;semi_auto_parallel&quot; and &quot;auto_parallel&quot;. Note the pynative mode only supports</span>
<span class="sd">                     the &quot;stand_alone&quot; and &quot;data_parallel&quot; mode. Default: &quot;stand_alone&quot;.</span>

<span class="sd">                     - stand_alone: Only one processor is working.</span>

<span class="sd">                     - data_parallel: Distributes the data across different processors.</span>

<span class="sd">                     - hybrid_parallel: Achieves data parallelism and model parallelism manually.</span>

<span class="sd">                     - semi_auto_parallel: Achieves data and model parallelism by setting parallel strategies.</span>

<span class="sd">                     - auto_parallel: Achieving parallelism automatically.</span>
<span class="sd">        search_mode (str): There are three kinds of shard strategy search modes: &quot;recursive_programming&quot;,</span>
<span class="sd">                     &quot;dynamic_programming&quot; and &quot;sharding_propagation&quot;. Default: &quot;dynamic_programming&quot;.</span>

<span class="sd">                     - recursive_programming: Recursive programming search mode.</span>

<span class="sd">                     - dynamic_programming: Dynamic programming search mode.</span>

<span class="sd">                     - sharding_propagation: Propagate shardings from configured ops to non-configured ops.</span>
<span class="sd">        auto_parallel_search_mode (str): This is the old version of &#39;search_mode&#39;. Here, remaining this attribute is</span>
<span class="sd">                     for forward compatibility, and this attribute will be deleted in a future MindSpore version.</span>
<span class="sd">        parameter_broadcast (bool): Whether to broadcast parameters before training. Before training, in order to have</span>
<span class="sd">                     the same network initialization parameter values for all devices, broadcast the parameters</span>
<span class="sd">                     on device 0 to other devices. Parameter broadcasting in different parallel modes is different,</span>
<span class="sd">                     data_parallel mode, all parameters are broadcast except for the parameter whose attribute</span>
<span class="sd">                     layerwise_parallel is True. Hybrid_parallel, semi_auto_parallel and auto_parallel mode, the</span>
<span class="sd">                     segmented parameters do not participate in broadcasting. Default: False.</span>
<span class="sd">        strategy_ckpt_load_file (str): The path to load parallel strategy checkpoint. Default: &#39;&#39;</span>
<span class="sd">        strategy_ckpt_save_file (str): The path to save parallel strategy checkpoint. Default: &#39;&#39;</span>
<span class="sd">        full_batch (bool): If you load whole batch datasets in auto_parallel mode, this parameter</span>
<span class="sd">                       should be set as True. Default: False. The interface is not to be recommended currently,</span>
<span class="sd">                       it is better using &#39;dataset_strategy&#39; to replace it.</span>
<span class="sd">        dataset_strategy (Union[str, tuple]): Dataset sharding strategy. Default: &quot;data_parallel&quot;.</span>
<span class="sd">                       dataset_strategy=&quot;data_parallel&quot; is equal to full_batch=False, dataset_strategy=&quot;full_batch&quot; is</span>
<span class="sd">                       equal to full_batch=True. For dataset load into net by model parallel strategy likes</span>
<span class="sd">                       ds_stra ((1, 8), (1, 8)), it requires using set_auto_parallel_context(dataset_strategy=ds_stra).</span>
<span class="sd">        enable_parallel_optimizer (bool): This is a developing feature, which shards the weight update computation for</span>
<span class="sd">                       data parallel training in the benefit of time and memory saving. Currently, auto and semi auto</span>
<span class="sd">                       parallel mode support all optimizers in both Ascend and GPU. Data parallel mode only supports</span>
<span class="sd">                       `Lamb` and `AdamWeightDecay` in Ascend . Default: False.</span>
<span class="sd">        enable_alltoall (bool): A switch that allows AllToAll operators to be generated during communication. If its</span>
<span class="sd">                        value is False, there will be a combination of operators such as AllGather, Split and Concat</span>
<span class="sd">                        instead of AllToAll. Default: False.</span>
<span class="sd">        all_reduce_fusion_config (list): Set allreduce fusion strategy by parameters indices. Only support ReduceOp.SUM</span>
<span class="sd">                       and HCCL_WORLD_GROUP/NCCL_WORLD_GROUP. No Default, if it is not set, the fusion is closed.</span>
<span class="sd">        pipeline_stages (int): Set the stage information for pipeline parallel. This indicates how the devices are</span>
<span class="sd">                        distributed alone in the pipeline. The total devices will be divided into &#39;pipeline_stags&#39;</span>
<span class="sd">                        stages. Currently, this could only be used when parallel mode semi_auto_parallel is enabled.</span>
<span class="sd">                        Default: 1.</span>
<span class="sd">        grad_accumulation_step (int): Set the accumulation steps of gradients in auto and semi auto parallel mode.</span>
<span class="sd">                        This should be a positive int. Default: 1.</span>
<span class="sd">        parallel_optimizer_config (dict): A dict contains the keys and values for setting the parallel optimizer</span>
<span class="sd">                        configure. The configure provides more detailed behavior control about parallel training</span>
<span class="sd">                        when parallel optimizer is enabled. Currently it supports the key `gradient_accumulation_shard`.</span>
<span class="sd">                        The configure will be effective when we use</span>
<span class="sd">                        mindspore.set_auto_parallel_context(enable_parallel_optimizer=True).</span>
<span class="sd">                        It supports the following keys.</span>

<span class="sd">                        - gradient_accumulation_shard(bool): If true, the accumulation gradient parameters will be</span>
<span class="sd">                          sharded across the data parallel devices. This will</span>
<span class="sd">                          introduce additional communication(ReduceScatter) at</span>
<span class="sd">                          each step when accumulate the gradients, but saves a</span>
<span class="sd">                          lot of device memories, thus can make model be trained</span>
<span class="sd">                          with larger batch size. This configure is effective only</span>
<span class="sd">                          when the model runs on pipeline training or gradient</span>
<span class="sd">                          accumulation with data parallel. Default True.</span>

<span class="sd">                        - parallel_optimizer_threshold(int): Set the threshold of parallel optimizer. When parallel</span>
<span class="sd">                          optimizer is enabled, parameters with size smaller than this threshold will not be sharded</span>
<span class="sd">                          across the devices. Parameter size = shape[0] \* ... \* shape[n] \* size(dtype). Non-negative.</span>
<span class="sd">                          Unit: KB. Default: 64.</span>

<span class="sd">        comm_fusion (dict): A dict contains the types and configurations for setting the communication fusion. each</span>
<span class="sd">                        communication fusion config has two keys: &quot;mode&quot; and &quot;config&quot;.</span>
<span class="sd">                        It supports following communication fusion types and configurations:</span>

<span class="sd">                        - allreduce: If communication fusion type is `allreduce`. The `mode` contains: `auto`, `size`</span>
<span class="sd">                          and `index`. In `auto` mode, AllReduce fusion is configured by gradients size and the default</span>
<span class="sd">                          fusion threshold is `64` MB. In &#39;size&#39; mode, AllReduce fusion is configured by gradients size</span>
<span class="sd">                          manually, and the fusion threshold must be larger than `0` MB. In `index` mode, it is same as</span>
<span class="sd">                          `all_reduce_fusion_config`.</span>

<span class="sd">                        - allgather: If communication fusion type is `allgather`. The `mode` contains: `auto`, `size`.</span>
<span class="sd">                          In `auto` mode, AllGather fusion is configured by gradients size, and the default fusion</span>
<span class="sd">                          threshold is `64` MB. In &#39;size&#39; mode, AllGather fusion is configured by gradients size</span>
<span class="sd">                          manually, and the fusion threshold must be larger than `0` MB.</span>

<span class="sd">                        - reducescatter: If communication fusion type is `reducescatter`. The `mode` contains: `auto`</span>
<span class="sd">                          and `size`. Config is same as `allgather`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If input key is not attribute in auto parallel context.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; ms.set_auto_parallel_context(device_num=8)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_auto_parallel_context(global_rank=0)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_auto_parallel_context(gradients_mean=True)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_auto_parallel_context(gradient_fp32_sync=False)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_auto_parallel_context(parallel_mode=&quot;auto_parallel&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_auto_parallel_context(search_mode=&quot;dynamic_programming&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_auto_parallel_context(auto_parallel_search_mode=&quot;dynamic_programming&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_auto_parallel_context(parameter_broadcast=False)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_auto_parallel_context(strategy_ckpt_load_file=&quot;./strategy_stage1.ckpt&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_auto_parallel_context(strategy_ckpt_save_file=&quot;./strategy_stage1.ckpt&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_auto_parallel_context(dataset_strategy=((1, 8), (1, 8)))</span>
<span class="sd">        &gt;&gt;&gt; ms.set_auto_parallel_context(enable_parallel_optimizer=False)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_auto_parallel_context(enable_alltoall=False)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_auto_parallel_context(all_reduce_fusion_config=[8, 160])</span>
<span class="sd">        &gt;&gt;&gt; ms.set_auto_parallel_context(pipeline_stages=2)</span>
<span class="sd">        &gt;&gt;&gt; parallel_config = {&quot;gradient_accumulation_shard&quot;: True, &quot;parallel_optimizer_threshold&quot;: 24}</span>
<span class="sd">        &gt;&gt;&gt; ms.set_auto_parallel_context(parallel_optimizer_config=parallel_config, enable_parallel_optimizer=True)</span>
<span class="sd">        &gt;&gt;&gt; config = {&quot;allreduce&quot;: {&quot;mode&quot;: &quot;size&quot;, &quot;config&quot;: 32}, &quot;allgather&quot;: {&quot;mode&quot;: &quot;size&quot;, &quot;config&quot;: 32}}</span>
<span class="sd">        &gt;&gt;&gt; ms.set_auto_parallel_context(comm_fusion=config)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_set_auto_parallel_context</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>


<div class="viewcode-block" id="get_auto_parallel_context"><a class="viewcode-back" href="../../api_python/mindspore/mindspore.get_auto_parallel_context.html#mindspore.get_auto_parallel_context">[docs]</a><span class="k">def</span> <span class="nf">get_auto_parallel_context</span><span class="p">(</span><span class="n">attr_key</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get auto parallel context attribute value according to the key.</span>

<span class="sd">    Args:</span>
<span class="sd">        attr_key (str): The key of the attribute.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Returns attribute value according to the key.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If input key is not attribute in auto parallel context.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; parallel_mode = ms.get_auto_parallel_context(&quot;parallel_mode&quot;)</span>
<span class="sd">        &gt;&gt;&gt; dataset_strategy = ms.get_auto_parallel_context(&quot;dataset_strategy&quot;)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_get_auto_parallel_context</span><span class="p">(</span><span class="n">attr_key</span><span class="p">)</span></div>


<div class="viewcode-block" id="reset_auto_parallel_context"><a class="viewcode-back" href="../../api_python/mindspore/mindspore.reset_auto_parallel_context.html#mindspore.reset_auto_parallel_context">[docs]</a><span class="k">def</span> <span class="nf">reset_auto_parallel_context</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reset auto parallel context attributes to the default values:</span>

<span class="sd">    - device_num: 1.</span>
<span class="sd">    - global_rank: 0.</span>
<span class="sd">    - gradients_mean: False.</span>
<span class="sd">    - gradient_fp32_sync: True.</span>
<span class="sd">    - parallel_mode: &#39;stand_alone&#39;.</span>
<span class="sd">    - search_mode: &#39;dynamic_programming&#39;.</span>
<span class="sd">    - auto_parallel_search_mode: &#39;dynamic_programming&#39;.</span>
<span class="sd">    - parameter_broadcast: False.</span>
<span class="sd">    - strategy_ckpt_load_file: &#39;&#39;.</span>
<span class="sd">    - strategy_ckpt_save_file: &#39;&#39;.</span>
<span class="sd">    - full_batch: False.</span>
<span class="sd">    - enable_parallel_optimizer: False.</span>
<span class="sd">    - enable_alltoall: False.</span>
<span class="sd">    - pipeline_stages: 1.</span>
<span class="sd">    - fusion_threshold: 64.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_reset_auto_parallel_context</span><span class="p">()</span></div>


<span class="k">def</span> <span class="nf">_check_target_specific_cfgs</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">arg_key</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Checking whether a config is suitable for a specified device&quot;&quot;&quot;</span>
    <span class="n">device_cfgs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;enable_graph_kernel&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Ascend&#39;</span><span class="p">,</span> <span class="s1">&#39;GPU&#39;</span><span class="p">,</span> <span class="s1">&#39;CPU&#39;</span><span class="p">],</span>
        <span class="s1">&#39;graph_kernel_flags&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Ascend&#39;</span><span class="p">,</span> <span class="s1">&#39;GPU&#39;</span><span class="p">,</span> <span class="s1">&#39;CPU&#39;</span><span class="p">],</span>
        <span class="s1">&#39;enable_reduce_precision&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Ascend&#39;</span><span class="p">],</span>
        <span class="s1">&#39;print_file_path&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Ascend&#39;</span><span class="p">],</span>
        <span class="s1">&#39;variable_memory_max_size&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Ascend&#39;</span><span class="p">],</span>
        <span class="s1">&#39;auto_tune_mode&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Ascend&#39;</span><span class="p">],</span>
        <span class="s1">&#39;max_device_memory&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Ascend&#39;</span><span class="p">,</span> <span class="s1">&#39;GPU&#39;</span><span class="p">],</span>
        <span class="s1">&#39;mempool_block_size&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;GPU&#39;</span><span class="p">,</span> <span class="s1">&#39;Ascend&#39;</span><span class="p">],</span>
        <span class="s1">&#39;disable_format_transform&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;GPU&#39;</span><span class="p">]</span>
    <span class="p">}</span>
    <span class="c1"># configs not in map device_cfgs are supposed to be suitable for all devices</span>
    <span class="k">if</span> <span class="n">arg_key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">device_cfgs</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="n">supported_devices</span> <span class="o">=</span> <span class="n">device_cfgs</span><span class="p">[</span><span class="n">arg_key</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">device</span> <span class="ow">in</span> <span class="n">supported_devices</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.set_context&#39;, when set the argument &#39;</span><span class="si">{</span><span class="n">arg_key</span><span class="si">}</span><span class="s2">&#39;, &quot;</span>
                   <span class="sa">f</span><span class="s2">&quot;the argument &#39;device_target&#39; only supports devices in &#39;</span><span class="si">{</span><span class="n">supported_devices</span><span class="si">}</span><span class="s2">&#39;, &quot;</span>
                   <span class="sa">f</span><span class="s2">&quot;but got &#39;</span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&#39;, ignore it.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">False</span>


<div class="viewcode-block" id="set_context"><a class="viewcode-back" href="../../api_python/mindspore/mindspore.set_context.html#mindspore.set_context">[docs]</a><span class="nd">@args_unreset_check</span><span class="p">(</span><span class="n">device_id</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">variable_memory_max_size</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">max_device_memory</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">mempool_block_size</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>
<span class="nd">@args_type_check</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">precompile_only</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">device_id</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">save_graphs</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span>
                 <span class="n">save_graphs_path</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">enable_dump</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">auto_tune_mode</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
                 <span class="n">save_dump_path</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">enable_reduce_precision</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">variable_memory_max_size</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
                 <span class="n">enable_auto_mixed_precision</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span>
                 <span class="n">enable_graph_kernel</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">reserve_class_name_in_scope</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">check_bprop</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span>
                 <span class="n">max_device_memory</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">print_file_path</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">max_call_depth</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">env_config_path</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
                 <span class="n">graph_kernel_flags</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">save_compile_cache</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">runtime_num_threads</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">load_compile_cache</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span>
                 <span class="n">grad_for_scalar</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">pynative_synchronize</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">mempool_block_size</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">disable_format_transform</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span>
                 <span class="n">op_timeout</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">set_context</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Set context for running environment.</span>

<span class="sd">    Context should be configured before running your program. If there is no configuration,</span>
<span class="sd">    it will be automatically set according to the device target by default.</span>

<span class="sd">    Note:</span>
<span class="sd">        Attribute name is required for setting attributes.</span>
<span class="sd">        The mode is not recommended to be changed after net was initialized because the implementations of some</span>
<span class="sd">        operations are different in graph mode and pynative mode. Default: GRAPH_MODE.</span>

<span class="sd">    Some configurations are device specific, see the below table for details:</span>

<span class="sd">    +-------------------------+------------------------------+----------------------------+</span>
<span class="sd">    | Function Classification |   Configuration Parameters   |   Hardware Platform Support|</span>
<span class="sd">    +=========================+==============================+============================+</span>
<span class="sd">    | System Configuration    |   device_id                  |   CPU/GPU/Ascend           |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |   device_target              |   CPU/GPU/Ascend           |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  max_device_memory           |  GPU/Ascend                |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  variable_memory_max_size    |  Ascend                    |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  mempool_block_size          |  GPU/Ascend                |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  op_timeout                  |  Ascend                    |</span>
<span class="sd">    +-------------------------+------------------------------+----------------------------+</span>
<span class="sd">    | Debug Configuration     |  save_graphs                 |  CPU/GPU/Ascend            |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  save_graphs_path            |  CPU/GPU/Ascend            |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  enable_dump                 |  Ascend                    |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  save_dump_path              |  Ascend                    |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  print_file_path             |  Ascend                    |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  env_config_path             |  CPU/GPU/Ascend            |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  precompile_only             |  CPU/GPU/Ascend            |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  reserve_class_name_in_scope |  CPU/GPU/Ascend            |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  pynative_synchronize        |  GPU/Ascend                |</span>
<span class="sd">    +-------------------------+------------------------------+----------------------------+</span>
<span class="sd">    | Executive Control       |   mode                       |   CPU/GPU/Ascend           |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  enable_graph_kernel         |  Ascend/GPU                |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  graph_kernel_flags          |  Ascend/GPU                |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  enable_reduce_precision     |  Ascend                    |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  auto_tune_mode              |  Ascend                    |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  check_bprop                 |  CPU/GPU/Ascend            |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  max_call_depth              |  CPU/GPU/Ascend            |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  grad_for_scalar             |  CPU/GPU/Ascend            |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  enable_compile_cache        |  CPU/GPU/Ascend            |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  runtime_num_threads         |  CPU/GPU/Ascend            |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  compile_cache_path          |  CPU/GPU/Ascend            |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  disable_format_transform    |  GPU                       |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  support_binary              |  CPU/GPU/Ascend            |</span>
<span class="sd">    +-------------------------+------------------------------+----------------------------+</span>

<span class="sd">    Args:</span>
<span class="sd">        device_id (int): ID of the target device, the value must be in [0, device_num_per_host-1],</span>
<span class="sd">            while device_num_per_host should be no more than 4096. Default: 0.</span>
<span class="sd">        device_target (str): The target device to run, support &quot;Ascend&quot;, &quot;GPU&quot;, and &quot;CPU&quot;.</span>
<span class="sd">            If device target is not set, the version of MindSpore package is used.</span>
<span class="sd">        max_device_memory (str): Set the maximum memory available for devices. The format is &quot;xxGB&quot;. Default: &quot;1024GB&quot;.</span>
<span class="sd">            The actual used memory size is the minimum of the available memory of the device and max_device_memory.</span>
<span class="sd">        variable_memory_max_size (str): This parameter is deprecated, and will be removed in a future version.</span>
<span class="sd">            Please use parameter &#39;max_device_memory&#39; instead.</span>
<span class="sd">        mempool_block_size (str): Set the size of the memory pool block in PyNative mode for devices.</span>
<span class="sd">            The format is &quot;xxGB&quot;. Default: &quot;1GB&quot;. Minimum size is &quot;1G&quot;. The actual used memory block size is the minimum</span>
<span class="sd">            of the available memory of the device and mempool_block_size.</span>
<span class="sd">        op_timeout (int): Set the maximum duration of executing an operator in seconds.</span>
<span class="sd">            If the execution time exceeds this value, system will terminate the task. 0 means endless wait.</span>
<span class="sd">            Default: 0.</span>
<span class="sd">        save_graphs (bool): Whether to save graphs. Default: False.</span>
<span class="sd">            When the `save_graphs` attribute is set as True, attribute of `save_graphs_path` is used to set the</span>
<span class="sd">            intermediate compilation graph storage path. By default, the graphs are saved in the current directory.</span>
<span class="sd">        save_graphs_path (str): Path to save graphs. Default: &quot;.&quot;.</span>
<span class="sd">            If the specified directory does not exist, the system will automatically create the directory.</span>
<span class="sd">            During distributed training, graphs will be saved to the directory of</span>
<span class="sd">            `save_graphs_path/rank_${rank_id}/`. `rank_id` is the ID of the current device in the cluster.</span>
<span class="sd">        enable_dump (bool): This parameters is deprecated, and will be deleted in the next version.</span>
<span class="sd">        save_dump_path (str): This parameters is deprecated, and will be deleted in the next version.</span>
<span class="sd">        print_file_path (str): The path of saving print data. If this parameter is set, print data is saved to</span>
<span class="sd">            a file by default, and print_file_path is not set, the screen will be displayed.</span>
<span class="sd">            If the saved file already exists, the timestamp suffix will be added to the file. Saving data to a file</span>
<span class="sd">            solves the problem of data loss in screen printing when a large amount of data is generated.</span>
<span class="sd">            If it is not set, an error will be reported: prompt to set the upper absolute path.</span>
<span class="sd">        env_config_path (str): Config path for DFX.</span>
<span class="sd">            Through mindspore.set_context(env_config_path=&quot;./mindspore_config.json&quot;)</span>

<span class="sd">            configure RDR:</span>

<span class="sd">            - enable: controls whether the RDR is enabled to collect the key data during training and</span>
<span class="sd">              save key data in the fault scenario. When set to true, the RDR will be turned on.</span>
<span class="sd">              When set to false, the RDR will be turned off.</span>
<span class="sd">            - mode: sets the mode of RDR on exporting data. When set to 1, the RDR only exports data</span>
<span class="sd">              in the fault scenario. When set to 2, the RDR exports data in the fault scenario and the</span>
<span class="sd">              normal end scenario. Default is 1.</span>
<span class="sd">            - path: sets the path where RDR saves data. The current path must be absolute.</span>

<span class="sd">            Memory reuse:</span>

<span class="sd">            - mem_Reuse: controls whether the memory reuse function is turned on. When set to True,</span>
<span class="sd">            - the memory reuse function is turned on. When set to False, the memory reuse function is turned off.</span>

<span class="sd">        precompile_only (bool): Whether to only precompile the network. Default: False.</span>
<span class="sd">            If set to True, the network will only be compiled, not executed.</span>
<span class="sd">        reserve_class_name_in_scope (bool) : Whether to save the network class name in the scope. Default: True.</span>
<span class="sd">            Each node has a scope. A scope of a subnode is the name of its parent node. If reserve_class_name_in_scope</span>
<span class="sd">            is set to True, the class name will be saved after keyword &#39;net-&#39; in the scope.</span>
<span class="sd">            For example:</span>

<span class="sd">            Default/net-Net1/net-Net2 (reserve_class_name_in_scope=True)</span>

<span class="sd">            Default/net/net (reserve_class_name_in_scope=False)</span>

<span class="sd">        pynative_synchronize (bool): Whether to enable synchronous execution of the device in PyNative mode.</span>
<span class="sd">            Default: False. When the value is set to False, the operator is executed asynchronously on the device.</span>
<span class="sd">            When an error occurs in the execution of the operator, the specific error script code location cannot</span>
<span class="sd">            be located, when the value is set to True, the operator is executed synchronously on the device. It will</span>
<span class="sd">            reduce the execution performance of the program. At this time, when an error occurs in the execution of</span>
<span class="sd">            the operator, the location of the error script code can be located according to the call stack of the error.</span>
<span class="sd">        mode (int): Running in GRAPH_MODE(0) or PYNATIVE_MODE(1). Default: GRAPH_MODE(0).</span>
<span class="sd">            GRAPH_MODE or PYNATIVE_MODE can be set by `mode` attribute and both modes support all backends, default</span>
<span class="sd">            mode is GRAPH_MODE.</span>
<span class="sd">        enable_graph_kernel (bool): Whether to enable graph kernel fusion to optimize network execution performance.</span>
<span class="sd">            Default: False.</span>
<span class="sd">            Indicates whether to enable image-computing convergence to optimize network execution performance.</span>
<span class="sd">            If enable_graph_kernel is set to True, acceleration can be enabled.</span>
<span class="sd">            For details of graph kernel fusion, please check</span>
<span class="sd">            `Enabling Graph Kernel Fusion</span>
<span class="sd">            &lt;https://www.mindspore.cn/tutorials/experts/en/r1.9/debug/graph_fusion_engine.html&gt;`_.</span>
<span class="sd">        graph_kernel_flags (str):</span>
<span class="sd">            Optimization options of graph kernel fusion, and the priority is higher when it conflicts</span>
<span class="sd">            with enable_graph_kernel. Only for experienced users.</span>
<span class="sd">            For example, mindspore.set_context(graph_kernel_flags=&quot;--opt_level=2 --dump_as_text&quot;). Some general options:</span>

<span class="sd">            - opt_level: Set the optimization level.</span>
<span class="sd">              Default: 2. Graph kernel fusion can be enabled equivalently by setting opt_level greater than 0.</span>
<span class="sd">              Available values are:</span>

<span class="sd">              - 0: disables graph kernel fusion;</span>
<span class="sd">              - 1: enables the basic fusion of operators;</span>
<span class="sd">              - 2: includes all optimizations of level 1,</span>
<span class="sd">                and turns on more optimizations such as CSE, arithmetic simplification and so on;</span>
<span class="sd">              - 3: includes all optimizations of level 2, and turns on more optimizations such as SitchingFusion,</span>
<span class="sd">                ParallelFusion and so on. Optimizations of this level are radical and unstable in some scenarios.</span>
<span class="sd">                Be caution when using this level.</span>

<span class="sd">            - dump_as_text: dumps detail info as text files. Default: false.</span>

<span class="sd">            More options can refer to the implementation code.</span>
<span class="sd">        enable_reduce_precision (bool): Whether to enable precision reduction.</span>
<span class="sd">            If the operator does not support the user-specified precision, the precision will</span>
<span class="sd">            be changed automatically. Default: True.</span>
<span class="sd">        auto_tune_mode (str): The mode of auto tune when op building, get the best tiling performance.</span>
<span class="sd">            Default: NO_TUNE. The value must be in [&#39;RL&#39;, &#39;GA&#39;, &#39;RL,GA&#39;].</span>

<span class="sd">            - RL: Reinforcement Learning tune.</span>
<span class="sd">            - GA: Genetic Algorithm tune.</span>
<span class="sd">            - RL,GA: When both RL and GA optimization are enabled, the tool automatically selects RL or GA based on</span>
<span class="sd">              different types of operators in the network model. The sequence of RL and GA is not differentiated.</span>
<span class="sd">              (Automatic selection).</span>

<span class="sd">            For more information about the enable operator tuning tool settings, please check</span>
<span class="sd">            `Enable the operator optimization tool</span>
<span class="sd">            &lt;https://www.mindspore.cn/tutorials/experts/en/r1.9/debug/auto_tune.html&gt;`_.</span>
<span class="sd">        check_bprop (bool): Whether to check back propagation nodes. The checking ensures that the shape and dtype</span>
<span class="sd">            of back propagation node outputs is the same as input parameters. Default: False.</span>
<span class="sd">        max_call_depth (int): Specify the maximum depth of function call. Must be positive integer. Default: 1000.</span>
<span class="sd">            The max_call_depth parameter needs to be set when the nested call is too deep or the number</span>
<span class="sd">            of subgraphs is too large. If max_call_depth is set larger than before, the system max stack depth should be</span>
<span class="sd">            set larger too, otherwise a `core dumped` exception may be raised because of system stack overflow.</span>
<span class="sd">        grad_for_scalar (bool):  Whether to get gradient for scalar. Default: False.</span>
<span class="sd">            When grad_for_scalar is set to True, the function&#39;s scalar input can be derived.</span>
<span class="sd">            The default value is False. Because the back-end does not support scaling operations currently,</span>
<span class="sd">            this interface only supports simple operations that can be deduced by the front-end.</span>
<span class="sd">        enable_compile_cache (bool): Whether to save or load the cache of the graph compiled by front-end.</span>
<span class="sd">            After enable_compile_cache is set to True, during the first execution, a hardware-independent</span>
<span class="sd">            compilation cache is generated and exported to a MINDIR file. When the network is executed again,</span>
<span class="sd">            if enable_compile_cache is still set to True and the network scripts are not changed,</span>
<span class="sd">            the compile cache is loaded. Note that only limited automatic detection for the changes of</span>
<span class="sd">            python scripts is supported by now, which means that there is a correctness risk. Default: False.</span>
<span class="sd">            This is an experimental prototype that is subject to change and/or deletion.</span>
<span class="sd">        compile_cache_path (str): Path to save the cache of the graph compiled by front-end. Default: &quot;.&quot;.</span>
<span class="sd">            If the specified directory does not exist, the system will automatically create the directory.</span>
<span class="sd">            The cache will be saved to the directory of `compile_cache_path/rank_${rank_id}/`. The `rank_id` is</span>
<span class="sd">            the ID of the current device in the cluster.</span>
<span class="sd">        runtime_num_threads(int): The thread pool number of cpu kernel and actor used in runtime,</span>
<span class="sd">            which must bigger than 0. Default value is 30, if you run many processes at</span>
<span class="sd">            the same time, you should set the value smaller to avoid thread contention.</span>
<span class="sd">        disable_format_transform (bool): Whether to disable the automatic format transform function from NCHW to NHWC.</span>
<span class="sd">            When the network training performance of fp16 is worse than fp32,</span>
<span class="sd">            `disable_format_transform` can be set to True to try to improve training performance. Default: False.</span>
<span class="sd">        support_binary (bool): Whether to support run .pyc or .so in graph mode. If want to support run .so or .pyc</span>
<span class="sd">            in graph mode, coulde set &#39;support_binary&#39; to be True, and run once .py file. It would save the source</span>
<span class="sd">            of the interfaces would be compiled by MindSpore to the interfaces definition .py file that should be</span>
<span class="sd">            guaranteed to be writable. Then compile the .py file to the .pyc or .so file, and could run in Graph mode.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If input key is not an attribute in context.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(mode=ms.PYNATIVE_MODE)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(precompile_only=True)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(device_target=&quot;Ascend&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(device_id=0)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(save_graphs=True, save_graphs_path=&quot;./model.ms&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(enable_reduce_precision=True)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(enable_graph_kernel=True)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(graph_kernel_flags=&quot;--opt_level=2 --dump_as_text&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(reserve_class_name_in_scope=True)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(variable_memory_max_size=&quot;6GB&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(check_bprop=True)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(max_device_memory=&quot;3.5GB&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(mempool_block_size=&quot;1GB&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(print_file_path=&quot;print.pb&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(max_call_depth=80)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(env_config_path=&quot;./env_config.json&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(auto_tune_mode=&quot;GA,RL&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(grad_for_scalar=True)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(enable_compile_cache=True, compile_cache_path=&quot;./cache.ms&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(pynative_synchronize=True)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(runtime_num_threads=10)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(disable_format_transform=True)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="p">()</span>
    <span class="c1"># set device target first</span>
    <span class="k">if</span> <span class="s1">&#39;device_target&#39;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">set_device_target</span><span class="p">(</span><span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;device_target&#39;</span><span class="p">])</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">get_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">device_target</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">device</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="n">__device_target__</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.set_context&#39;, package type </span><span class="si">{</span><span class="n">__package_name__</span><span class="si">}</span><span class="s2"> support &#39;device_target&#39; &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;type </span><span class="si">{</span><span class="n">__device_target__</span><span class="si">}</span><span class="s2">, but got </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">get_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">device_target</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="s1">&#39;enable_sparse&#39;</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.set_context&#39;, &#39;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&#39; parameter is deprecated, &quot;</span>
                           <span class="s2">&quot;and will be removed in the next version.&quot;</span><span class="p">)</span>
            <span class="k">continue</span>
        <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;enable_auto_mixed_precision&#39;</span><span class="p">,</span> <span class="s1">&#39;enable_dump&#39;</span><span class="p">,</span> <span class="s1">&#39;save_dump_path&#39;</span><span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.set_context&#39;, &#39;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&#39; parameter is deprecated. &quot;</span>
                           <span class="s2">&quot;For details, please see the interface parameter API comments&quot;</span><span class="p">)</span>
            <span class="k">continue</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_target_specific_cfgs</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
            <span class="k">continue</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
            <span class="k">continue</span>
        <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">ctx</span><span class="o">.</span><span class="n">setters</span><span class="p">:</span>
            <span class="n">ctx</span><span class="o">.</span><span class="n">setters</span><span class="p">[</span><span class="n">key</span><span class="p">](</span><span class="n">ctx</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
            <span class="k">continue</span>
        <span class="c1"># enum variables beginning with &#39;_&#39; are for internal use</span>
        <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">__members__</span> <span class="ow">and</span> <span class="n">key</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="s1">&#39;_&#39;</span><span class="p">:</span>
            <span class="n">ctx</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">__members__</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="n">value</span><span class="p">)</span>
            <span class="k">continue</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.set_context&#39;, the keyword argument </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2"> is not recognized! For detailed &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;usage of &#39;set_context&#39;, please refer to the Mindspore official website.&quot;</span><span class="p">)</span></div>


<div class="viewcode-block" id="get_context"><a class="viewcode-back" href="../../api_python/mindspore/mindspore.get_context.html#mindspore.get_context">[docs]</a><span class="k">def</span> <span class="nf">get_context</span><span class="p">(</span><span class="n">attr_key</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get context attribute value according to the input key.</span>
<span class="sd">    If some attributes are not set, they will be automatically obtained.</span>

<span class="sd">    Args:</span>
<span class="sd">        attr_key (str): The key of the attribute.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Object, The value of given attribute key.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If input key is not an attribute in context.</span>
<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; ms.get_context(&quot;device_target&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ms.get_context(&quot;device_id&quot;)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="p">()</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">get_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">device_target</span><span class="p">)</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">_check_target_specific_cfgs</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">attr_key</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">attr_key</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">attr_key</span><span class="p">)</span>
    <span class="c1"># enum variables beginning with &#39;_&#39; are for internal use</span>
    <span class="k">if</span> <span class="n">attr_key</span> <span class="ow">in</span> <span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">__members__</span> <span class="ow">and</span> <span class="n">attr_key</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="s1">&#39;_&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">ctx</span><span class="o">.</span><span class="n">get_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">__members__</span><span class="p">[</span><span class="n">attr_key</span><span class="p">])</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.get_context&#39;, the argument </span><span class="si">{</span><span class="n">attr_key</span><span class="si">}</span><span class="s2"> is not recognized! For detailed &quot;</span>
                     <span class="sa">f</span><span class="s2">&quot;usage of &#39;get_context&#39;, please refer to the Mindspore official website.&quot;</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_get_mode</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get execution mode. Only for internal using.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Object: The Value of execution mode.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">ctx</span><span class="o">.</span><span class="n">get_mode</span><span class="p">()</span>


<div class="viewcode-block" id="ParallelMode"><a class="viewcode-back" href="../../api_python/mindspore/mindspore.ParallelMode.html#mindspore.ParallelMode">[docs]</a><span class="k">class</span> <span class="nc">ParallelMode</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Parallel mode options.</span>

<span class="sd">    There are five kinds of parallel modes, &quot;STAND_ALONE&quot;, &quot;DATA_PARALLEL&quot;,</span>
<span class="sd">    &quot;HYBRID_PARALLEL&quot;, &quot;SEMI_AUTO_PARALLEL&quot; and &quot;AUTO_PARALLEL&quot;. Default: &quot;STAND_ALONE&quot;.</span>

<span class="sd">    - STAND_ALONE: Only one processor is working.</span>
<span class="sd">    - DATA_PARALLEL: Distributes the data across different processors.</span>
<span class="sd">    - HYBRID_PARALLEL: Achieves data parallelism and model parallelism manually.</span>
<span class="sd">    - SEMI_AUTO_PARALLEL: Achieves data parallelism and model parallelism by setting parallel strategies.</span>
<span class="sd">    - AUTO_PARALLEL: Achieves parallelism automatically.</span>

<span class="sd">    MODE_LIST: The list of all supported parallel modes.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">STAND_ALONE</span> <span class="o">=</span> <span class="s2">&quot;stand_alone&quot;</span>
    <span class="n">DATA_PARALLEL</span> <span class="o">=</span> <span class="s2">&quot;data_parallel&quot;</span>
    <span class="n">HYBRID_PARALLEL</span> <span class="o">=</span> <span class="s2">&quot;hybrid_parallel&quot;</span>
    <span class="n">SEMI_AUTO_PARALLEL</span> <span class="o">=</span> <span class="s2">&quot;semi_auto_parallel&quot;</span>
    <span class="n">AUTO_PARALLEL</span> <span class="o">=</span> <span class="s2">&quot;auto_parallel&quot;</span>
    <span class="n">MODE_LIST</span> <span class="o">=</span> <span class="p">[</span><span class="n">STAND_ALONE</span><span class="p">,</span> <span class="n">DATA_PARALLEL</span><span class="p">,</span> <span class="n">HYBRID_PARALLEL</span><span class="p">,</span> <span class="n">SEMI_AUTO_PARALLEL</span><span class="p">,</span> <span class="n">AUTO_PARALLEL</span><span class="p">]</span></div>


<div class="viewcode-block" id="set_ps_context"><a class="viewcode-back" href="../../api_python/mindspore/mindspore.set_ps_context.html#mindspore.set_ps_context">[docs]</a><span class="nd">@args_type_check</span><span class="p">(</span><span class="n">enable_ps</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">set_ps_context</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Set parameter server training mode context.</span>

<span class="sd">    Note:</span>
<span class="sd">        Some other environment variables should also be set for parameter server training mode.</span>
<span class="sd">        These environment variables are listed below:</span>

<span class="sd">        MS_SERVER_NUM: Server number</span>

<span class="sd">        MS_WORKER_NUM: Worker number</span>

<span class="sd">        MS_SCHED_HOST: Scheduler IP address</span>

<span class="sd">        MS_SCHED_PORT: Scheduler port</span>

<span class="sd">        MS_ROLE: The role of this process:</span>

<span class="sd">        MS_SCHED: represents the scheduler,</span>

<span class="sd">        MS_WORKER: represents the worker,</span>

<span class="sd">        MS_PSERVER/MS_SERVER: represents the Server</span>

<span class="sd">    Args:</span>
<span class="sd">        enable_ps (bool): Whether to enable parameter server training mode.</span>
<span class="sd">                          Only after enable_ps is set True, the environment variables will be effective.</span>
<span class="sd">                          Default: False.</span>
<span class="sd">        config_file_path (string): Configuration file path used by recovery, parameter server training mode only</span>
<span class="sd">                                   supports Server disaster recovery currently. Default: &#39;&#39;.</span>
<span class="sd">        scheduler_manage_port (int): Scheduler manage port used to scale out/in. Default: 11202.</span>
<span class="sd">        enable_ssl (bool): Set PS SSL mode enabled or disabled. Default: False.</span>
<span class="sd">        client_password (str): Password to decrypt the secret key stored in the client certificate. Default: &#39;&#39;.</span>
<span class="sd">        server_password (str): Password to decrypt the secret key stored in the server certificate. Default: &#39;&#39;.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If input key is not the attribute in parameter server training mode context.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; ms.set_ps_context(enable_ps=True, enable_ssl=True, client_password=&#39;123456&#39;, server_password=&#39;123456&#39;)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_set_ps_context</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>


<div class="viewcode-block" id="get_ps_context"><a class="viewcode-back" href="../../api_python/mindspore/mindspore.get_ps_context.html#mindspore.get_ps_context">[docs]</a><span class="k">def</span> <span class="nf">get_ps_context</span><span class="p">(</span><span class="n">attr_key</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get parameter server training mode context attribute value according to the key.</span>

<span class="sd">    Args:</span>
<span class="sd">        attr_key (str): The key of the attribute:</span>

<span class="sd">            - enable_ps (bool): Whether to enable parameter server training mode.</span>
<span class="sd">            - config_file_path (string): Configuration file path used by recovery, parameter server training mode only</span>
<span class="sd">              supports Server disaster recovery currently. Default: &#39;&#39;.</span>
<span class="sd">            - scheduler_manage_port (int): Scheduler manage port used to scale out/in. Default: 11202.</span>
<span class="sd">            - enable_ssl (bool): Set PS SSL mode enabled or disabled. Default: False.</span>
<span class="sd">            - client_password (str): Password to decrypt the secret key stored in the client certificate. Default: &#39;&#39;.</span>
<span class="sd">            - server_password (str): Password to decrypt the secret key stored in the server certificate. Default: &#39;&#39;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Returns attribute value according to the key.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If input key is not attribute in auto parallel context.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; ms.get_ps_context(&quot;enable_ps&quot;)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_get_ps_context</span><span class="p">(</span><span class="n">attr_key</span><span class="p">)</span></div>


<div class="viewcode-block" id="reset_ps_context"><a class="viewcode-back" href="../../api_python/mindspore/mindspore.reset_ps_context.html#mindspore.reset_ps_context">[docs]</a><span class="k">def</span> <span class="nf">reset_ps_context</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reset parameter server training mode context attributes to the default values:</span>

<span class="sd">    - enable_ps: False.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_reset_ps_context</span><span class="p">()</span></div>


<span class="n">_hccl_connect_timeout</span> <span class="o">=</span> <span class="s1">&#39;600&#39;</span>


<span class="k">def</span> <span class="nf">_init_parallel_env</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Set hccl connect timeout.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="s1">&#39;ascend&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">__device_target__</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="k">if</span> <span class="s1">&#39;HCCL_CONNECT_TIMEOUT&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">:</span>
        <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;HCCL_CONNECT_TIMEOUT&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">_hccl_connect_timeout</span>


<span class="n">_init_parallel_env</span><span class="p">()</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>