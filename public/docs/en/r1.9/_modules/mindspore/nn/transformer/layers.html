

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>mindspore.nn.transformer.layers &mdash; MindSpore master documentation</title>
  

  
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/training.css" type="text/css" />
   
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/js/training.js"></script>
        
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/auto_gradient.html">Functional Differential Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/distributed_training_design.html">Distributed Training Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/mindir.html">MindSpore IR (MindIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/thor.html">Second Order Optimizer</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.9/training_visual_design.html">Design of Visualization↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Specification</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/benchmark.html">Benchmarks</a></li>
<li class="toctree-l1"><a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.9/README.md#table-of-contents">Network List↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/operator_list.html">API List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/syntax_list.html">Syntax Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.audio.html">mindspore.dataset.audio</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.config.html">mindspore.dataset.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.text.html">mindspore.dataset.text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.vision.html">mindspore.dataset.vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.transformer.html">mindspore.nn.transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.function.html">mindspore.ops.function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.9/api_cpp/mindspore.html">C++ API↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Mapping</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/api_mapping/pytorch_api_mapping.html">PyTorch and MindSpore API Mapping Table</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/api_mapping/tensorflow_api_mapping.html">TensorFlow and MindSpore API Mapping Table</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Migration Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/enveriment_preparation.html">Environment Preparation and Information Acquisition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/analysis_and_preparation.html">Model Analysis and Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/model_development/model_development.html">Constructing MindSpore Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/debug_and_tune.html">Debugging and Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/sample_code.html">Network Migration Debugging Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/faq.html">FAQs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/typical_api_comparision.html">Differences Between MindSpore and PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/use_third_party_op.html">Using Third-party Operator Libraries Based on Customized Interfaces</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/data_processing.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/implement_problem.html">Implement Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/network_compilation.html">Network Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/operators_compile.html">Operators Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/usage_migrate_3rd.html">Migration from a Third-party Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/precision_tuning.html">Precision Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/distributed_configure.html">Distributed Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/feature_advice.html">Feature Advice</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../RELEASE.html">Release Notes</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
      <li>mindspore.nn.transformer.layers</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for mindspore.nn.transformer.layers</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2021 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">The basic layer of the Transformer Networks. This is an experimental interface that is subject to</span>
<span class="sd">change or deletion.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span>

<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">wraps</span><span class="p">,</span> <span class="n">partial</span>
<span class="kn">import</span> <span class="nn">inspect</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">mindspore.common.parameter</span> <span class="kn">import</span> <span class="n">Parameter</span>
<span class="kn">from</span> <span class="nn">mindspore.common.initializer</span> <span class="kn">import</span> <span class="n">initializer</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="kn">import</span> <span class="nn">mindspore.common.dtype</span> <span class="k">as</span> <span class="nn">mstype</span>
<span class="kn">from</span> <span class="nn">mindspore.common.seed</span> <span class="kn">import</span> <span class="n">_get_graph_seed</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="kn">from</span> <span class="nn">mindspore._extends</span> <span class="kn">import</span> <span class="n">cell_attr_register</span>
<span class="kn">from</span> <span class="nn">mindspore.nn.cell</span> <span class="kn">import</span> <span class="n">Cell</span>
<span class="kn">from</span> <span class="nn">mindspore.nn.layer.activation</span> <span class="kn">import</span> <span class="n">get_activation</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">mindspore._checkparam</span> <span class="kn">import</span> <span class="n">Validator</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.primitive</span> <span class="kn">import</span> <span class="n">constexpr</span>
<span class="kn">from</span> <span class="nn">mindspore.parallel._utils</span> <span class="kn">import</span> <span class="n">_get_parallel_mode</span><span class="p">,</span> <span class="n">_is_sharding_propagation</span>
<span class="kn">from</span> <span class="nn">mindspore.context</span> <span class="kn">import</span> <span class="n">ParallelMode</span>
<span class="kn">from</span> <span class="nn">mindspore.nn.transformer.op_parallel_config</span> <span class="kn">import</span> <span class="n">default_dpmp_config</span><span class="p">,</span> <span class="n">OpParallelConfig</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;FixedSparseAttention&quot;</span>
<span class="p">]</span>


<span class="k">def</span> <span class="nf">_args_type_validator_check</span><span class="p">(</span><span class="o">*</span><span class="n">type_args</span><span class="p">,</span> <span class="o">**</span><span class="n">type_kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check whether input data type is correct.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">type_check</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
        <span class="n">sig</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
        <span class="n">bound_types</span> <span class="o">=</span> <span class="n">sig</span><span class="o">.</span><span class="n">bind_partial</span><span class="p">(</span><span class="o">*</span><span class="n">type_args</span><span class="p">,</span> <span class="o">**</span><span class="n">type_kwargs</span><span class="p">)</span><span class="o">.</span><span class="n">arguments</span>

        <span class="nd">@wraps</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
        <span class="k">def</span> <span class="nf">wrapper</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="k">nonlocal</span> <span class="n">bound_types</span>
            <span class="n">bound_values</span> <span class="o">=</span> <span class="n">sig</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

            <span class="n">argument_dict</span> <span class="o">=</span> <span class="n">bound_values</span><span class="o">.</span><span class="n">arguments</span>
            <span class="k">if</span> <span class="s2">&quot;kwargs&quot;</span> <span class="ow">in</span> <span class="n">bound_types</span><span class="p">:</span>
                <span class="n">bound_types</span> <span class="o">=</span> <span class="n">bound_types</span><span class="p">[</span><span class="s2">&quot;kwargs&quot;</span><span class="p">]</span>
            <span class="k">if</span> <span class="s2">&quot;kwargs&quot;</span> <span class="ow">in</span> <span class="n">argument_dict</span><span class="p">:</span>
                <span class="n">argument_dict</span> <span class="o">=</span> <span class="n">argument_dict</span><span class="p">[</span><span class="s2">&quot;kwargs&quot;</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">argument_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">bound_types</span><span class="p">:</span>
                    <span class="n">bound_types</span><span class="p">[</span><span class="n">name</span><span class="p">](</span><span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">wrapper</span>

    <span class="k">return</span> <span class="n">type_check</span>


<span class="k">def</span> <span class="nf">_valid_type_checks</span><span class="p">(</span><span class="n">types</span><span class="p">,</span> <span class="n">class_name</span><span class="p">):</span>
    <span class="c1"># types should be a list of types, this function check if the type is in the valid dtypes</span>
    <span class="k">def</span> <span class="nf">validator_check_func</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="c1"># The args of Validator.check_type_name is (arg_name, arg_type, valid_types, prim_name)</span>
        <span class="c1"># as the input of _args_type_validator_check is fixed, so we need to manually change the input order</span>
        <span class="n">partial_check</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_type_name</span><span class="p">,</span> <span class="n">valid_types</span><span class="o">=</span><span class="n">types</span><span class="p">,</span> <span class="n">prim_name</span><span class="o">=</span><span class="n">class_name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">partial_check</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">value</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">validator_check_func</span>


<span class="k">def</span> <span class="nf">_valid_value_checks</span><span class="p">(</span><span class="n">types</span><span class="p">,</span> <span class="n">class_name</span><span class="p">):</span>
    <span class="c1"># the value should be a list of types, this function check if the value is in the valid dtypes</span>
    <span class="k">def</span> <span class="nf">validator_check_func</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="c1"># The args of Validator.check_type_name is (arg_name, arg_type, valid_types, prim_name)</span>
        <span class="c1"># as the input of _args_type_validator_check is fixed, so we need to manually change the input order</span>
        <span class="n">partial_check</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_type_name</span><span class="p">,</span> <span class="n">valid_types</span><span class="o">=</span><span class="n">types</span><span class="p">,</span> <span class="n">prim_name</span><span class="o">=</span><span class="n">class_name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">partial_check</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">validator_check_func</span>


<span class="k">class</span> <span class="nc">_LayerInputCheck</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">       A input check class for the inputs of the transformer model.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">check_shape_length</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">func_name</span><span class="p">,</span> <span class="n">target_len</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Check the input shape&#39;s length is equal to the expected shape</span>
<span class="sd">        :param input_shape(list): a list of the tensor shapes.</span>
<span class="sd">        :param param_name(str): the name of the checked parameter.</span>
<span class="sd">        :param func_name(str): the name of the function.</span>
<span class="sd">        :param target_len: the expected length of the shape.</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target_len</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">target_len</span> <span class="o">=</span> <span class="p">[</span><span class="n">target_len</span><span class="p">]</span>
        <span class="n">matched</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">target_len</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">==</span> <span class="n">item</span><span class="p">:</span>
                <span class="n">matched</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">matched</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">func_name</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">param_name</span><span class="si">}</span><span class="s2"> shape length must be one of </span><span class="si">{</span><span class="n">target_len</span><span class="si">}</span><span class="s2"> dimension, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got shape </span><span class="si">{</span><span class="n">input_shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">check_shape_equal</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">func_name</span><span class="p">,</span> <span class="n">target_shape</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Check the input shape&#39;s is equal to the expected shape</span>
<span class="sd">        :param input_shape(list): a list of the tensor shapes.</span>
<span class="sd">        :param param_name(str): the name of the checked parameter.</span>
<span class="sd">        :param func_name(str): the name of the function.</span>
<span class="sd">        :param target_shape: the expected shape.</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">target_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">target_shape</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="n">input_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
        <span class="n">_LayerInputCheck</span><span class="o">.</span><span class="n">check_shape_length</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">func_name</span><span class="p">,</span>
                                            <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">item</span><span class="p">)</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">target_shape</span><span class="p">])</span>
        <span class="n">matched</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">target_shape</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">item</span> <span class="o">==</span> <span class="n">input_shape</span><span class="p">:</span>
                <span class="n">matched</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="k">break</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">matched</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">func_name</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">param_name</span><span class="si">}</span><span class="s2"> shape must be one of </span><span class="si">{</span><span class="n">target_shape</span><span class="si">}</span><span class="s2">,&quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">input_shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">check_shape_value_on_axis</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">,</span> <span class="n">target_value</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Check whether the input_shape[dim] is equal to target value&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">input_shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">!=</span> <span class="n">target_value</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">cls_name</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">param_name</span><span class="si">}</span><span class="s2"> at </span><span class="si">{</span><span class="n">dim</span><span class="si">}</span><span class="s2"> shape must be </span><span class="si">{</span><span class="n">target_value</span><span class="si">}</span><span class="s2">,&quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">input_shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">True</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_past_none_input_none</span><span class="p">(</span><span class="n">use_past</span><span class="p">,</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">func_name</span><span class="p">,</span> <span class="n">default_value</span><span class="p">,</span> <span class="n">is_tensor</span><span class="p">,</span> <span class="n">is_default</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; If the past is True, check whether the inputs is None&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">use_past</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">is_tensor</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">func_name</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">param_name</span><span class="si">}</span><span class="s2"> must be </span><span class="si">{</span><span class="n">default_value</span><span class="si">}</span><span class="s2">, if use_pat is False, but found &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;a tensor&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_default</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">func_name</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">param_name</span><span class="si">}</span><span class="s2"> must be </span><span class="si">{</span><span class="n">default_value</span><span class="si">}</span><span class="s2">, if use_pat is False.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_tensor</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">func_name</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">param_name</span><span class="si">}</span><span class="s2"> must be tensor, if use_pat is True&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">True</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_input_dtype</span><span class="p">(</span><span class="n">input_dtype</span><span class="p">,</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">allow_dtypes</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">):</span>
    <span class="n">Validator</span><span class="o">.</span><span class="n">check_type_name</span><span class="p">(</span><span class="n">param_name</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">,</span> <span class="n">allow_dtypes</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">)</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_input_shape</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">func_name</span><span class="p">,</span> <span class="n">target_len</span><span class="p">):</span>
    <span class="c1"># check the input length</span>
    <span class="n">_LayerInputCheck</span><span class="o">.</span><span class="n">check_shape_length</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">func_name</span><span class="p">,</span> <span class="n">target_len</span><span class="p">)</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_shape_equal</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">func_name</span><span class="p">,</span> <span class="n">target_shape</span><span class="p">):</span>
    <span class="c1"># check the input length</span>
    <span class="n">_LayerInputCheck</span><span class="o">.</span><span class="n">check_shape_equal</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">func_name</span><span class="p">,</span> <span class="n">target_shape</span><span class="p">)</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_input_shape_value</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">,</span> <span class="n">target_value</span><span class="p">):</span>
    <span class="n">_LayerInputCheck</span><span class="o">.</span><span class="n">check_shape_value_on_axis</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">,</span> <span class="n">target_value</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_Dropout</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A Dropout Implements with P.DropoutGenMask and  P.DropoutDoMask for parallel training.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_prob</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">_Dropout</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">keep_prob</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">keep_prob</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;dropout probability should be a number in range (0, 1], but got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">keep_prob</span><span class="p">))</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;keep_prob&#39;</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">keep_prob</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_ascend</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s1">&#39;device_target&#39;</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;Ascend&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_ascend</span><span class="p">:</span>
            <span class="n">seed0</span><span class="p">,</span> <span class="n">seed1</span> <span class="o">=</span> <span class="n">_get_graph_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;dropout&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">seed0</span> <span class="o">=</span> <span class="n">seed0</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">seed1</span> <span class="o">=</span> <span class="n">seed1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">get_shape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout_gen_mask</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">DropoutGenMask</span><span class="p">(</span><span class="n">Seed0</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">seed0</span><span class="p">,</span> <span class="n">Seed1</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">seed1</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout_do_mask</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">DropoutDoMask</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cast</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">keep_prob</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">           Input: a tensor</span>
<span class="sd">           Returns: a tensor</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_ascend</span><span class="p">:</span>
            <span class="n">out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">out</span>

        <span class="n">shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">keep_prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_gen_mask</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_do_mask</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extend_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s1">&#39;keep_prob=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">shard</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">strategy</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_ascend</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout_gen_mask</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout_do_mask</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_LayerNorm</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A self-defined layer norm operation using reduce sum and reduce mean</span>

<span class="sd">        Args:</span>
<span class="sd">            normalized_shape (tuple): The shape of the input tensor</span>
<span class="sd">            eps (float): The epsilon value of the denominator. Default 1e-5.</span>
<span class="sd">            param_init_type: The param init type.</span>
<span class="sd">        Inputs:</span>
<span class="sd">            - **x** (Tensor) - Tensor of shape :math:`(batch, seq\_length, hidden\_size)`.</span>

<span class="sd">        Outputs:</span>
<span class="sd">            Tensor of shape :math:`(batch, seq_length, hidden_size)`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">normalized_shape</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">param_init_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">is_self_defined</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">_LayerNorm</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">param_init_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;The type of parameter &#39;param_init_type&#39; should in [float32, float16], &quot;</span>
                            <span class="s2">&quot;but got the type : </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">param_init_type</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_self_defined</span> <span class="o">=</span> <span class="n">is_self_defined</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_self_defined</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">begin_norm_axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                                          <span class="n">begin_params_axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                                          <span class="n">epsilon</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="s1">&#39;ones&#39;</span><span class="p">,</span> <span class="n">normalized_shape</span><span class="p">,</span> <span class="n">param_init_type</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;gamma&quot;</span><span class="p">,</span>
                               <span class="n">parallel_optimizer</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="n">normalized_shape</span><span class="p">,</span> <span class="n">param_init_type</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;beta&quot;</span><span class="p">,</span>
                              <span class="n">parallel_optimizer</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceMean</span><span class="p">(</span><span class="n">keep_dims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">square</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Square</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sqrt</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Sqrt</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sub1</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Sub</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sub2</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Sub</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add2</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">real_div</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">RealDiv</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">          x : batch x seq_length x hidden_size</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_self_defined</span><span class="p">:</span>
            <span class="n">mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">diff</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sub1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mean</span><span class="p">)</span>
            <span class="n">variance</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">diff</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">variance_eps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">variance</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">))</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">real_div</span><span class="p">(</span><span class="n">diff</span><span class="p">,</span> <span class="n">variance_eps</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">output</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">shard</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">strategy</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set the shard for the layer norm. the strategy size should be equal to the inputs.</span>

<span class="sd">        Note:</span>
<span class="sd">            It is valid only in semi auto parallel or auto parallel mode.</span>
<span class="sd">            In other parallel modes, strategies set here will be ignored.</span>

<span class="sd">        Args:</span>
<span class="sd">            strategy (tuple): The strategy for the dropout. Should be the same shape as the inputs.</span>
<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import mindspore</span>
<span class="sd">            &gt;&gt;&gt; net = mindspore.parallel.nn.transformer.LayerNorm(normalized_shape=(1024, 10))</span>
<span class="sd">            &gt;&gt;&gt; net.shard(((10, 2, 1),))</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_self_defined</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">square</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sqrt</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sub1</span><span class="o">.</span><span class="n">shard</span><span class="p">((</span><span class="n">strategy</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">strategy</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sub2</span><span class="o">.</span><span class="n">shard</span><span class="p">((</span><span class="n">strategy</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">strategy</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="o">.</span><span class="n">shard</span><span class="p">((</span><span class="n">strategy</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">()))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mul</span><span class="o">.</span><span class="n">shard</span><span class="p">((</span><span class="n">strategy</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add2</span><span class="o">.</span><span class="n">shard</span><span class="p">((</span><span class="n">strategy</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">real_div</span><span class="o">.</span><span class="n">shard</span><span class="p">((</span><span class="n">strategy</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">strategy</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="o">.</span><span class="n">shard</span><span class="p">((</span><span class="n">strategy</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)))</span>

        <span class="k">return</span> <span class="bp">self</span>


<span class="k">class</span> <span class="nc">_Linear</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The dense connected layer. Once the parallel mode is enabled, the input shape should be</span>
<span class="sd">    3-D tensor.</span>

<span class="sd">    Applies dense connected layer for the input. This layer implements the operation as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{outputs} = \text{activation}(\text{X} * \text{kernel} + \text{bias}),</span>

<span class="sd">    where :math:`X` is the input tensors, :math:`\text{activation}` is the activation function passed as the activation</span>
<span class="sd">    argument (if passed in), :math:`\text{kernel}` is a weight matrix with the same</span>
<span class="sd">    data type as the :math:`X` created by the layer, and :math:`\text{bias}` is a bias vector</span>
<span class="sd">    with the same data type as the :math:`X` created by the layer (only if has_bias is True).</span>

<span class="sd">    Args:</span>
<span class="sd">        in_channels (int): The number of channels in the input space.</span>
<span class="sd">        out_channels (int): The number of channels in the output space.</span>
<span class="sd">        weight_init (Union[Tensor, str, Initializer, numbers.Number]): The trainable weight_init parameter. The dtype</span>
<span class="sd">            is same as `x`. The values of str refer to the function `initializer`. Default: &#39;normal&#39;.</span>
<span class="sd">        bias_init (Union[Tensor, str, Initializer, numbers.Number]): The trainable bias_init parameter. The dtype is</span>
<span class="sd">            same as `x`. The values of str refer to the function `initializer`. Default: &#39;zeros&#39;.</span>
<span class="sd">        has_bias (bool): Specifies whether the layer uses a bias vector. Default: True.</span>
<span class="sd">        activation (str): activate function applied to the output of the fully connected layer,</span>
<span class="sd">            eg. &#39;ReLU&#39;.Default: None.</span>
<span class="sd">        expert_num (int): The number of experts used in this Linear. Here, for the case expert_num &gt; 1, BatchMatMul is</span>
<span class="sd">            used and the first dimension in BatchMatMul indicate expert_num. Default: 1.</span>
<span class="sd">        outer_batch (int): The replication number of experts. The replication is effective only when MoE is applied.</span>
<span class="sd">            Default: 1.</span>
<span class="sd">        expert_group_size (int): The number of tokens in each data parallel group. Default: None.</span>
<span class="sd">        compute_dtype (dtype.Number): The computation type. Default: mstype.float16</span>
<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Tensor of shape :math:`(*, in\_channels)`. The `in_channels` in `Args` should be equal</span>
<span class="sd">          to :math:`in\_channels` in `Inputs`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor of shape :math:`(*, out\_channels)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `in_channels` or `out_channels` is not an int.</span>
<span class="sd">        TypeError: If `has_bias` is not a bool.</span>
<span class="sd">        TypeError: If `activation` is not one of str, Cell, Primitive, None.</span>
<span class="sd">        ValueError: If length of shape of `weight_init` is not equal to 2 or shape[0] of `weight_init`</span>
<span class="sd">                    is not equal to `out_channels` or shape[1] of `weight_init` is not equal to `in_channels`.</span>
<span class="sd">        ValueError: If length of shape of `bias_init` is not equal to 1</span>
<span class="sd">                    or shape[0] of `bias_init` is not equal to `out_channels`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@cell_attr_register</span>
    <span class="nd">@_args_type_validator_check</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">out_channels</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">has_bias</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">,</span>
                                <span class="n">activation</span><span class="o">=</span><span class="n">_valid_type_checks</span><span class="p">([</span><span class="nb">type</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span> <span class="nb">str</span><span class="p">],</span> <span class="s2">&quot;Linear&quot;</span><span class="p">),</span>
                                <span class="n">transpose_b</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">,</span>
                                <span class="n">expert_num</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">outer_batch</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">param_init_type</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                                                                    <span class="s2">&quot;Linear&quot;</span><span class="p">),</span>
                                <span class="n">compute_dtype</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                                                                  <span class="s2">&quot;Linear&quot;</span><span class="p">))</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">in_channels</span><span class="p">,</span>
                 <span class="n">out_channels</span><span class="p">,</span>
                 <span class="n">weight_init</span><span class="o">=</span><span class="s1">&#39;normal&#39;</span><span class="p">,</span>
                 <span class="n">bias_init</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span>
                 <span class="n">has_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">transpose_b</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">expert_num</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">outer_batch</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">expert_group_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">param_init_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">compute_dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">_Linear</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">in_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">=</span> <span class="n">out_channels</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_init</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">weight_init</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">2</span> <span class="ow">or</span> <span class="n">weight_init</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">out_channels</span> <span class="ow">or</span> \
                                                <span class="n">weight_init</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">in_channels</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The shape of parameter &#39;weight_init&#39; is error, please check shape of &#39;weight_init&#39;.&quot;</span><span class="p">)</span>
        <span class="n">weight_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">]</span> <span class="k">if</span> <span class="n">transpose_b</span> <span class="k">else</span> <span class="p">[</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">expert_num</span> <span class="o">=</span> <span class="n">expert_num</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">outer_batch</span> <span class="o">=</span> <span class="n">outer_batch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">expert_group_size</span> <span class="o">=</span> <span class="n">expert_group_size</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">expert_num</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">expert_flag</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="n">weight_init</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">expert_num</span><span class="p">]</span> <span class="o">+</span> <span class="n">weight_shape</span><span class="p">,</span> <span class="n">param_init_type</span><span class="p">),</span>
                                    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;weight&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BatchMatMul</span><span class="p">(</span><span class="n">transpose_b</span><span class="o">=</span><span class="n">transpose_b</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">expert_flag</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="n">weight_init</span><span class="p">,</span> <span class="n">weight_shape</span><span class="p">,</span> <span class="n">param_init_type</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;weight&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">MatMul</span><span class="p">(</span><span class="n">transpose_b</span><span class="o">=</span><span class="n">transpose_b</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_expert_group_size</span> <span class="o">=</span> <span class="n">_get_parallel_mode</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">,)</span> \
                                     <span class="ow">and</span> <span class="ow">not</span> <span class="n">_is_sharding_propagation</span><span class="p">()</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">expert_flag</span> <span class="ow">is</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_expert_group_size</span> <span class="ow">is</span> <span class="kc">True</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">expert_group_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;expert_group_size&#39; should be configured as an integer in MoEConfig.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">has_bias</span> <span class="o">=</span> <span class="n">has_bias</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_bias</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">bias_init</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">bias_init</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">bias_init</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">out_channels</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The shape of parameter &#39;bias_init&#39; is error, please check shape of &#39;bias_init&#39;.&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">expert_flag</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="n">bias_init</span><span class="p">,</span>
                                                  <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">expert_num</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">],</span> <span class="n">param_init_type</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;bias&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="n">bias_init</span><span class="p">,</span> <span class="p">[</span><span class="n">out_channels</span><span class="p">],</span> <span class="n">param_init_type</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;bias&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">parallel_optimizer</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias_add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">act_name</span> <span class="o">=</span> <span class="n">activation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">get_activation</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">activation</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">activation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation_flag</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">compute_dtype</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cast</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out_shape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()(</span><span class="n">x</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span><span class="p">))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">expert_flag</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_expert_group_size</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">expert_num</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">expert_group_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">outer_batch</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">expert_num</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span><span class="p">))</span>
        <span class="n">ori_dtype</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_bias</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_flag</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ori_dtype</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()(</span><span class="n">x</span><span class="p">,</span> <span class="n">out_shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">shard</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">strategy_matmul</span><span class="p">,</span> <span class="n">strategy_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">strategy_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set the shard for the linear. the strategy size should be equal to the inputs.</span>

<span class="sd">        Note:</span>
<span class="sd">            It is valid only in semi auto parallel or auto parallel mode.</span>
<span class="sd">            In other parallel modes, strategies set here will be ignored.</span>

<span class="sd">        Args:</span>
<span class="sd">            strategy_matmul (tuple): The strategy for the matmul. Should be the same shape as the inputs.</span>
<span class="sd">            strategy_bias (tuple): The strategy for the bias_add. Should be the same shape as the inputs.</span>
<span class="sd">            strategy_activation (tuple): The strategy for the strategy_activation. Should be the same shape as</span>
<span class="sd">            the inputs.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy_matmul</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_bias</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias_add</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy_bias</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_flag</span><span class="p">:</span>
            <span class="c1"># some operations has many primitives, need to manually set the shard</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">act_name</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;leakyrelu&quot;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">select_op</span><span class="o">.</span><span class="n">shard</span><span class="p">((</span><span class="n">strategy_activation</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">strategy_activation</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">act_name</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;logsigmoid&quot;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">mul</span><span class="o">.</span><span class="n">shard</span><span class="p">((</span><span class="n">strategy_activation</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">()))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">exp</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy_activation</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">add</span><span class="o">.</span><span class="n">shard</span><span class="p">((</span><span class="n">strategy_activation</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">()))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">rec</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy_activation</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">log</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy_activation</span><span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">act_name</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;logsoftmax&quot;</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The &#39;LogSoftmax&#39; function is not supported in semi auto parallel &quot;</span>
                                 <span class="s2">&quot;or auto parallel mode.&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">act_name</span><span class="p">)</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy_activation</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span>


<div class="viewcode-block" id="FixedSparseAttention"><a class="viewcode-back" href="../../../../api_python/mindspore.nn.transformer.html#mindspore.nn.transformer.FixedSparseAttention">[docs]</a><span class="k">class</span> <span class="nc">FixedSparseAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fixed Sparse Attention Layer.</span>

<span class="sd">    This function contains the sparse attention primitives used in Sparse Transformers (see paper)</span>
<span class="sd">    `Generating Long Sequences with Sparse Transformers &lt;https://arxiv.org/abs/1904.10509&gt;`_.</span>

<span class="sd">    Specifically, it includes the following:</span>

<span class="sd">    1. A faster implementation of normal attention (the upper triangle is not computed, and many operations are fused).</span>
<span class="sd">    2. An implementation of &quot;strided&quot; and &quot;fixed&quot; attention, as in the Sparse Transformers paper.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch_size(int): Number of input batch size.</span>
<span class="sd">        num_heads(int): Number of attention heads.</span>
<span class="sd">        size_per_head(int): An integer determining embedding size of each attention head,</span>
<span class="sd">            only supports 64, 128 for now.</span>
<span class="sd">        block_size(int): An integer determining the block size. Current implementation of sparse self-attention</span>
<span class="sd">            is based on blocked sparse matrices. In which this parameter defines the size of such blocks,</span>
<span class="sd">            Block X Block. Only supports 64 for now.</span>
<span class="sd">        seq_length(int): length of input sequence, only supports 1024 for now. Default 1024.</span>
<span class="sd">        num_different_global_patterns(int): An integer determining the number of different global attentions layouts.</span>
<span class="sd">            While global attention can be fixed by which block/s are representative of</span>
<span class="sd">            any local window, since there are multi-heads, each head can use a</span>
<span class="sd">            different global representative, only supports 4 for now. Default 4.</span>
<span class="sd">        parallel_config(OpParallelConfig): The config of parallel setting, see `OpParallelConfig`.</span>
<span class="sd">            Default `default_dpmp_config`, an instance of `OpParallelConfig` with</span>
<span class="sd">            default args.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **q** (Tensor) - Tensor query (:class:`mstype.fp16` [batch_size, seq_length, hidden_size]): Sequence of</span>
<span class="sd">          queries to query the context.</span>
<span class="sd">        - **k** (Tensor) - Tensor key (:class:`mstype.fp16` [batch_size, seq_length, hidden_size]): Sequence of</span>
<span class="sd">          queries to query the context.</span>
<span class="sd">        - **v** (Tensor) - Tensor value (:class:`mstype.fp16` [batch size, sequence length, Embedding Size]):</span>
<span class="sd">          Sequence of queries to query the context.</span>
<span class="sd">        - **attention_mask** (Tensor) - Float Tensor the mask of (:class:`mstype.fp32`, :class:`mstype.fp16`</span>
<span class="sd">          [batch_size, seq_length, seq_length]): Lower triangular matrix to pass masked information.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        A Tensor. The output of the attention with shape [batch_size, seq_length, hidden_size]</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.nn.transformer import FixedSparseAttention</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; model = FixedSparseAttention(batch_size=2,</span>
<span class="sd">        ...                              num_heads=8,</span>
<span class="sd">        ...                              size_per_head=64,</span>
<span class="sd">        ...                              block_size=64)</span>
<span class="sd">        &gt;&gt;&gt; q = Tensor(np.ones((2, 1024, 8*64)), mstype.float16)</span>
<span class="sd">        &gt;&gt;&gt; k = Tensor(np.ones((2, 1024, 8*64)), mstype.float16)</span>
<span class="sd">        &gt;&gt;&gt; v = Tensor(np.ones((2, 1024, 8*64)), mstype.float16)</span>
<span class="sd">        &gt;&gt;&gt; attention_mask = Tensor(np.ones((2, 1024, 1024)), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = model(q, k, v, attention_mask)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 1024, 512)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@_args_type_validator_check</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">num_heads</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">size_per_head</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">block_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">seq_length</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">num_different_global_patterns</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">parallel_config</span><span class="o">=</span><span class="n">_valid_type_checks</span><span class="p">([</span><span class="n">OpParallelConfig</span><span class="p">],</span> <span class="s2">&quot;FixedSparseAttention&quot;</span><span class="p">))</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">batch_size</span><span class="p">,</span>
                 <span class="n">num_heads</span><span class="p">,</span>
                 <span class="n">size_per_head</span><span class="p">,</span>
                 <span class="n">block_size</span><span class="p">,</span>
                 <span class="n">seq_length</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
                 <span class="n">num_different_global_patterns</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                 <span class="n">parallel_config</span><span class="o">=</span><span class="n">default_dpmp_config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FixedSparseAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">dp</span><span class="p">,</span> <span class="n">mp</span> <span class="o">=</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span>
        <span class="k">if</span> <span class="n">num_heads</span> <span class="o">%</span> <span class="n">mp</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The number of heads </span><span class="si">{</span><span class="n">num_heads</span><span class="si">}</span><span class="s2"> must be a &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;multiple of parallel_config.model_parallel </span><span class="si">{</span><span class="n">mp</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">batch_size</span> <span class="o">%</span> <span class="n">dp</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The batch_size </span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s2"> must be a &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;multiple of parallel_config.data_parallel </span><span class="si">{</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span> <span class="o">=</span> <span class="n">seq_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">size_per_head</span> <span class="o">*</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="n">block_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block_num</span> <span class="o">=</span> <span class="n">seq_length</span> <span class="o">//</span> <span class="n">block_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">size_per_head</span> <span class="o">=</span> <span class="n">size_per_head</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">global_size</span> <span class="o">=</span> <span class="n">seq_length</span> <span class="o">//</span> <span class="mi">4</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transpose</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Transpose</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">dp</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">mp</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_matmul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BatchMatMul</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">dp</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">dp</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">multiply</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">dp</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">multiply_data</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">10000.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">parallel_config</span> <span class="o">=</span> <span class="n">parallel_config</span>
        <span class="n">size_per_head_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span> <span class="o">!=</span> <span class="mi">1024</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;FixedSparseAttention&#39;, the class variable &#39;seq_length&#39; must be 1024, &quot;</span>
                             <span class="s2">&quot;but got the value : </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">seq_length</span><span class="p">))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">!=</span> <span class="mi">64</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;FixedSparseAttention&#39;, the class variable &#39;block_size&#39; must be 64, &quot;</span>
                             <span class="s2">&quot;but got the value : </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">block_size</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">num_different_global_patterns</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;FixedSparseAttention&#39;, the class variable &#39;num_different_global_patterns&#39; &quot;</span>
                             <span class="s2">&quot;must be 4, but got the value : </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_different_global_patterns</span><span class="p">))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">size_per_head</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">size_per_head_list</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;FixedSparseAttention&#39;, the class variable &#39;size_per_head&#39; only supports </span><span class="si">{}</span><span class="s2">, &quot;</span>
                             <span class="s2">&quot;but got the value : </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">size_per_head_list</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size_per_head</span><span class="p">))</span>
        <span class="n">local_ones</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span><span class="p">),</span>
                             <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
        <span class="n">global_mask_original</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">global_size</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">i</span> <span class="o">//</span> <span class="mi">16</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="n">j</span> <span class="o">//</span> <span class="mi">16</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mi">4</span><span class="p">:</span>
                    <span class="n">global_mask_original</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>

        <span class="n">global_mask_original</span> <span class="o">=</span> <span class="o">-</span><span class="mi">10000</span> <span class="o">*</span> <span class="n">global_mask_original</span>
        <span class="n">global_mask_fx</span> <span class="o">=</span> <span class="n">global_mask_original</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span> <span class="o">//</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_size</span> <span class="o">//</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span>
        <span class="n">global_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">global_mask_fx</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
        <span class="n">global_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">global_mask</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:],</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">global_mask</span> <span class="o">=</span> <span class="n">global_mask</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_size</span> <span class="o">//</span> <span class="mi">16</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span> <span class="o">//</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">global_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">global_mask</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">local_mask_triangle</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">local_ones</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale_factor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">((</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size_per_head</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">matmul_dds</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">MatmulDDS</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">mp</span><span class="p">,</span> <span class="n">dp</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                                                                              <span class="p">(</span><span class="n">mp</span><span class="p">,</span> <span class="n">dp</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                                                                              <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dp</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                                                                              <span class="p">(</span><span class="n">dp</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">matmul_dsd</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">DSDMatmul</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">dp</span><span class="p">,</span> <span class="n">mp</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">dp</span><span class="p">,</span> <span class="n">mp</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">dp</span><span class="p">,</span> <span class="n">mp</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sub1</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Sub</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,),</span> <span class="p">(</span><span class="n">dp</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mul1</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">dp</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transpose1</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Transpose</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">dp</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transpose2</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Transpose</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">dp</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transpose3</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Transpose</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">dp</span><span class="p">,</span> <span class="n">mp</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transpose4</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Transpose</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">dp</span><span class="p">,</span> <span class="n">mp</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">div</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">RealDiv</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">mp</span><span class="p">,</span> <span class="n">dp</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">()))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">slice1</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">StridedSlice</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">dp</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">):</span>
        <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">q</span><span class="p">),</span> <span class="s2">&quot;q&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                           <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">])</span>
        <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">q</span><span class="p">),</span> <span class="s2">&quot;q&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">k</span><span class="p">),</span> <span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                           <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">])</span>
        <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">k</span><span class="p">),</span> <span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">v</span><span class="p">),</span> <span class="s2">&quot;v&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                           <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">])</span>
        <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">v</span><span class="p">),</span> <span class="s2">&quot;v&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">),</span> <span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                           <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">])</span>
        <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">),</span> <span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>

        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transpose_inputs</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        <span class="n">local_mask</span><span class="p">,</span> <span class="n">global_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generate_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scale_factor</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">q</span><span class="p">)))</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scale_factor</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">k</span><span class="p">)))</span>
        <span class="n">local_prob</span><span class="p">,</span> <span class="n">global_prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul_dds</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">local_mask</span><span class="p">,</span> <span class="n">global_mask</span><span class="p">)</span>
        <span class="n">attention</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul_dsd</span><span class="p">(</span><span class="n">local_prob</span><span class="p">,</span> <span class="n">global_prob</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        <span class="n">attention_merge</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose3</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
        <span class="n">attention_merge</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">attention_merge</span><span class="p">,</span>
            <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size_per_head</span><span class="p">))</span>
        <span class="n">attention_merge</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose4</span><span class="p">(</span><span class="n">attention_merge</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
        <span class="n">attention_merge</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">attention_merge</span><span class="p">,</span>
            <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size_per_head</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">attention_merge</span>

    <span class="k">def</span> <span class="nf">_generate_attention_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        generate global attention mask and local attention mask from origin attention mask</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">))</span>
        <span class="n">input_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">slice1</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
                                 <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">input_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">input_mask</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">))</span>
        <span class="n">input_shape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()(</span><span class="n">input_mask</span><span class="p">)</span>  <span class="c1"># bs, seq_length</span>
        <span class="c1"># bs, block_num, 1, block_size</span>
        <span class="n">local_shape_right</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_num</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span><span class="p">)</span>
        <span class="c1"># bs, block_num, block_size, 1</span>
        <span class="n">local_shape_left</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_num</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">local_mask_left</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">input_mask</span><span class="p">,</span> <span class="n">local_shape_left</span><span class="p">)</span>
        <span class="n">local_mask_right</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">input_mask</span><span class="p">,</span> <span class="n">local_shape_right</span><span class="p">)</span>
        <span class="c1"># bs, block_num, block_size, block_size</span>
        <span class="n">local_attention_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_matmul</span><span class="p">(</span><span class="n">local_mask_left</span><span class="p">,</span> <span class="n">local_mask_right</span><span class="p">)</span>
        <span class="n">lower_triangle</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">()(</span><span class="bp">self</span><span class="o">.</span><span class="n">local_mask_triangle</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">local_attention_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">local_attention_mask</span><span class="p">,</span> <span class="n">lower_triangle</span><span class="p">)</span>
        <span class="n">local_multiplied_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sub1</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()(</span><span class="n">F</span><span class="o">.</span><span class="n">tuple_to_array</span><span class="p">((</span><span class="mf">1.0</span><span class="p">,)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
                                         <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()(</span><span class="n">local_attention_mask</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
        <span class="n">local_adder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul1</span><span class="p">(</span><span class="n">local_multiplied_out</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">multiply_data</span><span class="p">)</span>
        <span class="n">local_mask_original</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose1</span><span class="p">(</span><span class="n">local_adder</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
        <span class="n">local_mask_original</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">local_mask_original</span><span class="p">,</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_num</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span><span class="p">))</span>
        <span class="n">local_mask_fx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">local_mask_original</span><span class="p">,</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">//</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">block_num</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">//</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span>
        <span class="n">local_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose2</span><span class="p">(</span><span class="n">local_mask_fx</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
        <span class="n">global_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_mask</span>

        <span class="k">return</span> <span class="n">local_mask</span><span class="p">,</span> <span class="n">global_mask</span>

    <span class="k">def</span> <span class="nf">_transpose_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        do reshape and transpose to inputs</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="n">q</span><span class="p">,</span>
                <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">size_per_head</span> <span class="o">//</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">)),</span>
            <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="n">k</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">size_per_head</span> <span class="o">//</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">)),</span>
            <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="n">v</span><span class="p">,</span>
                <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">size_per_head</span> <span class="o">//</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">)),</span>
            <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span></div>
</pre></div>

           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
</body>
</html>