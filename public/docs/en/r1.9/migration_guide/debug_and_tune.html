<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Debugging and Tuning &mdash; MindSpore master documentation</title>
      <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/training.css" type="text/css" /><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/js/training.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Network Migration Debugging Example" href="sample_code.html" />
    <link rel="prev" title="Inference and Training Process" href="model_development/training_and_evaluation_procession.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../design/auto_gradient.html">Functional Differential Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/distributed_training_design.html">Distributed Training Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/mindir.html">MindSpore IR (MindIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/thor.html">Second Order Optimizer</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.9/training_visual_design.html">Design of Visualization↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Specification</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/benchmark.html">Benchmarks</a></li>
<li class="toctree-l1"><a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.9/README.md#table-of-contents">Network List↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/operator_list.html">API List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/syntax_list.html">Syntax Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.audio.html">mindspore.dataset.audio</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.config.html">mindspore.dataset.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.text.html">mindspore.dataset.text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.vision.html">mindspore.dataset.vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.transformer.html">mindspore.nn.transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.function.html">mindspore.ops.function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.9/api_cpp/mindspore.html">C++ API↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Mapping</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/pytorch_api_mapping.html">PyTorch and MindSpore API Mapping Table</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/tensorflow_api_mapping.html">TensorFlow and MindSpore API Mapping Table</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Migration Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="enveriment_preparation.html">Environment Preparation and Information Acquisition</a></li>
<li class="toctree-l1"><a class="reference internal" href="analysis_and_preparation.html">Model Analysis and Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_development/model_development.html">Constructing MindSpore Network</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Debugging and Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="sample_code.html">Network Migration Debugging Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">FAQs</a></li>
<li class="toctree-l1"><a class="reference internal" href="typical_api_comparision.html">Differences Between MindSpore and PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="use_third_party_op.html">Using Third-party Operator Libraries Based on Customized Interfaces</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/data_processing.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/implement_problem.html">Implement Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/network_compilation.html">Network Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/operators_compile.html">Operators Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/usage_migrate_3rd.html">Migration from a Third-party Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/precision_tuning.html">Precision Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/distributed_configure.html">Distributed Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/feature_advice.html">Feature Advice</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Debugging and Tuning</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/migration_guide/debug_and_tune.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="debugging-and-tuning">
<h1>Debugging and Tuning<a class="headerlink" href="#debugging-and-tuning" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.9/docs/mindspore/source_en/migration_guide/debug_and_tune.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.9/resource/_static/logo_source_en.png"></a></p>
<section id="function-debugging">
<h2>Function Debugging<a class="headerlink" href="#function-debugging" title="Permalink to this headline"></a></h2>
<p>During network migration, you are advised to use the PyNative mode for debugging. In PyNative mode, you can perform debugging, and log printing is user-friendly. After the debugging is complete, the graph mode is used. The graph mode is more user-friendly in execution performance. You can also find some problems in network compilation. For example, gradient truncation caused by third-party operators.</p>
</section>
<section id="accuracy-debugging">
<h2>Accuracy Debugging<a class="headerlink" href="#accuracy-debugging" title="Permalink to this headline"></a></h2>
<p>The accuracy debugging process is as follows:</p>
<section id="1-checking-parameters">
<h3>1. Checking Parameters<a class="headerlink" href="#1-checking-parameters" title="Permalink to this headline"></a></h3>
<p>This part includes checking all parameters and the number of trainable parameters, and checking the shape of all parameters.</p>
<section id="obtaining-mindspore-parameters">
<h4>Obtaining MindSpore Parameters<a class="headerlink" href="#obtaining-mindspore-parameters" title="Permalink to this headline"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">Parameter</span></code> is used for MindSpore trainable and untrainable parameters.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">msNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">msNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">weight_init</span><span class="o">=</span><span class="s1">&#39;normal&#39;</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>

<span class="n">msnet</span> <span class="o">=</span> <span class="n">msNet</span><span class="p">()</span>
<span class="c1"># Obtain all parameters.</span>
<span class="n">all_parameter</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">msnet</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">():</span>
    <span class="n">all_parameter</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">item</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">item</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;all parameter numbers: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">all_parameter</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Obtain trainable parameters.</span>
<span class="n">trainable_params</span> <span class="o">=</span> <span class="n">msnet</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()</span>
<span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">trainable_params</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">item</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">item</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;trainable parameter numbers: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">trainable_params</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>    fc.weight (1, 1)
    fc.bias (1,)
    all parameter numbers: 2
    fc.weight (1, 1)
    fc.bias (1,)
    trainable parameter numbers: 2
</pre></div>
</div>
</section>
<section id="obtaining-pytorch-parameters">
<h4>Obtaining PyTorch Parameters<a class="headerlink" href="#obtaining-pytorch-parameters" title="Permalink to this headline"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">Parameter</span></code> is used for PyTorch trainable parameters, and <code class="docutils literal notranslate"><span class="pre">requires_grad=False</span></code> or <code class="docutils literal notranslate"><span class="pre">buffer</span></code> is used for PyTorch untrainable parameters.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">ptNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ptNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>


<span class="n">ptnet</span> <span class="o">=</span> <span class="n">ptNet</span><span class="p">()</span>
<span class="n">all_parameter</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">trainable_params</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c1"># Obtain network parameters.</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">ptnet</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">item</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
        <span class="n">trainable_params</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
    <span class="n">all_parameter</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">item</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">buffer</span> <span class="ow">in</span> <span class="n">ptnet</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">():</span>
    <span class="n">all_parameter</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">buffer</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">buffer</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;all parameter numbers: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">all_parameter</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;trainable parameter numbers: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">trainable_params</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>    fc.weight torch.Size([1, 1])
    fc.bias torch.Size([1])
    all parameter numbers: 2
    trainable parameter numbers: 2
</pre></div>
</div>
<p>The parameters of MindSpore and PyTorch are similar except BatchNorm. Note that MindSpore does not have parameters corresponding to <code class="docutils literal notranslate"><span class="pre">num_batches_tracked</span></code>. You can replace this parameter with <code class="docutils literal notranslate"><span class="pre">global_step</span></code> in the optimizer.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>MindSpore</p></th>
<th class="head"><p>PyTorch</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>gamma</p></td>
<td><p>weight</p></td>
</tr>
<tr class="row-odd"><td><p>beta</p></td>
<td><p>bias</p></td>
</tr>
<tr class="row-even"><td><p>moving_mean</p></td>
<td><p>running_mean</p></td>
</tr>
<tr class="row-odd"><td><p>moving_variance</p></td>
<td><p>running_var</p></td>
</tr>
<tr class="row-even"><td><p>-</p></td>
<td><p>num_batches_tracked</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="2-model-verification">
<h3>2. Model Verification<a class="headerlink" href="#2-model-verification" title="Permalink to this headline"></a></h3>
<p>The implementation of the model algorithm is irrelevant to the framework. The trained parameters can be converted into the <a class="reference external" href="https://www.mindspore.cn/tutorials/en/r1.9/beginner/save_load.html">checkpoint</a> file of MindSpore and loaded to the network for inference verification.</p>
<p>For details about the model verification process, see <a class="reference external" href="https://www.mindspore.cn/docs/en/r1.9/migration_guide/sample_code.html#model-validation">ResNet Network Migration</a>.</p>
</section>
<section id="3-inference-verification">
<h3>3. Inference Verification<a class="headerlink" href="#3-inference-verification" title="Permalink to this headline"></a></h3>
<p>After confirming that the model structures are the same, you are advised to perform inference verification again. In addition to models, the entire inference process also involves datasets and metrics. When the inference results are inconsistent, you can use the control variable method to gradually rectify the fault.</p>
<p>For details about the inference verification process, see <a class="reference external" href="https://www.mindspore.cn/docs/en/r1.9/migration_guide/sample_code.html#inference-process">ResNet Network Migration</a>.</p>
</section>
<section id="4-training-accuracy">
<h3>4. Training Accuracy<a class="headerlink" href="#4-training-accuracy" title="Permalink to this headline"></a></h3>
<p>After the inference verification is complete, the basic model, data processing, and metrics calculation are normal. If the training accuracy is still abnormal, how do we locate the fault?</p>
<ul class="simple">
<li><p>Add loss scale. On Ascend, operators such as Conv, Sort, and TopK can only be float16. MatMul is recommended to be float16 due to performance problems. Therefore, it is recommended that loss scale be used as a standard configuration for network training.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="c1"># Model</span>
<span class="n">loss_scale_manager</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">FixedLossScaleManager</span><span class="p">(</span><span class="n">drop_overflow_update</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="c1"># Static loss scale</span>
<span class="c1"># loss_scale_manager = ms.DynamicLossScaleManager()   # Dynamic loss scale</span>

<span class="c1"># 1. General process</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">msnet</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">network</span><span class="o">=</span><span class="n">msnet</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">loss_scale_manager</span><span class="o">=</span><span class="n">loss_scale_manager</span><span class="p">)</span>

<span class="c1"># 2. Self-packaged forward network and loss function</span>
<span class="n">msnet</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="n">loss</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">net_with_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">WithLossCell</span><span class="p">(</span><span class="n">msnet</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
<span class="c1"># It is recommended that loss_fn be used for the mixed precision of the model. Otherwise, float16 is used for calculation of the loss part, which may cause overflow.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">network</span><span class="o">=</span><span class="n">net_with_loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">)</span>

<span class="c1"># 3. Self-packaged training process</span>
<span class="n">scale_sense</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">FixedLossScaleUpdateCell</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="c1">#(config.loss_scale) # Static loss scale</span>
<span class="c1"># scale_sense = nn.DynamicLossScaleUpdateCell(loss_scale_value=config.loss_scale,</span>
<span class="c1">#                                             scale_factor=2, scale_window=1000) # Dynamic loss scale</span>
<span class="n">train_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">TrainOneStepWithLossScaleCell</span><span class="p">(</span><span class="n">net_with_loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">scale_sense</span><span class="o">=</span><span class="n">scale_sense</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">network</span><span class="o">=</span><span class="n">train_net</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Check whether overflow occurs. When loss scale is added, overflow detection is added by default to monitor the overflow result. If overflow occurs continuously, you are advised to use the <a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.9/debugger.html">debugger</a> or <a class="reference external" href="https://mindspore.cn/tutorials/experts/en/r1.9/debug/dump.html">dump data</a> of MindInsight to check why overflow occurs.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dataset</span> <span class="k">as</span> <span class="n">ds</span>

<span class="k">def</span> <span class="nf">get_data</span><span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="n">w</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">3.0</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mf">10.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">)</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">w</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="n">noise</span>
        <span class="k">yield</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">y</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">create_dataset</span><span class="p">(</span><span class="n">num_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">repeat_size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">GeneratorDataset</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">get_data</span><span class="p">(</span><span class="n">num_data</span><span class="p">)),</span> <span class="n">column_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span><span class="p">])</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">repeat_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">input_data</span>

<span class="n">train_net</span><span class="o">.</span><span class="n">set_train</span><span class="p">()</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="mi">1600</span><span class="p">)</span>
<span class="n">iterator</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_tuple_iterator</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">iterator</span><span class="p">):</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">overflow</span><span class="p">,</span> <span class="n">scaling_sens</span> <span class="o">=</span> <span class="n">train_net</span><span class="p">(</span><span class="o">*</span><span class="n">data</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;step: </span><span class="si">{}</span><span class="s2">, loss: </span><span class="si">{}</span><span class="s2">, overflow:</span><span class="si">{}</span><span class="s2">, scale:</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">overflow</span><span class="p">,</span> <span class="n">scaling_sens</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>    step: 0, loss: 138.42825, overflow:False, scale:1.0
    step: 1, loss: 118.172104, overflow:False, scale:1.0
    step: 2, loss: 159.14542, overflow:False, scale:1.0
    step: 3, loss: 150.65671, overflow:False, scale:1.0
    ... ...
    step: 97, loss: 69.513245, overflow:False, scale:1.0
    step: 98, loss: 51.903114, overflow:False, scale:1.0
    step: 99, loss: 42.250656, overflow:False, scale:1.0
</pre></div>
</div>
<ul class="simple">
<li><p>Check the optimizer, loss, and parameter initialization. In addition to the model and dataset, only the optimizer, loss, and parameter initialization are added in the entire training process. If the training is abnormal, check the optimizer, loss, and parameter initialization. Especially for loss and parameter initialization, there is a high probability that the problem occurs.</p></li>
<li><p>Check whether to add seeds for multiple devices to ensure that the initialization of multiple SIM cards is consistent. Determine whether to perform gradient aggregation during <a class="reference external" href="https://www.mindspore.cn/docs/en/r1.9/migration_guide/model_development/training_and_gradient.html#customizing-training-cell">customized training</a>.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># The random seeds of MindSpore, NumPy, and dataset are fixed. The random seed of the API needs to be set in the API attribute.</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Check whether the data processing meets the expectation through visualization. Focus on data shuffle and check whether data mismatch occurs.</p></li>
</ul>
<p>For details about more accuracy debugging policies, see <a class="reference external" href="https://mindspore.cn/mindinsight/docs/en/r1.9/accuracy_problem_preliminary_location.html">Accuracy Debugging</a>.</p>
</section>
</section>
<section id="performance-tuning">
<h2>Performance Tuning<a class="headerlink" href="#performance-tuning" title="Permalink to this headline"></a></h2>
<p>The performance tuning directions are as follows:</p>
<ol class="arabic simple">
<li><p>Operator performance tuning</p></li>
<li><p>Framework enabling performance tuning</p></li>
<li><p>Multi-Node synchronization performance tuning</p></li>
<li><p>Data processing performance tuning</p></li>
</ol>
<p>For details, see <a class="reference external" href="https://www.mindspore.cn/docs/en/r1.9/migration_guide/sample_code.html">ResNet Network Migration</a>.</p>
<blockquote>
<div><p>Some networks are large or there are many <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r1.9/network/control_flow.html">process control statements</a>. In this case, the build is slow in graph mode. During performance tuning, distinguish graph build from network execution. This section describes the performance tuning policies in the network execution phase. If graph build is slow, try <a class="reference external" href="https://mindspore.cn/tutorials/experts/en/r1.9/debug/op_compilation.html">incremental operator build</a> or contact <a class="reference external" href="https://gitee.com/mindspore/mindspore/issues">MindSpore community</a> for feedback.</p>
</div></blockquote>
<section id="operator-performance-tuning">
<h3>Operator Performance Tuning<a class="headerlink" href="#operator-performance-tuning" title="Permalink to this headline"></a></h3>
<section id="poor-operator-performance">
<h4>Poor Operator Performance<a class="headerlink" href="#poor-operator-performance" title="Permalink to this headline"></a></h4>
<p>If a single operator takes a long time and the performance of the same operator varies greatly in different shapes or data types, the problem is caused by the operator performance. The solution is as follows:</p>
<ol class="arabic simple">
<li><p>Use data types with less computational workload. For example, if there is no obvious difference between the precision of the same operator in float16 and float32 modes, you can use the float16 format with less calculation workload.</p></li>
<li><p>Use other operators with the same algorithm to avoid this problem.</p></li>
<li><p>Pay attention to 16-alignment in the Ascend environment. Due to the design of the Ascend AI Processors, it is recommended that the calculation on the AI core be 16-alignment (each dimension in the shape is a multiple of 16).</p></li>
<li><p><a class="reference external" href="https://mindspore.cn/tutorials/experts/en/r1.9/debug/auto_tune.html">Operator Tuning</a>.</p></li>
</ol>
<p>If you find an operator with poor performance, you are advised to contact <a class="reference external" href="https://gitee.com/mindspore/mindspore/issues">MindSpore community</a> for feedback. We will optimize the operator in time after confirming that the problem is caused by poor performance.</p>
</section>
</section>
<section id="framework-enabling-performance-tuning">
<h3>Framework Enabling Performance Tuning<a class="headerlink" href="#framework-enabling-performance-tuning" title="Permalink to this headline"></a></h3>
<section id="using-the-static-graph-mode">
<h4>Using the Static Graph Mode<a class="headerlink" href="#using-the-static-graph-mode" title="Permalink to this headline"></a></h4>
<p>Generally, MindSpore in static graph mode is much faster than that in PyNative mode. It is recommended that training and inference be performed in static graph mode.</p>
</section>
<section id="on-device-execution">
<h4>On-device Execution<a class="headerlink" href="#on-device-execution" title="Permalink to this headline"></a></h4>
<p>You only need to set <code class="docutils literal notranslate"><span class="pre">dataset_sink_mode=True</span></code> in <code class="docutils literal notranslate"><span class="pre">model.train</span></code>. Note that this configuration is <code class="docutils literal notranslate"><span class="pre">True</span></code> by default. When this configuration is enabled, one epoch returns the result of only one network. You are advised to change the value to <code class="docutils literal notranslate"><span class="pre">False</span></code> during debugging.</p>
</section>
<section id="using-automatic-mixed-precision">
<h4>Using Automatic Mixed Precision<a class="headerlink" href="#using-automatic-mixed-precision" title="Permalink to this headline"></a></h4>
<p>The mixed precision training method accelerates the deep neural network training process by mixing the single-precision floating-point data format and the half-precision floating-point data format without compromising the network accuracy. Mixed precision training can accelerate the computing process, reduce memory usage and retrieval, and enable a larger model or batch size to be trained on specific hardware.</p>
<p>For details, see <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r1.9/others/mixed_precision.html">Mixed Precision Tutorial</a>.</p>
</section>
<section id="enabling-graph-kernel-fusion">
<h4>Enabling Graph Kernel Fusion<a class="headerlink" href="#enabling-graph-kernel-fusion" title="Permalink to this headline"></a></h4>
<p>Graph kernel fusion is a unique network performance optimization technology of MindSpore. It can automatically analyze and optimize the logic of existing network computational graphs, simplify and replace computational graphs, split and fuse operators, and build operators in a special way based on the target hardware capability to improve the computing resource utilization of devices and optimize the overall network performance. Compared with traditional optimization technologies, the graph kernel fusion technology has unique advantages, such as joint optimization of multiple operators across boundaries, cross-layer collaboration with operator compilation, and real-time compilation of operators based on Polyhedral. In addition, the entire optimization process of graph kernel fusion can be automatically completed after users enable the corresponding configuration. Network developers do not need to perform extra perception, so that users can focus on network algorithm implementation.</p>
<p>Graph kernel fusion applies to scenarios that have high requirements on network execution time. Basic operators are combined to implement customized combination operators and these basic operators are automatically fused to improve the performance of the customized combination operators.</p>
</section>
<section id="others">
<h4>Others<a class="headerlink" href="#others" title="Permalink to this headline"></a></h4>
<p>If there are too many conversion operators (TransData and Cast operators) and the conversion takes a long time, analyze the necessity of the manually added Cast operator. If the accuracy is not affected, delete the redundant Cast and TransData operators.</p>
<p>If there are too many conversion operators automatically generated by MindSpore, the MindSpore framework may not be fully optimized for some special cases. In this case, contact <a class="reference external" href="https://gitee.com/mindspore/mindspore/issues">MindSpore community</a> for feedback.</p>
<p>In <a class="reference external" href="https://www.mindspore.cn/docs/en/r1.9/migration_guide/analysis_and_preparation.html">dynamic shape scenario</a>, continuous graph build is required, which may cause a long end-to-end training time. You are advised to <a class="reference external" href="https://www.mindspore.cn/docs/en/r1.9/migration_guide/model_development/model_and_loss.html">avoid dynamic shape</a>.</p>
</section>
</section>
<section id="multi-node-synchronization-performance-tuning">
<h3>Multi-Node Synchronization Performance Tuning<a class="headerlink" href="#multi-node-synchronization-performance-tuning" title="Permalink to this headline"></a></h3>
<p>During distributed training, after forward propagation and gradient calculation are complete in a step training process, each machine starts to perform AllReduce gradient synchronization. The AllReduce synchronization time is mainly affected by the number of weights and machines. For a more complex network with a larger machine scale, the AllReduce gradient update time is longer. In this case, you can perform AllReduce segmentation to reduce the time consumption.</p>
<p>In normal cases, AllReduce gradient synchronization waits until all backward operators are executed. That is, after the gradient of all gradients is calculated, the gradients of all machines are synchronized at a time. After AllReduce segmentation is used, the gradients of some weights can be calculated, gradient synchronization of this part of weights is immediately performed. In this way, gradient synchronization and gradient calculation of remaining operators can be performed concurrently, and this part of AllReduce gradient synchronization time is hidden. The shard strategy is usually manually tried to find an optimal solution (more than two shards are supported).
The <a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.9/official/cv/resnet/train.py">ResNet-50</a> is used as an example. The network has 160 weights. [85, 160] indicates that gradient synchronization is performed immediately after the gradients of weights 0 to 85 are calculated, and gradient synchronization is performed after the gradients of weights 86 to 160 are calculated. The network is divided into two shards. Therefore, gradient synchronization needs to be performed twice. The sample code is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span>

<span class="n">device_id</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;DEVICE_ID&#39;</span><span class="p">,</span> <span class="s1">&#39;0&#39;</span><span class="p">))</span>
<span class="n">rank_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;RANK_SIZE&#39;</span><span class="p">,</span> <span class="s1">&#39;1&#39;</span><span class="p">))</span>
<span class="n">rank_id</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;RANK_ID&#39;</span><span class="p">,</span> <span class="s1">&#39;0&#39;</span><span class="p">))</span>

<span class="c1"># init context</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s1">&#39;Ascend&#39;</span><span class="p">,</span> <span class="n">device_id</span><span class="o">=</span><span class="n">device_id</span><span class="p">)</span>
<span class="k">if</span> <span class="n">rank_size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">device_num</span><span class="o">=</span><span class="n">rank_size</span><span class="p">,</span> <span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">DATA_PARALLEL</span><span class="p">,</span>
                                <span class="n">gradients_mean</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">all_reduce_fusion_config</span><span class="o">=</span><span class="p">[</span><span class="mi">85</span><span class="p">,</span> <span class="mi">160</span><span class="p">])</span>
    <span class="n">init</span><span class="p">()</span>
</pre></div>
</div>
<p>For details, see <a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.9/performance_profiling_of_cluster.html">Cluster Performance Profiling</a>.</p>
</section>
<section id="data-processing-performance-tuning">
<h3>Data Processing Performance Tuning<a class="headerlink" href="#data-processing-performance-tuning" title="Permalink to this headline"></a></h3>
<p>The performance jitter of a single step and the empty data queue for a period of time are caused by the poor performance of the data preprocessing part. As a result, the data processing speed cannot keep up with the iteration speed of a single step. The two symptoms usually occur in pairs.</p>
<p>When the data processing speed is slow, the empty queue is gradually consumed from the beginning when the queue is full. The training process starts to wait for the empty queue to fill in data. Once new data is filled in, the network continues single-step training. Because no queue is used as the buffer for data processing, the performance jitter of data processing is directly reflected by the performance of a single step. Therefore, the performance jitter of a single step is also caused.</p>
<p>For details about data performance problems, see <a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.9/performance_profiling_ascend.html#data-preparation-performance-analysis">Data Preparation Performance Analysis</a> of MindInsight. This describes common data performance problems and solutions.</p>
<p>For more performance debugging methods, see <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r1.9/debug/performance_optimization.html">Performance Tuning</a>.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="model_development/training_and_evaluation_procession.html" class="btn btn-neutral float-left" title="Inference and Training Process" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="sample_code.html" class="btn btn-neutral float-right" title="Network Migration Debugging Example" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>