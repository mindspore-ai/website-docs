<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore.ops.function.math_func &mdash; MindSpore master documentation</title>
      <link rel="stylesheet" href="../../../../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/css/training.css" type="text/css" /><link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/js/training.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/overview.html">MindSpore Design Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/auto_gradient.html">Functional Differential Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/distributed_training_design.html">Distributed Training Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/dynamic_graph_and_static_graph.html">Combination of Dynamic and Static Graphs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/graph_fusion_engine.html">Graph-Kernel Fusion Acceleration Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/mindir.html">MindSpore IR (MindIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/all_scenarios.html">Full-scenarios Unification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/thor.html">Second Order Optimizer</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r2.0.0-alpha/training_visual_design.html">Design of Visualization↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/data_engine.html">High Performance Data Processing Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Specification</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/benchmark.html">Benchmarks</a></li>
<li class="toctree-l1"><a class="reference external" href="https://gitee.com/mindspore/models/blob/r2.0.0-alpha/README.md#table-of-contents">Network List↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/operator_list.html">API List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/syntax_list.html">Syntax Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.transformer.html">mindspore.nn.transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/api_cpp/mindspore.html">C++ API↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Mapping</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/api_mapping/pytorch_api_mapping.html">PyTorch and MindSpore API Mapping Table</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/api_mapping/tensorflow_api_mapping.html">TensorFlow and MindSpore API Mapping Table</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Migration Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/overview.html">Overview of Migration Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/enveriment_preparation.html">Environment Preparation and Information Acquisition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/analysis_and_preparation.html">Model Analysis and Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/model_development/model_development.html">Constructing MindSpore Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/debug_and_tune.html">Debugging and Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/sample_code.html">Network Migration Debugging Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/faq.html">FAQs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/typical_api_comparision.html">Differences Between MindSpore and PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/use_third_party_op.html">Using Third-party Operator Libraries Based on Customized Interfaces</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/data_processing.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/implement_problem.html">Implement Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/network_compilation.html">Network Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/operators_compile.html">Operators Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/usage_migrate_3rd.html">Migration from a Third-party Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/precision_tuning.html">Precision Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/distributed_configure.html">Distributed Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/feature_advice.html">Feature Advice</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
      <li>mindspore.ops.function.math_func</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for mindspore.ops.function.math_func</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2022 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>

<span class="sd">&quot;&quot;&quot;Defines math operators with functional form.&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">zip_longest</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">deque</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">mindspore.common</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.primitive</span> <span class="kn">import</span> <span class="n">constexpr</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">composite</span> <span class="k">as</span> <span class="n">C</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations._inner_ops</span> <span class="kn">import</span> <span class="n">Cummin</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations.math_ops</span> <span class="kn">import</span> <span class="n">STFT</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations.math_ops</span> <span class="kn">import</span> <span class="n">ReduceStd</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations.math_ops</span> <span class="kn">import</span> <span class="n">Logit</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations.math_ops</span> <span class="kn">import</span> <span class="n">LuUnpack</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations.math_ops</span> <span class="kn">import</span> <span class="n">Roll</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">layer</span>
<span class="kn">from</span> <span class="nn">mindspore._checkparam</span> <span class="kn">import</span> <span class="n">check_is_number</span>
<span class="kn">from</span> <span class="nn">mindspore._checkparam</span> <span class="kn">import</span> <span class="n">Rel</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations.math_ops</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Bernoulli</span><span class="p">,</span>
    <span class="n">BesselI0</span><span class="p">,</span>
    <span class="n">BesselI1</span><span class="p">,</span>
    <span class="n">BesselJ0</span><span class="p">,</span>
    <span class="n">BesselJ1</span><span class="p">,</span>
    <span class="n">BesselK0</span><span class="p">,</span>
    <span class="n">BesselK0e</span><span class="p">,</span>
    <span class="n">BesselY0</span><span class="p">,</span>
    <span class="n">BesselY1</span><span class="p">,</span>
    <span class="n">BesselK1</span><span class="p">,</span>
    <span class="n">BesselK1e</span><span class="p">,</span>
    <span class="n">MatrixExp</span><span class="p">,</span>
    <span class="n">MatrixSolve</span><span class="p">,</span>
    <span class="n">Median</span><span class="p">,</span>
    <span class="n">Orgqr</span><span class="p">,</span>
    <span class="n">Renorm</span><span class="p">,</span>
    <span class="n">Hypot</span><span class="p">,</span>
    <span class="n">Heaviside</span><span class="p">,</span>
    <span class="n">Lcm</span><span class="p">,</span>
    <span class="n">Gcd</span><span class="p">,</span>
    <span class="n">Sinc</span><span class="p">,</span>
    <span class="n">NanToNum</span><span class="p">,</span>
    <span class="n">SparseSegmentMean</span><span class="p">,</span>
    <span class="n">TriuIndices</span><span class="p">,</span>
    <span class="n">InplaceIndexAdd</span><span class="p">,</span>
    <span class="n">InplaceUpdateV2</span><span class="p">,</span>
    <span class="n">Igamma</span><span class="p">,</span>
    <span class="n">Igammac</span><span class="p">,</span>
    <span class="n">Angle</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">mindspore.common.tensor</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">mindspore._checkparam</span> <span class="kn">import</span> <span class="n">Validator</span> <span class="k">as</span> <span class="n">validator</span>
<span class="kn">from</span> <span class="nn">mindspore.ops._primitive_cache</span> <span class="kn">import</span> <span class="n">_get_cache_prim</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">Tensor</span> <span class="k">as</span> <span class="n">Tensor_</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_make_tensor</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns the tensor with value `val` and dtype `dtype`.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">get_x_shape</span><span class="p">(</span><span class="n">x_shape</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">x_shape</span><span class="p">:</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">s</span> <span class="o">*</span> <span class="n">i</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">s</span><span class="p">,)</span>


<span class="c1">#####################################</span>
<span class="c1"># Public Operation Functions.</span>
<span class="c1">#####################################</span>
<span class="n">absolute_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Abs</span><span class="p">()</span>
<span class="n">tensor_ceil</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Ceil</span><span class="p">()</span>
<span class="n">tensor_add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span>
<span class="n">neg_tensor</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Neg</span><span class="p">()</span>
<span class="n">tensor_sub</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Sub</span><span class="p">()</span>
<span class="n">tensor_mul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span>
<span class="n">tensor_div</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">RealDiv</span><span class="p">()</span>
<span class="n">tensor_floordiv</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">FloorDiv</span><span class="p">()</span>
<span class="n">floordiv</span> <span class="o">=</span> <span class="n">tensor_floordiv</span>
<span class="n">xdivy_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Xdivy</span><span class="p">()</span>
<span class="n">tensor_pow</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Pow</span><span class="p">()</span>
<span class="n">pows</span> <span class="o">=</span> <span class="n">tensor_pow</span>
<span class="n">tensor_mod</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">FloorMod</span><span class="p">()</span>
<span class="n">floormod</span> <span class="o">=</span> <span class="n">tensor_mod</span>
<span class="n">tensor_exp</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Exp</span><span class="p">()</span>
<span class="n">tensor_expm1</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Expm1</span><span class="p">()</span>
<span class="n">tensor_lt</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Less</span><span class="p">()</span>
<span class="n">tensor_le</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">LessEqual</span><span class="p">()</span>
<span class="n">tensor_gt</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Greater</span><span class="p">()</span>
<span class="n">tensor_ge</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">GreaterEqual</span><span class="p">()</span>
<span class="n">not_equal</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">NotEqual</span><span class="p">()</span>
<span class="n">size_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Size</span><span class="p">()</span>
<span class="n">transpose_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Transpose</span><span class="p">()</span>

<span class="c1">#####################################</span>
<span class="c1"># Private Operation Functions.</span>
<span class="c1">#####################################</span>
<span class="n">addn_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">AddN</span><span class="p">()</span>
<span class="n">angle_</span> <span class="o">=</span> <span class="n">Angle</span><span class="p">()</span>
<span class="n">log_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Log</span><span class="p">()</span>
<span class="n">floor_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Floor</span><span class="p">()</span>
<span class="n">logical_not_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">LogicalNot</span><span class="p">()</span>
<span class="n">logical_or_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">LogicalOr</span><span class="p">()</span>
<span class="n">logical_and_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">LogicalAnd</span><span class="p">()</span>
<span class="n">sin_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Sin</span><span class="p">()</span>
<span class="n">sinc_</span> <span class="o">=</span> <span class="n">Sinc</span><span class="p">()</span>
<span class="n">cos_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cos</span><span class="p">()</span>
<span class="n">tan_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Tan</span><span class="p">()</span>
<span class="n">asin_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Asin</span><span class="p">()</span>
<span class="n">acos_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ACos</span><span class="p">()</span>
<span class="n">atan_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Atan</span><span class="p">()</span>
<span class="n">sinh_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Sinh</span><span class="p">()</span>
<span class="n">cosh_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cosh</span><span class="p">()</span>
<span class="n">tanh_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
<span class="n">asinh_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Asinh</span><span class="p">()</span>
<span class="n">acosh_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Acosh</span><span class="p">()</span>
<span class="n">atanh_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Atanh</span><span class="p">()</span>
<span class="n">bitwise_and_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BitwiseAnd</span><span class="p">()</span>
<span class="n">bitwise_or_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BitwiseOr</span><span class="p">()</span>
<span class="n">bitwise_xor_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BitwiseXor</span><span class="p">()</span>
<span class="n">inv_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">math_ops</span><span class="o">.</span><span class="n">Inv</span><span class="p">()</span>
<span class="n">invert_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Invert</span><span class="p">()</span>
<span class="n">erf_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Erf</span><span class="p">()</span>
<span class="n">erfc_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Erfc</span><span class="p">()</span>
<span class="n">bessel_j1_</span> <span class="o">=</span> <span class="n">BesselJ1</span><span class="p">()</span>
<span class="n">bessel_j0_</span> <span class="o">=</span> <span class="n">BesselJ0</span><span class="p">()</span>
<span class="n">bessel_i0_</span> <span class="o">=</span> <span class="n">BesselI0</span><span class="p">()</span>
<span class="n">bessel_i0e_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BesselI0e</span><span class="p">()</span>
<span class="n">bessel_k0_</span> <span class="o">=</span> <span class="n">BesselK0</span><span class="p">()</span>
<span class="n">bessel_k0e_</span> <span class="o">=</span> <span class="n">BesselK0e</span><span class="p">()</span>
<span class="n">bessel_y0_</span> <span class="o">=</span> <span class="n">BesselY0</span><span class="p">()</span>
<span class="n">bessel_y1_</span> <span class="o">=</span> <span class="n">BesselY1</span><span class="p">()</span>
<span class="n">bessel_i1_</span> <span class="o">=</span> <span class="n">BesselI1</span><span class="p">()</span>
<span class="n">bessel_i1e_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BesselI1e</span><span class="p">()</span>
<span class="n">bessel_k1_</span> <span class="o">=</span> <span class="n">BesselK1</span><span class="p">()</span>
<span class="n">bessel_k1e_</span> <span class="o">=</span> <span class="n">BesselK1e</span><span class="p">()</span>
<span class="n">equal_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Equal</span><span class="p">()</span>
<span class="n">isfinite_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">IsFinite</span><span class="p">()</span>
<span class="n">isnan_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">IsNan</span><span class="p">()</span>
<span class="n">maximum_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Maximum</span><span class="p">()</span>
<span class="n">minimum_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Minimum</span><span class="p">()</span>
<span class="n">lerp_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Lerp</span><span class="p">()</span>
<span class="n">tensor_round_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Round</span><span class="p">()</span>
<span class="n">linspace_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">LinSpace</span><span class="p">()</span>
<span class="n">matrix_determinant_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">MatrixDeterminant</span><span class="p">()</span>
<span class="n">log_matrix_determinant_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">LogMatrixDeterminant</span><span class="p">()</span>
<span class="n">matrix_exp_</span> <span class="o">=</span> <span class="n">MatrixExp</span><span class="p">()</span>
<span class="n">exp2_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Pow</span><span class="p">()</span>
<span class="n">truncate_div_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TruncateDiv</span><span class="p">()</span>
<span class="n">truncate_mod_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TruncateMod</span><span class="p">()</span>
<span class="n">sparse_segment_mean_</span> <span class="o">=</span> <span class="n">SparseSegmentMean</span><span class="p">()</span>
<span class="n">xlogy_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Xlogy</span><span class="p">()</span>
<span class="n">square_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Square</span><span class="p">()</span>
<span class="n">sqrt_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Sqrt</span><span class="p">()</span>
<span class="n">cumsum_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">CumSum</span><span class="p">()</span>


<span class="c1">#####################################</span>
<span class="c1"># Element-wise Operation Functions.</span>
<span class="c1">#####################################</span>

<div class="viewcode-block" id="addn"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.addn.html#mindspore.ops.addn">[docs]</a><span class="k">def</span> <span class="nf">addn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes addition of all input tensors element-wise.</span>

<span class="sd">    All input tensors must have the same shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union(tuple[Tensor], list[Tensor])): A tuple or list composed of Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as each Tensor of `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is neither tuple nor list.</span>
<span class="sd">        ValueError: If there are Tensors with different shapes in `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4, 5, 6]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.addn([x, y, x, y])</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [10. 14. 18.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">addn_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="abs"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.abs.html#mindspore.ops.abs">[docs]</a><span class="k">def</span> <span class="nf">abs</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns absolute value of a tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = |x_i|</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor. The shape of tensor is</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-1.0, 1.0, 0.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.abs(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 1. 0.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">absolute_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="absolute"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.absolute.html#mindspore.ops.absolute">[docs]</a><span class="k">def</span> <span class="nf">absolute</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Alias for :func:`mindspore.ops.abs` .</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">abs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="add"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.add.html#mindspore.ops.add">[docs]</a><span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adds two input tensors element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = x_{i} + y_{i}</span>

<span class="sd">    .. note::</span>
<span class="sd">        - Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">        - The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">        - When the inputs are two tensors,</span>
<span class="sd">          dtypes of them cannot be bool at the same time, and the shapes of them can be broadcast.</span>
<span class="sd">        - When the inputs are one tensor and one scalar, the scalar could only be a constant.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[Tensor, number.Number, bool]): The first input is a number.Number or</span>
<span class="sd">            a bool or a tensor whose data type is</span>
<span class="sd">            `number &lt;https://www.mindspore.cn/docs/en/r2.0.0-alpha/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">            `bool_ &lt;https://www.mindspore.cn/docs/en/r2.0.0-alpha/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>
<span class="sd">        y (Union[Tensor, number.Number, bool]): The second input, when the first input is a Tensor,</span>
<span class="sd">            the second input should be a number.Number or bool value, or a Tensor whose data type is number or bool\_.</span>
<span class="sd">            When the first input is Scalar, the second input must be a Tensor whose data type is number or bool\_.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one of the input `x` , `y` after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` and `y` is not one of the following: Tensor, number.Number, bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # case 1: x and y are both Tensor.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4, 5, 6]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.add(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [5. 7. 9.]</span>
<span class="sd">        &gt;&gt;&gt; # case 2: x is a scalar and y is a Tensor</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(1, mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4, 5, 6]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.add(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [5. 6. 7.]</span>
<span class="sd">        &gt;&gt;&gt; # the data type of x is int32, the data type of y is float32,</span>
<span class="sd">        &gt;&gt;&gt; # and the output is the data format of higher precision float32.</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Float32</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="addcdiv"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.addcdiv.html#mindspore.ops.addcdiv">[docs]</a><span class="k">def</span> <span class="nf">addcdiv</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs the element-wise division of tensor x1 by tensor x2,</span>
<span class="sd">    multiply the result by the scalar value and add it to input_data.</span>

<span class="sd">    .. math::</span>
<span class="sd">        y[i] = input\_data[i] + value[i] * (x1[i] / x2[i])</span>

<span class="sd">    Args:</span>
<span class="sd">        input_data (Tensor): The tensor to be added.</span>
<span class="sd">        x1 (Tensor): The numerator tensor.</span>
<span class="sd">        x2 (Tensor): The denominator tensor.</span>
<span class="sd">        value (Tensor): The multiplier for tensor x1/x2.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as x1/x2.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x1`, `x2`, `value`, `input_data` is not tensor.</span>
<span class="sd">        ValueError: If `x1` could not be broadcast to a tensor with shape of `x2`.</span>
<span class="sd">        ValueError: If `value` could not be broadcast to tensors with shapes of `x1/x2`.</span>
<span class="sd">        ValueError: If `input_data` could not be broadcast to tensors with shapes of `value*(x1/x2)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_data = Tensor(np.array([1, 1, 1, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([1, 2, 3, 4]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([4, 3, 2, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; value = Tensor([1], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = ops.addcdiv(input_data, x1, x2, value)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [1.25      1.6666667 2.5       5.       ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Addcdiv</span><span class="p">)()(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span></div>


<div class="viewcode-block" id="addcmul"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.addcmul.html#mindspore.ops.addcmul">[docs]</a><span class="k">def</span> <span class="nf">addcmul</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs the element-wise product of tensor x1 and tensor x2,</span>
<span class="sd">    multiply the result by the scalar value and add it to input_data.</span>

<span class="sd">    .. math::</span>
<span class="sd">        output[i] = input\_data[i] + value[i] * (x1[i] * x2[i])</span>

<span class="sd">    Args:</span>
<span class="sd">        input_data (Tensor): The tensor to be added.</span>
<span class="sd">        x1 (Tensor): The tensor to be multiplied.</span>
<span class="sd">        x2 (Tensor): The tensor to be multiplied.</span>
<span class="sd">        value (Tensor): The multiplier for tensor x1*x2.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as x1*x2.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x1`, `x2`, `value`, `input_data` is not tensor.</span>
<span class="sd">        TypeError: If dtype of `input_data` is not one of: float32, float16, int32.</span>
<span class="sd">        TypeError: If dtype of `x1` or `x2` is not one of: float32, float16, int32.</span>
<span class="sd">        TypeError: If dtype of `value` is not one of: float32, float16, int32.</span>
<span class="sd">        ValueError: If `x1` could not be broadcast to a tensor with shape of `x2`.</span>
<span class="sd">        ValueError: If `value` could not be broadcast to tensors with shapes of `x1` * `x2`.</span>
<span class="sd">        ValueError: If `input_data` could not be broadcast to tensors with shapes of `value*(x1*x2)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_data = Tensor(np.array([1, 1, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([[1], [2], [3]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([[1, 2, 3]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; value = Tensor([1], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = ops.addcmul(input_data, x1, x2, value)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[ 2.  3.  4.]</span>
<span class="sd">         [ 3.  5.  7.]</span>
<span class="sd">         [ 4.  7. 10.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Addcmul</span><span class="p">)()(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span></div>


<div class="viewcode-block" id="angle"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.angle.html#mindspore.ops.angle">[docs]</a><span class="k">def</span> <span class="nf">angle</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the element-wise argument of a complex tensor.</span>
<span class="sd">    The elements in input are considered to be complex numbers of the form a+bj, where a is the real part and b</span>
<span class="sd">    is the imaginary part. The argument returned by this function is of the form atan2(b,a).</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor. types: complex64, complex128.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the float32 or float64 type and the same shape as input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If the dtype of input is not one of: complex64, complex128.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor([-1.5 + 7.8j, 3 + 5.75j], mindspore.complex64)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.angle(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.7607845 1.0899091]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">angle_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">exp2</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the base two exponential function of input.</span>

<span class="sd">    Calculates ``2^x``.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">     Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([2, 3, 4]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.exp2(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 4.  8. 16.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">tensor_2</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
        <span class="n">tensor_2</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">exp2_</span><span class="p">(</span><span class="n">tensor_2</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>


<div class="viewcode-block" id="argmin"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.argmin.html#mindspore.ops.argmin">[docs]</a><span class="k">def</span> <span class="nf">argmin</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the indices of the minimum value of a tensor across the axis.</span>

<span class="sd">    If the shape of input tensor is :math:`(x_1, ..., x_N)`, the shape of the output tensor is</span>
<span class="sd">    :math:`(x_1, ..., x_{axis-1}, x_{axis+1}, ..., x_N)`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input tensor. The shape is :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        axis (Union[int, None], optional): Axis where the Argmin operation applies to. Default: None.</span>
<span class="sd">        keepdims (bool, optional): Whether the output tensor retains the specified</span>
<span class="sd">            dimension. Ignored if `axis` is None. Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, indices of the min value of input tensor across the axis.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([2.0, 3.1, 1.2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; index = ops.argmin(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(index)</span>
<span class="sd">        2</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">():</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">is_axis_none</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,))</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">is_axis_none</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Argmin</span><span class="p">)(</span><span class="n">axis</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">keepdims</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_axis_none</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">()(</span><span class="n">out</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span></div>


<span class="n">neg_tensor</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Neg</span><span class="p">()</span>


<div class="viewcode-block" id="neg"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.neg.html#mindspore.ops.neg">[docs]</a><span class="k">def</span> <span class="nf">neg</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a tensor with negative values of the input tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = - x_{i}</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor with a dtype of Number, its rank must be in [0, 7] inclusive.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, -1, 2, 0, -3.5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.neg(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-1.  -2.   1.  -2.   0.   3.5]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">neg_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="negative"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.negative.html#mindspore.ops.negative">[docs]</a><span class="k">def</span> <span class="nf">negative</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Alias for :func:`mindspore.ops.neg` .</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">neg_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="positive"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.positive.html#mindspore.ops.positive">[docs]</a><span class="k">def</span> <span class="nf">positive</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return self Tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        x(Tensor): Input Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, self input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the dtype of self Tensor is bool type.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-5.0, 1.5, 3.0, 100.0]), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; print(ops.positive(x))</span>
<span class="sd">        [ -5.    1.5   3.  100. ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For positive, the type of tensor can not be bool.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span></div>


<div class="viewcode-block" id="numel"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.numel.html#mindspore.ops.numel">[docs]</a><span class="k">def</span> <span class="nf">numel</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a Scalar of type int that represents the total number of elements in the Tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        int. A scalar representing the total of elements in the Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[2, 2], [2, 2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; print(ops.numel(input_x))</span>
<span class="sd">        4</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">size_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="permute"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.permute.html#mindspore.ops.permute">[docs]</a><span class="k">def</span> <span class="nf">permute</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dims</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Permutes the dimensions of the input tensor according to input `dims` .</span>

<span class="sd">    Args:</span>
<span class="sd">        x(Tensor): Input Tensor.</span>
<span class="sd">        dims(Union[tuple(int), list(int), int]): Permute will permute the tensor to the input `dims` order.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dimension as input tensor, with `dims` suitably permuted.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If `dims` is None.</span>
<span class="sd">        ValueError: If the number of elements of `dims` is not equal to `x` ndim.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_perm = (0, 2, 1)</span>
<span class="sd">        &gt;&gt;&gt; print(ops.permute(input_x, input_perm))</span>
<span class="sd">        [[[ 1.  4.]</span>
<span class="sd">          [ 2.  5.]</span>
<span class="sd">          [ 3.  6.]]</span>
<span class="sd">         [[ 7. 10.]</span>
<span class="sd">          [ 8. 11.]</span>
<span class="sd">          [ 9. 12.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">transpose_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dims</span><span class="p">)</span></div>


<div class="viewcode-block" id="ceil"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ceil.html#mindspore.ops.ceil">[docs]</a><span class="k">def</span> <span class="nf">ceil</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Rounds a tensor up to the closest integer element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \lceil x_i \rceil = \lfloor x_i \rfloor + 1</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor with a dtype of float16 or float32, its rank must be in [0, 7] inclusive.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16 or float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.1, 2.5, -1.5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.ceil(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 2.  3. -1.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_ceil</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="round"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.round.html#mindspore.ops.round">[docs]</a><span class="k">def</span> <span class="nf">round</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns half to even of a tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i \approx x_i</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">         &gt;&gt;&gt; x = Tensor(np.array([0.8, 1.5, 2.3, 2.5, -4.5]), mindspore.float32)</span>
<span class="sd">         &gt;&gt;&gt; output = ops.round(x)</span>
<span class="sd">         &gt;&gt;&gt; print(output)</span>
<span class="sd">         [ 1.  2.  2.  2. -4.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_round_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="sub"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.sub.html#mindspore.ops.sub">[docs]</a><span class="k">def</span> <span class="nf">sub</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Subtracts the second input tensor from the first input tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = x_{i} - y_{i}</span>

<span class="sd">    .. note::</span>
<span class="sd">        - Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">        - The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">        - When the inputs are two tensors,</span>
<span class="sd">          dtypes of them cannot be bool at the same time, and the shapes of them can be broadcast.</span>
<span class="sd">        - When the inputs are one tensor and one scalar, the scalar could only be a constant.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[Tensor, number.Number, bool]): The first input is a number.Number or</span>
<span class="sd">            a bool or a tensor whose data type is</span>
<span class="sd">            `number &lt;https://www.mindspore.cn/docs/en/r2.0.0-alpha/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">            `bool_ &lt;https://www.mindspore.cn/docs/en/r2.0.0-alpha/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>
<span class="sd">        y (Union[Tensor, number.Number, bool]): The second input, when the first input is a Tensor,</span>
<span class="sd">            the second input should be a number.Number or bool value, or a Tensor whose data type is number or bool\_.</span>
<span class="sd">            When the first input is Scalar, the second input must be a Tensor whose data type is number or bool\_.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` and `y` is not a number.Number or a bool or a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4, 5, 6]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.sub(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-3 -3 -3]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_sub</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="subtract"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.subtract.html#mindspore.ops.subtract">[docs]</a><span class="k">def</span> <span class="nf">subtract</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs the element-wise subtraction of input tensors.</span>

<span class="sd">    .. math::</span>
<span class="sd">        output[i] = x[i] - alpha * y[i]</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[Tensor, number.Number]): Tensor or Number involved in subtraction.</span>
<span class="sd">        other (Union[Tensor, number.Number]): The tensor or number to be subtracted.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        alpha (Number): The multiplier for `other`. Default: 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as input tensors.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: `x` or `other` is neither Tensor nor number.Number.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([4, 5, 6]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; z = ops.subtract(x, y, alpha=1)</span>
<span class="sd">        &gt;&gt;&gt; print(z)</span>
<span class="sd">        [3. 3. 3.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_sub</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="true_divide"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.true_divide.html#mindspore.ops.true_divide">[docs]</a><span class="k">def</span> <span class="nf">true_divide</span><span class="p">(</span><span class="n">dividend</span><span class="p">,</span> <span class="n">divisor</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Alias for :func:`mindspore.ops.div` with :math:`rounding\_mode=None`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">div</span><span class="p">(</span><span class="n">dividend</span><span class="p">,</span> <span class="n">divisor</span><span class="p">,</span> <span class="n">rounding_mode</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></div>


<div class="viewcode-block" id="mul"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.mul.html#mindspore.ops.mul">[docs]</a><span class="k">def</span> <span class="nf">mul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multiplies two tensors element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = x_{i} * y_{i}</span>
<span class="sd">    .. note::</span>
<span class="sd">        - Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">        - The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">        - When the inputs are two tensors,</span>
<span class="sd">          dtypes of them cannot be bool at the same time, and the shapes of them can be broadcast.</span>
<span class="sd">        - When the inputs are one tensor and one scalar, the scalar could only be a constant.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[Tensor, number.Number, bool]): The first input is a number.Number or</span>
<span class="sd">            a bool or a tensor whose data type is</span>
<span class="sd">            `number &lt;https://www.mindspore.cn/docs/en/r2.0.0-alpha/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">            `bool_ &lt;https://www.mindspore.cn/docs/en/r2.0.0-alpha/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>
<span class="sd">        y (Union[Tensor, number.Number, bool]): The second input, when the first input is a Tensor,</span>
<span class="sd">            the second input should be a number.Number or bool value, or a Tensor whose data type is number or bool\_.</span>
<span class="sd">            When the first input is Scalar, the second input must be a Tensor whose data type is number or bool\_.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` and `y` is not one of the following: Tensor, number.Number, bool.</span>
<span class="sd">        ValueError: If `x` and `y` are not the same shape.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4.0, 5.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.mul(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 4. 10. 18.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_mul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="multiply"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.multiply.html#mindspore.ops.multiply">[docs]</a><span class="k">def</span> <span class="nf">multiply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Refer to :func:`mindspore.ops.mul` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_mul</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="div"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.div.html#mindspore.ops.div">[docs]</a><span class="k">def</span> <span class="nf">div</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="n">rounding_mode</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Divides the first input tensor by the second input tensor in floating-point type element-wise.</span>

<span class="sd">    Inputs of `input` and `other` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be bool at the same time, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar, the scalar could only be a constant.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = input_{i} / other_{i}</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, Number, bool]): The first input is a number or</span>
<span class="sd">            a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        other (Union[Tensor, Number, bool]): The second input is a number or</span>
<span class="sd">            a bool when the first input is a tensor or a tensor whose data type is number or bool.</span>
<span class="sd">        rounding_mode (str, optional): Type of rounding applied to the result. Three types are defined as,</span>

<span class="sd">            - None: Default behavior. Equivalent to true division in Python or `true_divide` in NumPy.</span>

<span class="sd">            - &quot;floor&quot;: Rounds the results of the division down. Equivalent to floor division in Python</span>
<span class="sd">              or `floor_divide` in NumPy.</span>

<span class="sd">            - &quot;trunc&quot;: Rounds the results of the division towards zero. Equivalent to C-style integer division.</span>
<span class="sd">              Default: None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` and `other` is not one of the following: Tensor, Number, bool.</span>
<span class="sd">        ValueError: If `rounding_mode` value is not None, &quot;floor&quot; or &quot;trunc&quot;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4.0, 5.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.div(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.25 0.4 0.5]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">rounding_mode</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">rounding_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;floor&#39;</span><span class="p">,</span> <span class="s1">&#39;trunc&#39;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For ops.div, rounding_mode value should be None, &#39;floor&#39; or &#39;trunc&#39;.&quot;</span><span class="p">)</span>

    <span class="n">output</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Div</span><span class="p">)()(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">rounding_mode</span> <span class="o">==</span> <span class="s1">&#39;floor&#39;</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Floor</span><span class="p">)()(</span><span class="n">output</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">rounding_mode</span> <span class="o">==</span> <span class="s1">&#39;trunc&#39;</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Trunc</span><span class="p">)()(</span><span class="n">output</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">output</span></div>


<div class="viewcode-block" id="divide"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.divide.html#mindspore.ops.divide">[docs]</a><span class="k">def</span> <span class="nf">divide</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">rounding_mode</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Alias for :func:`mindspore.ops.div` .</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">div</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="n">rounding_mode</span><span class="p">)</span></div>


<div class="viewcode-block" id="floor_div"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.floor_div.html#mindspore.ops.floor_div">[docs]</a><span class="k">def</span> <span class="nf">floor_div</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Divides the first input tensor by the second input tensor element-wise and round down to the closest integer.</span>

<span class="sd">    Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be bool at the same time, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = \\text{floor}( \\frac{x_i}{y_i})</span>

<span class="sd">    where the :math:`floor` indicates the Floor operator, for more details,</span>
<span class="sd">    please refer to the :class:`mindspore.ops.Floor` operator.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[Tensor, Number, bool]): The first input is a number or</span>
<span class="sd">            a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        y (Union[Tensor, Number, bool]): The second input is a number or</span>
<span class="sd">            a bool when the first input is a tensor, or it can be a tensor whose data type is number or bool.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `x` nor `y` is a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([2, 4, -1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([3, 3, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.floor_div(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0  1 -1]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_floordiv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="pow"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.pow.html#mindspore.ops.pow">[docs]</a><span class="k">def</span> <span class="nf">pow</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the `y` power of each element in `x`.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = x_{i} ^{ y_{i}}</span>

<span class="sd">    .. note::</span>
<span class="sd">        - Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">        - The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">        - When the inputs are two tensors,</span>
<span class="sd">          dtypes of them cannot be bool at the same time, and the shapes of them can be broadcast.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[Tensor, number.Number, bool]): The first input is a number.Number or</span>
<span class="sd">            a bool or a tensor whose data type is</span>
<span class="sd">            `number &lt;https://www.mindspore.cn/docs/en/r2.0.0-alpha/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">            `bool_ &lt;https://www.mindspore.cn/docs/en/r2.0.0-alpha/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>
<span class="sd">        y (Union[Tensor, number.Number, bool]): The second input, when the first input is a Tensor,</span>
<span class="sd">            the second input should be a number.Number or bool value, or a Tensor whose data type is number or bool\_.</span>
<span class="sd">            When the first input is Scalar, the second input must be a Tensor whose data type is number or bool\_.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` and `y` is not one of the following: Tensor, number.Number or bool.</span>
<span class="sd">        ValueError: If the shape of `x` and `y` are different.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = 3.0</span>
<span class="sd">        &gt;&gt;&gt; output = ops.pow(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1.  8. 64.]</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([2.0, 4.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.pow(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1. 16. 64.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_pow</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="floor_mod"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.floor_mod.html#mindspore.ops.floor_mod">[docs]</a><span class="k">def</span> <span class="nf">floor_mod</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the remainder of division element-wise. It&#39;s a flooring divide.</span>
<span class="sd">    E.g. :math:`floor(x / y) * y + mod(x, y) = x`.</span>

<span class="sd">    Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be both bool, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} =\text{floor}(x_{i} // y_{i})</span>

<span class="sd">    where the :math:`floor` indicates the Floor operator, for more details,</span>
<span class="sd">    please refer to the :class:`mindspore.ops.Floor` operator.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - Data of input `y` should not be 0, or the maximum value of its dtype will be returned.</span>
<span class="sd">        - When the elements of input exceeds 2048 , the accuracy of operator cannot guarantee the requirement of</span>
<span class="sd">          double thousandths in the mini form.</span>
<span class="sd">        - Due to different architectures, the calculation results of this operator on NPU and CPU may be inconsistent.</span>
<span class="sd">        - If shape is expressed as (D1,D2... ,Dn), then D1\*D2... \*DN&lt;=1000000,n&lt;=8.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[Tensor, Number, bool]): The first input is a number or</span>
<span class="sd">            a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        y (Union[Tensor, Number, bool]): The second input is a number or</span>
<span class="sd">            a bool when the first input is a tensor, or it can be a tensor whose data type is number or bool.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision of the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `x` nor `y` is a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([2, 4, -1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([3, 3, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.floor_mod(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2 1 2]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_mod</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="exp"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.exp.html#mindspore.ops.exp">[docs]</a><span class="k">def</span> <span class="nf">exp</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns exponential of a tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = e^{x_i}</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor, its rank must be in [0, 7] inclusive.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.exp(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 2.718282  7.389056 54.598152]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="expm1"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.expm1.html#mindspore.ops.expm1">[docs]</a><span class="k">def</span> <span class="nf">expm1</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns exponential then minus 1 of a tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = e^{x_i} - 1</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor with a dtype of float16 or float32, its rank must be in [0, 7] inclusive.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.0, 1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.expm1(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0.        1.718282  6.389056 53.598152]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_expm1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="log"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.log.html#mindspore.ops.log">[docs]</a><span class="k">def</span> <span class="nf">log</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the natural logarithm of a tensor element-wise.</span>

<span class="sd">    .. math::</span>
<span class="sd">        y_i = log_e(x_i)</span>

<span class="sd">    .. note::</span>
<span class="sd">        The dimension of the input Tensor on Ascend should be less than or equal to 8, and the dimension of the</span>
<span class="sd">        input Tensor on the CPU should be less than 8.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        If the input value of operator Log is within the range (0, 0.01] or [0.95, 1.05], the output accuracy may</span>
<span class="sd">        be affacted.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input Tensor of any dimension. The value must be greater than 0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64 on GPU and CPU.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16 or float32 on Ascend.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.log(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.        0.6931472 1.3862944]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">log_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="floor"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.floor.html#mindspore.ops.floor">[docs]</a><span class="k">def</span> <span class="nf">floor</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Rounds a tensor down to the closest integer element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \lfloor x_i \rfloor</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor, its rank must be in [0, 7] inclusive</span>
<span class="sd">            and data type must be float16, float32 or float64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not in [float16, float32, float64].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.1, 2.5, -1.5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.floor(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1.  2. -2.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_floor</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Floor</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">_floor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="i0"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.i0.html#mindspore.ops.i0">[docs]</a><span class="k">def</span> <span class="nf">i0</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Alias for :func:`mindspore.ops.bessel_i0` .</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">bessel_i0</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="inplace_update"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.inplace_update.html#mindspore.ops.inplace_update">[docs]</a><span class="k">def</span> <span class="nf">inplace_update</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates specified rows with values in `v`.</span>

<span class="sd">    Note:</span>
<span class="sd">            `indices` refers to the left-most dimension.</span>

<span class="sd">    Args:</span>
<span class="sd">        indices (Union[int, tuple], Tensor): Indices into the left-most dimension of `x`, and determines which rows of x</span>
<span class="sd">            to update with v. It is an int or tuple, whose value is in [0, the first dimension size of x). If the type</span>
<span class="sd">            is Tensor, it supports dynamic shape. Otherwise, it only supports static shape.</span>
<span class="sd">        x (Tensor): A tensor which to be inplace updated. It can be one of the following data types:</span>
<span class="sd">            float32, float16 and int32.</span>
<span class="sd">        v (Tensor): A tensor with the same type as `x` and the same dimension size as `x` except</span>
<span class="sd">            the first dimension, which must be the same as the size of `indices`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as the input `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `indices` is neither int nor tuple.</span>
<span class="sd">        TypeError: If `indices` is a tuple and its element is not an int.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; indices = (0, 1)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2], [3, 4], [5, 6]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; v = Tensor(np.array([[0.5, 1.0], [1.0, 1.5]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.inplace_update(x, v, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0.5 1. ]</span>
<span class="sd">         [1.  1.5]</span>
<span class="sd">         [5.  6. ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="n">inplace_update_inner</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">InplaceUpdate</span><span class="p">)(</span><span class="n">indices</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">inplace_update_inner</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">inplace_update_inner</span> <span class="o">=</span> <span class="n">InplaceUpdateV2</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">inplace_update_inner</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span></div>


<div class="viewcode-block" id="inplace_add"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.inplace_add.html#mindspore.ops.inplace_add">[docs]</a><span class="k">def</span> <span class="nf">inplace_add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adds `v` into specified rows of `x`. Computes `y` = `x`; y[i,] += `v`.</span>

<span class="sd">    Note:</span>
<span class="sd">            `indices` refers to the left-most dimension.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The first input is a tensor whose data type is float16, float32, float64 or int32.</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions, its rank should be less than 8.</span>
<span class="sd">        v (Tensor): The second input is a tensor that has the same dimension sizes as `x` except</span>
<span class="sd">            the first dimension, which must be the same as indices&#39; size. It has the same data type with `x`.</span>
<span class="sd">        indices (Union[int, tuple]): Indices into the left-most dimension of `x`, and determines which rows of `x`</span>
<span class="sd">            to add with `v`. It is an integer or a tuple, whose value is in [0, the first dimension size of `x`).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `indices` is neither int nor tuple.</span>
<span class="sd">        TypeError: If `indices` is a tuple whose elements are not all int.</span>
<span class="sd">        ValueError: If the rank of `x` is not equal to the rank of `v`.</span>
<span class="sd">        ValueError: If the length of `indices` is not equal to `v.shape[0]`.</span>
<span class="sd">        ValueError: If the values of `indices` are not in range of `[0, x.shape[0])`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; indices = (0, 1)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2], [3, 4], [5, 6]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_v = Tensor(np.array([[0.5, 1.0], [1.0, 1.5]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.inplace_add(x, input_v, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1.5 3. ]</span>
<span class="sd">         [4.  5.5]</span>
<span class="sd">         [5.  6. ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inplace_add_inner</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">InplaceAdd</span><span class="p">)(</span><span class="n">indices</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">inplace_add_inner</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">inplace_index_add</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adds tensor `updates` to specified axis and indices of tensor `var`. The axis should be in [0,  len(var.dim) - 1],</span>
<span class="sd">    and indices should be in [0, the size of `var` - 1] at the axis dimension.</span>

<span class="sd">    Args:</span>
<span class="sd">        var (Parameter): The input Parameter to add to, with data type uint8, int8, int16, int32,</span>
<span class="sd">            float16, float32, float64.</span>
<span class="sd">        indices (Tensor): Add the value of `var` and `updates` along the dimension of the `axis` according to the</span>
<span class="sd">            specified index value, with data type int32.</span>
<span class="sd">            The `indices` must be 1D with the same size as the size of `updates` in the `axis` dimension. The values</span>
<span class="sd">            of `indices` should be in [0, b), where the b is the size of `var` in the `axis` dimension.</span>
<span class="sd">        updates (Tensor): The input tensor with the value to add. Must have same data type as `var`.</span>
<span class="sd">            The shape must be the same as `var` except the `axis` th dimension.</span>
<span class="sd">        axis (int): The dimension along which to index.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as `var`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `var` is not a Parameter.</span>
<span class="sd">        TypeError: If neither `indices` nor `updates` is a Tensor.</span>
<span class="sd">        ValueError: If axis is out of `var` rank&#39;s range.</span>
<span class="sd">        ValueError: If `var` rank is not the same as `updates` rank.</span>
<span class="sd">        ValueError: If shape of `indices` is not 1D or size of `indices` is not equal to dimension of updates[axis].</span>
<span class="sd">        ValueError: If `updates`&#39;s shape is not the same as `var` except the `axis` th dimension.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; var = Parameter(np.array([[1, 2], [3, 4], [5, 6]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[0.5, 1.0], [1.0, 1.5]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; var = ops.inplace_index_add(var, indices, updates, axis=0)</span>
<span class="sd">        &gt;&gt;&gt; print(var)</span>
<span class="sd">        [[1.5 3. ]</span>
<span class="sd">         [4.  5.5]</span>
<span class="sd">         [5.  6. ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">inplace_index_add_</span> <span class="o">=</span> <span class="n">InplaceIndexAdd</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">inplace_index_add_</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span>


<div class="viewcode-block" id="inplace_sub"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.inplace_sub.html#mindspore.ops.inplace_sub">[docs]</a><span class="k">def</span> <span class="nf">inplace_sub</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Subtracts `v` into specified rows of `x`. Computes :math:`y = x`; :math:`y[i,] -= input\_v`.</span>

<span class="sd">    Note:</span>
<span class="sd">        `indices` refers to the left-most dimension.</span>

<span class="sd">    Args:</span>
<span class="sd">        indices (Union[int, tuple]): Indices into the left-most dimension of `x`, and determines which rows of `x`</span>
<span class="sd">            to subtract with `v`. It is an int or tuple, whose value is in [0, the first dimension size of `x`).</span>
<span class="sd">        x (Tensor): The first input is a tensor whose data type is float16, float32, float64 or int32.</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions, its rank should be less than 8.</span>
<span class="sd">        v (Tensor): The second input is a tensor who has the same dimension sizes as `x` except</span>
<span class="sd">            the first dimension, which must be the same as indices&#39; size. It has the same data type with `x`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `indices` is neither int nor tuple.</span>
<span class="sd">        TypeError: If `indices` is a tuple whose elements are not all int.</span>
<span class="sd">        ValueError: If the rank of `x` is not equal to the rank of `v`.</span>
<span class="sd">        ValueError: If the length of `indices` is not equal to `v.shape[0]`.</span>
<span class="sd">        ValueError: If the values of `indices` are not in range of `[0, x.shape[0])`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; indices = (0, 1)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2], [3, 4], [5, 6]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_v = Tensor(np.array([[0.5, 1.0], [1.0, 1.5]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.inplace_sub(x, input_v, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0.5 1. ]</span>
<span class="sd">         [2.  2.5]</span>
<span class="sd">         [5.  6. ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inplace_sub_inner</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">InplaceSub</span><span class="p">)(</span><span class="n">indices</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">inplace_sub_inner</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span></div>


<div class="viewcode-block" id="logical_not"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.logical_not.html#mindspore.ops.logical_not">[docs]</a><span class="k">def</span> <span class="nf">logical_not</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the &quot;logical NOT&quot; of a tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = \\neg x_{i}</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor whose dtype is bool.</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means,any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the `x`, and the dtype is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not a bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([True, False, True]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.logical_not(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False  True False]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">logical_not_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="logical_or"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.logical_or.html#mindspore.ops.logical_or">[docs]</a><span class="k">def</span> <span class="nf">logical_or</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the &quot;logical OR&quot; of two tensors element-wise.</span>

<span class="sd">    Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one bool.</span>
<span class="sd">    When the inputs are two tensors, the shapes of them could be broadcast,</span>
<span class="sd">    and the data types of them must be bool.</span>
<span class="sd">    When the inputs are one tensor and one bool, the bool object could only be a constant,</span>
<span class="sd">    and the data type of the tensor must be bool.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = x_{i} \\vee y_{i}</span>

<span class="sd">    Note:</span>
<span class="sd">        LogicalOr supports broadcasting.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[Tensor, bool]): The first input is a bool or a tensor whose data type is bool.</span>
<span class="sd">        y (Union[Tensor, bool]): The second input is a bool when the first input is a tensor or</span>
<span class="sd">            a tensor whose data type is bool.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `x` nor `y` is a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([True, False, True]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([True, True, False]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.logical_or(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True  True  True]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">logical_or_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="logical_and"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.logical_and.html#mindspore.ops.logical_and">[docs]</a><span class="k">def</span> <span class="nf">logical_and</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the &quot;logical AND&quot; of two tensors element-wise.</span>

<span class="sd">    Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one bool.</span>
<span class="sd">    When the inputs are two tensors, the shapes of them could be broadcast,</span>
<span class="sd">    and the data types of them must be bool.</span>
<span class="sd">    When the inputs are one tensor and one bool, the bool object could only be a constant,</span>
<span class="sd">    and the data type of the tensor must be bool.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = x_{i} \wedge y_{i}</span>

<span class="sd">    Note:</span>
<span class="sd">        LogicalAnd supports broadcasting.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[Tensor, bool]): The first input is a bool or a tensor whose data type is bool.</span>
<span class="sd">        y (Union[Tensor, bool]): The second input is a bool when the first input is a tensor or</span>
<span class="sd">            a tensor whose data type is bool.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `x` nor `y` is a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([True, False, True]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([True, True, False]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.logical_and(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True False False]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">logical_and_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="sin"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.sin.html#mindspore.ops.sin">[docs]</a><span class="k">def</span> <span class="nf">sin</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes sine of the input element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = sin(x_i)</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The shape of tensor is</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64, complex64, complex128.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.62, 0.28, 0.43, 0.62]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.sin(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.5810352 0.27635565 0.41687083 0.5810352]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">sin_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">sinc</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the normalized sinc of input.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = sinc(x_i)</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as the `x`. The dtype of output is float32 when dtype of `x` is in</span>
<span class="sd">        [uint8, uint8, uint16, int16, uint32, int32, uint64, int64, bool]. Otherwise output has the</span>
<span class="sd">        same dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.62, 0.28, 0.43, 0.62]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.sinc(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.47735003 0.8759357  0.7224278  0.47735003]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">sinc_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<div class="viewcode-block" id="cos"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.cos.html#mindspore.ops.cos">[docs]</a><span class="k">def</span> <span class="nf">cos</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes cosine of input element-wise.</span>

<span class="sd">    .. math::</span>
<span class="sd">        out_i = cos(x_i)</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Supported dtypes are float16 and float32, and using float64 may</span>
<span class="sd">        cause a problem of missing precision.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The shape of tensor is</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64, complex64, complex128.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.24, 0.83, 0.31, 0.09]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.cos(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.971338 0.6748758 0.95233357 0.9959527]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">cos_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="tan"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.tan.html#mindspore.ops.tan">[docs]</a><span class="k">def</span> <span class="nf">tan</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes tangent of `x` element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = tan(x_i)</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input Tensor, valid for any dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-1.0, 0.0, 1.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.tan(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-1.5574081 0. 1.5574081]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tan_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="xlogy"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.xlogy.html#mindspore.ops.xlogy">[docs]</a><span class="k">def</span> <span class="nf">xlogy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the first input tensor multiplied by the logarithm of second input tensor element-wise.</span>
<span class="sd">    Returns zero when `x` is zero.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = x_{i}\ln{y_{i}}</span>

<span class="sd">    Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be bool at the same time, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - On Ascend, the data type of `x` and `y` must be float16 or float32.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[Tensor, number.Number, bool]): The first input is a number.Number or</span>
<span class="sd">            a bool or a tensor whose data type is</span>
<span class="sd">            `number &lt;https://www.mindspore.cn/docs/en/r2.0.0-alpha/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">            `bool_ &lt;https://www.mindspore.cn/docs/en/r2.0.0-alpha/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>
<span class="sd">        y (Union[Tensor, number.Number, bool]): The second input is a number.Number or</span>
<span class="sd">            a bool when the first input is a tensor or a tensor whose data type is number or bool\_.</span>
<span class="sd">            When the first input is Scalar, the second input must be a Tensor whose data type is number or bool\_.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` and `y` is not a number.Number or a bool or a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` and &#39;y&#39; is not in [float16, float32, float64, complex64, complex128].</span>
<span class="sd">        ValueError: If `x` could not be broadcast to a tensor with shape of `y`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-5, 0, 4]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([2, 2, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.xlogy(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-3.465736   0.        2.7725887]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">xlogy_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="arccosh"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.arccosh.html#mindspore.ops.arccosh">[docs]</a><span class="k">def</span> <span class="nf">arccosh</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    For details, please refer to :func:`mindspore.ops.acosh`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">acosh_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="arcsin"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.arcsin.html#mindspore.ops.arcsin">[docs]</a><span class="k">def</span> <span class="nf">arcsin</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    For details, please refer to :func:`mindspore.ops.asin`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">asin_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="arctan"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.arctan.html#mindspore.ops.arctan">[docs]</a><span class="k">def</span> <span class="nf">arctan</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    For details, please refer to :func:`mindspore.ops.atan`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">atan_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="arctan2"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.arctan2.html#mindspore.ops.arctan2">[docs]</a><span class="k">def</span> <span class="nf">arctan2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    For details, please refer to :func:`mindspore.ops.atan2`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_atan2</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Atan2</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">_atan2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="asin"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.asin.html#mindspore.ops.asin">[docs]</a><span class="k">def</span> <span class="nf">asin</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes arcsine of input tensors element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = sin^{-1}(x_i)</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The shape of tensor is</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32, float64, complex64, complex128.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.74, 0.04, 0.30, 0.56]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.asin(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.8330704  0.04001067 0.30469266 0.5943858 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">asin_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="acos"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.acos.html#mindspore.ops.acos">[docs]</a><span class="k">def</span> <span class="nf">acos</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes arccosine of input tensors element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = cos^{-1}(x_i)</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The shape of tensor is</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64, complex64, complex128.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.74, 0.04, 0.30, 0.56]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.acos(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.737726  1.5307857 1.2661036 0.9764105]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">acos_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="arccos"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.arccos.html#mindspore.ops.arccos">[docs]</a><span class="k">def</span> <span class="nf">arccos</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Alias for :func:`mindspore.ops.acos` .</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">acos</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="atan"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.atan.html#mindspore.ops.atan">[docs]</a><span class="k">def</span> <span class="nf">atan</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the trigonometric inverse tangent of the input element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = tan^{-1}(x_i)</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The shape of tensor is</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">            The data type should be one of the following types: float16, float32.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A Tensor, has the same type as the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16 or float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 0.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.atan(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.7853982 0.       ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">atan_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="sinh"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.sinh.html#mindspore.ops.sinh">[docs]</a><span class="k">def</span> <span class="nf">sinh</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes hyperbolic sine of the input element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \sinh(x_i)</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor of hyperbolic sine function, its rank must be in [0, 7] inclusive.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.62, 0.28, 0.43, 0.62]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.sinh(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.6604918  0.28367308 0.44337422 0.6604918 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">sinh_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="cosh"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.cosh.html#mindspore.ops.cosh">[docs]</a><span class="k">def</span> <span class="nf">cosh</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes hyperbolic cosine of input element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \cosh(x_i)</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor of hyperbolic cosine function, its rank must be in [0, 7] inclusive</span>
<span class="sd">            and data type must be float16, float32, float64, complex64 or complex128.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the dtype of `x` is not one of the following types:</span>
<span class="sd">                   float16, float32, float64, complex64, complex128.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.24, 0.83, 0.31, 0.09]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.cosh(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.0289385 1.364684 1.048436 1.0040528]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">cosh_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="tanh"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.tanh.html#mindspore.ops.tanh">[docs]</a><span class="k">def</span> <span class="nf">tanh</span><span class="p">(</span><span class="n">input_x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes hyperbolic tangent of input element-wise. The Tanh function is defined as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        tanh(x_i) = \frac{\exp(x_i) - \exp(-x_i)}{\exp(x_i) + \exp(-x_i)} = \frac{\exp(2x_i) - 1}{\exp(2x_i) + 1},</span>

<span class="sd">    where :math:`x_i` is an element of the input Tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Tensor of shape :math:`(N, *)`, where :math:`*` means, any number of</span>
<span class="sd">            additional dimensions, with float16 or float32 data type.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as the `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `input_x` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``  ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3, 4, 5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.tanh(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.7615941 0.9640276 0.9950547 0.9993293 0.9999092]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tanh_</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="asinh"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.asinh.html#mindspore.ops.asinh">[docs]</a><span class="k">def</span> <span class="nf">asinh</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes inverse hyperbolic sine of the input element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \sinh^{-1}(input_i)</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor of inverse hyperbolic sine function, its rank must be in [0, 7] inclusive.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-5.0, 1.5, 3.0, 100.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.asinh(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-2.3124382  1.1947632  1.8184465  5.298342 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">asinh_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="acosh"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.acosh.html#mindspore.ops.acosh">[docs]</a><span class="k">def</span> <span class="nf">acosh</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes inverse hyperbolic cosine of the inputs element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \cosh^{-1}(input_i)</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Given an input tensor x, the function computes inverse hyperbolic cosine of every element.</span>
<span class="sd">        Input range is [1, inf].</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor of inverse hyperbolic cosine function, its rank must be in [0, 7] inclusive.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 1.5, 3.0, 100.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.acosh(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.        0.9624237 1.7627472 5.298292 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">acosh_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="atanh"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.atanh.html#mindspore.ops.atanh">[docs]</a><span class="k">def</span> <span class="nf">atanh</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes inverse hyperbolic tangent of the input element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \tanh^{-1}(x_{i})</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental prototype that is subject to change and/or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The shape of tensor is</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">            The data type should be one of the following types: float16, float32.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A Tensor, has the same type as the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16 or float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0, -0.5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.atanh(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0.         -0.54930615]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">atanh_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="atan2"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.atan2.html#mindspore.ops.atan2">[docs]</a><span class="k">def</span> <span class="nf">atan2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns arctangent of x/y element-wise.</span>

<span class="sd">    It returns :math:`\theta\ \in\ [-\pi, \pi]`</span>
<span class="sd">    such that :math:`x = r*\sin(\theta), y = r*\cos(\theta)`, where :math:`r = \sqrt{x^2 + y^2}`.</span>

<span class="sd">    Args of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower precision data type will be converted to</span>
<span class="sd">    the relatively highest precision data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor.</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">            The data type should be one of the following types: float16, float32, float64</span>
<span class="sd">        y (Tensor): The input tensor. It has the same shape with `x`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,and the data type is same as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` or `y` is not a Tensor.</span>
<span class="sd">        RuntimeError: If the data type of `x` and `y` conversion of Parameter is required</span>
<span class="sd">                      when data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([1, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.atan2(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.        0.7853982]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_atan2</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Atan2</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">_atan2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="bitwise_and"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bitwise_and.html#mindspore.ops.bitwise_and">[docs]</a><span class="k">def</span> <span class="nf">bitwise_and</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns bitwise `and` of two tensors element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = x_{i} \wedge y_{i}</span>

<span class="sd">    Args of `x` and `y` comply with the implicit type conversion rules to</span>
<span class="sd">    make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The first input tensor with shape :math:`(N,*)` where :math:`*` means</span>
<span class="sd">            any number of additional dimensions. The supported data types are:</span>
<span class="sd">            int8, uint8, int16, uint16, int32, uint32, int64 and uint64.</span>
<span class="sd">        y (Tensor): The second input tensor with the same dtype as `x`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same type as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` or `y` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0, 0, 1, -1, 1, 1, 1]), mindspore.int16)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([0, 1, 1, -1, -1, 2, 3]), mindspore.int16)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bitwise_and(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0  0  1 -1  1  0  1]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">bitwise_and_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="bitwise_or"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bitwise_or.html#mindspore.ops.bitwise_or">[docs]</a><span class="k">def</span> <span class="nf">bitwise_or</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns bitwise `or` of two tensors element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = x_{i} \mid y_{i}</span>

<span class="sd">    Args of `x` and `y` comply with the implicit type conversion rules to</span>
<span class="sd">    make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The first input tensor with shape :math:`(N,*)` where :math:`*` means</span>
<span class="sd">            any number of additional dimensions. The supported data types are:</span>
<span class="sd">            int8, uint8, int16, uint16, int32, uint32, int64 and uint64.</span>
<span class="sd">        y (Tensor): The second input tensor with the same dtype as `x`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same type as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` or `y` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0, 0, 1, -1, 1, 1, 1]), mindspore.int16)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([0, 1, 1, -1, -1, 2, 3]), mindspore.int16)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bitwise_or(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0  1  1 -1 -1  3  3]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">bitwise_or_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="bitwise_xor"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bitwise_xor.html#mindspore.ops.bitwise_xor">[docs]</a><span class="k">def</span> <span class="nf">bitwise_xor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns bitwise `xor` of two tensors element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = x_{i} \oplus y_{i}</span>

<span class="sd">    Args of `x` and `y` comply with the implicit type conversion rules to</span>
<span class="sd">    make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The first input tensor with shape :math:`(N,*)` where :math:`*` means</span>
<span class="sd">            any number of additional dimensions. The supported data types are:</span>
<span class="sd">            int8, uint8, int16, uint16, int32, uint32, int64 and uint64.</span>
<span class="sd">        y (Tensor): The second input tensor with the same dtype as `x`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same type as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` or `y` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0, 0, 1, -1, 1, 1, 1]), mindspore.int16)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([0, 1, 1, -1, -1, 2, 3]), mindspore.int16)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bitwise_xor(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0  1  0  0 -2  3  2]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">bitwise_xor_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="inv"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.inv.html#mindspore.ops.inv">[docs]</a><span class="k">def</span> <span class="nf">inv</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes Reciprocal of input tensor element-wise.</span>

<span class="sd">    .. math::</span>
<span class="sd">        out_i = \frac{1}{x_{i} }</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Tensor of any dimension. Must be one of the following types: float16, float32 or int32.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same type and shape as input shape value.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not one of float16, float32, int32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.25, 0.4, 0.31, 0.52]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.inv(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [4.        2.5       3.2258065 1.923077 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">inv_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="invert"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.invert.html#mindspore.ops.invert">[docs]</a><span class="k">def</span> <span class="nf">invert</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Flips all bits of input tensor element-wise.</span>

<span class="sd">    .. math::</span>
<span class="sd">        out_i = \sim x_{i}</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">            The data type should be one of the following types: int16, uint16.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x` is neither int16 nor uint16.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([25, 4, 13, 9]), mindspore.int16)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.invert(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-26 -5 -14 -10]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">invert_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="erf"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.erf.html#mindspore.ops.erf">[docs]</a><span class="k">def</span> <span class="nf">erf</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Gauss error function of `x` element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        erf(x)=\frac{2} {\sqrt{\pi}} \int\limits_0^{x} e^{-t^{2}} dt</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor of Gaussian error function. Its rank must be in [0, 7] inclusive</span>
<span class="sd">            and data type must be float16 float32 or float64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-1, 0, 1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.erf(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-0.8427168   0.          0.8427168   0.99530876  0.99997765]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">erf_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="erfc"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.erfc.html#mindspore.ops.erfc">[docs]</a><span class="k">def</span> <span class="nf">erfc</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the complementary error function of `x` element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        erfc(x) = 1 - \frac{2} {\sqrt{\pi}} \int\limits_0^{x} e^{-t^{2}} dt</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor with a dtype of float16, float32 or float64,</span>
<span class="sd">            its rank should be in [0, 7] inclusive.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-1, 0, 1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.erfc(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.8427168e+00 1.0000000e+00 1.5728319e-01 4.6912432e-03 2.2351742e-05]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">erfc_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="bessel_j0"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bessel_j0.html#mindspore.ops.bessel_j0">[docs]</a><span class="k">def</span> <span class="nf">bessel_j0</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Bessel j0 function of x element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor. The data type must be float16, float32 or float64.</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.5, 1., 2., 4.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bessel_j0(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.93846981  0.76519769  0.22389078  -0.39714981]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">bessel_j0_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="bessel_j1"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bessel_j1.html#mindspore.ops.bessel_j1">[docs]</a><span class="k">def</span> <span class="nf">bessel_j1</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Bessel j1 function of x element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor. The data type must be float16, float32 or float64.</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.5, 1., 2., 4.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bessel_j1(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.24226846  0.44005059  0.57672481 -0.06604333]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">bessel_j1_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="bessel_i0"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bessel_i0.html#mindspore.ops.bessel_i0">[docs]</a><span class="k">def</span> <span class="nf">bessel_i0</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Bessel i0 function of x element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor. The data type must be float16, float32 or float64.</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-1, -0.5, 0.5, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bessel_i0(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.26606588  1.06348337  1.06348337  1.26606588]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">bessel_i0_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="bessel_i0e"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bessel_i0e.html#mindspore.ops.bessel_i0e">[docs]</a><span class="k">def</span> <span class="nf">bessel_i0e</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Bessel i0e function of x element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor. The data type must be float16, float32 or float64.</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-1, -0.5, 0.5, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bessel_i0e(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.46575961  0.64503527  0.64503527  0.46575961]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">bessel_i0e_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="bessel_k0"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bessel_k0.html#mindspore.ops.bessel_k0">[docs]</a><span class="k">def</span> <span class="nf">bessel_k0</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Bessel k0 function of x element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor. The data type must be float16, float32 or float64.</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.5, 1., 2., 4.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bessel_k0(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.92441907  0.42102444  0.11389387  0.01115968]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">bessel_k0_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="bessel_k0e"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bessel_k0e.html#mindspore.ops.bessel_k0e">[docs]</a><span class="k">def</span> <span class="nf">bessel_k0e</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Bessel k0e function of x element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor. The data type must be float16, float32 or float64.</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.5, 1., 2., 4.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bessel_k0e(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.52410939  1.14446308  0.84156822  0.60929767]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">bessel_k0e_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="bessel_y0"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bessel_y0.html#mindspore.ops.bessel_y0">[docs]</a><span class="k">def</span> <span class="nf">bessel_y0</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Bessel y0 function of x element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor. The data type must be float16, float32 or float64.</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.5, 1., 2., 4.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bessel_y0(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-0.44451874  0.08825696  0.51037567  -0.01694074]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">bessel_y0_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="bessel_y1"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bessel_y1.html#mindspore.ops.bessel_y1">[docs]</a><span class="k">def</span> <span class="nf">bessel_y1</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Bessel y1 function of x element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor. The data type must be float16, float32 or float64.</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.5, 1., 2., 4.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bessel_y1(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-1.47147239  -0.78121282  -0.10703243  0.39792571]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">bessel_y1_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="linspace"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.linspace.html#mindspore.ops.linspace">[docs]</a><span class="k">def</span> <span class="nf">linspace</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">stop</span><span class="p">,</span> <span class="n">num</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a Tensor whose value is `num` evenly spaced in the interval `start` and `stop` (including `start` and</span>
<span class="sd">    `stop`), and the length of the output Tensor is `num`.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{aligned}</span>
<span class="sd">        &amp;step = (stop - start)/(num - 1)\\</span>
<span class="sd">        &amp;output = [start, start+step, start+2*step, ... , stop]</span>
<span class="sd">        \end{aligned}</span>

<span class="sd">    Args:</span>
<span class="sd">        start (Tensor): Start value of interval. The tensor data type must be float32 or float64 and with shape of 0-D.</span>
<span class="sd">        stop (Tensor): Last value of interval. The tensor data type must be float32 or float64 and with shape of 0-D.</span>
<span class="sd">        num (Union[Tensor, int]): Number of ticks in the interval, inclusive of start and stop.</span>
<span class="sd">            Must be positive int number or 0D int32/int64 Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype as `start`, and the shape of :math:`(num)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `start` or `stop` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `start` or dtype of `stop` is not float32 or float64.</span>
<span class="sd">        ValueError: If shape of `start` or shape of `stop` is not 0-D.</span>
<span class="sd">        TypeError: If `num` is not int or 0D int32/int64 Tensor.</span>
<span class="sd">        ValueError: If `num` is not positive int number.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; start = Tensor(1, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; stop = Tensor(10, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; num = 5</span>
<span class="sd">        &gt;&gt;&gt; output = ops.linspace(start, stop, num)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1.    3.25  5.5   7.75 10.  ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">linspace_</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">stop</span><span class="p">,</span> <span class="n">num</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">matrix_determinant</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the determinant of one or more square matrices.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): A matrix to be calculated, its shape should be :math:`[..., M, M]` who must</span>
<span class="sd">          have at least two dimensions, and the last two</span>
<span class="sd">          dimensions must be the same size. Data type must be float32, float64, complex64 or complex128.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor. The shape is :math:`x.shape[:-2]`, and the dtype is same as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` not float32, float64, complex64 or complex128.</span>
<span class="sd">        ValueError: If the last two dimensions of `x` is not same size.</span>
<span class="sd">        ValueError: If the dimension of `x` is less than 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[-4.5, -1.5], [7.0, 6.0]], [[2.5, 0.5], [3.0, 9.0]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.matrix_determinant(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-16.5 21. ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">matrix_determinant_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<div class="viewcode-block" id="matrix_exp"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.matrix_exp.html#mindspore.ops.matrix_exp">[docs]</a><span class="k">def</span> <span class="nf">matrix_exp</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the matrix exponential of a square matrix. Supports batched inputs.</span>

<span class="sd">    .. math::</span>

<span class="sd">        matrix\_exp(x) = \sum_{k=0}^{\infty} \frac{1}{k !} x^{k} \in \mathbb{K}^{n \times n}</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The shape of tensor is :math:`(*, n, n)` where * is zero or more batch dimensions.</span>
<span class="sd">          Must be one of the following types: float16, float32, float64, complex64, complex128.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If the dtype of `x` is not one of the following dtype:</span>
<span class="sd">                   float16, float32, float64, complex64, complex128.</span>
<span class="sd">        ValueError: If the rank of `x` is less than 2.</span>
<span class="sd">        ValueError: If the last two dimensions of `x` are not equal.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2], [0, 1]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.matrix_exp(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[2.7182817 5.436563 ]</span>
<span class="sd">        [0.        2.7182817]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">matrix_exp_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">log_matrix_determinant</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the sign and the log of the absolute value of the determinant of one or more square matrices.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): A matrix to be calculated, its shape is :math:`[..., M, M]`.</span>
<span class="sd">          The matrix must be at least two dimensions, and the last two</span>
<span class="sd">          dimensions must be the same size. Data type must be float32, float64, complex64 or complex128.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor. The signs of the log determinants. The shape is :math:`x.shape[:-2]`,</span>
<span class="sd">        and the dtype is same as `x`.</span>
<span class="sd">        Tensor. The absolute values of the log determinants. The shape is :math:`x.shape[:-2]`, and</span>
<span class="sd">        the dtype is same as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` not float32, float64, complex64 or complex128.</span>
<span class="sd">        ValueError: If the last two dimensions of `x` is not same size.</span>
<span class="sd">        ValueError: If the dimension of `x` is less than 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[-4.5, -1.5], [7.0, 6.0]], [[2.5, 0.5], [3.0, 9.0]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; sign, output = ops.log_matrix_determinant(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(sign)</span>
<span class="sd">        [-1.   1.]</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2.80336046e+00    3.04452229e+00]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">log_matrix_determinant_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<div class="viewcode-block" id="matrix_solve"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.matrix_solve.html#mindspore.ops.matrix_solve">[docs]</a><span class="k">def</span> <span class="nf">matrix_solve</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="n">adjoint</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-outer-name</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Solves systems of linear equations.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{aligned}</span>
<span class="sd">        &amp;matrix[..., M, M] * x[..., M, K] = rhs[..., M, K] \\</span>
<span class="sd">        &amp;adjoint(matrix[..., M, M]) * x[..., M, K] = rhs[..., M, K]</span>
<span class="sd">        \end{aligned}</span>

<span class="sd">    .. warning::</span>
<span class="sd">        On GPU, if the matrix is irreversible, an error may be reported or an unknown result may be returned.</span>

<span class="sd">    Args:</span>
<span class="sd">        matrix (Tensor): The shape of tensor is :math:`[..., M, M]`.</span>
<span class="sd">        rhs (Tensor): The shape of tensor is :math:`[..., M, K]`. `rhs` must have the same dtype as `matrix`.</span>
<span class="sd">        adjoint(bool): Indicating whether to solve with matrix or its (block-wise) adjoint. Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        x (Tensor), The dtype and shape is the same as &#39;rhs&#39;.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If adjoint is not the type of bool.</span>
<span class="sd">        TypeError: If the type of matrix is not one of the following dtype:</span>
<span class="sd">                   mstype.float16, mstype.float32, mstype.float64, mstype.complex64, mstype.complex128.</span>
<span class="sd">        TypeError: If the type of `matrix` is not the same as that of `rhs`.</span>
<span class="sd">        ValueError: If the rank of `matrix` less than 2.</span>
<span class="sd">        ValueError: If the dimension of `matrix` is not the same as `rhs`.</span>
<span class="sd">        ValueError: If the inner-most 2 dimension of `matrix` is not the same.</span>
<span class="sd">        ValueError: If the inner-most 2 dimension of `rhs` does not match `matrix`.</span>
<span class="sd">        ValueError: If the `matrix` is irreversible.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; matrix = Tensor([[5, 4], [3, 1]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; rhs = Tensor([[7], [2]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; result = ops.matrix_solve(matrix, rhs)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [[0.14285707]</span>
<span class="sd">         [1.5714287 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">matrix_solve_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">MatrixSolve</span><span class="p">)(</span><span class="n">adjoint</span><span class="o">=</span><span class="n">adjoint</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">matrix_solve_</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">)</span></div>


<div class="viewcode-block" id="truncate_div"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.truncate_div.html#mindspore.ops.truncate_div">[docs]</a><span class="k">def</span> <span class="nf">truncate_div</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Divides the first input tensor by the second input tensor element-wise for integer types, negative numbers will</span>
<span class="sd">    round fractional quantities towards zero.</span>

<span class="sd">    Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be bool at the same time, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    Note:</span>
<span class="sd">        Broadcasting is supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x(Union[Tensor, Number, bool]): The first input is a number, or a bool,</span>
<span class="sd">            or a tensor whose data type is number or bool.</span>
<span class="sd">        y(Union[Tensor, Number, bool]): The second input is a number, or a bool when the first input</span>
<span class="sd">            is a tensor, or a tensor whose data type is number or bool.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` and `y` is not one of the following: Tensor, Number, bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([2, 4, -1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([3, 3, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.truncate_div(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0 1 0]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">truncate_div_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="truncate_mod"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.truncate_mod.html#mindspore.ops.truncate_mod">[docs]</a><span class="k">def</span> <span class="nf">truncate_mod</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the remainder of division element-wise.</span>

<span class="sd">    Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be bool at the same time, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - The input data does not support 0.</span>
<span class="sd">        - When the elements of input exceed 2048 , the accuracy of operator cannot guarantee the requirement of</span>
<span class="sd">          double thousandths in the mini form.</span>
<span class="sd">        - Due to different architectures, the calculation results of this operator on NPU and CPU may be inconsistent.</span>
<span class="sd">        - If shape is expressed as (D1,D2... ,Dn), then D1\*D2... \*DN&lt;=1000000,n&lt;=8.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[Tensor, numbers.Number, bool]): The first input is a number, or a bool,</span>
<span class="sd">            or a tensor whose data type is number or bool.</span>
<span class="sd">        y (Union[Tensor, numbers.Number, bool]): The second input is a number, or a bool when the first input</span>
<span class="sd">            is a tensor, or a tensor whose data type is number or bool.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `x` nor `y` is one of the following: Tensor, number, bool.</span>
<span class="sd">        TypeError: If neither `x` nor `y` is a Tensor.</span>
<span class="sd">        ValueError: If the shape `x` and `y` cannot be broadcasted to each other.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([2, 4, -1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([3, 3, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.truncate_mod(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 2  1 -1]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">truncate_mod_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="trunc"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.trunc.html#mindspore.ops.trunc">[docs]</a><span class="k">def</span> <span class="nf">trunc</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a new tensor with the truncated integer values of the elements of the input tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the same shape and data type as the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([3.4742, 0.5466, -0.8008, -3.9079]),mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.trunc(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [3. 0. 0. -3.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Trunc</span><span class="p">)()(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="ldexp"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ldexp.html#mindspore.ops.ldexp">[docs]</a><span class="k">def</span> <span class="nf">ldexp</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multiplies input by :math:`2^{other}` .</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = x_{i} * ( 2_{i} ^{other} )</span>

<span class="sd">    Note:</span>
<span class="sd">        Typically this function can create floating point numbers</span>
<span class="sd">        by multiplying mantissas in input with powers of intger 2</span>
<span class="sd">        from the exponents in `other`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor.</span>
<span class="sd">        other (Tensor): A tensor of exponents, typically integers.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the output tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `other` is not a Tensor.</span>
<span class="sd">        ValueError: If the size of tensor `x` and `other` are different at non-singleton dimension, and not equal to 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor(np.array([1, 2, 3, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; out = ops.ldexp(x, other)</span>
<span class="sd">        &gt;&gt;&gt; print(out)</span>
<span class="sd">        [ 2.  4.  8. 16.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">pow_ops</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Pow</span><span class="p">)()</span>
    <span class="n">mul_ops</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Mul</span><span class="p">)()</span>

    <span class="n">out</span> <span class="o">=</span> <span class="n">mul_ops</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pow_ops</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">other</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="logit"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.logit.html#mindspore.ops.logit">[docs]</a><span class="k">def</span> <span class="nf">logit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the logit of a tensor element-wise. When eps is not None, element in `x` is clamped to [eps, 1-eps].</span>
<span class="sd">    When eps is None, input `x` is not clamped.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{align}</span>
<span class="sd">        y_{i} &amp; = \ln(\frac{z_{i}}{1 - z_{i}}) \\</span>
<span class="sd">        z_{i} &amp; = \begin{cases}</span>
<span class="sd">        x_{i} &amp; \text{if eps is None} \\</span>
<span class="sd">        \text{eps} &amp; \text{if } x_{i} \lt \text{eps} \\</span>
<span class="sd">        x_{i} &amp; \text{if } \text{eps} \leq x_{i} \leq 1 - \text{eps} \\</span>
<span class="sd">        1 - \text{eps} &amp; \text{if } x_{i} \gt 1 - \text{eps}</span>
<span class="sd">        \end{cases}</span>
<span class="sd">        \end{align}</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor.</span>
<span class="sd">        eps (float, optional): The epsilon. The input clamp bound is defined as [eps, 1-eps]. Default: None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `eps` is not a float.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.1, 0.2, 0.3]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.logit(x, eps=1e-5)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-2.1972246 -1.3862944 -0.8472978]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">eps</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.0</span>
    <span class="n">logit_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Logit</span><span class="p">)(</span><span class="n">eps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logit_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<span class="c1">#####################################</span>
<span class="c1"># Comparison Operation Functions.</span>
<span class="c1">#####################################</span>


<div class="viewcode-block" id="less"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.less.html#mindspore.ops.less">[docs]</a><span class="k">def</span> <span class="nf">less</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the boolean value of :math:`x &lt; y` element-wise.</span>

<span class="sd">    Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be bool at the same time, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} =\begin{cases}</span>
<span class="sd">            &amp; \text{True,    if } x_{i}&lt;y_{i} \\</span>
<span class="sd">            &amp; \text{False,   if } x_{i}&gt;=y_{i}</span>
<span class="sd">            \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[Tensor, Number, bool]): The first input is a number or</span>
<span class="sd">            a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        y (Union[Tensor, Number, bool]): The second input is a number or</span>
<span class="sd">            a bool when the first input is a tensor, or it can be a tensor whose data type is number or bool.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,and the data type is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` and `y` is not one of the following: Tensor, Number, bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([1, 1, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.less(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False False True]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_lt</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="le"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.le.html#mindspore.ops.le">[docs]</a><span class="k">def</span> <span class="nf">le</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the boolean value of :math:`x &lt;= y` element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} =\begin{cases}</span>
<span class="sd">            &amp; \text{True,    if } x_{i}&lt;=y_{i} \\</span>
<span class="sd">            &amp; \text{False,   if } x_{i}&gt;y_{i}</span>
<span class="sd">            \end{cases}</span>

<span class="sd">    .. note::</span>
<span class="sd">        - Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">        - The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">        - When the inputs are two tensors,</span>
<span class="sd">          dtypes of them cannot be both bool , and the shapes of them can be broadcast.</span>
<span class="sd">        - When the inputs are one tensor and one scalar, the scalar could only be a constant.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[Tensor, number.Number, bool]): The first input is a number.Number or</span>
<span class="sd">            a bool or a tensor whose data type is</span>
<span class="sd">            `number &lt;https://www.mindspore.cn/docs/en/r2.0.0-alpha/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">            `bool_ &lt;https://www.mindspore.cn/docs/en/r2.0.0-alpha/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>
<span class="sd">        y (Union[Tensor, number.Number, bool]): The second input, when the first input is a Tensor,</span>
<span class="sd">            the second input should be a number.Number or bool value, or a Tensor whose data type is number or bool\_.</span>
<span class="sd">            When the first input is Scalar, the second input must be a Tensor whose data type is number or bool\_.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `x` nor `y` is a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([1, 1, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.le(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True False  True]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_le</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="gt"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.gt.html#mindspore.ops.gt">[docs]</a><span class="k">def</span> <span class="nf">gt</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compare the value of the input parameters :math:`x,y` element-wise, and the output result is a bool value.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} =\begin{cases}</span>
<span class="sd">            &amp; \text{True,    if } x_{i}&gt;y_{i} \\</span>
<span class="sd">            &amp; \text{False,   if } x_{i}&lt;=y_{i}</span>
<span class="sd">            \end{cases}</span>

<span class="sd">    Note:</span>
<span class="sd">        - Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">        - The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">        - When the inputs are two tensors, dtypes of them cannot be bool at the same time,</span>
<span class="sd">          and the shapes of them can be broadcast.</span>
<span class="sd">        - When the inputs are one tensor and one scalar, the scalar could only be a constant.</span>
<span class="sd">        - Broadcasting is supported.</span>
<span class="sd">        - If the input Tensor can be broadcast, the low dimension will be extended to the corresponding high dimension</span>
<span class="sd">          in another input by copying the value of the dimension.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[Tensor, number.Number, bool]): The first input is a number.Number or</span>
<span class="sd">            a bool or a tensor whose data type is</span>
<span class="sd">            `number &lt;https://www.mindspore.cn/docs/en/r2.0.0-alpha/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">            `bool_ &lt;https://www.mindspore.cn/docs/en/r2.0.0-alpha/api_python/mindspore.html#mindspore.dtype&gt;`_ .</span>
<span class="sd">        y (Union[Tensor, number.Number, bool]): The second input, when the first input is a Tensor,</span>
<span class="sd">            the second input should be a number.Number or bool value, or a Tensor whose data type is number or bool\_.</span>
<span class="sd">            When the first input is Scalar, the second input must be a Tensor whose data type is number or bool\_.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `x` nor `y` is a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([1, 1, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.gt(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False True False]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_greater</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Greater</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">_greater</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="ge"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ge.html#mindspore.ops.ge">[docs]</a><span class="k">def</span> <span class="nf">ge</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the boolean value of :math:`x &gt;= y` element-wise.</span>

<span class="sd">    Note:</span>
<span class="sd">        - Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">        - The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">        - When the inputs are two tensors, dtypes of them cannot be bool at the same time,</span>
<span class="sd">          and the shapes of them can be broadcast.</span>
<span class="sd">        - When the inputs are one tensor and one scalar, the scalar could only be a constant.</span>
<span class="sd">        - Broadcasting is supported.</span>
<span class="sd">        - If the input Tensor can be broadcast, the low dimension will be extended to the corresponding high dimension</span>
<span class="sd">          in another input by copying the value of the dimension.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} =\begin{cases}</span>
<span class="sd">            &amp; \text{True,    if } x_{i}&gt;=y_{i} \\</span>
<span class="sd">            &amp; \text{False,   if } x_{i}&lt;y_{i}</span>
<span class="sd">            \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[Tensor, Number, bool]): The first input is a number or</span>
<span class="sd">            a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        y (Union[Tensor, Number, bool]): The second input is a number or</span>
<span class="sd">            a bool when the first input is a tensor or a tensor whose data type is number or bool.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `x` nor `y` is a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([1, 1, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.ge(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [True True False]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_greater_equal</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">GreaterEqual</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">_greater_equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="equal"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.equal.html#mindspore.ops.equal">[docs]</a><span class="k">def</span> <span class="nf">equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the equivalence between two tensors element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} =\begin{cases}</span>
<span class="sd">            &amp; \text{True,    if } x_{i} = y_{i} \\</span>
<span class="sd">            &amp; \text{False,   if } x_{i} \ne y_{i}</span>
<span class="sd">            \end{cases}</span>

<span class="sd">    Note:</span>
<span class="sd">        - Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">        - The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">        - When the inputs are two tensors, the shapes of them could be broadcast.</span>
<span class="sd">        - When the inputs are one tensor and one scalar, the scalar could only be a constant.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[Tensor, Number]): The first input is a number or</span>
<span class="sd">            a tensor whose data type is number.</span>
<span class="sd">        y (Union[Tensor, Number]): The second input is a number</span>
<span class="sd">            when the first input is a tensor or a tensor whose data type is number.</span>
<span class="sd">            The data type is the same as the first input.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,and the data type is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `x` nor `y` is a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # case 1: The shape of two inputs are different</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.equal(x, 2.0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False True False]</span>
<span class="sd">        &gt;&gt;&gt; # case 2: The shape of two inputs are the same</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([1, 2, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.equal(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True  True False]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">equal_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="ne"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ne.html#mindspore.ops.ne">[docs]</a><span class="k">def</span> <span class="nf">ne</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the non-equivalence of two tensors element-wise.</span>

<span class="sd">    Note:</span>
<span class="sd">        - Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">        - The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">        - When the inputs are two tensors, the shapes of them could be broadcast.</span>
<span class="sd">        - When the inputs are one tensor and one scalar, the scalar could only be a constant.</span>
<span class="sd">        - Broadcasting is supported.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} =\begin{cases}</span>
<span class="sd">            &amp; \text{True,    if } x_{i} \ne y_{i} \\</span>
<span class="sd">            &amp; \text{False,   if } x_{i} = y_{i}</span>
<span class="sd">            \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[Tensor, Number, bool]): The first input is a number or</span>
<span class="sd">            a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        y (Union[Tensor, Number, bool]): The second input is a number or</span>
<span class="sd">            a bool when the first input is a tensor or a tensor whose data type is number or bool.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,and the data type is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` and `y` is not one of the following: Tensor, Number, bool.</span>
<span class="sd">        TypeError: If neither `x` nor `y` is a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.ne(x, 2.0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True False  True]</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([1, 2, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.ne(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False False  True]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">not_equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="approximate_equal"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.approximate_equal.html#mindspore.ops.approximate_equal">[docs]</a><span class="k">def</span> <span class="nf">approximate_equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">tolerance</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns True if abs(x-y) is smaller than tolerance element-wise, otherwise False.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \begin{cases}</span>
<span class="sd">        &amp; \text{ if } \left | x_{i} - y_{i} \right | &lt; \text{tolerance},\ \ True  \\</span>
<span class="sd">        &amp; \text{ if } \left | x_{i} - y_{i} \right | \ge \text{tolerance},\ \  False</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    where `tolerance` indicates Acceptable maximum tolerance.</span>

<span class="sd">    Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower precision data type will be converted to</span>
<span class="sd">    the relatively highest precision data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): A tensor. Must be one of the following types: float32, float16.</span>
<span class="sd">          :math:`(N,*)` where :math:`*` means, any number of additional dimensions, its rank should be less than 8.</span>
<span class="sd">        y (Tensor): A tensor of the same type and shape as `x`.</span>
<span class="sd">        tolerance (float): The maximum deviation that two elements can be considered equal. Default: 1e-05.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the shape of `x`, and the data type is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `tolerance` is not a float.</span>
<span class="sd">        RuntimeError: If the data type of `x`, `y` conversion of Parameter is given</span>
<span class="sd">                      but data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; tol = 1.5</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([2, 4, 6]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.approximate_equal(Tensor(x), Tensor(y), tol)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True  False  False]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">P</span><span class="o">.</span><span class="n">ApproximateEqual</span><span class="p">(</span><span class="n">tolerance</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="isfinite"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.isfinite.html#mindspore.ops.isfinite">[docs]</a><span class="k">def</span> <span class="nf">isfinite</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Determines which elements are finite for each position.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \begin{cases}</span>
<span class="sd">          &amp; \text{ if } x_{i} = \text{Finite},\ \ True\  \\</span>
<span class="sd">          &amp; \text{ if } x_{i} \ne \text{Finite},\ \ False</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor.</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape of input, and the dtype is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([np.log(-1), 1, np.log(0)]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.isfinite(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False  True False]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">isfinite_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="isnan"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.isnan.html#mindspore.ops.isnan">[docs]</a><span class="k">def</span> <span class="nf">isnan</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Determines which elements are NaN for each position.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \begin{cases}</span>
<span class="sd">          &amp; \ True,\ \text{ if } x_{i} = \text{Nan} \\</span>
<span class="sd">          &amp; \ False,\ \text{ if } x_{i} \ne  \text{Nan}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    where :math:`Nan` means not a number.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor.</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape of input, and the dtype is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([np.log(-1), 1, np.log(0)]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.isnan(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True False False]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">isnan_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="isclose"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.isclose.html#mindspore.ops.isclose">[docs]</a><span class="k">def</span> <span class="nf">isclose</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">,</span> <span class="n">equal_nan</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a new Tensor with boolean elements representing if each element of `x1`</span>
<span class="sd">    is “close” to the corresponding element of `x2`. Closeness is defined as:</span>

<span class="sd">    .. math::</span>

<span class="sd">            ∣x1−x2∣  ≤  atol + rtol × ∣x2∣</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Tensor): First Tensor to compare, with data type belongs to float32, float16, int32.</span>
<span class="sd">        x2 (Tensor): Second Tensor to compare, with data type belongs to float32, float16, int32.</span>
<span class="sd">        rtol (float, optional): Relative tolerance. Default: 1e-05.</span>
<span class="sd">        atol (float, optional): Absolute tolerance. Default: 1e-08.</span>
<span class="sd">        equal_nan (bool, optional): If True, then two NaNs will be considered equal. Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A bool Tensor, with the shape as broadcasted result of the input `x1` and `x2`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If either of `x1` and `x2` is not Tensor.</span>
<span class="sd">        TypeError: If either of `x1` and `x2` is not float16, float32 or int32.</span>
<span class="sd">        TypeError: If either of `atol` and `rtol` is not float.</span>
<span class="sd">        TypeError: If `equal_nan` is not bool.</span>
<span class="sd">        TypeError: If the dtype of `x1` is not same as the `x2`.</span>
<span class="sd">        ValueError: If `x1` and `x2` can not be broadcast.</span>
<span class="sd">        ValueError: If either of `atol` and `rtol` is less than zero.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([1.3, 2.1, 3.2, 4.1, 5.1]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor(np.array([1.3, 3.3, 2.3, 3.1, 5.1]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.isclose(input, other)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True False False False  True]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">is_close</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">IsClose</span><span class="p">)(</span><span class="n">rtol</span><span class="o">=</span><span class="n">rtol</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="n">atol</span><span class="p">,</span> <span class="n">equal_nan</span><span class="o">=</span><span class="n">equal_nan</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">is_close</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">isreal</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a new tensor with boolean elements representing whether each element of `x` is real-valued.</span>
<span class="sd">    All real value types are considered real numbers.</span>
<span class="sd">    A complex value is considered real when its imaginary part is 0.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape of input, and the dtype is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops, Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([1, 1+1j, 2+0j], mstype.complex64)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.isreal(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True False  True]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;the input x must be Tensor!&quot;</span><span class="p">)</span>

    <span class="c1"># Note: Integral and Floating tensor values are always real</span>
    <span class="n">ones_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Ones</span><span class="p">)()</span>
    <span class="n">real_dtype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int_type</span> <span class="o">+</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint_type</span> <span class="o">+</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float_type</span> <span class="o">+</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">,)</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="n">real_dtype</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">ones_op</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>

    <span class="n">imag_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Imag</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">imag_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>


<span class="k">def</span> <span class="nf">nan_to_num</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">posinf</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">neginf</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replaces `NaN`, positive infinity, and negative infinity values in the `x` with the values</span>
<span class="sd">    specified by `nan`, `posinf`, and `neginf`, respectively. By default, NaN is replaced by 0,</span>
<span class="sd">    positive infinity is replaced by the largest finite value representable by the x dtype,</span>
<span class="sd">    and negative infinity is replaced by the smallest finite value representable by the x dtype.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The shape of tensor is :math:`(x_1, x_2, ..., x_R)`. With float32 or float16 data type.</span>
<span class="sd">        nan (float): The value to replace `NaN`. Default value is 0.0.</span>
<span class="sd">        posinf (float): If a Number, the value to replace positive infinity values with. If None, positive</span>
<span class="sd">          infinity values are replaced with the greatest finite value representable by `x`&#39;s dtype.</span>
<span class="sd">          Default value is None.</span>
<span class="sd">        neginf (float): if a Number, the value to replace negative infinity values with. If None, negative</span>
<span class="sd">          infinity values are replaced with the lowest finite value representable by `x`&#39;s dtype.</span>
<span class="sd">          Default value is None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16 or float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([float(&#39;nan&#39;), float(&#39;inf&#39;), -float(&#39;inf&#39;), 3.14]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.nan_to_num(x, 1.0, 2.0, 3.0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.  2.  3.  3.14]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;the input x must be Tensor!&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">nan</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">nan</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;the parameter nan&#39;s dtype must be float.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">nan</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">if</span> <span class="n">posinf</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">posinf</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;the parameter posinf&#39;s dtype must be float.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
            <span class="n">posinf</span> <span class="o">=</span> <span class="p">(</span><span class="nb">float</span><span class="p">)(</span><span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
            <span class="n">posinf</span> <span class="o">=</span> <span class="p">(</span><span class="nb">float</span><span class="p">)(</span><span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">neginf</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">neginf</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;the parameter neginf&#39;s dtype must be float.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
            <span class="n">neginf</span> <span class="o">=</span> <span class="p">(</span><span class="nb">float</span><span class="p">)(</span><span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
            <span class="n">neginf</span> <span class="o">=</span> <span class="p">(</span><span class="nb">float</span><span class="p">)(</span><span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">)</span>
    <span class="n">_nan_to_num</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NanToNum</span><span class="p">)(</span><span class="n">nan</span><span class="o">=</span><span class="n">nan</span><span class="p">,</span> <span class="n">posinf</span><span class="o">=</span><span class="n">posinf</span><span class="p">,</span> <span class="n">neginf</span><span class="o">=</span><span class="n">neginf</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_nan_to_num</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<div class="viewcode-block" id="maximum"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.maximum.html#mindspore.ops.maximum">[docs]</a><span class="k">def</span> <span class="nf">maximum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the maximum of input tensors element-wise.</span>

<span class="sd">    Note:</span>
<span class="sd">        - Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">        - The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">        - When the inputs are two tensors,</span>
<span class="sd">          dtypes of them cannot be bool at the same time, and the shapes of them could be broadcast.</span>
<span class="sd">        - When the inputs are one tensor and one scalar,</span>
<span class="sd">          the scalar could only be a constant.</span>
<span class="sd">        - Broadcasting is supported.</span>
<span class="sd">        - If one of the elements being compared is a NaN, then that element is returned.</span>

<span class="sd">    .. math::</span>
<span class="sd">        output_i = max(x_i, y_i)</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[Tensor, Number, bool]): The first input is a number or</span>
<span class="sd">            a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        y (Union[Tensor, Number, bool]): The second input is a number or</span>
<span class="sd">            a bool when the first input is a tensor or a tensor whose data type is number or bool.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` and `y` is not one of the following: Tensor, Number, bool.</span>
<span class="sd">        ValueError: If `x` and `y` are not the same shape.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # case 1 : same data type</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 5.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4.0, 2.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.maximum(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [4. 5. 6.]</span>
<span class="sd">        &gt;&gt;&gt; # case 2 : different data type</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 5.0, 3.0]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4.0, 2.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.maximum(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Float32</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">maximum_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="minimum"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.minimum.html#mindspore.ops.minimum">[docs]</a><span class="k">def</span> <span class="nf">minimum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the minimum of input tensors element-wise.</span>

<span class="sd">    Note:</span>
<span class="sd">        - Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">        - The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">        - When the inputs are two tensors, dtypes of them cannot be bool at the same time.</span>
<span class="sd">        - When the inputs are one tensor and one scalar, the scalar could only be a constant.</span>
<span class="sd">        - Shapes of them are supposed to be broadcast.</span>
<span class="sd">        - If one of the elements being compared is a NaN, then that element is returned.</span>

<span class="sd">    .. math::</span>
<span class="sd">        output_i = min(x_i, y_i)</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[Tensor, Number, bool]): The first input is a number or</span>
<span class="sd">            a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        y (Union[Tensor, Number, bool]): The second input is a number or</span>
<span class="sd">            a bool when the first input is a tensor or a tensor whose data type is number or bool.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` and `y` is not one of the following: Tensor, Number, bool.</span>
<span class="sd">        ValueError: If `x` and `y` are not the same shape after broadcast.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # case 1 : same data type</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 5.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4.0, 2.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.minimum(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 2. 3.]</span>
<span class="sd">        &gt;&gt;&gt; # case 2 : different data type</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 5.0, 3.0]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4.0, 2.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.minimum(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Float32</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">minimum_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="median"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.median.html#mindspore.ops.median">[docs]</a><span class="k">def</span> <span class="nf">median</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the median and indices of input tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): A Tensor of any dimension whose data type is int16, int32, int64, float32 or float64.</span>
<span class="sd">        axis (int, optional): The dimension need to reduce. Default: -1.</span>
<span class="sd">        keepdims (bool, optional): Whether the output tensor need to retain `axis` dimension or not. Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        y (Tensor), has the same dtype as the `x`. If `keepdims` is true,</span>
<span class="sd">        the `y` has the same shape as the `x` except the shape of `y` in dimension `axis` is size 1.</span>
<span class="sd">        Otherwise, the `y` lacks `axis` dimension than input.</span>

<span class="sd">        indices (Tensor), has the same shape as the `y`, but dtype is int64.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x` is not one of the following: int16, int32, int64, float32, float64.</span>
<span class="sd">        TypeError: If input `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not a int.</span>
<span class="sd">        TypeError: If `keepdims` is not a bool.</span>
<span class="sd">        ValueError: If `axis` is not in range of [-x.dim, x.dim-1].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[0.57, 0.11, 0.21],[0.38, 0.50, 0.57], [0.36, 0.16, 0.44]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; y = ops.median(x, axis=0, keepdims=False)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        (Tensor(shape=[3], dtype=Float32, value=[0.38, 0.16, 0.44]), Tensor(shape=[3], dtype=Int64, value=[1, 2, 2]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">Median</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">orgqr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tau</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the first :math:`N` columns of a product of</span>
<span class="sd">    `Householder &lt;https://en.wikipedia.org/wiki/Householder_transformation#Householder_matrix&gt;`_</span>
<span class="sd">    matrices. Take the case of input without batch</span>
<span class="sd">    as an example. Suppose input `x` is a matrix of size :math:`(M, N)` after householder transformation.</span>
<span class="sd">    When the diagonal of `x` is set to 1, every colunm of lower triangular in `x` is</span>
<span class="sd">    denoted as :math:`w_j` for :math:`j` for</span>
<span class="sd">    :math:`j=1, \ldots, M`, this function returns the first :math:`N` columns of the matrix</span>

<span class="sd">    .. math::</span>
<span class="sd">        H_{1} H_{2} \ldots H_{k} \quad \text { with } \quad H_{j}=\mathrm{I}_{M}-\tau_{j} w_{j} w_{j}^{\mathrm{H}}</span>

<span class="sd">    where :math:`\mathrm{I}_{M}` is the :math:`M`-dimensional identity matrix. And when :math:`w` is complex,</span>
<span class="sd">    :math:`w^{\mathrm{H}}` is the conjugate transpose, otherwise the transpose.</span>
<span class="sd">    The output matrix is the same size as the input matrix `x`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Tensor of shape :math:`(*, M, N)`, indicating 2D or 3D matrices,</span>
<span class="sd">            with float32, float64, complex64 and complex128 data type.</span>
<span class="sd">        tau (Tensor): Tensor of shape :math:`(*, K)`, where `K` is less than or equal to `N`, indicating the</span>
<span class="sd">            reflecting coefficient in Householder transformation, which have the same type as `x`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and data type as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` or `tau` are not Tensors.</span>
<span class="sd">        TypeError: If dtype of `x` and `tau` is not one of: float64, float32, complex64, complex128.</span>
<span class="sd">        ValueError: If `x` and `tau` have different batch size.</span>
<span class="sd">        ValueError: If x.shape[-2] &lt; x.shape[-1].</span>
<span class="sd">        ValueError: If x.shape[-1] &lt; tau.shape[-1].</span>
<span class="sd">        ValueError: If rank(x) - rank(tau) != 1.</span>
<span class="sd">        ValueError: If rank(x) != 2 or 3.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[-114.6, 10.9, 1.1], [-0.304, 38.07, 69.38], [-0.45, -0.17, 62.]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; tau = Tensor(np.array([1.55, 1.94, 0.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; net = ops.Orgqr()</span>
<span class="sd">        &gt;&gt;&gt; y = net(x, tau)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[-0.54999995 -0.2128925   0.8137956 ]</span>
<span class="sd">         [ 0.47119996 -0.8752807   0.08240613]</span>
<span class="sd">         [ 0.69749993  0.42560163  0.57772595]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">orgqr_</span> <span class="o">=</span> <span class="n">Orgqr</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">orgqr_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tau</span><span class="p">)</span>


<div class="viewcode-block" id="hypot"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.hypot.html#mindspore.ops.hypot">[docs]</a><span class="k">def</span> <span class="nf">hypot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes hypotenuse of input tensors element-wise as legs of a right triangle.</span>
<span class="sd">    The shape of two inputs should be broadcastable, and data type of them should be</span>
<span class="sd">    one of: float32, float64</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The first input tensor.</span>
<span class="sd">        other (Tensor): The second input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is one</span>
<span class="sd">        with higher precision in the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If data type `x` or `other` is not float32 or float64.</span>
<span class="sd">        ValueError: If shape of two inputs are not broadcastable.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([3., 5., 7.]))</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor(np.array([4., 12., 24.]))</span>
<span class="sd">        &gt;&gt;&gt; y = ops.hypot(x, other)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [ 5. 13. 25.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">hypot_</span> <span class="o">=</span> <span class="n">Hypot</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">hypot_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="heaviside"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.heaviside.html#mindspore.ops.heaviside">[docs]</a><span class="k">def</span> <span class="nf">heaviside</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">values</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Heaviside step function for each element in input.</span>

<span class="sd">    .. math::</span>
<span class="sd">            \text { heaviside }(\text { x, values })=\left\{\begin{array}{ll}</span>
<span class="sd">            0, &amp; \text { if x }&lt;0 \\</span>
<span class="sd">            \text { values, } &amp; \text { if x }==0 \\</span>
<span class="sd">            1, &amp; \text { if x }&gt;0</span>
<span class="sd">            \end{array}\right.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor. With real number data type.</span>
<span class="sd">        values (Tensor): The values to use where x is zero. Values can be broadcast with x.</span>
<span class="sd">            &#39;x&#39; should have the same dtype with &#39;values&#39;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same type as &#39;x&#39; and &#39;values&#39;.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` or `values` is not Tensor.</span>
<span class="sd">        TypeError: If data type `x` and `values` is different.</span>
<span class="sd">        ValueError: If shape of two inputs are not broadcastable.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-1.5, 0., 2.]))</span>
<span class="sd">        &gt;&gt;&gt; values = Tensor(np.array([0.5]))</span>
<span class="sd">        &gt;&gt;&gt; y = ops.heaviside(x, values)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [ 0.  0.5 1. ]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">heaviside_</span> <span class="o">=</span> <span class="n">Heaviside</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">heaviside_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">values</span><span class="p">)</span></div>


<div class="viewcode-block" id="logaddexp"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.logaddexp.html#mindspore.ops.logaddexp">[docs]</a><span class="k">def</span> <span class="nf">logaddexp</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the logarithm of the sum of exponentiations of the inputs.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = log(exp(x1_i) + exp(x2_i))</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Tensor): Input Tensor.</span>
<span class="sd">        x2 (Tensor): Input Tensor. If the shape of `x1` is not equal to the shape of `x2`, they must be broadcastable to</span>
<span class="sd">            a common shape (which becomes the shape of the output).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x1`, `x2` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([1, 2, 3]).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array(2).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.logaddexp(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2.312 2.693 3.312]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">log_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Log</span><span class="p">)()</span>
    <span class="n">exp_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Exp</span><span class="p">)()</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">log_op</span><span class="p">(</span><span class="n">exp_op</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span> <span class="o">+</span> <span class="n">exp_op</span><span class="p">(</span><span class="n">x2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">y</span></div>


<div class="viewcode-block" id="logaddexp2"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.logaddexp2.html#mindspore.ops.logaddexp2">[docs]</a><span class="k">def</span> <span class="nf">logaddexp2</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the logarithm of the sum of exponentiations in base of 2 of the inputs.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = log_2(2^{x1_i} + 2^{x2_i})</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Tensor): Input tensor.</span>
<span class="sd">        x2 (Tensor): Input tensor. If ``x1.shape != x2.shape``, they must be broadcastable to</span>
<span class="sd">            a common shape (which becomes the shape of the output).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x1`, `x2` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([2, 4, 8]).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([2]).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.logaddexp2(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [3. 4.32 8.02]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">log_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Log</span><span class="p">)()</span>
    <span class="n">pow_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Pow</span><span class="p">)()</span>
    <span class="n">add_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">)()</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;The input x1 must be Tensor.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;The input x2 must be Tensor.&quot;</span><span class="p">)</span>

    <span class="n">add_exp</span> <span class="o">=</span> <span class="n">add_op</span><span class="p">(</span><span class="n">pow_op</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">x1</span><span class="p">),</span> <span class="n">pow_op</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">x2</span><span class="p">))</span>
    <span class="n">tensor_2</span> <span class="o">=</span> <span class="n">_make_tensor</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">add_exp</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">log_op</span><span class="p">(</span><span class="n">add_exp</span><span class="p">)</span> <span class="o">/</span> <span class="n">log_op</span><span class="p">(</span><span class="n">tensor_2</span><span class="p">)</span></div>


<div class="viewcode-block" id="std"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.std.html#mindspore.ops.std">[docs]</a><span class="k">def</span> <span class="nf">std</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(),</span> <span class="n">unbiased</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the standard-deviation and mean of each row of the input tensor by default,</span>
<span class="sd">    or it can calculate them in specified dimension `axis`.</span>
<span class="sd">    If `axis` is a list of dimensions, reduce over all of them.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor[Number]): Input tensor with a dtype of number.Number, its shape should be :math:`(N, *)`</span>
<span class="sd">          where :math:`*` means any number of additional dims, its rank should be less than 8.</span>
<span class="sd">        axis (Union[int, tuple(int), list(int)]): The dimensions to reduce. Default: (), reduce all dimensions.</span>
<span class="sd">                                                  Only constant value is allowed.</span>
<span class="sd">                                                  Must be in the range [-rank(`input_x`), rank(`input_x`)).</span>
<span class="sd">        unbiased (bool):  Whether to use Bessel’s correction.</span>
<span class="sd">                          If true, will use the Bessel correction unbiased estimation.</span>
<span class="sd">                          If false, will through the biased estimation to calculate the standard deviation.</span>
<span class="sd">        keep_dims (bool): Whether the output tensor has dim retained or not.</span>
<span class="sd">                          If true, keep these reduced dimensions and the length is 1.</span>
<span class="sd">                          If false, don&#39;t keep these dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tuple of 2 Tensors (output_std, output_mean) containing the standard deviation and mean.</span>
<span class="sd">        Suppose the shape of `input_x` is :math:`(x_0, x_1, ..., x_R)`:</span>

<span class="sd">        - If `axis` is () and `keep_dims` is set to False, returns a 0-D Tensor, indicating</span>
<span class="sd">          the standard deviation of all elements in `input_x`.</span>
<span class="sd">        - If `axis` is int 1 and `keep_dims` is set to False, then the returned Tensor</span>
<span class="sd">          has shape :math:`(x_0, x_2, ..., x_R)`.</span>
<span class="sd">        - If `axis` is tuple(int) or list(int), e.g. (1, 2) and `keep_dims` is set to False,</span>
<span class="sd">          then the returned Tensor has shape :math:`(x_0, x_2, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not one of the following: int, tuple or list.</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        ValueError: If `axis` is out of range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1, 2, 3], [-1, 1, 4]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.std(input_x, 1, True, False)</span>
<span class="sd">        &gt;&gt;&gt; output_std, output_mean = output[0], output[1]</span>
<span class="sd">        &gt;&gt;&gt; print(output_std)</span>
<span class="sd">        [1.        2.5166116]</span>
<span class="sd">        &gt;&gt;&gt; print(output_mean)</span>
<span class="sd">        [2.        1.3333334]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">reduce_std_op</span> <span class="o">=</span> <span class="n">ReduceStd</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">unbiased</span><span class="o">=</span><span class="n">unbiased</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="n">keep_dims</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">reduce_std_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span></div>


<div class="viewcode-block" id="sqrt"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.sqrt.html#mindspore.ops.sqrt">[docs]</a><span class="k">def</span> <span class="nf">sqrt</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns sqrt of a tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = \\sqrt{x_{i}}</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor with a dtype of number.Number, its rank must be in [0, 7] inclusive.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 4.0, 9.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.sqrt(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 2. 3.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">sqrt_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="square"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.square.html#mindspore.ops.square">[docs]</a><span class="k">def</span> <span class="nf">square</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns square of a tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = (x_{i})^2</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor with a dtype of Number, its rank must be in [0, 7] inclusive.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.square(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 4. 9.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">square_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">outer</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return outer product of `x1` and `x2`. If `x1` is a vector of size n and `x2` is a vector of size m,</span>
<span class="sd">    then output must be a matrix of size n x m.</span>

<span class="sd">    Note:</span>
<span class="sd">        This function does not broadcast.</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Tensor): 1-D input vector.</span>
<span class="sd">        x2 (Tensor): 1-D input vector.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        out (Tensor, optional) : optional output matrix.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x1` is not a Tensor.</span>
<span class="sd">        TypeError: If `x2` is not a Tensor.</span>
<span class="sd">        ValueError: Expected 1-D input `x1`, but got n-D.</span>
<span class="sd">        ValueError: Expected 1-D input `x2`, but got n-D.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; out = ops.outer(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(out)</span>
<span class="sd">        [[1 2 3]</span>
<span class="sd">         [2 4 6]</span>
<span class="sd">         [3 6 9]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;the input x1 must be Tensor!&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;the input x2 must be Tensor!&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;the input x1 must be a 1-D vector!&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;the input x2 must be a 1-D vector!&quot;</span><span class="p">)</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">x1</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">mul_ops</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Mul</span><span class="p">)()</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">mul_ops</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span>


<span class="k">def</span> <span class="nf">mv</span><span class="p">(</span><span class="n">mat</span><span class="p">,</span> <span class="n">vec</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multiplies matrix `mat` and vector `vec`.</span>

<span class="sd">    If mat is a :math:`(N, M)` tensor, vec is a 1-D tensor of size :math:`M`, out will be 1-D of size :math:`N`.</span>

<span class="sd">    Args:</span>
<span class="sd">        mat (Tensor): Input matrix of the tensor. The shape of the tensor is :math:`(N, M)`.</span>
<span class="sd">        vec (Tensor): Input vector of the tensor. The shape of the tensor is :math:`(M,)`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape of the output tensor is :math:`(N,)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `mat`, `vec` is not a Tensor.</span>
<span class="sd">        ValueError: If `mat` is not a 2-D Tensor.</span>
<span class="sd">            If `vec` is not a 1-D Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; mat = Tensor(np.array([[3., 4.], [1., 6.], [1., 3.]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; vec = Tensor(np.array([1., 2.]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = mv(mat, vec)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [11. 13. 7.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">matmul_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">MatMul</span><span class="p">)()</span>
    <span class="n">reshape_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">)()</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mat</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;The input mat must be Tensor.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">vec</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;The input vec must be Tensor.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">mat</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The input mat must be 2-D Tensor.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">vec</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The input vec must be 1-D Tensor.&quot;</span><span class="p">)</span>

    <span class="n">length_vec</span> <span class="o">=</span> <span class="n">get_x_shape</span><span class="p">(</span><span class="n">vec</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">vec</span> <span class="o">=</span> <span class="n">reshape_op</span><span class="p">(</span><span class="n">vec</span><span class="p">,</span> <span class="p">(</span><span class="n">length_vec</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>

    <span class="n">out</span> <span class="o">=</span> <span class="n">matmul_op</span><span class="p">(</span><span class="n">mat</span><span class="p">,</span> <span class="n">vec</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">T</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">out</span>


<div class="viewcode-block" id="addbmm"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.addbmm.html#mindspore.ops.addbmm">[docs]</a><span class="k">def</span> <span class="nf">addbmm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">batch1</span><span class="p">,</span> <span class="n">batch2</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies batch matrix multiplication to `batch1` and `batch2`, with a reduced add step. The matrix `x` is add to</span>
<span class="sd">    final result.</span>

<span class="sd">    The optional values `alpha` and `beta` are the matrix-matrix product between `batch1` and `batch2` and the scale</span>
<span class="sd">    factor for the added tensor `x` respectively. If `beta` is 0, then `x` will be ignored.</span>

<span class="sd">    .. math::</span>
<span class="sd">        output = \beta x + \alpha (\sum_{i=0}^{b-1} {batch1 @ batch2})</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Tensor to be added.</span>
<span class="sd">        batch1 (Tensor): The first batch of tensor to be multiplied.</span>
<span class="sd">        batch2 (Tensor): The second batch of tensor to be multiplied.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        beta (Union[int, float], optional): Multiplier for `x`. Default: 1.</span>
<span class="sd">        alpha (Union[int, float], optional): Multiplier for `batch1` @ `batch2`. Default: 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If `batch1`, `batch2` cannot apply batch matrix multiplication.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">bmm_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">BatchMatMul</span><span class="p">)()</span>
    <span class="n">bmm_res</span> <span class="o">=</span> <span class="n">bmm_op</span><span class="p">(</span><span class="n">batch1</span><span class="p">,</span> <span class="n">batch2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">bmm_res</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span></div>


<div class="viewcode-block" id="addmm"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.addmm.html#mindspore.ops.addmm">[docs]</a><span class="k">def</span> <span class="nf">addmm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mat1</span><span class="p">,</span> <span class="n">mat2</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multiplies matrix `mat1` and matrix `mat2`. The matrix `x` is added to the final result.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Tensor to be added.</span>
<span class="sd">        mat1 (Tensor): The first tensor to be multiplied.</span>
<span class="sd">        mat2 (Tensor): The second tensor to be multiplied.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        beta (Union[int, float], optional): Multiplier for `x`. Default: 1.</span>
<span class="sd">        alpha (Union[int, float], optional): Multiplier for `mat1` @ `mat2`. Default: 1.</span>

<span class="sd">    .. math::</span>
<span class="sd">        output = \beta x + \alpha (mat1 @ mat2)</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If `mat1`, `mat2` cannot apply matrix multiplication.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">matmul_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">MatMul</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">matmul_op</span><span class="p">(</span><span class="n">mat1</span><span class="p">,</span> <span class="n">mat2</span><span class="p">))</span></div>


<span class="k">def</span> <span class="nf">addmv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mat</span><span class="p">,</span> <span class="n">vec</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multiplies matrix `mat` and vector `vec`. The vector `x` is added to the final result.</span>

<span class="sd">    If mat is a :math:`(N, M)` tensor, vec is a 1-D tensor of size :math:`M`, then `x` must be broadcastable</span>
<span class="sd">    with a 1-D tensor of size :math:`N` and `out` will be 1-D tensor of size :math:`N`.</span>

<span class="sd">    The optional values `beta` and `alpha` are the matrix-vector product between `mat` and `vec` and the scale</span>
<span class="sd">    factor for the added tensor `x` respectively. If `beta` is 0, then `x` will be ignored.</span>

<span class="sd">    .. math::</span>
<span class="sd">        output = β x + α (mat @ vec)</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Vector to be added. The shape of the tensor is :math:`(N,)`.</span>
<span class="sd">        mat (Tensor): The first tensor to be multiplied. The shape of the tensor is :math:`(N, M)`.</span>
<span class="sd">        vec (Tensor): The second tensor to be multiplied. The shape of the tensor is :math:`(M,)`.</span>
<span class="sd">        beta (scalar[int, float, bool], optional): Multiplier for `x` (β). The `beta` must be int or</span>
<span class="sd">            float or bool, Default: 1.</span>
<span class="sd">        alpha (scalar[int, float, bool], optional): Multiplier for `mat` @ `vec` (α). The `alpha` must</span>
<span class="sd">            be int or float or bool, Default: 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape of the output tensor is :math:`(N,)`, has the same dtype as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `mat`, `vec`, `x` is not a Tensor.</span>
<span class="sd">        TypeError: If inputs `x`, `mat`, &#39;vec&#39; are not the same dtype.</span>
<span class="sd">        ValueError: If `mat` is not a 2-D Tensor.</span>
<span class="sd">            If `x`, `vec` is not a 1-D Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([2., 3.]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; mat = Tensor(np.array([[2., 5., 3.], [4., 2., 2.]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; vec = Tensor(np.array([3., 2., 4.]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = addmv(x, mat, vec)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [30. 27.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">dtypeop</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()</span>
    <span class="n">input_dtype</span> <span class="o">=</span> <span class="n">dtypeop</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mat</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">vec</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For Addmv, inputs must be all tensors.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">input_dtype</span> <span class="o">==</span> <span class="n">dtypeop</span><span class="p">(</span><span class="n">mat</span><span class="p">)</span> <span class="ow">and</span> <span class="n">input_dtype</span> <span class="o">==</span> <span class="n">dtypeop</span><span class="p">(</span><span class="n">vec</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For Addmv, the inputs should be the same dtype.&quot;</span><span class="p">)</span>
    <span class="n">_check_input_1d</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="s2">&quot;Addmv&quot;</span><span class="p">)</span>
    <span class="n">_check_input_1d</span><span class="p">(</span><span class="n">vec</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;vec&quot;</span><span class="p">,</span> <span class="s2">&quot;Addmv&quot;</span><span class="p">)</span>
    <span class="n">_check_input_2d</span><span class="p">(</span><span class="n">mat</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;mat&quot;</span><span class="p">,</span> <span class="s2">&quot;Addmv&quot;</span><span class="p">)</span>
    <span class="n">_check_input_dtype</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">,</span>
                       <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
                        <span class="n">mstype</span><span class="o">.</span><span class="n">int16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">],</span> <span class="s2">&quot;Addmv&quot;</span><span class="p">)</span>
    <span class="n">_check_attr_dtype</span><span class="p">(</span><span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">],</span> <span class="s2">&quot;Addmv&quot;</span><span class="p">)</span>
    <span class="n">_check_attr_dtype</span><span class="p">(</span><span class="s2">&quot;beta&quot;</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">],</span> <span class="s2">&quot;Addmv&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">input_dtype</span> <span class="ow">in</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">):</span>
        <span class="n">scalar_cast</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScalarCast</span><span class="p">()</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="n">scalar_cast</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="n">scalar_cast</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">mv</span><span class="p">(</span><span class="n">mat</span><span class="p">,</span> <span class="n">vec</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>


<div class="viewcode-block" id="adjoint"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.adjoint.html#mindspore.ops.adjoint">[docs]</a><span class="k">def</span> <span class="nf">adjoint</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a view of the tensor conjugated and with the last two dimensions transposed.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the calculated result.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_dtype</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">_dim</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span>
    <span class="n">perm</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">_dim</span><span class="p">)]</span>
    <span class="n">perm</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">perm</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">perm</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">perm</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">perm</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">_dtype</span> <span class="ow">in</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">complex64</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">complex128</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">t</span><span class="o">.</span><span class="n">conj</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">t</span></div>


<div class="viewcode-block" id="addr"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.addr.html#mindspore.ops.addr">[docs]</a><span class="k">def</span> <span class="nf">addr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">vec1</span><span class="p">,</span> <span class="n">vec2</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Executes the outer-product of `vec1` and `vec2` and adds it to the matrix `x`.</span>

<span class="sd">    If `vec1` is a vector of size :math:`N` and `vec2` is a vector of size :math:`M`, then `x` must be broadcastable</span>
<span class="sd">    with a matrix of size :math:`(N, M)` and `out` will be a matrix of size :math:`(N, M)`.</span>

<span class="sd">    The optional values `beta` and `alpha` are the scale factors on the outer product between `vec1` and `vec2`</span>
<span class="sd">    and the added matrix `x` respectively. If `beta` is 0, then `x` will be ignored.</span>

<span class="sd">    .. math::</span>
<span class="sd">        output = β x + α (vec1 ⊗ vec2)</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Vector to be added. The shape of the tensor is :math:`(N, M)`.</span>
<span class="sd">        vec1 (Tensor): The first tensor to be multiplied. The shape of the tensor is :math:`(N,)`.</span>
<span class="sd">        vec2 (Tensor): The second tensor to be multiplied. The shape of the tensor is :math:`(M,)`.</span>
<span class="sd">        beta (scalar[int, float, bool], optional): Multiplier for `x` (β). The `beta` must be int or</span>
<span class="sd">            float or bool, Default: 1.</span>
<span class="sd">        alpha (scalar[int, float, bool], optional): Multiplier for `vec1` ⊗ `vec2` (α). The `alpha` must</span>
<span class="sd">            be int or float or bool, Default: 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape of the output tensor is :math:`(N, M)`, has the same dtype as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x`, `vec1`, `vec2` is not a Tensor.</span>
<span class="sd">        TypeError: If inputs `x`, `vec1`, `vec2` are not the same dtype.</span>
<span class="sd">        ValueError: If `x` is not a 2-D Tensor.</span>
<span class="sd">            If `vec1`, `vec2` is not a 1-D Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[2., 2.], [3., 2.], [3., 4.]], np.float32))</span>
<span class="sd">        &gt;&gt;&gt; vec1 = Tensor(np.array([2., 3., 2.], np.float32))</span>
<span class="sd">        &gt;&gt;&gt; vec2 = Tensor(np.array([3, 4], np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.addr(x, vec1, vec2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 8. 10.]</span>
<span class="sd">         [12. 14.]</span>
<span class="sd">         [ 9. 12.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">dtypeop</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()</span>
    <span class="n">input_dtype</span> <span class="o">=</span> <span class="n">dtypeop</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">vec1</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">vec2</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For Addr, inputs must be all tensors.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">input_dtype</span> <span class="o">==</span> <span class="n">dtypeop</span><span class="p">(</span><span class="n">vec1</span><span class="p">)</span> <span class="ow">and</span> <span class="n">input_dtype</span> <span class="o">==</span> <span class="n">dtypeop</span><span class="p">(</span><span class="n">vec2</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For Addr, the inputs should be the same dtype.&quot;</span><span class="p">)</span>
    <span class="n">_check_input_1d</span><span class="p">(</span><span class="n">vec1</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;vec1&quot;</span><span class="p">,</span> <span class="s2">&quot;Addr&quot;</span><span class="p">)</span>
    <span class="n">_check_input_1d</span><span class="p">(</span><span class="n">vec2</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;vec2&quot;</span><span class="p">,</span> <span class="s2">&quot;Addr&quot;</span><span class="p">)</span>
    <span class="n">_check_input_2d</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="s2">&quot;Addr&quot;</span><span class="p">)</span>
    <span class="n">_check_input_dtype</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">,</span>
                       <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
                        <span class="n">mstype</span><span class="o">.</span><span class="n">int16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">],</span> <span class="s2">&quot;Addmv&quot;</span><span class="p">)</span>
    <span class="n">_check_attr_dtype</span><span class="p">(</span><span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">],</span> <span class="s2">&quot;Addr&quot;</span><span class="p">)</span>
    <span class="n">_check_attr_dtype</span><span class="p">(</span><span class="s2">&quot;beta&quot;</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">],</span> <span class="s2">&quot;Addr&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">input_dtype</span> <span class="ow">in</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">):</span>
        <span class="n">scalar_cast</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScalarCast</span><span class="p">()</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="n">scalar_cast</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="n">scalar_cast</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">matmul_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>
    <span class="n">reshape_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()</span>

    <span class="n">length_vec1</span> <span class="o">=</span> <span class="n">get_x_shape</span><span class="p">(</span><span class="n">vec1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">vec1</span> <span class="o">=</span> <span class="n">reshape_op</span><span class="p">(</span><span class="n">vec1</span><span class="p">,</span> <span class="p">(</span><span class="n">length_vec1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">length_vec2</span> <span class="o">=</span> <span class="n">get_x_shape</span><span class="p">(</span><span class="n">vec2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">vec2</span> <span class="o">=</span> <span class="n">reshape_op</span><span class="p">(</span><span class="n">vec2</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">length_vec2</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

    <span class="n">out</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">matmul_op</span><span class="p">(</span><span class="n">vec1</span><span class="p">,</span> <span class="n">vec2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="lcm"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.lcm.html#mindspore.ops.lcm">[docs]</a><span class="k">def</span> <span class="nf">lcm</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes least common multiplier of input tensors element-wise.</span>
<span class="sd">    The shape of two inputs should be broadcastable, and data type of them should be</span>
<span class="sd">    one of: int32, int64</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Tensor): The first input tensor.</span>
<span class="sd">        x2 (Tensor): The second input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is one</span>
<span class="sd">        with higher digits in the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If data type `x1` or `x2` is not int32 or int64.</span>
<span class="sd">        ValueError: If shape of two inputs are not broadcastable.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([7, 8, 9]))</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([14, 6, 12]))</span>
<span class="sd">        &gt;&gt;&gt; y = ops.lcm(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [14 24 36]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">lcm_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Lcm</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">lcm_</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span></div>


<div class="viewcode-block" id="cdist"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.cdist.html#mindspore.ops.cdist">[docs]</a><span class="k">def</span> <span class="nf">cdist</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">2.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes p-norm distance between each pair of row vectors of two input Tensors.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input tensor of shape :math:`(B, P, M)`.</span>
<span class="sd">          Letter :math:`B` represents 0 or positive int number.</span>
<span class="sd">          When :math:`B` is equal to 0, it means this dimension can be ignored,</span>
<span class="sd">          i.e. shape of the tensor is :math:`(P, M)`. The supported dtype is</span>
<span class="sd">          [float32, float64] on GPU, or [float32] on CPU.</span>
<span class="sd">        y (Tensor): Input tensor of shape :math:`(B, R, M)`, has the same dtype as `x`.</span>
<span class="sd">        p (float, optional): P value for the p-norm distance to calculate between each</span>
<span class="sd">          vector pair, P ∈ [0,∞]. Default: 2.0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, p-norm distance, has the same dtype as `x`, its shape is :math:`(B, P, R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` or `y` is not Tensor.</span>
<span class="sd">        TypeError: If dtype of x or y is not in [float32, float64] on GPU, or is not in [float32] on CPU.</span>
<span class="sd">        TypeError: If `p` is not float32.</span>
<span class="sd">        ValueError: If `p` is negative.</span>
<span class="sd">        ValueError: If dimension of `x` is not the same as `y`.</span>
<span class="sd">        ValueError: If dimension of `x` or `y` is neither 2 nor 3.</span>
<span class="sd">        ValueError: If the batch shape of `x` is not the same as the shape of `y`.</span>
<span class="sd">        ValueError: If the number of columns of `x` is not the same as the number of `y`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[1.0, 1.0], [2.0, 2.0]]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([[[3.0, 3.0], [3.0, 3.0]]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.cdist(x, y, 2.0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[2.8284273 2.8284273]</span>
<span class="sd">          [1.4142137 1.4142137]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cdist_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Cdist</span><span class="p">)(</span><span class="n">p</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cdist_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">gcd</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes greatest common divisor of input tensors element-wise.</span>
<span class="sd">    The shape of two inputs should be broadcastable, and data type of them should be</span>
<span class="sd">    one of: int32, int64</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x1** (Tensor) - The first input tensor.</span>
<span class="sd">        - **x2** (Tensor) - The second input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is one</span>
<span class="sd">        with higher digits in the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If data type `x1` or `x2` is not int32 or int64.</span>
<span class="sd">        ValueError: If shape of two inputs are not broadcastable.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([7, 8, 9]))</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([14, 6, 12]))</span>
<span class="sd">        &gt;&gt;&gt; y = ops.gcd(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [7 2 3]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">gcd_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Gcd</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">gcd_</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>


<div class="viewcode-block" id="lerp"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.lerp.html#mindspore.ops.lerp">[docs]</a><span class="k">def</span> <span class="nf">lerp</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Does a linear interpolation of two tensors start and end based on a float or tensor weight.</span>

<span class="sd">    If `weight` is a tensor, the shapes of three inputs need to be broadcast;</span>
<span class="sd">    If `weight` is a float, the shapes of `start` and `end` need to be broadcast.</span>

<span class="sd">    .. math::</span>

<span class="sd">        output_{i} = start_{i} + weight_{i} * (end_{i} - start_{i})</span>

<span class="sd">    Args:</span>
<span class="sd">        start (Tensor): The tensor with the starting points. Data type must be float16 or float32.</span>
<span class="sd">        end (Tensor): The tensor with the ending points. Data type must be the same as `start`.</span>
<span class="sd">        weight (Union[float, Tensor]): The weight for the interpolation formula. Must be a float</span>
<span class="sd">            or a scalar tensor with float16 or float32 data type.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same type and shape as input `start`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `start` or `end` is not a tensor.</span>
<span class="sd">        TypeError: If `weight` is neither scalar(float) nor tensor.</span>
<span class="sd">        TypeError: If dtype of `start` or `end` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `weight` is neither float16 nor float32 when it is a tensor.</span>
<span class="sd">        TypeError: If `start` and `end` have different data types.</span>
<span class="sd">        TypeError: If `start`, `end` and `weight` have different data types when `weight` is a tensor.</span>
<span class="sd">        ValueError: If `end` could not be broadcast to a tensor with shape of `start`.</span>
<span class="sd">        ValueError: If `weight` could not be broadcast to tensors with shapes of `start` and `end` when it is a tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; start = Tensor(np.array([1., 2., 3., 4.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; end = Tensor(np.array([10., 10., 10., 10.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.lerp(start, end, 0.5)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [5.5 6. 6.5 7. ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">lerp_</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span></div>


<div class="viewcode-block" id="bernoulli"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bernoulli.html#mindspore.ops.bernoulli">[docs]</a><span class="k">def</span> <span class="nf">bernoulli</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">seed</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Randomly set the elements of output to 0 or 1 with the probability of P which follows the Bernoulli distribution.</span>

<span class="sd">    .. math::</span>
<span class="sd">        out_{i} \sim Bernoulli(p_{i})</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Tensor of shape :math:`(N,*)` where :math:`*` means, any number of additional dimensions. Data</span>
<span class="sd">                    type must be int8, uint8, int16, int32, int64, bool, float32 or float64.</span>
<span class="sd">        p (Union[Tensor, float], optional): The shape of p need to be broadcast. Data type must be float32 or float64.</span>
<span class="sd">                                            The elements of p represent the probability of setting 1 for the</span>
<span class="sd">                                            corresponding broadcast position of the current Tensor. The value of `p`</span>
<span class="sd">                                            must be in the range `[0, 1]`. Default: 0.5.</span>
<span class="sd">        seed (int, optional): The seed value for random generating. The value of `seed` must be -1 or a positive</span>
<span class="sd">                              integer. Default: -1, which means using the current timestamp.</span>

<span class="sd">    Returns:</span>
<span class="sd">        output (Tensor), with the same shape and type as x.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x` is not one of: int8, uint8, int16, int32, int64, bool, float32, float64.</span>
<span class="sd">        TypeError: If dtype of `p` is not one of: float32, float64.</span>
<span class="sd">        TypeError: If dtype of `seed` is not int.</span>
<span class="sd">        ValueError: If `p` is not in range [0, 1].</span>
<span class="sd">        ValueError: If `seed` is less than 0 and not -1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3]), mindspore.int8)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bernoulli(input_x, p=1.0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1 1 1]</span>
<span class="sd">        &gt;&gt;&gt; input_p = Tensor(np.array([0.0, 1.0, 1.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bernoulli(input_x, input_p)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0 1 1]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">bernoulli_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Bernoulli</span><span class="p">)(</span><span class="n">seed</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">bernoulli_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span></div>


<div class="viewcode-block" id="bessel_i1"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bessel_i1.html#mindspore.ops.bessel_i1">[docs]</a><span class="k">def</span> <span class="nf">bessel_i1</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Bessel i1 function of x element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor. The data type must be float16, float32 or float64.</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-1, -0.5, 0.5, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bessel_i1(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-0.5651591  -0.25789431  0.25789431  0.5651591]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">bessel_i1_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="bessel_i1e"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bessel_i1e.html#mindspore.ops.bessel_i1e">[docs]</a><span class="k">def</span> <span class="nf">bessel_i1e</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Bessel i1e function of x element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor. The data type must be float16, float32 or float64.</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-1, -0.5, 0.5, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bessel_i1e(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-0.20791042  -0.15642083  0.15642083  0.20791042]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">bessel_i1e_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="bessel_k1"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bessel_k1.html#mindspore.ops.bessel_k1">[docs]</a><span class="k">def</span> <span class="nf">bessel_k1</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Bessel k1 function of x element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor. The data type must be float16, float32 or float64.</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.5, 1., 2., 4.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bessel_k1(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.65644112  0.60190723  0.13986588  0.0124835]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">bessel_k1_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="bessel_k1e"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bessel_k1e.html#mindspore.ops.bessel_k1e">[docs]</a><span class="k">def</span> <span class="nf">bessel_k1e</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Bessel k1e function of x element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor. The data type must be float16, float32 or float64.</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.5, 1., 2., 4.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bessel_k1e(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2.73100971  1.63615349  1.03347685  0.68157595]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">bessel_k1e_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_input_dtype</span><span class="p">(</span><span class="n">param_name</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">,</span> <span class="n">allow_dtypes</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">):</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_type_name</span><span class="p">(</span><span class="n">param_name</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">,</span> <span class="n">allow_dtypes</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">)</span>


<div class="viewcode-block" id="deg2rad"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.deg2rad.html#mindspore.ops.deg2rad">[docs]</a><span class="k">def</span> <span class="nf">deg2rad</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates a new tensor with each of the elements of `x` converted from angles in degrees to radians.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor[Number]): The input tensor. It must be a positive-definite matrix.</span>
<span class="sd">            With float16, float32 or float64 data type.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` isn&#39;t float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[90.0, -90.0], [180.0, -180.0], [270.0, -270.0]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.deg2rad(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 1.5707964 -1.5707964]</span>
<span class="sd">         [ 3.1415927 -3.1415927]</span>
<span class="sd">         [ 4.712389  -4.712389 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;The input x must be tensor&quot;</span><span class="p">)</span>
    <span class="n">dtype_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">)()</span>
    <span class="n">x_dtype</span> <span class="o">=</span> <span class="n">dtype_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">_check_input_dtype</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">],</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x_dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mf">180.0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mf">180.0</span>
    <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="rad2deg"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.rad2deg.html#mindspore.ops.rad2deg">[docs]</a><span class="k">def</span> <span class="nf">rad2deg</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a new tensor with each of the elements of `x` converted from angles in radians to degrees.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` isn&#39;t float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[6.283, -3.142],[1.570, -6.283],[3.142, -1.570]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.rad2deg(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 359.98935 -180.02333]</span>
<span class="sd">         [  89.95438 -359.98935]</span>
<span class="sd">         [ 180.02333  -89.95438]]</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;The input x must be tensor&quot;</span><span class="p">)</span>
    <span class="n">dtype_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">)()</span>
    <span class="n">x_dtype</span> <span class="o">=</span> <span class="n">dtype_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">_check_input_dtype</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">],</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x_dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="mf">180.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mf">180.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span>
    <span class="k">return</span> <span class="n">out</span></div>


<span class="k">def</span> <span class="nf">frac</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the fractional part of each element in the input</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - x (Tensor) - x is a tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and type as input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.common import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([2, 4.2, -2.5], mstype.float16)</span>
<span class="sd">        &gt;&gt;&gt; output = frac(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0.      0.1992 -0.5   ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">frac_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Mod</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">frac_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>


<span class="c1">#####################################</span>
<span class="c1"># Reduction Operation Functions.</span>
<span class="c1">#####################################</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_create_cummin_perm</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Insure axis is in [-len(x_shape),len(s_shape)-1]&quot;&quot;&quot;</span>
    <span class="n">len_axis</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The date type of &#39;axis&#39; must be Int, but got </span><span class="si">{</span><span class="n">axis</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="o">&lt;</span> <span class="o">-</span><span class="n">len_axis</span> <span class="ow">or</span> <span class="n">axis</span> <span class="o">&gt;</span> <span class="n">len_axis</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The value of axis must be in [</span><span class="si">{</span><span class="o">-</span><span class="n">len_axis</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">len_axis</span><span class="si">}</span><span class="s2">], but got </span><span class="si">{</span><span class="n">axis</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">prem</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">len_axis</span><span class="p">)]</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span> <span class="o">+</span> <span class="n">len_axis</span>
    <span class="n">prem</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">prem</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">=</span> <span class="n">axis</span><span class="p">,</span> <span class="mi">0</span>
    <span class="n">prem</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">prem</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">prem</span>


<div class="viewcode-block" id="cummin"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.cummin.html#mindspore.ops.cummin">[docs]</a><span class="k">def</span> <span class="nf">cummin</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a tuple (values,indices) where &#39;values&#39; is the cumulative minimum value of input Tensor `x`</span>
<span class="sd">    along the dimension `axis`, and `indices` is the index location of each minimum value.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            y_{i} = min(x_{1}, x_{2}, ... , x_{i})</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input Tensor, rank of `x` &gt; 0.</span>
<span class="sd">        axis (int): The dimension to do the operation over. The value of `axis` must be in the range</span>
<span class="sd">            `[-x.ndim, x.ndim - 1]`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        tuple [Tensor], tuple of 2 Tensors, containing the cumulative minimum of elements and the index,</span>
<span class="sd">        The shape of each output tensor is the same as input `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>
<span class="sd">        ValueError: If `axis` is out the range of `[-x.ndim, x.ndim - 1]`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; a = Tensor([-0.2284, -0.6628,  0.0975,  0.2680, -1.3298, -0.4220], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.cummin(a, axis=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output[0])</span>
<span class="sd">        [-0.2284 -0.6628 -0.6628 -0.6628 -1.3298 -1.3298]</span>
<span class="sd">        &gt;&gt;&gt; print(output[1])</span>
<span class="sd">        [0 1 1 1 4 4]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cummin_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Cummin</span><span class="p">)(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">out1</span><span class="p">,</span> <span class="n">out2</span> <span class="o">=</span> <span class="n">cummin_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">transpose</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Transpose</span><span class="p">)()</span>
        <span class="n">_shape_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">)()</span>
        <span class="n">x_shape</span> <span class="o">=</span> <span class="n">_shape_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">prem</span> <span class="o">=</span> <span class="n">_create_cummin_perm</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">prem</span><span class="p">)</span>
        <span class="n">out1</span><span class="p">,</span> <span class="n">out2</span> <span class="o">=</span> <span class="n">cummin_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out1</span> <span class="o">=</span> <span class="n">transpose</span><span class="p">(</span><span class="n">out1</span><span class="p">,</span> <span class="n">prem</span><span class="p">)</span>
        <span class="n">out2</span> <span class="o">=</span> <span class="n">transpose</span><span class="p">(</span><span class="n">out2</span><span class="p">,</span> <span class="n">prem</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">out1</span><span class="p">,</span> <span class="n">out2</span><span class="p">]</span></div>


<div class="viewcode-block" id="cummax"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.cummax.html#mindspore.ops.cummax">[docs]</a><span class="k">def</span> <span class="nf">cummax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a tuple (values,indices) where &#39;values&#39; is the cumulative maximum value of input Tensor `x`</span>
<span class="sd">    along the dimension `axis`, and `indices` is the index location of each maximum value.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            y_{i} = max(x_{1}, x_{2}, ... , x_{i})</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input Tensor, rank of `x` &gt; 0.</span>
<span class="sd">        axis (int): The dimension to do the operation over. The value of `axis` must be in the range</span>
<span class="sd">            `[-x.ndim, x.ndim - 1]`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        tuple [Tensor], tuple of 2 Tensors, containing the cumulative maximum of elements and the index,</span>
<span class="sd">        The shape of each output tensor is the same as input `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>
<span class="sd">        ValueError: If `axis` is out the range of `[-x.ndim, x.ndim - 1]`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[3, 4, 6, 10], [1, 6, 7, 9], [4, 3, 8, 7], [1, 3, 7, 9]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.cummax(x, axis=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output[0])</span>
<span class="sd">        [[ 3.  4.  6. 10.]</span>
<span class="sd">         [ 3.  6.  7. 10.]</span>
<span class="sd">         [ 4.  6.  8. 10.]</span>
<span class="sd">         [ 4.  6.  8. 10.]]</span>
<span class="sd">        &gt;&gt;&gt; print(output[1])</span>
<span class="sd">        [[0 0 0 0]</span>
<span class="sd">         [0 1 1 0]</span>
<span class="sd">         [2 1 2 0]</span>
<span class="sd">         [2 1 2 0]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_cummax</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">Cummax</span><span class="p">)(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_cummax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="cumsum"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.cumsum.html#mindspore.ops.cumsum">[docs]</a><span class="k">def</span> <span class="nf">cumsum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the cumulative sum of input Tensor along `axis`.</span>

<span class="sd">    .. math::</span>

<span class="sd">        y_i = x_1 + x_2 + x_3 + ... + x_i</span>

<span class="sd">    Note:</span>
<span class="sd">        On Ascend, the dtype of `x` only support :int8, uint8, int32, float16 or float32 in case of static shape.</span>
<span class="sd">        For the case of dynamic shape, the dtype of `x` only support int32, float16 or float32.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input Tensor to accumulate.</span>
<span class="sd">        axis (int): Axis along which the cumulative sum is computed.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): The desired dtype of returned Tensor. If specified,</span>
<span class="sd">            the input Tensor will be cast to `dtype` before the computation. This is useful for preventing overflows.</span>
<span class="sd">            If not specified, stay the same as original Tensor. Default: None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape of the output Tensor is consistent with the input Tensor&#39;s.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        ValueError: If the axis is out of range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[3, 4, 6, 10], [1, 6, 7, 9], [4, 3, 8, 7], [1, 3, 7, 9]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; # case 1: along the axis 0</span>
<span class="sd">        &gt;&gt;&gt; y = ops.cumsum(x, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[ 3.  4.  6. 10.]</span>
<span class="sd">         [ 4. 10. 13. 19.]</span>
<span class="sd">         [ 8. 13. 21. 26.]</span>
<span class="sd">         [ 9. 16. 28. 35.]]</span>
<span class="sd">        &gt;&gt;&gt; # case 2: along the axis 1</span>
<span class="sd">        &gt;&gt;&gt; y = ops.cumsum(x, 1)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[ 3.  7. 13. 23.]</span>
<span class="sd">         [ 1.  7. 14. 23.]</span>
<span class="sd">         [ 4.  7. 15. 22.]</span>
<span class="sd">         [ 1.  4. 11. 20.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">dtype</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cumsum_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="sparse_segment_mean"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.sparse_segment_mean.html#mindspore.ops.sparse_segment_mean">[docs]</a><span class="k">def</span> <span class="nf">sparse_segment_mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes a Tensor such that :math:`output_i = \frac{\sum_j x_{indices[j]}}{N}` where mean is over :math:`j` such</span>
<span class="sd">    that :math:`segment\_ids[j] == i` and :math:`N` is the total number of values summed. If the mean is empty for</span>
<span class="sd">    a given segment ID :math:`i`, :math:`output[i] = 0`.</span>

<span class="sd">    Note:</span>
<span class="sd">        - On CPU, values in `segment_ids` are always validated to be sorted, and an error is thrown for indices that</span>
<span class="sd">          are not increasing. Moreover, values in `indices` are validated to be bounded, and an error is thrown when</span>
<span class="sd">          `indices` are out of range[0, x.shape[0]).</span>
<span class="sd">        - On GPU, this does not throw an error for unsorted `segment_ids` and out-of-bound `indices`. Out-of-order</span>
<span class="sd">          `segment_ids` result in safe but unspecified behavior, while out-of-range `indices` will be ignored.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): A Tensor, and its rank must be greater than or equal to 1.</span>
<span class="sd">        indices (Tensor): A 1-D Tensor, with int32 or int64 data type.</span>
<span class="sd">        segment_ids (Tensor): A 1-D Tensor, must have the same dtype as `indices`.</span>
<span class="sd">            Values should be sorted and can be repeated.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, whose dtype and rank is the same as `x`. The first dimension is equal to the value of the last element</span>
<span class="sd">        of `segment_ids` plus one, and the other dimensions are the same as those of `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x`, `indices` or `segment_ids` is not a Tensor.</span>
<span class="sd">        TypeError: If the dtype of `x` is not one of the following dtype: float16, float32, float64.</span>
<span class="sd">        TypeError: If the dtype of `indices` and `segment_ids` are not one of the following dtype: int32, int64.</span>
<span class="sd">        TypeError: If the dtype of `indices` and `segment_ids` are not the same.</span>
<span class="sd">        ValueError: If the shape of `x`, &#39;indices&#39; or `segment_ids` don&#39;t meet the parameter description.</span>
<span class="sd">        ValueError: If the size of &#39;indices&#39; and `segment_ids` are not the same.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[0, 1, 2], [1, 2, 3], [3, 6, 7]], dtype=mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor([0, 1, 2], dtype=mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; segment_ids = Tensor([1,2,2], dtype=mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; out = ops.sparse_segment_mean(x, indices, segment_ids)</span>
<span class="sd">        &gt;&gt;&gt; print(out)</span>
<span class="sd">        [[0. 0. 0.]</span>
<span class="sd">         [0. 1. 2.]</span>
<span class="sd">         [2. 4. 5.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">sparse_segment_mean_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">triu_indices</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the indices of the upper triangular part of a row by col matrix in a 2-by-N Tensor,</span>
<span class="sd">    where the first row contains row coordinates of all indices and the second row contains column coordinates.</span>
<span class="sd">    Indices are ordered based on rows and then columns.</span>

<span class="sd">    The upper triangular part of the matrix is defined as the elements on and above the diagonal.</span>

<span class="sd">    Note:</span>
<span class="sd">        When running on CUDA, row * col must be less than 2^59 to prevent overflow during calculation.</span>

<span class="sd">    Args:</span>
<span class="sd">        row (int): number of rows in the 2-D matrix.</span>
<span class="sd">        col (int): number of columns in the 2-D matrix.</span>
<span class="sd">        offset (int): diagonal offset from the main diagonal. Default: 0.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`): The specified type of output tensor.</span>
<span class="sd">            An optional data type of `mindspore.int32` and `mindspore.int64`. Default: `mindspore.int32`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - indices of the elements in upper triangular part of matrix. The type is specified by `dtype`.</span>
<span class="sd">          The shape of output is :math:`(2, triu\_size)`, where :math:`triu\_size` is the number of elements in the</span>
<span class="sd">          upper triangular matrix.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `row`, `col` or `offset` is not an int.</span>
<span class="sd">        TypeError: If `dtype` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If `row` or `col` &lt; 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; output = ops.triu_indices(5, 4, 2, mindspore.int64)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0 0 1]</span>
<span class="sd">         [2 3 3]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Int64</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">triu_indices_</span> <span class="o">=</span> <span class="n">TriuIndices</span><span class="p">(</span><span class="n">row</span><span class="o">=</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="n">col</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="n">offset</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">triu_indices_</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">atleast_2d</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reshapes `inputs` as arrays with at least two dimensions.</span>
<span class="sd">    Input tensor with two or more dimensions will be returned as is.</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs (Union[tensor, List[tensor]]): one or more input tensors.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or list of tensors, each with ``a.ndim &gt;= 2``.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input is not a tensor or a list of tensors.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x1 = np.ones((2, 3))</span>
<span class="sd">        &gt;&gt;&gt; x2 = np.ones(())</span>
<span class="sd">        &gt;&gt;&gt; x3 = np.ones(5)</span>
<span class="sd">        &gt;&gt;&gt; out = ops.atleast_2d([x1, x2, x3])</span>
<span class="sd">        &gt;&gt;&gt; print(out)</span>
<span class="sd">        [Tensor(shape=[2, 3], dtype=Float32, value=</span>
<span class="sd">        [[1.00000000e+000, 1.00000000e+000, 1.00000000e+000],</span>
<span class="sd">         [1.00000000e+000, 1.00000000e+000, 1.00000000e+000]]), Tensor(shape=[1, 1], dtype=Float32, value=</span>
<span class="sd">        [[1.00000000e+000]]), Tensor(shape=[1, 5], dtype=Float32, value=</span>
<span class="sd">        [[1.00000000e+000, 1.00000000e+000, 1.00000000e+000, 1.00000000e+000, 1.00000000e+000]])]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_expand</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="s2">&quot;expect Tensor or list of tensors, but got &quot;</span> <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">_expand</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">arr</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">vstack</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Stacks tensors in sequence vertically.</span>

<span class="sd">    This is equivalent to concatenation along the first axis.</span>
<span class="sd">    1-D tensors should firstly be reshaped to :math:`(1, N)`, and then be concatenated along the first axis.</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs (Union(List[tensor], Tuple[tensor])): A sequence of 1-D or 2-D tensors.</span>
<span class="sd">            The tensors must have the same shape along all but the first axis.</span>
<span class="sd">            1-D tensors must have the same shape.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, formed by stacking the given tensors, will be at least 3-D.</span>
<span class="sd">        The output shape is similar to the output of `numpy.vstack()` function.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `inputs` is not list.</span>
<span class="sd">        ValueError: If `inputs` is empty.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x1 = np.array([1, 2, 3])</span>
<span class="sd">        &gt;&gt;&gt; x2 = np.array([4, 5, 6])</span>
<span class="sd">        &gt;&gt;&gt; out = ops.vstack([x1, x2])</span>
<span class="sd">        &gt;&gt;&gt; print(out)</span>
<span class="sd">        [[1 2 3]</span>
<span class="sd">         [4 5 6]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;List or tuple of tensors are required, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">inputs</span><span class="p">:</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="s2">&quot;Inputs can not be empty&quot;</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
    <span class="n">trans_tup</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Tensor is required, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()(</span><span class="n">tensor</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">shape</span><span class="p">,)</span>
            <span class="n">ndim_diff</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">ndim_diff</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">ndim_diff</span> <span class="o">+</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">shape</span><span class="p">]</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()(</span><span class="n">tensor</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">shape</span><span class="p">))</span>
        <span class="n">trans_tup</span> <span class="o">+=</span> <span class="p">(</span><span class="n">tensor</span><span class="p">,)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">trans_tup</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Need at least one tensor to concatenate.&quot;</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="mi">0</span><span class="p">)(</span><span class="n">trans_tup</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>


<div class="viewcode-block" id="copysign"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.copysign.html#mindspore.ops.copysign">[docs]</a><span class="k">def</span> <span class="nf">copysign</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create a new floating-point tensor with the magnitude of `x` and the sign of `other`, element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[Tensor]): Values to change the sign of.</span>
<span class="sd">        other (Union[int, float, Tensor]): The sign of `other` is copied to `x`. If `x.shape != other.shape`,</span>
<span class="sd">            `other` must be broadcastable to the shape of `x` (which is also the shape of the output).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor. The dtype of the tensor is float.</span>
<span class="sd">        The values of `x` with the sign of `other`, the shape is the same as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of the input is not in the given types or</span>
<span class="sd">            the input can not be converted to tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; x = np.array([[0.3, -0.7], [0.5, 0.5]])</span>
<span class="sd">        &gt;&gt;&gt; other = np.array([[-0.4, 0.6], [0.4, -0.6]])</span>
<span class="sd">        &gt;&gt;&gt; out = ops.copysign(x, other)</span>
<span class="sd">        &gt;&gt;&gt; print(out)</span>
<span class="sd">        [[-0.3  0.7]</span>
<span class="sd">         [ 0.5 -0.5]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_broadcast_to_shape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Broadcasts x from current shape to shape&quot;&quot;&quot;</span>
        <span class="n">ndim_to</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">_expand</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ndim_to</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">_broadcast_to</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()(</span><span class="n">x</span><span class="p">),</span> <span class="n">shape</span><span class="p">,</span> <span class="n">ndim_to</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Tensor is expected, but got &quot;</span> <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
            <span class="s2">&quot;integer, float or Tensor is expected, but got &quot;</span> <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">other</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">_type_convert</span><span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>
    <span class="n">other</span> <span class="o">=</span> <span class="n">_broadcast_to_shape</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()(</span><span class="n">x</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()(</span><span class="n">x</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;copysign does not accept dtype bool.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()(</span><span class="n">other</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;copysign does not accept dtype bool.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()(</span><span class="n">x</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">complex64</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;copysign does not accept dtype complex64.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()(</span><span class="n">other</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">complex64</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;copysign does not accept dtype complex64.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()(</span><span class="n">x</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">complex128</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;copysign does not accept dtype complex128.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()(</span><span class="n">other</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">complex128</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;copysign does not accept dtype complex128.&quot;</span><span class="p">)</span>

    <span class="n">x_float</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">x</span>
        <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="k">else</span> <span class="n">x</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">pos_tensor</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Abs</span><span class="p">()(</span><span class="n">x_float</span><span class="p">)</span>
    <span class="n">less_zero</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Less</span><span class="p">()(</span><span class="n">other</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">P</span><span class="o">.</span><span class="n">Select</span><span class="p">()(</span><span class="n">less_zero</span><span class="p">,</span> <span class="n">P</span><span class="o">.</span><span class="n">Neg</span><span class="p">()(</span><span class="n">pos_tensor</span><span class="p">),</span> <span class="n">pos_tensor</span><span class="p">)</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_type_convert</span><span class="p">(</span><span class="n">force</span><span class="p">,</span> <span class="n">obj</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Convert type of `obj` to `force`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">force</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span>


<div class="viewcode-block" id="logsumexp"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.logsumexp.html#mindspore.ops.logsumexp">[docs]</a><span class="k">def</span> <span class="nf">logsumexp</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces a dimension of a tensor by calculating exponential for all elements in the dimension,</span>
<span class="sd">    then calculate logarithm of the sum.</span>

<span class="sd">    .. math::</span>

<span class="sd">        logsumexp(x) = \log(\sum(e^(x-x_{max}))) + x_{max}</span>

<span class="sd">    Note:</span>
<span class="sd">        The dimension of input Tensor on Ascend should be less than or equal to 8,</span>
<span class="sd">        and the dimension of input Tensor on CPU should be less than 8.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor. With float16 or float32 data type.</span>
<span class="sd">        axis (Union[int, tuple(int), list(int)]): The dimensions to reduce. Default: (), reduce all dimensions.</span>
<span class="sd">            Only constant value is allowed.</span>
<span class="sd">        keep_dims (bool): If True, keep these reduced dimensions and the length is 1.</span>
<span class="sd">            If False, don&#39;t keep these dimensions.</span>
<span class="sd">            Default : False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype as the `x`.</span>

<span class="sd">        - If axis is (), and keep_dims is False,</span>
<span class="sd">          the output is a 0-D tensor representing the sum of all elements in the input tensor.</span>
<span class="sd">        - If axis is int, set as 2, and keep_dims is False,</span>
<span class="sd">          the shape of output is :math:`(x_1, x_3, ..., x_R)`.</span>
<span class="sd">        - If axis is tuple(int), set as (2, 3), and keep_dims is False,</span>
<span class="sd">          the shape of output is :math:`(x_1, x_4, ..., x_R)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.randn(3, 4, 5, 6).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.logsumexp(x, 1, keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (3, 1, 5, 6)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_exp</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Exp</span><span class="p">)()</span>
    <span class="n">_reduce_sum</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">)(</span><span class="n">keep_dims</span><span class="p">)</span>
    <span class="n">_log</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Log</span><span class="p">)()</span>

    <span class="n">x_max</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">x_exp</span> <span class="o">=</span> <span class="n">_exp</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x_max</span><span class="p">)</span>
    <span class="n">x_sumexp</span> <span class="o">=</span> <span class="n">_reduce_sum</span><span class="p">(</span><span class="n">x_exp</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
    <span class="n">x_logsumexp</span> <span class="o">=</span> <span class="n">_log</span><span class="p">(</span><span class="n">x_sumexp</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">keep_dims</span><span class="p">:</span>
        <span class="n">x_max</span> <span class="o">=</span> <span class="n">x_max</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_logsumexp</span> <span class="o">+</span> <span class="n">x_max</span></div>


<div class="viewcode-block" id="amin"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.amin.html#mindspore.ops.amin">[docs]</a><span class="k">def</span> <span class="nf">amin</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(),</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces all dimensions of a tensor by returning the minimum value in `x`, by default. And also can</span>
<span class="sd">    reduce a dimension of `x` along specified `axis`.  `keep_dims` determines whether the dimensions of</span>
<span class="sd">    output and input are the same.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor[Number]): The input tensor. The dtype of the tensor to be reduced is number.</span>
<span class="sd">          :math:`(N,*)` where :math:`*` means, any number of additional dimensions, its rank should be less than 8.</span>
<span class="sd">        axis (Union[int, tuple(int), list(int)]): The dimensions to reduce. Default: (), reduce all dimensions.</span>
<span class="sd">          Only constant value is allowed. Assume the rank of `x` is r, and the value range is [-r,r)..</span>
<span class="sd">        keep_dims (bool): If true, keep these reduced dimensions and the length is 1.</span>
<span class="sd">                          If false, don&#39;t keep these dimensions. Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same data type as input tensor.</span>

<span class="sd">        - If `axis` is (), and `keep_dims` is False,</span>
<span class="sd">          the output is a 0-D tensor representing the product of all elements in the input tensor.</span>
<span class="sd">        - If `axis` is int, set as 1, and `keep_dims` is False,</span>
<span class="sd">          the shape of output is :math:`(x_0, x_2, ..., x_R)`.</span>
<span class="sd">        - If `axis` is tuple(int), set as (1, 2), and `keep_dims` is False,</span>
<span class="sd">          the shape of output is :math:`(x_0, x_3, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not one of the following: int, tuple or list.</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        ValueError: If `axis` is out of range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.randn(3, 4, 5, 6).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.amin(x, 1, keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; result = output.shape</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        (3, 1, 5, 6)</span>
<span class="sd">        &gt;&gt;&gt; # case 1: Reduces a dimension by the minimum value of all elements in the dimension.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2], [3, 3, 3, 3, 3, 3]],</span>
<span class="sd">        ...                      [[4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5], [6, 6, 6, 6, 6, 6]],</span>
<span class="sd">        ...                      [[7, 7, 7, 7, 7, 7], [8, 8, 8, 8, 8, 8], [9, 9, 9, 9, 9, 9]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.amin(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        1.0</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        ()</span>
<span class="sd">        &gt;&gt;&gt; # case 2: Reduces a dimension along axis 0.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.amin(x, 0, True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1. 1. 1. 1. 1. 1.]</span>
<span class="sd">          [2. 2. 2. 2. 2. 2.]</span>
<span class="sd">          [3. 3. 3. 3. 3. 3.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: Reduces a dimension along axis 1.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.amin(x, 1, True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1. 1. 1. 1. 1. 1.]]</span>
<span class="sd">         [[4. 4. 4. 4. 4. 4.]]</span>
<span class="sd">         [[7. 7. 7. 7. 7. 7.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 4: Reduces a dimension along axis 2.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.amin(x, 2, True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1.]</span>
<span class="sd">          [2.]</span>
<span class="sd">          [3.]]</span>
<span class="sd">         [[4.]</span>
<span class="sd">          [5.]</span>
<span class="sd">          [6.]]</span>
<span class="sd">         [[7.]</span>
<span class="sd">          [8.]</span>
<span class="sd">          [9.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ReduceMin</span><span class="p">)(</span><span class="n">keep_dims</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="amax"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.amax.html#mindspore.ops.amax">[docs]</a><span class="k">def</span> <span class="nf">amax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(),</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces all dimensions of a tensor by returning the maximum value in `x`, by default. And also can</span>
<span class="sd">    reduce a dimension of `x` along specified `axis`.  `keep_dims` determines whether the dimensions of</span>
<span class="sd">    output and input are the same.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor[Number]): The input tensor. The dtype of the tensor to be reduced is number.</span>
<span class="sd">          :math:`(N,*)` where :math:`*` means, any number of additional dimensions, its rank should be less than 8.</span>
<span class="sd">        axis (Union[int, tuple(int), list(int)]): The dimensions to reduce. Default: (), reduce all dimensions.</span>
<span class="sd">          Only constant value is allowed. Assume the rank of `x` is r, and the value range is [-r,r).</span>
<span class="sd">        keep_dims (bool): If true, keep these reduced dimensions and the length is 1.</span>
<span class="sd">                          If false, don&#39;t keep these dimensions. Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same data type as input tensor.</span>

<span class="sd">        - If `axis` is (), and `keep_dims` is False,</span>
<span class="sd">          the output is a 0-D tensor representing the product of all elements in the input tensor.</span>
<span class="sd">        - If `axis` is int, set as 1, and `keep_dims` is False,</span>
<span class="sd">          the shape of output is :math:`(x_0, x_2, ..., x_R)`.</span>
<span class="sd">        - If `axis` is tuple(int), set as (1, 2), and `keep_dims` is False,</span>
<span class="sd">          the shape of output is :math:`(x_0, x_3, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not one of the following: int, tuple or list.</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        ValueError: If `axis` is out of range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.randn(3, 4, 5, 6).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.amax(x, 1, keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; result = output.shape</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        (3, 1, 5, 6)</span>
<span class="sd">        &gt;&gt;&gt; # case 1: Reduces a dimension by the maximum value of all elements in the dimension.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2], [3, 3, 3, 3, 3, 3]],</span>
<span class="sd">        ...                      [[4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5], [6, 6, 6, 6, 6, 6]],</span>
<span class="sd">        ...                      [[7, 7, 7, 7, 7, 7], [8, 8, 8, 8, 8, 8], [9, 9, 9, 9, 9, 9]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.amax(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        9.0</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        ()</span>
<span class="sd">        &gt;&gt;&gt; # case 2: Reduces a dimension along axis 0.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.amax(x, 0, True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[7. 7. 7. 7. 7. 7.]</span>
<span class="sd">          [8. 8. 8. 8. 8. 8.]</span>
<span class="sd">          [9. 9. 9. 9. 9. 9.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: Reduces a dimension along axis 1.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.amax(x, 1, True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[3. 3. 3. 3. 3. 3.]]</span>
<span class="sd">         [[6. 6. 6. 6. 6. 6.]]</span>
<span class="sd">         [[9. 9. 9. 9. 9. 9.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 4: Reduces a dimension along axis 2.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.amax(x, 2, True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1.]</span>
<span class="sd">          [2.]</span>
<span class="sd">          [3.]]</span>
<span class="sd">         [[4.]</span>
<span class="sd">          [5.]</span>
<span class="sd">          [6.]]</span>
<span class="sd">         [[7.]</span>
<span class="sd">          [8.]</span>
<span class="sd">          [9.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ReduceMax</span><span class="p">)(</span><span class="n">keep_dims</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="mean"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.mean.html#mindspore.ops.mean">[docs]</a><span class="k">def</span> <span class="nf">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(),</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces all dimension of a tensor by averaging all elements in the dimension, by default.</span>
<span class="sd">    And reduce a dimension of `x` along the specified `axis`. `keep_dims`</span>
<span class="sd">    determines whether the dimensions of the output and input are the same.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor[Number]): The input tensor. The dtype of the tensor to be reduced is number.</span>
<span class="sd">          :math:`(N,*)` where :math:`*` means, any number of additional dimensions, its rank should be less than 8.</span>
<span class="sd">        axis (Union[int, tuple(int), list(int)]): The dimensions to reduce. Default: (), reduce all dimensions.</span>
<span class="sd">          Only constant value is allowed. Assume the rank of `x` is r, and the value range is [-r,r).</span>
<span class="sd">        keep_dims (bool): If true, keep these reduced dimensions and the length is 1.</span>
<span class="sd">                          If false, don&#39;t keep these dimensions. Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same data type as input tensor.</span>

<span class="sd">        - If `axis` is (), and `keep_dims` is False,</span>
<span class="sd">          the output is a 0-D tensor representing the product of all elements in the input tensor.</span>
<span class="sd">        - If `axis` is int, set as 1, and `keep_dims` is False,</span>
<span class="sd">          the shape of output is :math:`(x_0, x_2, ..., x_R)`.</span>
<span class="sd">        - If `axis` is tuple(int), set as (1, 2), and `keep_dims` is False,</span>
<span class="sd">          the shape of output is :math:`(x_0, x_3, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not one of the following: int, tuple or list.</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        ValueError: If `axis` is out of range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.randn(3, 4, 5, 6).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.mean(x, 1, keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; result = output.shape</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        (3, 1, 5, 6)</span>
<span class="sd">        &gt;&gt;&gt; # case 1: Reduces a dimension by averaging all elements in the dimension.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2]],</span>
<span class="sd">        ... [[4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5], [6, 6, 6, 6, 6, 6]],</span>
<span class="sd">        ... [[6, 6, 6, 6, 6, 6], [8, 8, 8, 8, 8, 8], [10, 10, 10, 10, 10, 10]]]),</span>
<span class="sd">        ... mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.mean(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        5.0</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        ()</span>
<span class="sd">        &gt;&gt;&gt; # case 2: Reduces a dimension along the axis 0</span>
<span class="sd">        &gt;&gt;&gt; output = ops.mean(x, 0, True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[4. 4. 4. 4. 4. 4.]</span>
<span class="sd">          [5. 5. 5. 5. 5. 5.]</span>
<span class="sd">          [6. 6. 6. 6. 6. 6.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: Reduces a dimension along the axis 1</span>
<span class="sd">        &gt;&gt;&gt; output = ops.mean(x, 1, True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[2. 2. 2. 2. 2. 2.]]</span>
<span class="sd">         [[5. 5. 5. 5. 5. 5.]]</span>
<span class="sd">         [[8. 8. 8. 8. 8. 8.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 4: Reduces a dimension along the axis 2</span>
<span class="sd">        &gt;&gt;&gt; output = ops.mean(x, 2, True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[ 2.]</span>
<span class="sd">          [ 2.]</span>
<span class="sd">          [ 2.]]</span>
<span class="sd">         [[ 4.]</span>
<span class="sd">          [ 5.]</span>
<span class="sd">          [ 6.]]</span>
<span class="sd">         [[ 6.]</span>
<span class="sd">          [ 8.]</span>
<span class="sd">          [10.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ReduceMean</span><span class="p">)(</span><span class="n">keep_dims</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="prod"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.prod.html#mindspore.ops.prod">[docs]</a><span class="k">def</span> <span class="nf">prod</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(),</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces a dimension of a tensor by multiplying all elements in the dimension, by default. And also can</span>
<span class="sd">    reduce a dimension of `x` along the axis. Determine whether the dimensions of the output and input are the same by</span>
<span class="sd">    controlling `keep_dims`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor[Number]): The input tensor. The dtype of the tensor to be reduced is number.</span>
<span class="sd">          :math:`(N,*)` where :math:`*` means, any number of additional dimensions, its rank should be less than 8.</span>
<span class="sd">        axis (Union[int, tuple(int), list(int)]): The dimensions to reduce. Default: (), reduce all dimensions.</span>
<span class="sd">          Only constant value is allowed. Assume the rank of `x` is r, and the value range is [-r,r).</span>
<span class="sd">        keep_dims (bool): If true, keep these reduced dimensions and the length is 1.</span>
<span class="sd">                          If false, don&#39;t keep these dimensions. Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same data type as input tensor.</span>

<span class="sd">        - If `axis` is (), and `keep_dims` is False,</span>
<span class="sd">          the output is a 0-D tensor representing the product of all elements in the input tensor.</span>
<span class="sd">        - If `axis` is int, set as 1, and `keep_dims` is False,</span>
<span class="sd">          the shape of output is :math:`(x_0, x_2, ..., x_R)`.</span>
<span class="sd">        - If `axis` is tuple(int), set as (1, 2), and `keep_dims` is False,</span>
<span class="sd">          the shape of output is :math:`(x_0, x_3, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not one of the following: int, tuple or list.</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        ValueError: If `axis` is out of range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.randn(3, 4, 5, 6).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.prod(x, 1, keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; result = output.shape</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        (3, 1, 5, 6)</span>
<span class="sd">        &gt;&gt;&gt; # case 1: Reduces a dimension by multiplying all elements in the dimension.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2], [3, 3, 3, 3, 3, 3]],</span>
<span class="sd">        ...                      [[4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5], [6, 6, 6, 6, 6, 6]],</span>
<span class="sd">        ...                      [[7, 7, 7, 7, 7, 7], [8, 8, 8, 8, 8, 8], [9, 9, 9, 9, 9, 9]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.prod(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        2.2833798e+33</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        ()</span>
<span class="sd">        &gt;&gt;&gt; # case 2: Reduces a dimension along axis 0.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.prod(x, 0, True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[ 28.  28.  28.  28.  28.  28.]</span>
<span class="sd">          [ 80.  80.  80.  80.  80.  80.]</span>
<span class="sd">          [162. 162. 162. 162. 162. 162.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: Reduces a dimension along axis 1.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.prod(x, 1, True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[  6.   6.   6.   6.   6.   6.]]</span>
<span class="sd">         [[120. 120. 120. 120. 120. 120.]]</span>
<span class="sd">         [[504. 504. 504. 504. 504. 504.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 4: Reduces a dimension along axis 2.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.prod(x, 2, True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1.00000e+00]</span>
<span class="sd">          [6.40000e+01]</span>
<span class="sd">          [7.29000e+02]]</span>
<span class="sd">         [[4.09600e+03]</span>
<span class="sd">          [1.56250e+04]</span>
<span class="sd">          [4.66560e+04]]</span>
<span class="sd">         [[1.17649e+05]</span>
<span class="sd">          [2.62144e+05]</span>
<span class="sd">          [5.31441e+05]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ReduceProd</span><span class="p">)(</span><span class="n">keep_dims</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="norm"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.norm.html#mindspore.ops.norm">[docs]</a><span class="k">def</span> <span class="nf">norm</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the matrix norm or vector norm of a given tensor.</span>

<span class="sd">    .. math::</span>
<span class="sd">        output = sum(abs(input)**p)**(1/p)</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Input tensor. The dtype must be float32 or float16.</span>
<span class="sd">        axis (Union[int, list, tuple]): Specifies which dimension or dimensions of input to calculate the norm across.</span>
<span class="sd">        p (int): The order of norm. Default: 2. `p` is greater than or equal to 0.</span>
<span class="sd">        keep_dims (bool): Whether the output tensors have dim retained or not. Default: False.</span>
<span class="sd">        epsilon (float): A value added to the denominator for numerical stability. Default: 1e-12.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype as `input`, which shape depends on the args axis. For example, if the size of input</span>
<span class="sd">        is (2, 3, 4), axis is [0, 1], Outputs&#39; shape will be (4,).</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not one of: float16, float32.</span>
<span class="sd">        TypeError: If `p` is not an int.</span>
<span class="sd">        TypeError: If `axis` is not an int, a tuple or a list.</span>
<span class="sd">        TypeError: If `axis` is a tuple or a list, but the element of `axis` is not an int.</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        TypeError: If `epsilon` is not a float.</span>
<span class="sd">        ValueError: If the element of `axis` is out of the range (-len(input.shape), len(input.shape)).</span>
<span class="sd">        ValueError: If the length of shape of `axis` is bigger than the length of shape of `input`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.norm(input_x, [0, 1], p=2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 9.165152 10.954452]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">lp_norm_inner</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">LpNorm</span><span class="p">)(</span><span class="n">axis</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">lp_norm_inner</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">lu_unpack</span><span class="p">(</span><span class="n">LU_data</span><span class="p">,</span> <span class="n">LU_pivots</span><span class="p">,</span> <span class="n">unpack_data</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">unpack_pivots</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Unpack the LU_data and LU_pivots from a LU factorization of a tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        LU_data (Tensor): The packed LU factorization data. A tensor of size [*, M, N], where * is batch</span>
<span class="sd">          dimensions, with data type int8, uint8, int16, int32, int64, float16, float32, float64. The dims of LU_data</span>
<span class="sd">          must be equal to or greater than 2.</span>
<span class="sd">        LU_pivots (Tensor): The packed LU factorization pivots. A tensor of size [*, min(M, N)], where * is</span>
<span class="sd">          batch dimensions, with data type int8, uint8, int16, int32, int64.</span>
<span class="sd">        unpack_data (bool): A flag indicating if the LU_data should be unpacked. If False, then the returned L and U</span>
<span class="sd">            are None. Default: True.</span>
<span class="sd">        unpack_pivots (bool): A flag indicating if the LU_pivots should be unpacked into a permutation matrix P. If</span>
<span class="sd">            False, then the returned P is None. Default: True.</span>

<span class="sd">    Returns:</span>
<span class="sd">        pivots (Tensor) - The permutation matrix of LU factorization. The shape is `[*, M, M]`, the dtype is</span>
<span class="sd">        same as `LU_data`.</span>
<span class="sd">        L (Tensor) - The L matrix  of LU factorization. The dtype is same as `LU_data`.</span>
<span class="sd">        U (Tensor) - The U matrix  of LU factorization. The dtype is same as `LU_data`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the dtype of `LU_data` is not one of the following: int8, uint8, int16, int32,</span>
<span class="sd">                   int64, float16, float32, float64.</span>
<span class="sd">        TypeError: If the dtype of `LU_pivots` is not one of the following: int8, uint8, int16, int32, int64.</span>
<span class="sd">        ValueError: If the dimension of `LU_data` is less than 2.</span>
<span class="sd">        ValueError: If the dimension of `LU_pivots` is less than 1.</span>
<span class="sd">        ValueError: If the size of the last dimension of LU_pivots is not equal to the minimum of the sizes of the last</span>
<span class="sd">                    two dimensions of LU_data.</span>
<span class="sd">        ValueError: If the batch dimensions of LU_data&#39;s does not match LU_pivots&#39;s batch dimensions.</span>
<span class="sd">        ValueError: On the CPU platform, if the value of `LU_pivots` are out of range[1, LU_data.shape[-2]).</span>
<span class="sd">        RuntimeError: On the Ascend platform, if the value of `LU_pivots` are out of range[1, LU_data.shape[-2]).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import functional as F</span>
<span class="sd">        &gt;&gt;&gt; LU_data = Tensor(np.array([[[-0.3806, -0.4872,  0.5536],</span>
<span class="sd">        ...                             [-0.1287,  0.6508, -0.2396],</span>
<span class="sd">        ...                             [ 0.2583,  0.5239,  0.6902]],</span>
<span class="sd">        ...                            [[ 0.6706, -1.1782,  0.4574],</span>
<span class="sd">        ...                             [-0.6401, -0.4779,  0.6701],</span>
<span class="sd">        ...                             [ 0.1015, -0.5363,  0.6165]]]), mstype.float64)</span>
<span class="sd">        &gt;&gt;&gt; LU_pivots = Tensor(np.array([[1, 3, 3],</span>
<span class="sd">        ...                              [2, 3, 3]]), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; pivots, L, U = F.lu_unpack(LU_data, LU_pivots, unpack_data, unpack_pivots)</span>
<span class="sd">        &gt;&gt;&gt; print(pivots)</span>
<span class="sd">        [[[1. 0. 0.]</span>
<span class="sd">          [0. 0. 1.]</span>
<span class="sd">          [0. 1. 0.]]</span>
<span class="sd">         [[0. 0. 1.]</span>
<span class="sd">          [1. 0. 0.]</span>
<span class="sd">          [0. 1. 0.]]]</span>
<span class="sd">        &gt;&gt;&gt; print(L)</span>
<span class="sd">        [[[ 1.       0.       0.]</span>
<span class="sd">          [-0.1287   1.       0.]</span>
<span class="sd">          [ 0.2583   0.5239   1.]]</span>
<span class="sd">         [[ 1.0000   0.       0.]</span>
<span class="sd">          [-0.6401   1.       0.]</span>
<span class="sd">          [ 0.1015  -0.5363   1.]]]</span>
<span class="sd">        &gt;&gt;&gt; print(U)</span>
<span class="sd">        [[[-0.3806  -0.4872   0.5536]</span>
<span class="sd">          [ 0.       0.6508  -0.2396]</span>
<span class="sd">          [ 0.       0.       0.6902]]</span>
<span class="sd">         [[ 0.6706  -1.1782   0.4574]</span>
<span class="sd">          [ 0.      -0.4779   0.6701]</span>
<span class="sd">          [ 0.       0.       0.6165]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">pivots</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">u</span> <span class="o">=</span> <span class="n">LuUnpack</span><span class="p">(</span><span class="n">LU_data</span><span class="p">,</span> <span class="n">LU_pivots</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">unpack_data</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">unpack_pivots</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">pivots</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">u</span>
        <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">u</span>
    <span class="k">if</span> <span class="n">unpack_pivots</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">pivots</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
    <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>


<div class="viewcode-block" id="renorm"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.renorm.html#mindspore.ops.renorm">[docs]</a><span class="k">def</span> <span class="nf">renorm</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">maxnorm</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Renormalizes the sub-tensors along dimension `dim`, and each sub-tensor&#39;s p-norm should not exceed the</span>
<span class="sd">    &#39;maxnorm&#39;. The values of current sub-tensor don&#39;t need change if the p-norm of the sub-tensor is less than</span>
<span class="sd">    `maxnorm`. Otherwise the sub-tensor needs to be modified to the original value of the corresponding position</span>
<span class="sd">    divided by the p-norm of the substensor and then multiplied by `maxnorm`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): A Tensor, types: float32 or float16.</span>
<span class="sd">        p (int): Power of norm calculation.</span>
<span class="sd">        dim (int): The dimension that expected to get the slice-tensor.</span>
<span class="sd">        maxnorm (float32): Max norm.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype and shape as input_x.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `p` is not int.</span>
<span class="sd">        TypeError: If dtype of `dim` is not int.</span>
<span class="sd">        TypeError: If dtype of `maxnorm` is not float32.</span>
<span class="sd">        ValueError: If the value of `p` less than 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = ops.renorm(x, p=1, dim=0, maxnorm=5.)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[1.       1.        1.        ]</span>
<span class="sd">        [1.6666666 1.6666666 1.6666666 ]</span>
<span class="sd">        [1.6666667 1.6666667 1.6666667 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">renorm_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Renorm</span><span class="p">)(</span><span class="n">p</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">maxnorm</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">renorm_</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_attr_dtype</span><span class="p">(</span><span class="n">param_name</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">,</span> <span class="n">allow_dtypes</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">):</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="n">param_name</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">,</span> <span class="n">allow_dtypes</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">)</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_positive_float</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="n">arg_name</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">):</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_float</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="n">arg_name</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">)</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_int_range</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="n">lower_limit</span><span class="p">,</span> <span class="n">upper_limit</span><span class="p">,</span> <span class="n">arg_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prim_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="n">lower_limit</span><span class="p">,</span> <span class="n">upper_limit</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span> <span class="n">arg_name</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>


<div class="viewcode-block" id="gumbel_softmax"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.gumbel_softmax.html#mindspore.ops.gumbel_softmax">[docs]</a><span class="k">def</span> <span class="nf">gumbel_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hard</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the samples from the Gumbel-Softmax distribution and optionally discretizes. If `hard = True`, the returned</span>
<span class="sd">    samples will be one-hot, otherwise it will be probability distributions that sum to 1 across `dim`.</span>

<span class="sd">    Args:</span>
<span class="sd">        logits (Tensor): Unnormalized log probabilities. The data type must be float16 or float32.</span>
<span class="sd">        tau (float): The scalar temperature, which is a positive number. Default: 1.0.</span>
<span class="sd">        hard (bool): if `True`, the returned samples will be discretized as one-hot vectors, but will be differentiated</span>
<span class="sd">          as if it is the soft sample in autograd. Default: False.</span>
<span class="sd">        dim (int): Dim for softmax to compute. Default: -1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype and shape as `logits`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `logits` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `logits` is not one of: float16, float32.</span>
<span class="sd">        TypeError: If `tau` is not an float.</span>
<span class="sd">        TypeError: If `hard` is not a bool.</span>
<span class="sd">        TypeError: If `dim` is not a int.</span>
<span class="sd">        ValueError: If If `tau` is not positive.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.gumbel_softmax(input_x, 1.0, True, -1)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 3)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;The input logits must be tensor&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">logits</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For gumbel_softmax, the 0-D input is not supported.&quot;</span><span class="p">)</span>
    <span class="n">logits_dtype</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">)()(</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">_check_input_dtype</span><span class="p">(</span><span class="s2">&quot;logits&quot;</span><span class="p">,</span> <span class="n">logits_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="s2">&quot;gumbel_softmax&quot;</span><span class="p">)</span>
    <span class="n">_check_attr_dtype</span><span class="p">(</span><span class="s2">&quot;tau&quot;</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="s2">&quot;gumbel_softmax&quot;</span><span class="p">)</span>
    <span class="n">_check_attr_dtype</span><span class="p">(</span><span class="s2">&quot;hard&quot;</span><span class="p">,</span> <span class="n">hard</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="s2">&quot;gumbel_softmax&quot;</span><span class="p">)</span>
    <span class="n">_check_attr_dtype</span><span class="p">(</span><span class="s2">&quot;dim&quot;</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="s2">&quot;gumbel_softmax&quot;</span><span class="p">)</span>
    <span class="n">_check_positive_float</span><span class="p">(</span><span class="n">tau</span><span class="p">,</span> <span class="s2">&quot;tau&quot;</span><span class="p">,</span> <span class="s2">&quot;gumbel_softmax&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">hard</span><span class="p">:</span>
        <span class="n">_check_int_range</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="s1">&#39;dim&#39;</span><span class="p">,</span> <span class="s2">&quot;gumbel_softmax&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">_check_int_range</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="s1">&#39;dim&#39;</span><span class="p">,</span> <span class="s2">&quot;gumbel_softmax&quot;</span><span class="p">)</span>

    <span class="n">log_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Log</span><span class="p">)()</span>
    <span class="n">const_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ScalarToTensor</span><span class="p">)()</span>

    <span class="n">sample_shape</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">)()(</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">uniform</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">,</span> <span class="n">const_op</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">const_op</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
    <span class="n">uniform</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">)()(</span><span class="n">uniform</span><span class="p">,</span> <span class="n">logits_dtype</span><span class="p">)</span>
    <span class="n">gumbel</span> <span class="o">=</span> <span class="n">neg_tensor</span><span class="p">(</span><span class="n">log_op</span><span class="p">(</span><span class="n">neg_tensor</span><span class="p">(</span><span class="n">log_op</span><span class="p">(</span><span class="n">uniform</span><span class="p">))))</span>
    <span class="n">gumbel</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span> <span class="o">+</span> <span class="n">gumbel</span><span class="p">)</span> <span class="o">/</span> <span class="n">tau</span>
    <span class="n">y_soft</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Softmax</span><span class="p">)(</span><span class="n">dim</span><span class="p">)(</span><span class="n">gumbel</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">hard</span><span class="p">:</span>
        <span class="n">index</span> <span class="o">=</span> <span class="n">y_soft</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="n">y_hard</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">OneHot</span><span class="p">)(</span><span class="n">dim</span><span class="p">)(</span><span class="n">index</span><span class="p">,</span> <span class="n">sample_shape</span><span class="p">[</span><span class="n">dim</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">logits_dtype</span><span class="p">),</span>
                                                <span class="n">Tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">logits_dtype</span><span class="p">))</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">y_hard</span> <span class="o">-</span> <span class="n">ops</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">y_soft</span><span class="p">)</span> <span class="o">+</span> <span class="n">y_soft</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">y_soft</span>
    <span class="k">return</span> <span class="n">ret</span></div>


<span class="k">def</span> <span class="nf">stft</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_fft</span><span class="p">,</span> <span class="n">hop_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">win_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">center</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
         <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;REFLECT&quot;</span><span class="p">,</span> <span class="n">normalized</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">onesided</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_complex</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    STFTs can be used as a way of quantifying the change</span>
<span class="sd">    of a nonstationary signal’s frequency and phase content over time.</span>
<span class="sd">    Ignoring the optional batch dimension, this method computes the following expression:</span>
<span class="sd">    math:`X[\omega, m]=\sum_{k=0}^{\text {win_length-1 }}</span>
<span class="sd">    \text { window }[k] \text { input }[m \times \text { hop_length }+</span>
<span class="sd">    k] \exp \left(-j \frac{2 \pi \cdot \omega k}{\text { win_length }}\right)`</span>
<span class="sd">    where m is the index of the sliding window, and</span>
<span class="sd">    math:`\omegaω` is the frequency math:`0 \leq \omega &lt; \text{n\_fft}0≤ω&lt;n_fft`</span>
<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Time sequence of stft, must be either a 1-D time tensor or a 2-D tensor.</span>
<span class="sd">        n_fft (int): The size of Fourier transform.</span>
<span class="sd">        hop_length (int, optional): The distance between neighboring sliding window</span>
<span class="sd">            frames. Default: ``None`` (treated as equal to ``floor(n_fft / 4)``).</span>
<span class="sd">        win_length (int, optional): the size of window frame and STFT filter.</span>
<span class="sd">            Default: ``None``  (treated as equal to :attr:`n_fft`).</span>
<span class="sd">        window (Tensor, optional): the optional window function.</span>
<span class="sd">            Default: ``None`` (treated as window of all :math:`1` s).</span>
<span class="sd">        center (bool, optional): whether to pad :attr:`input` on both sides.</span>
<span class="sd">            Default: ``True``.</span>
<span class="sd">        pad_mode (string, optional): controls the padding method used when</span>
<span class="sd">            :attr:`center` is ``True``. Default: ``&quot;REFLECT&quot;``.</span>
<span class="sd">        normalized (bool, optional): controls whether to return the normalized STFT results</span>
<span class="sd">             Default: ``False``.</span>
<span class="sd">        onesided (bool, optional): controls whether to return half of results to</span>
<span class="sd">            avoid redundancy for real inputs.</span>
<span class="sd">            Default: ``True`` for real :attr:`input` and :attr:`window`, ``False`` otherwise.</span>
<span class="sd">        return_complex (bool, optional): whether to return a complex tensor, or</span>
<span class="sd">            a real tensor with an extra last dimension for the real and</span>
<span class="sd">            imaginary components.</span>
<span class="sd">            Default: ``True`` for complex :attr:`input` or :attr:`window`, ``False`` otherwise.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">        - **output** (Tensor) - A tensor containing the STFT result with shape described above.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the x is not a tensor.</span>
<span class="sd">        ValueError: If x arguments have values not specified above.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = ms.Tensor(np.random.rand(2,7192), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.stft(n_fft=64, x=x)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 33, 450, 2)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">hop_length</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">hop_length</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">n_fft</span> <span class="o">/</span> <span class="mi">4</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">win_length</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">win_length</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">n_fft</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">window</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">window</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Ones</span><span class="p">()((</span><span class="n">win_length</span><span class="p">,),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_is_complex</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">dtype</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">complex64</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">complex128</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">onesided</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">onesided</span> <span class="o">=</span> <span class="p">(</span><span class="ow">not</span> <span class="n">_is_complex</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="n">_is_complex</span><span class="p">(</span><span class="n">window</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">return_complex</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">return_complex</span> <span class="o">=</span> <span class="n">_is_complex</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="ow">or</span> <span class="n">_is_complex</span><span class="p">(</span><span class="n">window</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">center</span><span class="p">:</span>
        <span class="n">_check_attr_dtype</span><span class="p">(</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">center</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="s2">&quot;stft&quot;</span><span class="p">)</span>
        <span class="n">signal_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">pad</span> <span class="o">=</span> <span class="n">n_fft</span> <span class="o">//</span> <span class="mi">2</span>
        <span class="k">if</span> <span class="n">signal_dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">Pad</span><span class="p">(((</span><span class="n">pad</span><span class="p">,</span> <span class="n">pad</span><span class="p">),),</span> <span class="n">pad_mode</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">signal_dim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">Pad</span><span class="p">(((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="n">pad</span><span class="p">)),</span> <span class="n">pad_mode</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Expected a 1-D tensor or a 2-D tensor, but got </span><span class="si">{</span><span class="n">signal_dim</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">stft_</span> <span class="o">=</span> <span class="n">STFT</span><span class="p">(</span><span class="n">n_fft</span><span class="p">,</span> <span class="n">hop_length</span><span class="p">,</span> <span class="n">win_length</span><span class="p">,</span>
                 <span class="n">normalized</span><span class="p">,</span> <span class="n">onesided</span><span class="p">,</span> <span class="n">return_complex</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">stft_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">window</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_same_type</span><span class="p">(</span><span class="n">dtype1</span><span class="p">,</span> <span class="n">dtype2</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">dtype1</span> <span class="o">==</span> <span class="n">dtype2</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_max</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns the maximum value.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">max</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_min</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns the minimum value.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">min</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_infer_shape_rem</span><span class="p">(</span><span class="n">shape1</span><span class="p">,</span> <span class="n">shape2</span><span class="p">,</span> <span class="n">ndim1</span><span class="p">,</span> <span class="n">ndim2</span><span class="p">,</span> <span class="n">transpose_b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Infers the shape of the last two dimensions after performing matmul.&quot;&quot;&quot;</span>
    <span class="n">shape_rem</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">ndim1</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">shape_rem</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">shape1</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">transpose_b</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">ndim2</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">shape_rem</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">shape2</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">ndim1</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">shape_rem</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">shape2</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">shape_rem</span><span class="p">)</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_matmul_shapes</span><span class="p">(</span><span class="n">shape1</span><span class="p">,</span> <span class="n">shape2</span><span class="p">,</span> <span class="n">prim_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Checks shape1 and shape2 are valid to perform matmul, and returns output shape after broadcasting.&quot;&quot;&quot;</span>
    <span class="n">msg_prefix</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="n">prim_name</span><span class="si">}</span><span class="s2">&#39;, the&quot;</span> <span class="k">if</span> <span class="n">prim_name</span> <span class="k">else</span> <span class="s2">&quot;The&quot;</span>
    <span class="n">ndim1</span><span class="p">,</span> <span class="n">ndim2</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape1</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape2</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ndim1</span> <span class="o">&lt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">ndim2</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">msg_prefix</span><span class="si">}</span><span class="s2"> dimension of input operands must be at least 1, but got &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;the length of shape1: </span><span class="si">{</span><span class="n">ndim1</span><span class="si">}</span><span class="s2">, the length of shape2: </span><span class="si">{</span><span class="n">ndim2</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ndim2</span> <span class="o">&gt;=</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">shape1</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">shape2</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">msg_prefix</span><span class="si">}</span><span class="s2"> shape1[-1] must be equal to shape2[-2] when the length of shape2 &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;is greater than or equal to 2, but got shape1[-1]: </span><span class="si">{</span><span class="n">shape1</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">, &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;shape2[-2]: </span><span class="si">{</span><span class="n">shape2</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">shape_out</span> <span class="o">=</span> <span class="n">deque</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">items</span> <span class="ow">in</span> <span class="n">zip_longest</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">shape1</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]),</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">shape2</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]),</span> <span class="n">fillvalue</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">max_size</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">items</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">items</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">item</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_size</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">msg_prefix</span><span class="si">}</span><span class="s2"> operands could not be broadcast together with shape1 </span><span class="si">{</span><span class="n">shape1</span><span class="si">}</span><span class="s2"> and &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;shape2 </span><span class="si">{</span><span class="n">shape2</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">shape_out</span><span class="o">.</span><span class="n">appendleft</span><span class="p">(</span><span class="n">max_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">shape_out</span><span class="p">)</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_tile_size</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">out_shape</span><span class="p">,</span> <span class="n">ndim</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns tile_size such that shape*tile_size = out_shape&quot;&quot;&quot;</span>
    <span class="n">size</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">ndim</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">out_shape</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">j</span><span class="p">:</span>
            <span class="n">size</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">j</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_need_broadcast</span><span class="p">(</span><span class="n">shape1</span><span class="p">,</span> <span class="n">shape2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns True if broadcast is necessary for batchmatmul.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">shape1</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">!=</span> <span class="n">shape2</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_input_1d</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">func_name</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">func_name</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">param_name</span><span class="si">}</span><span class="s2"> should be 1d, but got shape </span><span class="si">{</span><span class="n">input_shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">True</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_input_2d</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">func_name</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">func_name</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">param_name</span><span class="si">}</span><span class="s2"> should be 2d, but got shape </span><span class="si">{</span><span class="n">input_shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">True</span>


<span class="k">def</span> <span class="nf">_expand</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ndim</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Expand x to ndim from axis, which can be 0 or -1.&quot;&quot;&quot;</span>
    <span class="n">rank_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Rank</span><span class="p">)()</span>
    <span class="n">expand_dims_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">)()</span>
    <span class="k">while</span> <span class="n">rank_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">ndim</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">expand_dims_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>


<span class="k">def</span> <span class="nf">_broadcast_to</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">shape_cur</span><span class="p">,</span> <span class="n">shape_to</span><span class="p">,</span> <span class="n">ndim_to</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Broadcasts x from shape_cur to shape_to.&quot;&quot;&quot;</span>
    <span class="n">tile_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Tile</span><span class="p">)()</span>
    <span class="n">size</span> <span class="o">=</span> <span class="n">_tile_size</span><span class="p">(</span><span class="n">shape_cur</span><span class="p">,</span> <span class="n">shape_to</span><span class="p">,</span> <span class="n">ndim_to</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tile_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>


<div class="viewcode-block" id="matmul"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.matmul.html#mindspore.ops.matmul">[docs]</a><span class="k">def</span> <span class="nf">matmul</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the matrix product of two tensors.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>
<span class="sd">        On GPU, the supported dtypes are np.float16 and np.float32.</span>
<span class="sd">        On CPU, the supported dtypes are np.float16 and np.float32.</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Tensor): Input tensor, scalar not allowed.</span>
<span class="sd">          The last dimension of `x1` must be the same size as the second last dimension of `x2`.</span>
<span class="sd">          And the shape of x1 and x2 could be broadcast.</span>
<span class="sd">        x2 (Tensor): Input tensor, scalar not allowed.</span>
<span class="sd">          The last dimension of `x1` must be the same size as the second last dimension of `x2`.</span>
<span class="sd">          And the shape of x1 and x2 could be broadcast.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, the matrix product of the inputs. This is a scalar only</span>
<span class="sd">        when both `x1`, `x2` are 1-d vectors.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the last dimension of `x1` is not the same size as the</span>
<span class="sd">            second-to-last dimension of `x2`, or if a scalar value is passed in.</span>
<span class="sd">        ValueError: If the shape of `x1` and `x2` could not broadcast together.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # case 1 : Reasonable application of broadcast mechanism</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.arange(2*3*4).reshape(2, 3, 4), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.arange(4*5).reshape(4, 5), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.matmul(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[  70.   76.   82.   88.   94.]</span>
<span class="sd">        [ 190.  212.  234.  256.  278.]</span>
<span class="sd">        [ 310.  348.  386.  424.  462.]]</span>
<span class="sd">        [[ 430.  484.  538.  592.  646.]</span>
<span class="sd">        [ 550.  620.  690.  760.  830.]</span>
<span class="sd">        [ 670.  756.  842.  928. 1014.]]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 3, 5)</span>
<span class="sd">        &gt;&gt;&gt; # case 2 : the rank of `x1` is 1</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.ones([1, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.ones([2,]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.matmul(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2.]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1,)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dtype_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">)()</span>
    <span class="n">rank_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Rank</span><span class="p">)()</span>
    <span class="n">shape_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">)()</span>
    <span class="n">reshape_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">)()</span>

    <span class="n">dtype1</span> <span class="o">=</span> <span class="n">dtype_op</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
    <span class="n">dtype2</span> <span class="o">=</span> <span class="n">dtype_op</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">dtype1</span><span class="p">,</span> <span class="n">dtype2</span><span class="p">):</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">x1</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">x2</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="n">ndim1_orig</span><span class="p">,</span> <span class="n">ndim2_orig</span> <span class="o">=</span> <span class="n">rank_op</span><span class="p">(</span><span class="n">x1</span><span class="p">),</span> <span class="n">rank_op</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
    <span class="n">shape1_orig</span><span class="p">,</span> <span class="n">shape2_orig</span> <span class="o">=</span> <span class="n">shape_op</span><span class="p">(</span><span class="n">x1</span><span class="p">),</span> <span class="n">shape_op</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
    <span class="n">transpose_b</span> <span class="o">=</span> <span class="n">ndim2_orig</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="n">shape_backbone</span> <span class="o">=</span> <span class="n">_check_matmul_shapes</span><span class="p">(</span><span class="n">shape1_orig</span><span class="p">,</span> <span class="n">shape2_orig</span><span class="p">,</span> <span class="s1">&#39;matmul&#39;</span><span class="p">)</span>
    <span class="c1"># infers the shape of the output</span>
    <span class="n">shape_out</span> <span class="o">=</span> <span class="n">shape_backbone</span> <span class="o">+</span> <span class="n">_infer_shape_rem</span><span class="p">(</span><span class="n">shape1_orig</span><span class="p">,</span> <span class="n">shape2_orig</span><span class="p">,</span>
                                                  <span class="n">ndim1_orig</span><span class="p">,</span> <span class="n">ndim2_orig</span><span class="p">,</span> <span class="n">transpose_b</span><span class="p">)</span>

    <span class="n">_matmul</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">MatMul</span><span class="p">)(</span><span class="kc">False</span><span class="p">,</span> <span class="n">transpose_b</span><span class="p">)</span>
    <span class="n">_batch_matmul</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">BatchMatMul</span><span class="p">)(</span><span class="kc">False</span><span class="p">,</span> <span class="n">transpose_b</span><span class="p">)</span>

    <span class="n">x1</span> <span class="o">=</span> <span class="n">_expand</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="n">_expand</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">rank_op</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">rank_op</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">x1</span> <span class="o">=</span> <span class="n">reshape_op</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape1_orig</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">_matmul</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># broadcasts x1.shape[:-2] with x2.shape[:-2]</span>
        <span class="n">ndim_aligned</span> <span class="o">=</span> <span class="n">_max</span><span class="p">(</span><span class="n">ndim1_orig</span><span class="p">,</span> <span class="n">ndim2_orig</span><span class="p">)</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">_expand</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">ndim_aligned</span><span class="p">)</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">_expand</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">ndim_aligned</span><span class="p">)</span>
        <span class="n">shape1_aligned</span><span class="p">,</span> <span class="n">shape2_aligned</span> <span class="o">=</span> <span class="n">shape_op</span><span class="p">(</span><span class="n">x1</span><span class="p">),</span> <span class="n">shape_op</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">_broadcast_to</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">shape1_aligned</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">shape_backbone</span><span class="p">,</span> <span class="n">ndim_aligned</span><span class="p">)</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">_broadcast_to</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">shape2_aligned</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">shape_backbone</span><span class="p">,</span> <span class="n">ndim_aligned</span><span class="p">)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">_batch_matmul</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">reshape_op</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">shape_out</span><span class="p">)</span></div>


<div class="viewcode-block" id="bmm"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bmm.html#mindspore.ops.bmm">[docs]</a><span class="k">def</span> <span class="nf">bmm</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">mat2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes matrix multiplication between two tensors by batch.</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{output}[..., :, :] = \text{matrix}(input_x[..., :, :]) * \text{matrix}(mat2[..., :, :])</span>

<span class="sd">    The dim of `input_x` can not be less than `3` and the dim of `mat2` can not be less than `2`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The first tensor to be multiplied. The shape of the tensor is :math:`(*B, N, C)`,</span>
<span class="sd">            where :math:`*B` represents the batch size which can be multidimensional, :math:`N` and :math:`C` are the</span>
<span class="sd">            size of the last two dimensions.</span>
<span class="sd">        mat2 (Tensor): The second tensor to be multiplied. The shape of the tensor is :math:`(*B, C, M)`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape of the output tensor is :math:`(*B, N, M)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If dim of `input_x` is less than `3` or dim of `mat2` is less than `2`.</span>
<span class="sd">        ValueError: If the length of the third dim of `input_x` is not equal to</span>
<span class="sd">            the length of the second dim of `mat2`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones(shape=[2, 4, 1, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; mat2 = Tensor(np.ones(shape=[2, 4, 3, 4]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bmm(input_x, mat2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[3. 3. 3. 3.]]</span>
<span class="sd">          [[3. 3. 3. 3.]]</span>
<span class="sd">          [[3. 3. 3. 3.]]</span>
<span class="sd">          [[3. 3. 3. 3.]]]</span>
<span class="sd">         [[[3. 3. 3. 3.]]</span>
<span class="sd">          [[3. 3. 3. 3.]]</span>
<span class="sd">          [[3. 3. 3. 3.]]</span>
<span class="sd">          [[3. 3. 3. 3.]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mat2</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For bmm op, inputs input_x and mat2 must be all tensors.&quot;</span><span class="p">)</span>

    <span class="n">bmm_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">BatchMatMul</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">bmm_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">mat2</span><span class="p">)</span></div>


<div class="viewcode-block" id="baddbmm"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.baddbmm.html#mindspore.ops.baddbmm">[docs]</a><span class="k">def</span> <span class="nf">baddbmm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">batch1</span><span class="p">,</span> <span class="n">batch2</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs a batch matrix-matrix product of matrices in batch1 and batch2. input is added to the final result.</span>
<span class="sd">    The formula is defined as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{out}_{i} = \beta \text{input}_{i} + \alpha (\text{batch1}_{i} \mathbin{@} \text{batch2}_{i})</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The tensor to be added.</span>
<span class="sd">        batch1 (Tensor): The first batch of matrices to be multiplied.</span>
<span class="sd">        batch2 (Tensor): The second batch of matrices to be multiplied.</span>
<span class="sd">        beta (Union[float, int], optional): multiplier for input. The default is 1.</span>
<span class="sd">        alpha (Union[float, int], optional): multiplier for `batch1 @ batch2`. The default is 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the output tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: The type of `x`, `batch1`, `batch2` is not Tensor.</span>
<span class="sd">        TypeError: The types of `x`, `batch1`, `batch2` are different.</span>
<span class="sd">        TypeError: For inputs of type FloatTensor or DoubleTensor, \</span>
<span class="sd">                    arguments beta and alpha not be real numbers, otherwise not be integers.</span>
<span class="sd">        TypeError: For Baddbmm, attributes alpha and beta are not real numbers</span>
<span class="sd">        ValueError: If `batch1` and `batch2` are not 3-D tensors.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.ones([1, 3, 3]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; batch1 = Tensor(np.ones([1, 3, 4]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; batch2 = Tensor(np.ones([1, 4, 3]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.baddbmm(input, batch1, batch2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[5. 5. 5.]</span>
<span class="sd">          [5. 5. 5.]</span>
<span class="sd">          [5. 5. 5.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dtypeop</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">)()</span>
    <span class="n">bmmop</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">BatchMatMul</span><span class="p">)(</span><span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch1</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch2</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For Baddbmm, inputs must be all tensors.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">3</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For batch1 and batch2 must be 3-D tensors each containing the same number of matrices, &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;but got length of batch1:&#39;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">batch1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="si">}</span><span class="s2">&#39;, length of batch2:&#39;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">batch2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="si">}</span><span class="s2">&#39;.&quot;</span><span class="p">)</span>
    <span class="n">input_dtype</span> <span class="o">=</span> <span class="n">dtypeop</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">input_dtype</span> <span class="o">==</span> <span class="n">dtypeop</span><span class="p">(</span><span class="n">batch1</span><span class="p">)</span> <span class="ow">and</span> <span class="n">input_dtype</span> <span class="o">==</span> <span class="n">dtypeop</span><span class="p">(</span><span class="n">batch2</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For Baddbmm, the inputs should be the same dtype.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">input_dtype</span> <span class="ow">in</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">))):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For attributes alpha and beta should be real numbers.&quot;</span><span class="p">)</span>
        <span class="n">check_is_number</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">))</span>
        <span class="n">check_is_number</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="nb">int</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For inputs of type not FloatTensor or DoubleTensor, &quot;</span>
                            <span class="s2">&quot;arguments beta and alpha must be integers.&quot;</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">bmmop</span><span class="p">(</span><span class="n">batch1</span><span class="p">,</span> <span class="n">batch2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">y</span></div>


<div class="viewcode-block" id="log2"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.log2.html#mindspore.ops.log2">[docs]</a><span class="k">def</span> <span class="nf">log2</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a new tensor with the logarithm to the base 2 of the elements of input.</span>

<span class="sd">    .. math::</span>
<span class="sd">        y_i = log_2(x_i)</span>

<span class="sd">    .. warning::</span>
<span class="sd">        If the input value of operator Log2 is within the range (0, 0.01] or [0.95, 1.05], the output accuracy may</span>
<span class="sd">        be affacted. If the input value of operator Log2 is less than or equal to 0, it will not raise Error.</span>

<span class="sd">    .. note::</span>
<span class="sd">        The dimension of the input Tensor on Ascend should be less than or equal to 8, and the dimension of the</span>
<span class="sd">        input Tensor on the CPU or GPU should be less than 8.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input Tensor of any dimension. The value must be greater than 0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16 or float32 or float64 on CPU and GPU, if dtype of `x` is not float16</span>
<span class="sd">            or float32 on Ascend.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([2, 4, 8]).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.log2(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 2. 3.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">dtype_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">)()</span>

    <span class="n">x_dtype</span> <span class="o">=</span> <span class="n">dtype_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">denominator</span> <span class="o">=</span> <span class="n">log_</span><span class="p">(</span><span class="n">_make_tensor</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">))</span>
    <span class="n">frac_log</span> <span class="o">=</span> <span class="n">log_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">frac_log</span> <span class="o">/</span> <span class="n">denominator</span>
    <span class="k">return</span> <span class="n">output</span></div>


<span class="k">def</span> <span class="nf">arrange</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">lists</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">lists</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">lists</span>


<span class="k">def</span> <span class="nf">rot90</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">dims</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.</span>
<span class="sd">    Rotation direction is from the first towards the second axis if k &gt; 0,</span>
<span class="sd">    and from the second towards the first for k &lt; 0.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input tensor.</span>
<span class="sd">        k (int): Number of times to rotate.</span>
<span class="sd">        dims (a list or tuple): Axis to rotate.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `k` is not integer.</span>
<span class="sd">        TypeError: If `dims` is not tuple of integers or list of ints.</span>
<span class="sd">        ValueError: If the length of `dims` is not `2`.</span>
<span class="sd">        ValueError: If any dims is out of range.</span>
<span class="sd">        RuntimeError: If rotation dims are not different.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[0, 1], [2, 3]])).astype(np.float32)</span>
<span class="sd">        &gt;&gt;&gt; k = 1</span>
<span class="sd">        &gt;&gt;&gt; dims = [0, 1]</span>
<span class="sd">        &gt;&gt;&gt; output = ops.rot90(x, k, dims)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 3.]</span>
<span class="sd">        [0. 2.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;the input x must be Tensor!&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;the input k must be int!&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;the input dims must be list or tuple!&quot;</span><span class="p">)</span>

    <span class="n">total_dims</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span>
    <span class="n">total_rot_dims</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dims</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">total_rot_dims</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;total rotation dims must be 2.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">or</span> <span class="p">(</span><span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="n">total_dims</span> <span class="ow">or</span> <span class="p">(</span><span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">==</span> <span class="n">total_dims</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;rotation dims must be different.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">total_dims</span> <span class="ow">or</span> <span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="o">-</span><span class="n">total_dims</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Rotation dim0 out of range, dim0 = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="k">if</span> <span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">total_dims</span> <span class="ow">or</span> <span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="o">-</span><span class="n">total_dims</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Rotation dim1 out of range, dim1 = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

    <span class="n">k</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span> <span class="o">+</span> <span class="p">(</span><span class="n">k</span> <span class="o">%</span> <span class="mi">4</span><span class="p">))</span> <span class="o">%</span> <span class="mi">4</span>

    <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span>
        <span class="k">return</span> <span class="n">out</span>
    <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">op1</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReverseV2</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">op1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">op2</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReverseV2</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">op2</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="n">axes_list</span> <span class="o">=</span> <span class="n">arrange</span><span class="p">(</span><span class="n">total_dims</span><span class="p">)</span>
    <span class="p">(</span><span class="n">axes_list</span><span class="p">[</span><span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="n">axes_list</span><span class="p">[</span><span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span> <span class="o">=</span> <span class="p">(</span><span class="n">axes_list</span><span class="p">[</span><span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span>
                                                <span class="n">axes_list</span><span class="p">[</span><span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>

    <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReverseV2</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">axes_list</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">axes_list</span><span class="p">)</span>
        <span class="n">op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReverseV2</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>


<div class="viewcode-block" id="roll"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.roll.html#mindspore.ops.roll">[docs]</a><span class="k">def</span> <span class="nf">roll</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">shifts</span><span class="p">,</span> <span class="n">dims</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Rolls the elements of a tensor along an axis.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input tensor.</span>
<span class="sd">        shifts (Union[list(int), tuple(int), int]): Specifies the number of places by which elements are shifted</span>
<span class="sd">            positively (towards larger indices) along the specified dimension. Negative shifts will roll the elements</span>
<span class="sd">            in the opposite direction.</span>
<span class="sd">        dims (Union[list(int), tuple(int), int], optional): Specifies the dimension indexes of shape to be rolled.</span>
<span class="sd">            Default: None. If `dims` is None, the Tensor will be flattened before rolling and then restored to the</span>
<span class="sd">            original shape.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `shifts` is not an int, a tuple or a list.</span>
<span class="sd">        TypeError: If `dims` is not an int, a tuple or a list.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([0, 1, 2, 3, 4]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.roll(input_x, shifts=2, dims=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [3. 4. 0. 1. 2.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">if</span> <span class="n">dims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">flatten_x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Roll</span><span class="p">(</span><span class="n">shifts</span><span class="p">,</span> <span class="mi">0</span><span class="p">)(</span><span class="n">flatten_x</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">_shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Roll</span><span class="p">(</span><span class="n">shifts</span><span class="p">,</span> <span class="n">dims</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="xdivy"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.xdivy.html#mindspore.ops.xdivy">[docs]</a><span class="k">def</span> <span class="nf">xdivy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Divides the first input tensor by the second input tensor element-wise. Returns zero when `x` is zero.</span>

<span class="sd">    Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be bool at the same time, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    .. note::</span>
<span class="sd">        When `x` and `y` are both of datatype complex, they should be both complex64 or complex128 at the same time.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[Tensor, Number, bool]):  Tensor of datatype number.Number或bool, or it can be a bool or number.</span>
<span class="sd">        y (Union[Tensor, Number, bool]): Tensor of datatype number.Number或bool, or it can be a bool or number.</span>
<span class="sd">            `x` and `y` can not be both bool at the same time.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` and `y` is not one of the following: Tensor, Number, bool.</span>
<span class="sd">        TypeError: If dtype of `x` and &#39;y&#39; is not in [float16, float32, float64, complex64, complex128, bool].</span>
<span class="sd">        ValueError: If `x` could not be broadcast to a tensor with shape of `y`.</span>
<span class="sd">        RuntimeError: If the data type of `x`, `y` conversion of Parameter is given</span>
<span class="sd">                      but data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([2, 4, -1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([2, 2, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.xdivy(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1.   2.  -0.5]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">xdivy_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="log10"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.log10.html#mindspore.ops.log10">[docs]</a><span class="k">def</span> <span class="nf">log10</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a new tensor with the logarithm to the base 10 of the elements of input.</span>

<span class="sd">    .. math::</span>
<span class="sd">        y_i = log_{10}(x_i)</span>

<span class="sd">    .. warning::</span>
<span class="sd">        If the input value of operator Log10 is within the range (0, 0.01] or [0.95, 1.05], the output accuracy may</span>
<span class="sd">        be affacted. If the input value of operator Log10 is less than or equal to 0, it will not raise Error.</span>

<span class="sd">    .. note::</span>
<span class="sd">        The dimension of the input Tensor on Ascend should be less than or equal to 8, and the dimension of the</span>
<span class="sd">        input Tensor on the CPU or GPU should be less than 8.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input Tensor of any dimension. The value must be greater than 0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16 or float32 or float64 on CPU and GPU.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16 or float32 on Ascend.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([2, 4, 10]).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.log10(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.301 0.602 1.   ]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">dtype_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()</span>

    <span class="n">x_dtype</span> <span class="o">=</span> <span class="n">dtype_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">denominator</span> <span class="o">=</span> <span class="n">log_</span><span class="p">(</span><span class="n">_make_tensor</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">))</span>
    <span class="n">frac_log</span> <span class="o">=</span> <span class="n">log_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">frac_log</span> <span class="o">/</span> <span class="n">denominator</span>
    <span class="k">return</span> <span class="n">output</span></div>


<div class="viewcode-block" id="log1p"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.log1p.html#mindspore.ops.log1p">[docs]</a><span class="k">def</span> <span class="nf">log1p</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the natural logarithm of one plus the input tensor element-wise.</span>

<span class="sd">    .. math::</span>
<span class="sd">        out_i = {log_e}(x_i + 1)</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor. With float16 or float32 data type.</span>
<span class="sd">            The value must be greater than -1.</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions,</span>
<span class="sd">            its rank should be less than 8.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.log1p(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.6931472 1.0986123 1.609438 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_log1p</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Log1p</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">_log1p</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">kron</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Kronecker product, denoted by ⊗, of `x` and `y`.</span>

<span class="sd">    If `x` is a :math:`(a_{0}` x :math:`a_{1}` x ... x :math:`a_{n})` tensor</span>
<span class="sd">    and `y` is a :math:`(b_{0}` x :math:`b_{1}` x ... x :math:`b_{n})` tensor,</span>
<span class="sd">    the result will be a :math:`(a_{0}*b_{0}` x :math:`a_{1}*b_{1}` x ... x :math:`a_{n}*b_{n})`</span>
<span class="sd">    tensor with the following entries:</span>

<span class="sd">    .. math::</span>
<span class="sd">            (x ⊗ y)_{k_{0},k_{1},...k_{n}} =</span>
<span class="sd">            x_{i_{0},i_{1},...i_{n}} * y_{j_{0},j_{1},...j_{n}},</span>

<span class="sd">    where :math:`k_{t} = i_{t} * b_{t} + j_{t}` for 0 ≤ `t` ≤ `n`. If one</span>
<span class="sd">    tensor has fewer dimensions than the other it is unsqueezed</span>
<span class="sd">    until it has the same number of dimensions.</span>

<span class="sd">    Note:</span>
<span class="sd">        Supports real-valued and complex-valued inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input tensor.</span>
<span class="sd">        y (Tensor): Input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `y` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[0, 1, 2], [3, 4, 5]])).astype(np.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([[-1, -2, -3], [-4, -6, -8]])).astype(np.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.kron(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[  0.   0.   0.  -1.  -2.  -3.  -2.  -4.  -6.]</span>
<span class="sd">         [  0.   0.   0.  -4.  -6.  -8.  -8. -12. -16.]</span>
<span class="sd">         [ -3.  -6.  -9.  -4.  -8. -12.  -5. -10. -15.]</span>
<span class="sd">         [-12. -18. -24. -16. -24. -32. -20. -30. -40.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;the input x must be Tensor!&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;the input y must be Tensor!&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">y</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>

    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;=</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span><span class="p">:</span>
        <span class="n">maxdim</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">maxdim</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span>
    <span class="n">pad_x</span> <span class="o">=</span> <span class="n">maxdim</span> <span class="o">-</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span>
    <span class="n">pad_y</span> <span class="o">=</span> <span class="n">maxdim</span> <span class="o">-</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span>
    <span class="n">x_reshape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">maxdim</span><span class="p">)]</span>
    <span class="n">y_reshape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">maxdim</span><span class="p">)]</span>
    <span class="n">result_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">maxdim</span><span class="p">)]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">maxdim</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="n">pad_x</span><span class="p">:</span>
            <span class="n">x_reshape</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="n">pad_x</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x_reshape</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">x_reshape</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">y_reshape</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="n">pad_y</span><span class="p">:</span>
            <span class="n">y_reshape</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="n">pad_y</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">y_reshape</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">result_shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">x_reshape</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">y_reshape</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x_reshape</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">y_reshape</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">result_shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>


<span class="k">def</span> <span class="nf">all</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(),</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces a dimension of a tensor by the &quot;logicalAND&quot; of all elements in the dimension, by default. And also can</span>
<span class="sd">    reduce a dimension of `x` along the axis. Determine whether the dimensions of the output and input are the same by</span>
<span class="sd">    controlling `keep_dims`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor[bool]): The input tensor. The dtype of the tensor to be reduced is bool.</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions, its rank should be less than 8.</span>
<span class="sd">        axis (Union[int, tuple(int), list(int)]): The dimensions to reduce. Default: (), reduce all dimensions.</span>
<span class="sd">            Only constant value is allowed. Must be in the range [-rank(x), rank(x)).</span>
<span class="sd">        keep_dims (bool): If true, keep these reduced dimensions and the length is 1.</span>
<span class="sd">                          If false, don&#39;t keep these dimensions. Default : False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the dtype is bool.</span>

<span class="sd">        - If axis is (), and keep_dims is False,</span>
<span class="sd">          the output is a 0-D tensor representing the &quot;logical and&quot; of all elements in the input tensor.</span>
<span class="sd">        - If axis is int, set as 2, and keep_dims is False,</span>
<span class="sd">          the shape of output is :math:`(x_1, x_3, ..., x_R)`.</span>
<span class="sd">        - If axis is tuple(int), set as (2, 3), and keep_dims is False,</span>
<span class="sd">          the shape of output is :math:`(x_1, x_4, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not one of the following: int, tuple or list.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[True, False], [True, True]]))</span>
<span class="sd">        &gt;&gt;&gt; # case 1: Reduces a dimension by the &quot;logicalAND&quot; of all elements in the dimension.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.all(x, keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[False]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 1)</span>
<span class="sd">        &gt;&gt;&gt; # case 2: Reduces a dimension along axis 0.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.all(x, axis=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [True False]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: Reduces a dimension along axis 1.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.all(x, axis=1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False True]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ReduceAll</span><span class="p">)(</span><span class="n">keep_dims</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">any</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(),</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces a dimension of a tensor by the &quot;logical OR&quot; of all elements in the dimension, by default. And also can</span>
<span class="sd">    reduce a dimension of `x` along the axis. Determine whether the dimensions of the output and input are the same by</span>
<span class="sd">    controlling `keep_dims`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor[bool]): The input tensor. The dtype of the tensor to be reduced is bool.</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions, its rank should be less than 8.</span>
<span class="sd">        axis (Union[int, tuple(int), list(int)]): The dimensions to reduce. Default: (), reduce all dimensions.</span>
<span class="sd">            Only constant value is allowed. Must be in the range [-rank(x), rank(x)).</span>
<span class="sd">        keep_dims (bool): If true, keep these reduced dimensions and the length is 1.</span>
<span class="sd">                         If false, don&#39;t keep these dimensions. Default : False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the dtype is bool.</span>

<span class="sd">        - If axis is (), and keep_dims is False,</span>
<span class="sd">          the output is a 0-D tensor representing the &quot;logical or&quot; of all elements in the input tensor.</span>
<span class="sd">        - If axis is int, set as 2, and keep_dims is False,</span>
<span class="sd">          the shape of output is :math:`(x_1, x_3, ..., x_R)`.</span>
<span class="sd">        - If axis is tuple(int), set as (2, 3), and keep_dims is False,</span>
<span class="sd">          the shape of output is :math:`(x_1, x_4, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not one of the following: int, tuple or list.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[True, False], [True, True]]))</span>
<span class="sd">        &gt;&gt;&gt; # case 1: Reduces a dimension by the &quot;logical OR&quot; of all elements in the dimension.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.any(x, keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ True]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 1)</span>
<span class="sd">        &gt;&gt;&gt; # case 2: Reduces a dimension along axis 0.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.any(x, axis=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [True True]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: Reduces a dimension along axis 1.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.any(x, axis=1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [True True]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ReduceAny</span><span class="p">)(</span><span class="n">keep_dims</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>


<div class="viewcode-block" id="remainder"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.remainder.html#mindspore.ops.remainder">[docs]</a><span class="k">def</span> <span class="nf">remainder</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the remainder of dividing the first input tensor by the second input tensor element-wise.</span>

<span class="sd">    Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar. When the inputs are two tensors,</span>
<span class="sd">    both dtypes cannot be bool, and the shapes of them could be broadcast. When the inputs are one tensor</span>
<span class="sd">    and one scalar, the scalar could only be a constant.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = input_{i} \text{ % } other_{i}</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - The input data does not support 0.</span>
<span class="sd">        - When the elements of input exceed 2048, the accuracy of operator cannot guarantee the requirement of</span>
<span class="sd">          double thousandths in the mini form.</span>
<span class="sd">        - Due to different architectures, the calculation results of this operator on NPU and CPU may be inconsistent.</span>
<span class="sd">        - If shape is expressed as (D1,D2... ,Dn), then D1\*D2... \*DN&lt;=1000000,n&lt;=8.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[Tensor, numbers.Number, bool]): The first input is a number, a bool</span>
<span class="sd">            or a tensor whose data type is number.</span>
<span class="sd">        y (Union[Tensor, numbers.Number, bool]): When the first input is a tensor, The second input</span>
<span class="sd">            could be a number, a bool or a tensor whose data type is number.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `x` nor `y` is one of the following: Tensor, number, bool.</span>
<span class="sd">        ValueError: If the shape `x` and `y` cannot be broadcasted to each other.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-4.0, 5.0, 6.0]).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([3.0, 2.0, 3.0]).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.remainder(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2.  1.  0.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">tensor_floordiv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">y</span>
    <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="accumulate_n"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.accumulate_n.html#mindspore.ops.accumulate_n">[docs]</a><span class="k">def</span> <span class="nf">accumulate_n</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes accumulation of all input tensors element-wise.</span>

<span class="sd">    :func:`mindspore.ops.accumulate_n` is similar to :func:`mindspore.ops.addn`,</span>
<span class="sd">    but there is a significant difference between them: accumulate_n will not wait</span>
<span class="sd">    for all of its inputs to be ready before summing. That is to say, accumulate_n is able to save memory when inputs</span>
<span class="sd">    are ready at different time since the minimum temporary storage is proportional to the output size rather than the</span>
<span class="sd">    input size.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union(tuple[Tensor], list[Tensor])): The input tuple or list is made up of multiple tensors whose dtype is</span>
<span class="sd">            number to be added together. Each element of tuple or list should have the same shape.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as each entry of `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is neither tuple nor list.</span>
<span class="sd">        ValueError: If there is an input element with a different shape.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4, 5, 6]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.accumulate_n([x, y, x, y])</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [10. 14. 18.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">accumulate_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">AccumulateNV2</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">accumulate_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="iou"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.iou.html#mindspore.ops.iou">[docs]</a><span class="k">def</span> <span class="nf">iou</span><span class="p">(</span><span class="n">anchor_boxes</span><span class="p">,</span> <span class="n">gt_boxes</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;iou&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates intersection over union for boxes.</span>

<span class="sd">    Computes the intersection over union (IOU) or the intersection over foreground (IOF) based on the ground-truth and</span>
<span class="sd">    predicted regions.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{IOU} = \frac{\text{Area of Overlap}}{\text{Area of Union}}</span>

<span class="sd">        \text{IOF} = \frac{\text{Area of Overlap}}{\text{Area of Ground Truth}}</span>

<span class="sd">    .. warning::</span>
<span class="sd">        In Ascend, only computation of float16 data is supported. To avoid overflow, the input length</span>
<span class="sd">        and width are scaled by 0.2 internally.</span>

<span class="sd">    Args:</span>
<span class="sd">        anchor_boxes (Tensor): Anchor boxes, tensor of shape (N, 4). &quot;N&quot; indicates the number of anchor boxes,</span>
<span class="sd">            and the value &quot;4&quot; refers to &quot;x0&quot;, &quot;y0&quot;, &quot;x1&quot;, and &quot;y1&quot;.</span>
<span class="sd">            Data type must be either float16， float32 or float64.</span>
<span class="sd">        gt_boxes (Tensor): Ground truth boxes, tensor of shape (M, 4). &quot;M&quot; indicates the number of ground</span>
<span class="sd">            truth boxes, and the value &quot;4&quot; refers to &quot;x0&quot;, &quot;y0&quot;, &quot;x1&quot;, and &quot;y1&quot;.</span>
<span class="sd">            Data type must be either float16, float32 or float64.</span>
<span class="sd">        mode (string): The mode is used to specify the calculation method,</span>
<span class="sd">            now supporting &#39;iou&#39; (intersection over union) or &#39;iof&#39; (intersection over foreground) mode.</span>
<span class="sd">            Default: &#39;iou&#39;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the &#39;iou&#39; values, tensor of shape (M, N), with the same data type as `anchor_boxes`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        KeyError: When `mode` is not &#39;iou&#39; or &#39;iof&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; anchor_boxes = Tensor(np.random.randint(1.0, 5.0, [3, 4]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; gt_boxes = Tensor(np.random.randint(1.0, 5.0, [3, 4]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; mode = &#39;iou&#39;</span>
<span class="sd">        &gt;&gt;&gt; output = ops.iou(anchor_boxes, gt_boxes, mode)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (3, 3)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">IOU</span><span class="p">)(</span><span class="n">mode</span><span class="p">)(</span><span class="n">anchor_boxes</span><span class="p">,</span> <span class="n">gt_boxes</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_check_is_float</span><span class="p">(</span><span class="n">dtype</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">dtype</span> <span class="ow">in</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_list_comprehensions</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">([</span><span class="n">item</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">obj</span><span class="p">)])</span>


<span class="k">def</span> <span class="nf">_tuple_setitem</span><span class="p">(</span><span class="n">tup</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="n">tup</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">tup</span><span class="p">)</span>
    <span class="n">tup</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">tup</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_dim_in_range</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">ndim</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;axes should be integers, not </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="o">-</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="n">dim</span> <span class="ow">or</span> <span class="n">dim</span> <span class="o">&gt;=</span> <span class="n">ndim</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;dim </span><span class="si">{</span><span class="n">dim</span><span class="si">}</span><span class="s1"> is out of bounds for array of dimension </span><span class="si">{</span><span class="n">ndim</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dim</span> <span class="o">%</span> <span class="n">ndim</span>


<span class="k">def</span> <span class="nf">dotrapezoid</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
    <span class="n">y_left</span> <span class="o">=</span> <span class="n">select_</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">y_right</span> <span class="o">=</span> <span class="n">select_</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">y_sum</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">y_sum</span> <span class="o">-</span> <span class="p">(</span><span class="n">y_left</span> <span class="o">+</span> <span class="n">y_right</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="n">dx</span>


<span class="k">def</span> <span class="nf">dotrapezoid_tensor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
    <span class="n">y_start_dim_left</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dim</span><span class="p">)]</span>
    <span class="n">y_start_dim_left</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">y_start_dim_left</span><span class="p">)</span>
    <span class="n">y_start_dim_right</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="n">dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>
    <span class="n">y_start_dim_right</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">y_start_dim_right</span><span class="p">)</span>
    <span class="n">y_slice_size</span> <span class="o">=</span> <span class="n">_tuple_setitem</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()(</span><span class="n">y</span><span class="p">),</span> <span class="n">dim</span><span class="p">,</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()(</span><span class="n">y</span><span class="p">)[</span><span class="n">dim</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">y_slice_left</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Slice</span><span class="p">()(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_start_dim_left</span> <span class="o">+</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">+</span> <span class="n">y_start_dim_right</span><span class="p">,</span> <span class="n">y_slice_size</span><span class="p">)</span>
    <span class="n">y_slice_right</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Slice</span><span class="p">()(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_start_dim_left</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="n">y_start_dim_right</span><span class="p">,</span> <span class="n">y_slice_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()(</span><span class="n">y_slice_left</span><span class="p">,</span> <span class="n">y_slice_right</span><span class="p">)</span> <span class="o">*</span> <span class="n">dx</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.</span>


<span class="k">def</span> <span class="nf">add_padding_to_shape</span><span class="p">(</span><span class="n">curr_shape</span><span class="p">,</span> <span class="n">target_n_dim</span><span class="p">):</span>
    <span class="n">curr_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">curr_shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">curr_size</span> <span class="o">&gt;=</span> <span class="n">target_n_dim</span><span class="p">:</span>
        <span class="n">target_n_dim</span> <span class="o">=</span> <span class="n">curr_size</span>
    <span class="n">new_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">target_n_dim</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">curr_size</span><span class="p">):</span>
        <span class="n">new_shape</span><span class="p">[</span><span class="n">target_n_dim</span> <span class="o">-</span> <span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">curr_shape</span><span class="p">[</span><span class="n">curr_size</span> <span class="o">-</span> <span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">new_shape</span>


<span class="k">def</span> <span class="nf">zeros_like_except</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
    <span class="n">_check_dim_in_range</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span> <span class="o">+</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="k">if</span> <span class="n">dim</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">dim</span>
    <span class="n">sizes</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="n">dim</span><span class="p">]</span> <span class="o">+</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">dim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:]</span>
    <span class="n">zeros</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Zeros</span><span class="p">()(</span><span class="n">sizes</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">zeros</span>


<span class="k">def</span> <span class="nf">trapezoid_tensor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    add trapezoid implementation when x is not None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">zeros_like_except</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="ow">and</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">x_start_dim_left</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dim</span><span class="p">)]</span>
        <span class="n">x_start_dim_left</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">x_start_dim_left</span><span class="p">)</span>
        <span class="n">x_start_dim_right</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="n">dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>
        <span class="n">x_start_dim_right</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">x_start_dim_right</span><span class="p">)</span>
        <span class="n">x_slice_size</span> <span class="o">=</span> <span class="n">_tuple_setitem</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">x_left</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Slice</span><span class="p">()(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_start_dim_left</span> <span class="o">+</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">+</span> <span class="n">x_start_dim_right</span><span class="p">,</span> <span class="n">x_slice_size</span><span class="p">)</span>
        <span class="n">x_right</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Slice</span><span class="p">()(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_start_dim_left</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="n">x_start_dim_right</span><span class="p">,</span> <span class="n">x_slice_size</span><span class="p">)</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="n">x_right</span> <span class="o">-</span> <span class="n">x_left</span>
        <span class="n">new_sizes</span> <span class="o">=</span> <span class="n">add_padding_to_shape</span><span class="p">(</span><span class="n">dx</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="n">dx</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">new_sizes</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">dotrapezoid_tensor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;There must be one `x` value for each sample point&quot;</span><span class="p">)</span>
        <span class="n">new_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">ndim</span><span class="p">)]</span>
        <span class="n">new_sizes</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">x_viewed</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">new_sizes</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">x_viewed</span> <span class="o">=</span> <span class="n">x</span>
    <span class="n">x_start_dim_left</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dim</span><span class="p">)]</span>
    <span class="n">x_start_dim_left</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">x_start_dim_left</span><span class="p">)</span>
    <span class="n">x_start_dim_right</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x_viewed</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="n">dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>
    <span class="n">x_start_dim_right</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">x_start_dim_right</span><span class="p">)</span>
    <span class="n">x_slice_size</span> <span class="o">=</span> <span class="n">_tuple_setitem</span><span class="p">(</span><span class="n">x_viewed</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">x_viewed</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">x_left</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Slice</span><span class="p">()(</span><span class="n">x_viewed</span><span class="p">,</span> <span class="n">x_start_dim_left</span> <span class="o">+</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">+</span> <span class="n">x_start_dim_right</span><span class="p">,</span> <span class="n">x_slice_size</span><span class="p">)</span>
    <span class="n">x_right</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Slice</span><span class="p">()(</span><span class="n">x_viewed</span><span class="p">,</span> <span class="n">x_start_dim_left</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="n">x_start_dim_right</span><span class="p">,</span> <span class="n">x_slice_size</span><span class="p">)</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">x_right</span> <span class="o">-</span> <span class="n">x_left</span>
    <span class="k">return</span> <span class="n">dotrapezoid_tensor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">trapezoid</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">zeros_like_except</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dotrapezoid</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">r</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">depth</span> <span class="o">==</span> <span class="n">dim</span><span class="p">:</span>
        <span class="n">r</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ts</span><span class="p">[</span><span class="n">index</span><span class="p">])</span>
        <span class="k">return</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">ts</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">get</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">select_</span><span class="p">(</span><span class="n">feat</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
    <span class="n">select_shape</span> <span class="o">=</span> <span class="n">feat</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">select_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">select_shape</span><span class="p">)</span>
    <span class="n">select_shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">new_shape</span> <span class="o">=</span> <span class="n">feat</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="n">dim</span><span class="p">]</span> <span class="o">+</span> <span class="n">feat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">dim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:]</span>
    <span class="n">indexes</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Ones</span><span class="p">()(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">select_shape</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">index</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">feat</span><span class="o">.</span><span class="n">gather_elements</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">indexes</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">new_shape</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">trapz</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dx</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes the trapezoidal rule along dim.</span>

<span class="sd">        Integrates `y` (x) along given dim. By default x-dim distances between points will be 1.0,</span>
<span class="sd">        alternatively they can be provided with x array or with dx scalar.</span>

<span class="sd">        .. math::</span>

<span class="sd">            \mathop{ \int }\nolimits_{{}}^{{}}{y}{ \left( {x} \right) } \text{d} x</span>

<span class="sd">        Args:</span>
<span class="sd">            y (Tensor): Input tensor to integrate.</span>
<span class="sd">            x (Tensor, optional): The sample points corresponding to the `y` values. If `x` is None,</span>
<span class="sd">            the sample points are assumed to be evenly spaced `dx` apart. The default is None.</span>
<span class="sd">            If x is not None, after subtracting 1 from the axis specified by dim, the shape of x should be same</span>
<span class="sd">            or can be broadcast to y.</span>
<span class="sd">            dx (float, optional): The spacing between sample points when `x` is None. The default is 1.0.</span>
<span class="sd">            dim (int, optional): The dim along which to integrate. Defaults to -1.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor of float, definite integral as approximated by trapezoidal rule.</span>
<span class="sd">            If y is a one-dimensional array, the result is a floating-point number. If y is an n-dimensional array,</span>
<span class="sd">            the result is an N-1-dimensional array because the dimension associated with the axis has been deleted.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If dim is out of range of ``[-y.ndim, y.ndim)``.</span>
<span class="sd">            RuntimeError: If x&#39;s ndim is 1, and x&#39;s shape[0] is not equal to y&#39;s shape[dim].</span>
<span class="sd">            TypeError: If y is not a Tensor.</span>
<span class="sd">            TypeError: If x is not None and is not a Tensor.</span>
<span class="sd">            TypeError: If dx is not a float number.</span>
<span class="sd">            TypeError: If dim is not a Integer.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; y = Tensor(np.array([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]]).astype(np.float32))</span>
<span class="sd">            &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3], [1, 3, 5], [1, 4, 7]]).astype(np.float32))</span>
<span class="sd">            &gt;&gt;&gt; output = ops.trapz(y, x)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            [2. 4. 6.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;The input y must be Tensor.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dx</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;The input dx must be float.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;The input dim must be int.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_is_float</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()(</span><span class="n">y</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">_check_dim_in_range</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span> <span class="o">+</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="k">if</span> <span class="n">dim</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">dim</span>
    <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">trapezoid</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;The input x must be Tensor.&quot;</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()(</span><span class="n">x</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">trapezoid_tensor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>


<div class="viewcode-block" id="cholesky"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.cholesky.html#mindspore.ops.cholesky">[docs]</a><span class="k">def</span> <span class="nf">cholesky</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Cholesky decomposition of a symmetric positive-definite matrix :math:`A`</span>
<span class="sd">    or for batches of symmetric positive-definite matrices.</span>

<span class="sd">    If `upper` is `True`, the returned matrix :math:`U` is upper-triangular, and the decomposition has the form:</span>

<span class="sd">    .. math::</span>
<span class="sd">        A = U^TU</span>

<span class="sd">    If `upper` is `False`, the returned matrix :math:`L` is lower-triangular, and the decomposition has the form:</span>

<span class="sd">    .. math::</span>
<span class="sd">        A = LL^T</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Tensor of shape :math:`(*, N, N)`, where :math:`*` is zero or more batch dimensions</span>
<span class="sd">            consisting of symmetric positive-definite matrices, with float32 or float64 data type.</span>
<span class="sd">        upper (bool): Flag that indicates whether to return a upper or lower triangular matrix.</span>
<span class="sd">            Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and data type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `upper` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `input_x` is not one of: float64, float32.</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        ValueError: If `input_x` is not a or a batch of square matrix.</span>
<span class="sd">        ValueError: If `input_x` is not symmetric positive definite.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1.0, 1.0], [1.0, 2.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.cholesky(input_x, upper=False)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 0.]</span>
<span class="sd">         [1. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cholesky_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Cholesky</span><span class="p">)(</span><span class="n">upper</span><span class="o">=</span><span class="n">upper</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cholesky_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="cholesky_inverse"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.cholesky_inverse.html#mindspore.ops.cholesky_inverse">[docs]</a><span class="k">def</span> <span class="nf">cholesky_inverse</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the inverse of the positive definite matrix using cholesky matrix factorization.</span>

<span class="sd">    If `upper` is `False`, :math:`U` is a lower triangular such that the output tensor is</span>

<span class="sd">    .. math::</span>

<span class="sd">        inv = (UU^{T})^{-1}</span>

<span class="sd">    If `upper` is `True`, :math:`U` is an upper triangular such that the output tensor is</span>

<span class="sd">    .. math::</span>

<span class="sd">        inv = (U^{T}U)^{-1}</span>

<span class="sd">    Note:</span>
<span class="sd">        The input must be either an upper triangular matrix or a lower triangular matrix.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The input tensor with a rank of 2. Supported dtypes: float32, float64.</span>
<span class="sd">        upper(bool): Whether to return a lower or upper triangular matrix. Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input_x` is not one of: float32, float64.</span>
<span class="sd">        ValueError: If the dimension of `input_x` is not equal to 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[2,0,0], [4,1,0], [-1,1,2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.cholesky_inverse(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 5.8125 -2.625   0.625 ]</span>
<span class="sd">         [-2.625   1.25   -0.25  ]</span>
<span class="sd">         [ 0.625  -0.25    0.25  ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cholesky_inv_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">CholeskyInverse</span><span class="p">)(</span><span class="n">upper</span><span class="o">=</span><span class="n">upper</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cholesky_inv_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="conj"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.conj.html#mindspore.ops.conj">[docs]</a><span class="k">def</span> <span class="nf">conj</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a tensor of complex numbers that are the complex conjugate of each element in input.</span>
<span class="sd">    The complex numbers in input must be of the form a + bj, where a is the real part and b is the imaginary part.</span>

<span class="sd">    The complex conjugate returned by this operation is of the form a - bj.</span>

<span class="sd">    If `input` is real, it is returned unchanged.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor to compute to. Must have numeric type.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the dtype of `input` is not a numeric type.</span>
<span class="sd">        TypeError: If the `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.asarray(np.complex(1.3+0.4j)), mindspore.complex64)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.conj(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (1.3-0.4j)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For conj op, input must be Tensor.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Conj</span><span class="p">)()(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="cross"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.cross.html#mindspore.ops.cross">[docs]</a><span class="k">def</span> <span class="nf">cross</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the cross product of vectors in dimension `dim` of input `input` and `other`. `input` and `other` must</span>
<span class="sd">    have the same shape and the same type, and the size of their `dim` dimension should be `3`.</span>
<span class="sd">    If `dim` is not given, it defaults to the first dimension found with the size `3`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): input is a tensor.</span>
<span class="sd">        other (Tensor):  The other Tensor, `other` must have the same shape and type as input `input`, and</span>
<span class="sd">            the size of their `dim` dimension should be `3`.</span>
<span class="sd">        dim (int): dimension to apply cross product in. Default: None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as input `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If `other` is not a Tensor.</span>
<span class="sd">        TypeError: If the type of `input` is not the same as that of `other`.</span>
<span class="sd">        ValueError: If `input` and `other` not have the same size, and the size of their `dim` dimension not be `3`.</span>
<span class="sd">        ValueError: If `input` and `other` not have the same shape.</span>
<span class="sd">        ValueError: If `dim` is out of range, `dim` should be [-len(input.shape), len(input.shape)-1].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([1, 2, 3], mstype.int8)</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor([1, 2, 3], mstype.int8)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.cross(x, other)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0 0 0]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="o">-</span><span class="mi">65530</span>
    <span class="n">cross_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Cross</span><span class="p">)(</span><span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cross_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="erfinv"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.erfinv.html#mindspore.ops.erfinv">[docs]</a><span class="k">def</span> <span class="nf">erfinv</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the inverse error function of input. The inverse error function is defined in the range `(-1, 1)` as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        erfinv(erf(x)) = x</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor to compute to, with data type float32, float16 or float64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `input` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0, 0.5, -0.9]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.erfinv(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0.          0.47695306 -1.1630805 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Erfinv</span><span class="p">)()(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="less_equal"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.less_equal.html#mindspore.ops.less_equal">[docs]</a><span class="k">def</span> <span class="nf">less_equal</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the boolean value of :math:`input\_x &lt;= other` element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} =\begin{cases}</span>
<span class="sd">            &amp; \text{True,    if } input\_x_{i}&lt;=other_{i} \\</span>
<span class="sd">            &amp; \text{False,   if } input\_x_{i}&gt;other_{i}</span>
<span class="sd">            \end{cases}</span>

<span class="sd">    .. note::</span>
<span class="sd">        - Inputs of `input` and `other` comply with the implicit type conversion rules to make the data types</span>
<span class="sd">          consistent.</span>
<span class="sd">        - The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">        - When the inputs are two tensors, dtypes of them cannot be both bool, and the shapes of them</span>
<span class="sd">          can be broadcast.</span>
<span class="sd">        - When the inputs are one tensor and one scalar, the scalar could only be a constant.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, number.Number, bool]): The first input is a number.Number or</span>
<span class="sd">            a bool or a tensor whose data type is</span>
<span class="sd">            `number &lt;https://www.mindspore.cn/docs/en/r2.0.0-alpha/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">            `bool_ &lt;https://www.mindspore.cn/docs/en/r2.0.0-alpha/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>
<span class="sd">        other (Union[Tensor, number.Number, bool]): The second input, when the first input is a Tensor,</span>
<span class="sd">            the second input should be a number.Number or bool value, or a Tensor whose data type is number or bool\_.</span>
<span class="sd">            When the first input is Scalar, the second input must be a Tensor whose data type is number or bool\_.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `input` nor `other` is a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor(np.array([1, 1, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.less_equal(x, other)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True False  True]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">LessEqual</span><span class="p">)()(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="cumprod"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.cumprod.html#mindspore.ops.cumprod">[docs]</a><span class="k">def</span> <span class="nf">cumprod</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the cumulative product of the `input` tensor along dimension `dim`.</span>
<span class="sd">    For example, if `input` is a vector of size `N`, the result will also be a vector of size `N`, with elements.</span>

<span class="sd">    .. math::</span>

<span class="sd">        y_i = x_1 * x_2 * x_3 * ... * x_i</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor[Number]): The input tensor.</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions, its rank should be less than 8.</span>
<span class="sd">        dim (int): The dimensions to compute the cumulative product. Only constant value is allowed.</span>
<span class="sd">        dtype: The desired data type of output. Default: None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `input` unless `dtype` is specified.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `dim` is not an int.</span>
<span class="sd">        TypeError: If `dtype` conversion is not acceptable.</span>
<span class="sd">        ValueError: If `dim` is None.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3], np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.cumprod(x, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 2. 6.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cumprod_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">CumProd</span><span class="p">)()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">cumprod_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">)()(</span><span class="n">output</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span></div>


<div class="viewcode-block" id="greater"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.greater.html#mindspore.ops.greater">[docs]</a><span class="k">def</span> <span class="nf">greater</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the boolean value of :math:`input &gt; other` element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, number.Number, bool]): The first input is a number.Number or</span>
<span class="sd">            a bool or a tensor whose data type is</span>
<span class="sd">            `number &lt;https://www.mindspore.cn/docs/en/r2.0.0-alpha/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">            `bool_ &lt;https://www.mindspore.cn/docs/en/r2.0.0-alpha/api_python/mindspore.html#mindspore.dtype&gt;`_ .</span>
<span class="sd">        other (Union[Tensor, number.Number, bool]): The second input, when the first input is a Tensor,</span>
<span class="sd">            the second input should be a number.Number or bool value, or a Tensor whose data type is number or bool\_.</span>
<span class="sd">            When the first input is Scalar, the second input must be a Tensor whose data type is number or bool\_.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([1, 1, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.greater(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False True False]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">greater_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Greater</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">greater_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="greater_equal"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.greater_equal.html#mindspore.ops.greater_equal">[docs]</a><span class="k">def</span> <span class="nf">greater_equal</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the boolean value of :math:`input &gt;= other` element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, number.Number, bool]): The first input is a number.Number or</span>
<span class="sd">            a bool or a tensor whose data type is</span>
<span class="sd">            `number &lt;https://www.mindspore.cn/docs/en/r2.0.0-alpha/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">            `bool_ &lt;https://www.mindspore.cn/docs/en/r2.0.0-alpha/api_python/mindspore.html#mindspore.dtype&gt;`_ .</span>
<span class="sd">        other (Union[Tensor, number.Number, bool]): The second input, when the first input is a Tensor,</span>
<span class="sd">            the second input should be a number.Number or bool value, or a Tensor whose data type is number or bool\_.</span>
<span class="sd">            When the first input is Scalar, the second input must be a Tensor whose data type is number or bool\_.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([1, 1, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.greater_equal(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [True True False]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">greater_equal_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">GreaterEqual</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">greater_equal_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="igamma"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.igamma.html#mindspore.ops.igamma">[docs]</a><span class="k">def</span> <span class="nf">igamma</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates lower regularized incomplete Gamma function.</span>

<span class="sd">    If we define `input` as `a` and `other` as `x`, the lower regularized incomplete Gamma function is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        P(a, x) = Gamma(a, x) / Gamma(a) = 1 - Q(a, x)</span>

<span class="sd">    where</span>

<span class="sd">    .. math::</span>
<span class="sd">        Gamma(a, x) = \int_0^x t^{a-1} \exp^{-t} dt</span>

<span class="sd">    is the lower incomplete Gamma function.</span>

<span class="sd">    Above :math:`Q(a, x)` is the upper regularized complete Gamma function.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental prototype that is subject to change and/or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The first input tensor. With type of float32 or float64.</span>
<span class="sd">        other (Tensor): The second input tensor. With float32 or float64 type. `other` should have</span>
<span class="sd">          the same dtype with `input`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype as `input` and `other`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` or `other` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of input `other` and a is not float32 nor float64.</span>
<span class="sd">        TypeError: If `other` has different dtype with `input`.</span>
<span class="sd">        ValueError: If `input` could not be broadcast to a tensor with shape of `other`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; a = Tensor(np.array([2.0, 4.0, 6.0, 8.0]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([2.0, 3.0, 4.0, 5.0]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.igamma(a, x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.593994 0.35276785 0.21486944 0.13337152]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">igamma_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Igamma</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">igamma_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="igammac"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.igammac.html#mindspore.ops.igammac">[docs]</a><span class="k">def</span> <span class="nf">igammac</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates upper regularized incomplete Gamma function.</span>

<span class="sd">    If we define `input` as `a` and `other` as `x`, the upper regularized incomplete Gamma function is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        Q(a, x) = Gamma(a, x) / Gamma(a) = 1 - P(a, x)</span>

<span class="sd">    where</span>

<span class="sd">    .. math::</span>
<span class="sd">        Gamma(a, x) = int_{x}^{\infty} t^{a-1} exp(-t) dt</span>

<span class="sd">    is the upper incomplete Gama function.</span>

<span class="sd">    Above :math:`P(a, x)` is the lower regularized complete Gamma function.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental prototype that is subject to change and/or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The first input tensor. With type of float32 or float64.</span>
<span class="sd">        other (Tensor): The second input tensor. With float32 or float64 type. `other` should have</span>
<span class="sd">            the same dtype with `input`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype as `input` and `other`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` or `other` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of input `other` and a is not float32 nor float64.</span>
<span class="sd">        TypeError: If `other` has different dtype with `input`.</span>
<span class="sd">        ValueError: If `input` could not be broadcast to a tensor with shape of `other`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; a = Tensor(np.array([2.0, 4.0, 6.0, 8.0]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([2.0, 3.0, 4.0, 5.0]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.igammac(a, x)</span>
<span class="sd">        &gt;&gt;&gt; print (output)</span>
<span class="sd">        [0.40600586 0.6472318 0.7851304 0.8666283]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">igammac_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Igammac</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">igammac_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="isinf"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.isinf.html#mindspore.ops.isinf">[docs]</a><span class="k">def</span> <span class="nf">isinf</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Determines which elements are inf or -inf for each position.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \begin{cases}</span>
<span class="sd">        &amp; \text{ if } x_{i} = \text{Inf},\ \ True \\</span>
<span class="sd">        &amp; \text{ if } x_{i} \ne \text{Inf},\ \ False</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    where :math:`Inf` means not a number.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor.</span>
<span class="sd">          :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape of input, and the dtype is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([np.log(-1), 1, np.log(0)]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.isinf(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False False True]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">isinf_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">IsInf</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">isinf_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="logical_xor"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.logical_xor.html#mindspore.ops.logical_xor">[docs]</a><span class="k">def</span> <span class="nf">logical_xor</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the &quot;logical XOR&quot; of two tensors element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = x_{i} \oplus y_{i}</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The first input is a tensor whose data type is bool.</span>
<span class="sd">        other (Tensor): The second input is a tensor to compute XOR with the first input.</span>
<span class="sd">          Datatype must be bool.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `input` nor `other` is a Tensor whose data type is bool.</span>
<span class="sd">        ValueError: If the shape of two inputs cannot be broadcast.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([True, False, True]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([True, True, False]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.logical_xor(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False True True]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">logical_xor_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">LogicalXor</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">logical_xor_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">imag</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a new tensor containing imaginary value of the `input`.</span>
<span class="sd">    If `input` is real, it will return zeros.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor to compute to.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">       TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.asarray(np.complex(1.3 + 0.4j)), mindspore.complex64)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.imag(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.4</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Imag</span><span class="p">)()(</span><span class="nb">input</span><span class="p">)</span>


<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;addn&#39;</span><span class="p">,</span>
    <span class="s1">&#39;absolute&#39;</span><span class="p">,</span>
    <span class="s1">&#39;abs&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_add&#39;</span><span class="p">,</span>
    <span class="s1">&#39;add&#39;</span><span class="p">,</span>
    <span class="s1">&#39;addbmm&#39;</span><span class="p">,</span>
    <span class="s1">&#39;addcdiv&#39;</span><span class="p">,</span>
    <span class="s1">&#39;addcmul&#39;</span><span class="p">,</span>
    <span class="s1">&#39;angle&#39;</span><span class="p">,</span>
    <span class="s1">&#39;argmin&#39;</span><span class="p">,</span>
    <span class="s1">&#39;arccosh&#39;</span><span class="p">,</span>
    <span class="s1">&#39;arcsin&#39;</span><span class="p">,</span>
    <span class="s1">&#39;arctan&#39;</span><span class="p">,</span>
    <span class="s1">&#39;arctan2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;neg_tensor&#39;</span><span class="p">,</span>
    <span class="s1">&#39;neg&#39;</span><span class="p">,</span>
    <span class="s1">&#39;negative&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_lt&#39;</span><span class="p">,</span>
    <span class="s1">&#39;less&#39;</span><span class="p">,</span>
    <span class="s1">&#39;logaddexp2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_le&#39;</span><span class="p">,</span>
    <span class="s1">&#39;lcm&#39;</span><span class="p">,</span>
    <span class="s1">&#39;le&#39;</span><span class="p">,</span>
    <span class="s1">&#39;lerp&#39;</span><span class="p">,</span>
    <span class="s1">&#39;norm&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_gt&#39;</span><span class="p">,</span>
    <span class="s1">&#39;logaddexp&#39;</span><span class="p">,</span>
    <span class="s1">&#39;mv&#39;</span><span class="p">,</span>
    <span class="s1">&#39;addmm&#39;</span><span class="p">,</span>
    <span class="s1">&#39;addmv&#39;</span><span class="p">,</span>
    <span class="s1">&#39;adjoint&#39;</span><span class="p">,</span>
    <span class="s1">&#39;outer&#39;</span><span class="p">,</span>
    <span class="s1">&#39;gt&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_ge&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ge&#39;</span><span class="p">,</span>
    <span class="s1">&#39;addr&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_sub&#39;</span><span class="p">,</span>
    <span class="s1">&#39;sub&#39;</span><span class="p">,</span>
    <span class="s1">&#39;subtract&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_mul&#39;</span><span class="p">,</span>
    <span class="s1">&#39;mul&#39;</span><span class="p">,</span>
    <span class="s1">&#39;multiply&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_div&#39;</span><span class="p">,</span>
    <span class="s1">&#39;div&#39;</span><span class="p">,</span>
    <span class="s1">&#39;divide&#39;</span><span class="p">,</span>
    <span class="s1">&#39;true_divide&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_floordiv&#39;</span><span class="p">,</span>
    <span class="s1">&#39;floor_div&#39;</span><span class="p">,</span>
    <span class="s1">&#39;floordiv&#39;</span><span class="p">,</span>
    <span class="s1">&#39;xdivy&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_pow&#39;</span><span class="p">,</span>
    <span class="s1">&#39;pow&#39;</span><span class="p">,</span>
    <span class="s1">&#39;pows&#39;</span><span class="p">,</span>
    <span class="s1">&#39;renorm&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_mod&#39;</span><span class="p">,</span>
    <span class="s1">&#39;floor_mod&#39;</span><span class="p">,</span>
    <span class="s1">&#39;floormod&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_exp&#39;</span><span class="p">,</span>
    <span class="s1">&#39;exp&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_expm1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;expm1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;equal&#39;</span><span class="p">,</span>
    <span class="s1">&#39;not_equal&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ne&#39;</span><span class="p">,</span>
    <span class="s1">&#39;numel&#39;</span><span class="p">,</span>
    <span class="s1">&#39;permute&#39;</span><span class="p">,</span>
    <span class="s1">&#39;inplace_update&#39;</span><span class="p">,</span>
    <span class="s1">&#39;inplace_add&#39;</span><span class="p">,</span>
    <span class="s1">&#39;inplace_sub&#39;</span><span class="p">,</span>
    <span class="s1">&#39;isfinite&#39;</span><span class="p">,</span>
    <span class="s1">&#39;isnan&#39;</span><span class="p">,</span>
    <span class="s1">&#39;isclose&#39;</span><span class="p">,</span>
    <span class="s1">&#39;isreal&#39;</span><span class="p">,</span>
    <span class="s1">&#39;log&#39;</span><span class="p">,</span>
    <span class="s1">&#39;log_matrix_determinant&#39;</span><span class="p">,</span>
    <span class="s1">&#39;matrix_determinant&#39;</span><span class="p">,</span>
    <span class="s1">&#39;linspace&#39;</span><span class="p">,</span>
    <span class="s1">&#39;matrix_solve&#39;</span><span class="p">,</span>
    <span class="s1">&#39;std&#39;</span><span class="p">,</span>
    <span class="s1">&#39;maximum&#39;</span><span class="p">,</span>
    <span class="s1">&#39;minimum&#39;</span><span class="p">,</span>
    <span class="s1">&#39;median&#39;</span><span class="p">,</span>
    <span class="s1">&#39;positive&#39;</span><span class="p">,</span>
    <span class="s1">&#39;floor&#39;</span><span class="p">,</span>
    <span class="s1">&#39;logical_not&#39;</span><span class="p">,</span>
    <span class="s1">&#39;logical_or&#39;</span><span class="p">,</span>
    <span class="s1">&#39;logical_and&#39;</span><span class="p">,</span>
    <span class="s1">&#39;logit&#39;</span><span class="p">,</span>
    <span class="s1">&#39;logsumexp&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ldexp&#39;</span><span class="p">,</span>
    <span class="s1">&#39;sqrt&#39;</span><span class="p">,</span>
    <span class="s1">&#39;square&#39;</span><span class="p">,</span>
    <span class="s1">&#39;sin&#39;</span><span class="p">,</span>
    <span class="s1">&#39;cos&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tan&#39;</span><span class="p">,</span>
    <span class="s1">&#39;asin&#39;</span><span class="p">,</span>
    <span class="s1">&#39;acos&#39;</span><span class="p">,</span>
    <span class="s1">&#39;arccos&#39;</span><span class="p">,</span>
    <span class="s1">&#39;atan&#39;</span><span class="p">,</span>
    <span class="s1">&#39;sinh&#39;</span><span class="p">,</span>
    <span class="s1">&#39;cosh&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tanh&#39;</span><span class="p">,</span>
    <span class="s1">&#39;asinh&#39;</span><span class="p">,</span>
    <span class="s1">&#39;acosh&#39;</span><span class="p">,</span>
    <span class="s1">&#39;atanh&#39;</span><span class="p">,</span>
    <span class="s1">&#39;atan2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;round&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bitwise_and&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bitwise_or&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bitwise_xor&#39;</span><span class="p">,</span>
    <span class="s1">&#39;inv&#39;</span><span class="p">,</span>
    <span class="s1">&#39;invert&#39;</span><span class="p">,</span>
    <span class="s1">&#39;erf&#39;</span><span class="p">,</span>
    <span class="s1">&#39;erfc&#39;</span><span class="p">,</span>
    <span class="s1">&#39;cdist&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ceil&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bernoulli&#39;</span><span class="p">,</span>
    <span class="s1">&#39;heaviside&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hypot&#39;</span><span class="p">,</span>
    <span class="s1">&#39;i0&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bessel_j0&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bessel_j1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bessel_i0&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bessel_i0e&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bessel_k0&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bessel_k0e&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bessel_y0&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bessel_y1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bessel_i1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bessel_i1e&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bessel_k1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bessel_k1e&#39;</span><span class="p">,</span>
    <span class="s1">&#39;exp2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;deg2rad&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stft&#39;</span><span class="p">,</span>
    <span class="s1">&#39;rad2deg&#39;</span><span class="p">,</span>
    <span class="s1">&#39;truncate_div&#39;</span><span class="p">,</span>
    <span class="s1">&#39;truncate_mod&#39;</span><span class="p">,</span>
    <span class="s1">&#39;trunc&#39;</span><span class="p">,</span>
    <span class="s1">&#39;gumbel_softmax&#39;</span><span class="p">,</span>
    <span class="s1">&#39;matmul&#39;</span><span class="p">,</span>
    <span class="s1">&#39;cummin&#39;</span><span class="p">,</span>
    <span class="s1">&#39;cummax&#39;</span><span class="p">,</span>
    <span class="s1">&#39;cumsum&#39;</span><span class="p">,</span>
    <span class="s1">&#39;amin&#39;</span><span class="p">,</span>
    <span class="s1">&#39;amax&#39;</span><span class="p">,</span>
    <span class="s1">&#39;mean&#39;</span><span class="p">,</span>
    <span class="s1">&#39;prod&#39;</span><span class="p">,</span>
    <span class="s1">&#39;all&#39;</span><span class="p">,</span>
    <span class="s1">&#39;any&#39;</span><span class="p">,</span>
    <span class="s1">&#39;sparse_segment_mean&#39;</span><span class="p">,</span>
    <span class="s1">&#39;atleast_2d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;vstack&#39;</span><span class="p">,</span>
    <span class="s1">&#39;copysign&#39;</span><span class="p">,</span>
    <span class="s1">&#39;log2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;xlogy&#39;</span><span class="p">,</span>
    <span class="s1">&#39;log10&#39;</span><span class="p">,</span>
    <span class="s1">&#39;log1p&#39;</span><span class="p">,</span>
    <span class="s1">&#39;approximate_equal&#39;</span><span class="p">,</span>
    <span class="s1">&#39;frac&#39;</span><span class="p">,</span>
    <span class="s1">&#39;kron&#39;</span><span class="p">,</span>
    <span class="s1">&#39;rot90&#39;</span><span class="p">,</span>
    <span class="s1">&#39;remainder&#39;</span><span class="p">,</span>
    <span class="s1">&#39;accumulate_n&#39;</span><span class="p">,</span>
    <span class="s1">&#39;iou&#39;</span><span class="p">,</span>
    <span class="s1">&#39;baddbmm&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bmm&#39;</span><span class="p">,</span>
    <span class="s1">&#39;trapz&#39;</span><span class="p">,</span>
    <span class="s1">&#39;cholesky&#39;</span><span class="p">,</span>
    <span class="s1">&#39;cholesky_inverse&#39;</span><span class="p">,</span>
    <span class="s1">&#39;conj&#39;</span><span class="p">,</span>
    <span class="s1">&#39;cross&#39;</span><span class="p">,</span>
    <span class="s1">&#39;erfinv&#39;</span><span class="p">,</span>
    <span class="s1">&#39;less_equal&#39;</span><span class="p">,</span>
    <span class="s1">&#39;cumprod&#39;</span><span class="p">,</span>
    <span class="s1">&#39;greater&#39;</span><span class="p">,</span>
    <span class="s1">&#39;greater_equal&#39;</span><span class="p">,</span>
    <span class="s1">&#39;igamma&#39;</span><span class="p">,</span>
    <span class="s1">&#39;igammac&#39;</span><span class="p">,</span>
    <span class="s1">&#39;isinf&#39;</span><span class="p">,</span>
    <span class="s1">&#39;logical_xor&#39;</span><span class="p">,</span>
    <span class="s1">&#39;imag&#39;</span><span class="p">,</span>
    <span class="s1">&#39;roll&#39;</span><span class="p">,</span>
    <span class="s1">&#39;matrix_exp&#39;</span>
<span class="p">]</span>
<span class="n">__all__</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>