

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>mindspore.ops.function.nn_func &mdash; MindSpore master documentation</title>
  

  
  <link rel="stylesheet" href="../../../../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/training.css" type="text/css" />
   
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/language_data.js"></script>
        <script src="../../../../_static/js/training.js"></script>
        
        
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/overview.html">MindSpore Design Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/auto_gradient.html">Functional Differential Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/distributed_training_design.html">Distributed Training Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/dynamic_graph_and_static_graph.html">Combination of Dynamic and Static Graphs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/graph_fusion_engine.html">Graph-Kernel Fusion Acceleration Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/mindir.html">MindSpore IR (MindIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/all_scenarios.html">Full-scenarios Unification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/thor.html">Second Order Optimizer</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r2.0.0-alpha/training_visual_design.html">Design of Visualization↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/data_engine.html">High Performance Data Processing Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/glossary.html">Glossary</a></li>
</ul>
<p class="caption"><span class="caption-text">Specification</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/benchmark.html">Benchmarks</a></li>
<li class="toctree-l1"><a class="reference external" href="https://gitee.com/mindspore/models/blob/r2.0.0-alpha/README.md#table-of-contents">Network List↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/operator_list.html">API List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/syntax_list.html">Syntax Support</a></li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.transformer.html">mindspore.nn.transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/api_cpp/mindspore.html">C++ API↗</a></li>
</ul>
<p class="caption"><span class="caption-text">API Mapping</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/api_mapping/pytorch_api_mapping.html">PyTorch and MindSpore API Mapping Table</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/api_mapping/tensorflow_api_mapping.html">TensorFlow and MindSpore API Mapping Table</a></li>
</ul>
<p class="caption"><span class="caption-text">Migration Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/overview.html">Overview of Migration Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/enveriment_preparation.html">Environment Preparation and Information Acquisition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/analysis_and_preparation.html">Model Analysis and Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/model_development/model_development.html">Constructing MindSpore Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/debug_and_tune.html">Debugging and Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/sample_code.html">Network Migration Debugging Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/faq.html">FAQs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/typical_api_comparision.html">Differences Between MindSpore and PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/use_third_party_op.html">Using Third-party Operator Libraries Based on Customized Interfaces</a></li>
</ul>
<p class="caption"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/data_processing.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/implement_problem.html">Implement Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/network_compilation.html">Network Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/operators_compile.html">Operators Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/usage_migrate_3rd.html">Migration from a Third-party Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/precision_tuning.html">Precision Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/distributed_configure.html">Distributed Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/feature_advice.html">Feature Advice</a></li>
</ul>
<p class="caption"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../RELEASE.html">Release Notes</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
        
      <li>mindspore.ops.function.nn_func</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for mindspore.ops.function.nn_func</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2022 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>

<span class="sd">&quot;&quot;&quot;Defines nn operators with functional form.&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">pi</span><span class="p">,</span> <span class="n">log</span>

<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.primitive</span> <span class="kn">import</span> <span class="n">constexpr</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations</span> <span class="kn">import</span> <span class="n">_inner_ops</span> <span class="k">as</span> <span class="n">inner</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations</span> <span class="kn">import</span> <span class="n">nn_ops</span> <span class="k">as</span> <span class="n">NN_OPS</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations</span> <span class="kn">import</span> <span class="n">image_ops</span> <span class="k">as</span> <span class="n">IMG</span>
<span class="kn">import</span> <span class="nn">mindspore.common.dtype</span> <span class="k">as</span> <span class="nn">mstype</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.function.math_func</span> <span class="kn">import</span> <span class="n">logsumexp</span>
<span class="kn">from</span> <span class="nn">mindspore.common.tensor</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">Tensor</span> <span class="k">as</span> <span class="n">Tensor_</span>
<span class="kn">from</span> <span class="nn">mindspore.ops._primitive_cache</span> <span class="kn">import</span> <span class="n">_get_cache_prim</span>
<span class="kn">from</span> <span class="nn">mindspore._checkparam</span> <span class="kn">import</span> <span class="n">Rel</span>
<span class="kn">from</span> <span class="nn">mindspore._checkparam</span> <span class="kn">import</span> <span class="n">Validator</span> <span class="k">as</span> <span class="n">validator</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.composite.multitype_ops._constexpr_utils</span> <span class="kn">import</span> <span class="n">raise_value_error</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations.nn_ops</span> <span class="kn">import</span> <span class="n">MaxUnpool2D</span><span class="p">,</span> <span class="n">MaxUnpool3D</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations.nn_ops</span> <span class="kn">import</span> <span class="n">FractionalMaxPoolWithFixedKsize</span><span class="p">,</span> <span class="n">FractionalMaxPool3DWithFixedKsize</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations.nn_ops</span> <span class="kn">import</span> <span class="n">PadV3</span>

<span class="n">slice_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Slice</span><span class="p">()</span>
<span class="n">fast_gelu_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">FastGeLU</span><span class="p">()</span>
<span class="n">softsign_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Softsign</span><span class="p">()</span>
<span class="n">hardswish_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">HSwish</span><span class="p">()</span>
<span class="n">mish_</span> <span class="o">=</span> <span class="n">NN_OPS</span><span class="o">.</span><span class="n">Mish</span><span class="p">()</span>
<span class="n">selu_</span> <span class="o">=</span> <span class="n">NN_OPS</span><span class="o">.</span><span class="n">SeLU</span><span class="p">()</span>
<span class="n">scalar_to_tensor_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScalarToTensor</span><span class="p">()</span>
<span class="n">sigmoid_</span> <span class="o">=</span> <span class="n">NN_OPS</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>


<div class="viewcode-block" id="adaptive_avg_pool2d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.adaptive_avg_pool2d.html#mindspore.ops.adaptive_avg_pool2d">[docs]</a><span class="k">def</span> <span class="nf">adaptive_avg_pool2d</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This operator applies a 2D adaptive average pooling to an input signal composed of multiple input planes.</span>
<span class="sd">    That is, for any input size, the size of the specified output is H x W.</span>
<span class="sd">    The number of output features is equal to the number of input features.</span>

<span class="sd">    The input and output data format can be &quot;NCHW&quot; and &quot;CHW&quot;. N is the batch size, C is the number of channels,</span>
<span class="sd">    H is the feature height, and W is the feature width.</span>

<span class="sd">    For adaptive average pooling for 2D:</span>

<span class="sd">    ..  math::</span>
<span class="sd">        \begin{align}</span>
<span class="sd">        h_{start} &amp;= floor(i * H_{in} / H_{out})\\</span>
<span class="sd">        h_{end} &amp;= ceil((i + 1) * H_{in} / H_{out})\\</span>
<span class="sd">        w_{start} &amp;= floor(j * W_{in} / W_{out})\\</span>
<span class="sd">        w_{end} &amp;= ceil((j + 1) * W_{in} / W_{out})\\</span>
<span class="sd">        Output(i,j) &amp;= \frac{\sum Input[h_{start}:h_{end}, w_{start}:w_{end}]}{(h_{end}- h_{start})</span>
<span class="sd">        * (w_{end}- w_{start})}</span>
<span class="sd">        \end{align}</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The input of adaptive_avg_pool2d, which is a 3D or 4D tensor,</span>
<span class="sd">          with float16, float32 or float64 data type.</span>
<span class="sd">        output_size (Union[int, tuple]): The target output size is H x W.</span>
<span class="sd">            `ouput_size` can be a tuple consisted of int type H and W, or a single H for H x H, or None.</span>
<span class="sd">            If it is None, it means the output size is the same as the input size.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type as the `input_x`.</span>

<span class="sd">        Shape of the output is `input_x_shape[:len(input_x_shape) - len(out_shape)] + out_shape`.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out\_shape = \begin{cases}</span>
<span class="sd">        input\_x\_shape[-2] + output\_size[1], &amp; \text{if output_size is (None, w);}\\</span>
<span class="sd">        output\_size[0] + input\_x\_shape[-1], &amp; \text{if output_size is (h, None);}\\</span>
<span class="sd">        input\_x\_shape[-2:], &amp; \text{if output_size is (None, None);}\\</span>
<span class="sd">        (h, h), &amp; \text{if output_size is h;}\\</span>
<span class="sd">        (h, w), &amp; \text{if output_size is (h, w)}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If `output_size` is a tuple and the length of `output_size` is not 2.</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input_x` is not float16, float32 or float64.</span>
<span class="sd">        ValueError: If the dimension of `input_x` is less than or equal to the dimension of `output_size`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # case 1: output_size=(None, 2)</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]],</span>
<span class="sd">        ...                            [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]],</span>
<span class="sd">        ...                            [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.adaptive_avg_pool2d(input_x, (None, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1.5 2.5]</span>
<span class="sd">          [4.5 5.5]</span>
<span class="sd">          [7.5 8.5]]</span>
<span class="sd">         [[1.5 2.5]</span>
<span class="sd">          [4.5 5.5]</span>
<span class="sd">          [7.5 8.5]]</span>
<span class="sd">         [[1.5 2.5]</span>
<span class="sd">          [4.5 5.5]</span>
<span class="sd">          [7.5 8.5]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 2: output_size=2</span>
<span class="sd">        &gt;&gt;&gt; output = ops.adaptive_avg_pool2d(input_x, 2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[3. 4.]</span>
<span class="sd">          [6. 7.]]</span>
<span class="sd">         [[3. 4.]</span>
<span class="sd">          [6. 7.]]</span>
<span class="sd">         [[3. 4.]</span>
<span class="sd">          [6. 7.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: output_size=(1, 2)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.adaptive_avg_pool2d(input_x, (1, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[4.5 5.5]]</span>
<span class="sd">         [[4.5 5.5]]</span>
<span class="sd">         [[4.5 5.5]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">adaptive_avgpool2d_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">AdaptiveAvgPool2D</span><span class="p">)(</span><span class="n">output_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">adaptive_avgpool2d_</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="adaptive_avg_pool3d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.adaptive_avg_pool3d.html#mindspore.ops.adaptive_avg_pool3d">[docs]</a><span class="k">def</span> <span class="nf">adaptive_avg_pool3d</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This operator applies a 3D adaptive average pooling to an input signal composed of multiple input planes.</span>
<span class="sd">    That is, for any input size, the size of the specified output is :math:`(D, H, W)`.</span>
<span class="sd">    The number of output features is equal to the number of input planes.</span>

<span class="sd">    Suppose the last 3 dimension size of x is :math:`(inD, inH, inW)`, the last 3 dimension size of output is</span>
<span class="sd">    :math:`(outD, outH, outW)`.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            \forall \quad od \in [0,outD-1], oh \in [0,outH-1], ow \in [0,outW-1]\\</span>
<span class="sd">            output[od,oh,ow] = \\</span>
<span class="sd">            \qquad mean(x[istartD:iendD+1,istartH:iendH+1,istartW:iendW+1])\\</span>
<span class="sd">            where,\\</span>
<span class="sd">            \qquad istartD= \left\lceil \frac{od * inD}{outD} \right\rceil \\</span>
<span class="sd">            \qquad iendD=\left\lfloor \frac{(od+1)* inD}{outD} \right\rfloor \\</span>
<span class="sd">            \qquad istartH=\left\lceil \frac{oh * inH}{outH} \right\rceil \\</span>
<span class="sd">            \qquad iendH=\left\lfloor \frac{(oh+1) * inH}{outH} \right\rfloor \\</span>
<span class="sd">            \qquad istartW=\left\lceil \frac{ow * inW}{outW} \right\rceil \\</span>
<span class="sd">            \qquad iendW=\left\lfloor \frac{(ow+1) * inW}{outW} \right\rfloor</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The input of adaptive_avg_pool3d, which is a 5D or 4D Tensor.</span>
<span class="sd">        output_size (Union[int, tuple]): The target output size. `ouput_size` can be a tuple :math:`(D, H, W)`,</span>
<span class="sd">            or an int D for :math:`(D, D, D)`. :math:`D`, :math:`H` and :math:`W` can be int or None</span>
<span class="sd">            which means the output size is the same as that of the input.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type as the `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input_x` is not float16, float32 or float64.</span>
<span class="sd">        ValueError: If the dimension of `input_x` is not 4D or 5D.</span>
<span class="sd">        ValueError: If `output_size` value is not positive.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # case 1: output_size=(3, 3, 4)</span>
<span class="sd">        &gt;&gt;&gt; output_size=(3, 3, 4)</span>
<span class="sd">        &gt;&gt;&gt; input_x_val = np.random.randn(4, 3, 5, 6, 7)</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(input_x_val, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.adaptive_avg_pool3d(input_x, output_size)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (4, 3, 3, 3, 4)</span>
<span class="sd">        &gt;&gt;&gt; # case 2: output_size=4</span>
<span class="sd">        &gt;&gt;&gt; output_size=5</span>
<span class="sd">        &gt;&gt;&gt; input_x_val = np.random.randn(2, 3, 8, 6, 12)</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(input_x_val, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.adaptive_avg_pool3d(input_x, output_size)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 3, 5, 5, 5)</span>
<span class="sd">        &gt;&gt;&gt; # case 3: output_size=(None, 4, 5)</span>
<span class="sd">        &gt;&gt;&gt; output_size=(None, 4, 5)</span>
<span class="sd">        &gt;&gt;&gt; input_x_val = np.random.randn(4, 1, 9, 10, 8)</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(input_x_val, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.adaptive_avg_pool3d(input_x, output_size)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (4, 1, 9, 4, 5)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">adaptive_avg_pool3d_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">AdaptiveAvgPool3D</span><span class="p">)(</span><span class="n">output_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">adaptive_avg_pool3d_</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_avgpool_1d_type_and_int</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">,</span> <span class="n">count_include_pad</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Checks the type of avgpool1d input&quot;&quot;&quot;</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="s1">&#39;avg_pool1d&#39;</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="s1">&#39;avg_pool1d&#39;</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;ceil_mode&#39;</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="s1">&#39;avg_pool1d&#39;</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;count_include_pad&#39;</span><span class="p">,</span> <span class="n">count_include_pad</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="s1">&#39;avg_pool1d&#39;</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="s1">&#39;avg_pool1d&#39;</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s2">&quot;stride&quot;</span><span class="p">,</span> <span class="s1">&#39;avg_pool1d&#39;</span><span class="p">)</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">check_non_negative_int</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="n">arg_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prim_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check argument is non-negative integer, which mean arg_value &gt;= 0.&quot;&quot;&quot;</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="n">arg_name</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">avg_pool1d</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">count_include_pad</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a 1D average pooling over an input Tensor which can be regarded as a composition of 1D input planes.</span>

<span class="sd">    Typically the input is of shape :math:`(N_{in}, C_{in}, L_{in})`, avg_pool1d outputs regional average in the</span>
<span class="sd">    :math:`(L_{in})`-dimension. Given kernel size :math:`ks = l_{ker}` and `stride` :math:`s = s_0`, the</span>
<span class="sd">    operation is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(N_i, C_j, l) = \frac{1}{l_{ker}} \sum_{n=0}^{l_{ker}-1}</span>
<span class="sd">        \text{input}(N_i, C_j, s_0 \times l + n)</span>

<span class="sd">    .. warning::</span>
<span class="sd">        `kernel_size` is in the range `[1, 255]`. `stride` is in the range `[1, 63]`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Tensor of shape :math:`(N, C_{in}, L_{in})`.</span>
<span class="sd">        kernel_size (int): The size of kernel window used to take the average value, Default: 1.</span>
<span class="sd">        stride (int): The distance of kernel moving, an int number that represents the height and</span>
<span class="sd">            width of movement are both strides, or a tuple of two int numbers that represent height and width of</span>
<span class="sd">            movement respectively. Default: 1.</span>
<span class="sd">        padding (Union(int, tuple[int])): The pad value to be filled. If `padding` is an integer, the paddings of left</span>
<span class="sd">            and right are the same, equal to pad. If `padding` is a tuple of `2` integers, the padding of left and right</span>
<span class="sd">            equal to `padding[0]` and `padding[1]` correspondingly. Default: 0.</span>
<span class="sd">        ceil_mode (bool): If True, apply ceil instead of floor to compute the output shape. Default: False.</span>
<span class="sd">        count_include_pad (bool): If True, include the zero-padding in the averaging calculation. Default: True.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor of shape :math:`(N, C_{out}, L_{out})`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not an Tensor.</span>
<span class="sd">        TypeError: If `kernel_size` or `stride` is not an int.</span>
<span class="sd">        TypeError: If `ceil_mode` or `count_include_pad` is not a bool.</span>
<span class="sd">        ValueError: If length of shape of `input_x` is not equal to `3`.</span>
<span class="sd">        ValueError: If `kernel_size` or `stride` is less than `1`.</span>
<span class="sd">        ValueError: If `padding` is not int nor a tuple whose length is equal to `2`.</span>
<span class="sd">        ValueError: If value(s) of `padding` is less than `0`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.random.randint(0, 10, [1, 3, 6]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.avg_pool1d(input_x, kernel_size=6, stride=1)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 3, 1)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For avg_pool1d, the input input_x must be tensor&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For avg_pool1d, input must have 3 dim, but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)))</span>

    <span class="n">_check_avgpool_1d_type_and_int</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">,</span> <span class="n">count_include_pad</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="s1">&#39;padding&#39;</span><span class="p">,</span> <span class="s1">&#39;avg_pool1d&#39;</span><span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">padding</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For avg_pool1d, padding should be int or tuple of length 2.&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">padding</span><span class="p">:</span>
            <span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s1">&#39;padding&#39;</span><span class="p">,</span> <span class="s1">&#39;avg_pool1d&#39;</span><span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">padding</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">padding</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For avg_pool1d, padding should be int or tuple of length 2.&quot;</span><span class="p">)</span>

    <span class="n">expand_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">)()</span>
    <span class="n">squeeze_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Squeeze</span><span class="p">)((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="n">avg_pool_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">AvgPool3D</span><span class="p">)(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">),</span>
                                               <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="p">),</span>
                                               <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span>
                                               <span class="n">pad</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
                                               <span class="n">ceil_mode</span><span class="o">=</span><span class="n">ceil_mode</span><span class="p">,</span>
                                               <span class="n">count_include_pad</span><span class="o">=</span><span class="n">count_include_pad</span><span class="p">)</span>
    <span class="n">input_x</span> <span class="o">=</span> <span class="n">expand_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">input_x</span> <span class="o">=</span> <span class="n">expand_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">input_x</span> <span class="o">=</span> <span class="n">avg_pool_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
    <span class="n">input_x</span> <span class="o">=</span> <span class="n">squeeze_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">input_x</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_avgpool_2d_kernel_size</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;check and calculate the avgpool2d kernel_size&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="s1">&#39;avg_pool2d&#39;</span><span class="p">)</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For avg_pool2d, kernel_size should be int or tuple of length 2.&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">kernel_size</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="s1">&#39;avg_pool2d&#39;</span><span class="p">)</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For avg_pool2d, kernel_size should be int or tuple of length 2.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">kernel_size</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_avgpool_2d_stride</span><span class="p">(</span><span class="n">stride</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;check and calculate the avgpool2d stride&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s2">&quot;stride&quot;</span><span class="p">,</span> <span class="s1">&#39;avg_pool2d&#39;</span><span class="p">)</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">stride</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For avg_pool2d, stride should be int or tuple of length 2.&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">stride</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s2">&quot;stride&quot;</span><span class="p">,</span> <span class="s1">&#39;avg_pool2d&#39;</span><span class="p">)</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For avg_pool2d, stride should be int or tuple of length 2.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">stride</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_avgpool_2d_padding</span><span class="p">(</span><span class="n">padding</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;check and calculate the avgpool2d padding&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="s1">&#39;padding&#39;</span><span class="p">,</span> <span class="s1">&#39;avg_pool2d&#39;</span><span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">padding</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For avg_pool2d, padding should be int or tuple of length 4.&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">padding</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s1">&#39;padding&#39;</span><span class="p">,</span> <span class="s1">&#39;avg_pool2d&#39;</span><span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">padding</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">padding</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">padding</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">padding</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For avg_pool2d, padding should be int or tuple of length 4.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">padding</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_avg_pool2d_type_and_value</span><span class="p">(</span><span class="n">ceil_mode</span><span class="p">,</span> <span class="n">count_include_pad</span><span class="p">,</span> <span class="n">divisor_override</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;check the type of avgpool2d input&quot;&quot;&quot;</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;ceil_mode&#39;</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="s1">&#39;avg_pool2d&#39;</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;count_include_pad&#39;</span><span class="p">,</span> <span class="n">count_include_pad</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="s1">&#39;avg_pool2d&#39;</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">divisor_override</span><span class="p">,</span> <span class="s1">&#39;divisor_override&#39;</span><span class="p">,</span> <span class="s1">&#39;avg_pool2d&#39;</span><span class="p">)</span>


<div class="viewcode-block" id="avg_pool2d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.avg_pool2d.html#mindspore.ops.avg_pool2d">[docs]</a><span class="k">def</span> <span class="nf">avg_pool2d</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">count_include_pad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
               <span class="n">divisor_override</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a 2D average pooling over an input Tensor which can be regarded as a composition of 2D input planes.</span>
<span class="sd">    Typically the input is of shape :math:`(N_{in}, C_{in}, H_{in}, W_{in})`, outputs regional average in the</span>
<span class="sd">    :math:`(H_{in}, W_{in})`-dimension. Given kernel size :math:`(k_{h}, k_{w})` and `strides` , the operation</span>
<span class="sd">    is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(N_i, C_j, h, w) = \frac{1}{k_{h} * k_{w}} \sum_{m=0}^{k_{h}-1} \sum_{n=0}^{k_{w}-1}</span>
<span class="sd">        \text{input}(N_i, C_j, stride[0] \times h + m, stride[1] \times w + n)</span>

<span class="sd">    .. warning::</span>
<span class="sd">        `kernel_size` is in the range `[1, 255]`. `stride` is in the range `[1, 63]`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of kernel used to take the average value. It is an int number</span>
<span class="sd">            that represents height and width of the kernel, or a tuple of two int numbers that represent height and</span>
<span class="sd">            width respectively. Default: 1.</span>
<span class="sd">        stride (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents the height and</span>
<span class="sd">            width of movement are both strides, or a tuple of two int numbers that represent height and width of</span>
<span class="sd">            movement respectively. Default: 1.</span>
<span class="sd">        padding (Union(int, tuple[int])): The pad value to be filled. Default: 0. If `padding` is an integer, the</span>
<span class="sd">            paddings of top, bottom, left and right are the same, equal to pad. If `padding` is a tuple of `4` integers,</span>
<span class="sd">            the padding of top, bottom, left and right equal to `padding[0]`, `padding[1]`, `padding[2]` and</span>
<span class="sd">            `padding[3]` correspondingly. Default: 0.</span>
<span class="sd">        ceil_mode (bool): If True, apply ceil instead of floor to compute the output shape. Default: False.</span>
<span class="sd">        count_include_pad (bool): If True, include the zero-padding in the averaging calculation. Default: True.</span>
<span class="sd">        divisor_override (int): If specified, it will be used as divisor in the averaging calculation, otherwise</span>
<span class="sd">            `kernel_size` will be used. Default: 0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with shape :math:`(N, C_{out}, H_{out}, W_{out})`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not an Tensor.</span>
<span class="sd">        TypeError: If `kernel_size` or `stride` is neither int nor tuple.</span>
<span class="sd">        TypeError: If `ceil_mode` or `count_include_pad` is not a bool.</span>
<span class="sd">        TypeError: If `divisor_override` is not an int.</span>
<span class="sd">        ValueError: If length of shape of `input_x` is not equal to `4`.</span>
<span class="sd">        ValueError: If `kernel_size` or `stride` is less than 1.</span>
<span class="sd">        ValueError: If `kernel_size` or `stride` is a tuple whose length is not equal to `2`.</span>
<span class="sd">        ValueError: If `padding` is not int nor a tuple whose length is equal to `4`.</span>
<span class="sd">        ValueError: If value(s) of `padding` is less than `0`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(1 * 3 * 3 * 4).reshape(1, 3, 3, 4), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.avg_pool2d(x, kernel_size=2, stride=1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[ 2.5   3.5   4.5]</span>
<span class="sd">           [ 6.5   7.5   8.5]]</span>
<span class="sd">          [[14.5  15.5  16.5]</span>
<span class="sd">           [18.5  19.5  20.5]]</span>
<span class="sd">          [[26.5  27.5  28.5]</span>
<span class="sd">           [30.5  31.5  32.5]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For avg_pool2d, the input input_x must be tensor&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For avg_pool2d, input must have 4 dim, but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)))</span>

    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_avgpool_2d_kernel_size</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">_check_avgpool_2d_stride</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="n">_check_avgpool_2d_padding</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
    <span class="n">_check_avg_pool2d_type_and_value</span><span class="p">(</span><span class="n">ceil_mode</span><span class="p">,</span> <span class="n">count_include_pad</span><span class="p">,</span> <span class="n">divisor_override</span><span class="p">)</span>

    <span class="n">expand_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">)()</span>
    <span class="n">squeeze_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Squeeze</span><span class="p">)(</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">avg_pool_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">AvgPool3D</span><span class="p">)(</span><span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
                                               <span class="n">strides</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
                                               <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span>
                                               <span class="n">pad</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
                                               <span class="n">ceil_mode</span><span class="o">=</span><span class="n">ceil_mode</span><span class="p">,</span>
                                               <span class="n">count_include_pad</span><span class="o">=</span><span class="n">count_include_pad</span><span class="p">,</span>
                                               <span class="n">divisor_override</span><span class="o">=</span><span class="n">divisor_override</span><span class="p">)</span>
    <span class="n">input_x</span> <span class="o">=</span> <span class="n">expand_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">input_x</span> <span class="o">=</span> <span class="n">avg_pool_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
    <span class="n">input_x</span> <span class="o">=</span> <span class="n">squeeze_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">input_x</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_avg_pool3d_padding</span><span class="p">(</span><span class="n">padding</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check the padding value in avg_pool3d op.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="s1">&#39;padding&#39;</span><span class="p">,</span> <span class="s1">&#39;avg_pool3d&#39;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">6</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For avg_pool3d, padding should be int or tuple of length 6.&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">padding</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s1">&#39;padding&#39;</span><span class="p">,</span> <span class="s1">&#39;avg_pool3d&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For avg_pool3d, padding should be int or tuple of length 6.&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="avg_pool3d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.avg_pool3d.html#mindspore.ops.avg_pool3d">[docs]</a><span class="k">def</span> <span class="nf">avg_pool3d</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">count_include_pad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
               <span class="n">divisor_override</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a 3D average pooling over an input Tensor which can be regarded as a composition of 3D input planes.</span>
<span class="sd">    Typically the input is of shape :math:`(N, C, D_{in}, H_{in}, W_{in})`, avg_pool3d outputs regional average in the</span>
<span class="sd">    :math:`(D_{in}, H_{in}, W_{in})`-dimension. Given kernel size :math:`ks = (d_{ker}, h_{ker}, w_{ker})` and stride</span>
<span class="sd">    :math:`s = (s_0, s_1, s_2)`, the operation is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(N_i, C_j, d, h, w) =</span>
<span class="sd">        \frac{1}{d_{ker} * h_{ker} * w_{ker}} \sum_{l=0}^{d_{ker}-1} \sum_{m=0}^{h_{ker}-1} \sum_{n=0}^{w_{ker}-1}</span>

<span class="sd">        \text{input}(N_i, C_j, s_0 \times d + l, s_1 \times h + m, s_2 \times w + n)</span>
<span class="sd">    .. warning::</span>
<span class="sd">        `kernel_size` is in the range `[1, 255]`. `stride` is in the range `[1, 63]`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Tensor of shape :math:`(N, C, D_{in}, H_{in}, W_{in})`. Currently support float16 and</span>
<span class="sd">            float32 data type.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of kernel used to take the average value, is an int number</span>
<span class="sd">            that represents depth, height and width are both `kernel_size`, or a tuple of three int numbers that</span>
<span class="sd">            represent depth, height and width respectively. Default: 1.</span>
<span class="sd">        stride (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents the depth,</span>
<span class="sd">            height and width of movement are both stride, or a tuple of three int numbers that represent depth, height</span>
<span class="sd">            and width of movement respectively. Default: 1.</span>
<span class="sd">        padding (Union(int, tuple[int])): The pad value to be filled. If `padding` is an integer, the addings of head,</span>
<span class="sd">            tail, top, bottom, left and right are the same, equal to pad. If `padding` is a tuple of six integers, the</span>
<span class="sd">            padding of head, tail, top, bottom, left and right equal to padding[0], padding[1], padding[2],</span>
<span class="sd">            padding[3], padding[4] and padding[5] correspondingly. Default: 0</span>
<span class="sd">        ceil_mode (bool): If True, ceil instead of floor to compute the output shape. Default: False.</span>
<span class="sd">        count_include_pad (bool): If True, averaging calculation will include the zero-padding. Default: True.</span>
<span class="sd">        divisor_override (int): If specified, it will be used as divisor in the averaging calculation, otherwise</span>
<span class="sd">            `kernel_size` will be used. Default: 0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with shape :math:`(N, C, D_{out}, H_{out}, W_{out})`. Has the same data type with `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not an Tensor.</span>
<span class="sd">        TypeError: If `kernel_size`, `stride` or `padding` is neither an int not a tuple.</span>
<span class="sd">        TypeError: If `ceil_mode` or `count_include_pad` is not a bool.</span>
<span class="sd">        TypeError: If `divisor_override` is not an int.</span>
<span class="sd">        ValueError: If length of shape of `input_x` is not equal to `5`.</span>
<span class="sd">        ValueError: If numbers in `kernel_size` or `stride` are not positive.</span>
<span class="sd">        ValueError: If `kernel_size` or `stride` is a tuple whose length is not equal to `3`.</span>
<span class="sd">        ValueError: If `padding` is a tuple whose length is not equal to `6`.</span>
<span class="sd">        ValueError: If element of `padding` is less than `0`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.arange(1 * 2 * 2 * 2 * 3).reshape((1, 2, 2, 2, 3)), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.avg_pool3d(input_x, kernel_size=2, stride=1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[[ 5.  6.]]]</span>
<span class="sd">          [[[17. 18.]]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For avg_pool3d, the input input_x must be tensor&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">5</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For avg_pool3d, input must have 5 dim, but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)))</span>

    <span class="n">_check_avg_pool3d_padding</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>

    <span class="n">avg_pool_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">AvgPool3D</span><span class="p">)(</span><span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
                                               <span class="n">strides</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
                                               <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span>
                                               <span class="n">pad</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
                                               <span class="n">ceil_mode</span><span class="o">=</span><span class="n">ceil_mode</span><span class="p">,</span>
                                               <span class="n">count_include_pad</span><span class="o">=</span><span class="n">count_include_pad</span><span class="p">,</span>
                                               <span class="n">divisor_override</span><span class="o">=</span><span class="n">divisor_override</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">avg_pool_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="adaptive_max_pool2d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.adaptive_max_pool2d.html#mindspore.ops.adaptive_max_pool2d">[docs]</a><span class="k">def</span> <span class="nf">adaptive_max_pool2d</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This operator applies a 2D adaptive max pooling to an input signal composed of multiple input planes.</span>
<span class="sd">    That is, for any input size, the size of the specified output is H x W.</span>
<span class="sd">    The number of output features is equal to the number of input planes.</span>

<span class="sd">    The input and output data format can be &quot;NCHW&quot; and &quot;CHW&quot;. N is the batch size, C is the number of channels,</span>
<span class="sd">    H is the feature height, and W is the feature width.</span>

<span class="sd">    .. math::</span>

<span class="sd">        \begin{align}</span>
<span class="sd">        h_{start} &amp;= floor(i * H_{in} / H_{out})\\</span>
<span class="sd">        h_{end} &amp;= ceil((i + 1) * H_{in} / H_{out})\\</span>
<span class="sd">        w_{start} &amp;= floor(j * W_{in} / W_{out})\\</span>
<span class="sd">        w_{end} &amp;= ceil((j + 1) * W_{in} / W_{out})\\</span>
<span class="sd">        Output(i,j) &amp;= {\max Input[h_{start}:h_{end}, w_{start}:w_{end}]}</span>
<span class="sd">        \end{align}</span>

<span class="sd">    Note:</span>
<span class="sd">        Ascend platform only supports float16 type for input_x.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The input of adaptive_max_pool2d, which is a 3D or 4D tensor,</span>
<span class="sd">            with float16, float32 or float64 data type.</span>

<span class="sd">        output_size (Union[int, tuple]): The target output size is H x W.</span>
<span class="sd">            ouput_size can be a tuple, or a single H for H x H, and H and W can be int or None</span>
<span class="sd">            which means the output size is the same as the input.</span>

<span class="sd">        return_indices (bool): If `return_indices` is True, the indices of max value would be output.</span>
<span class="sd">            Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type as the `input_x`.</span>

<span class="sd">        Shape of the output is `input_x_shape[:len(input_x_shape) - len(out_shape)] + out_shape`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `output_size` is not int or tuple.</span>
<span class="sd">        TypeError: If `input_x` is not a tensor.</span>
<span class="sd">        TypeError: If `return_indices` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `input_x` is not float16, float32 or float64.</span>
<span class="sd">        ValueError: If `output_size` is a tuple and the length of `output_size` is not 2.</span>
<span class="sd">        ValueError: If the dimension of `input_x` is not NCHW or CHW.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # case 1: output_size=(None, 2)</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]],</span>
<span class="sd">        ...                             [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]],</span>
<span class="sd">        ...                             [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.adaptive_max_pool2d(input_x, (None, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[2. 3.]</span>
<span class="sd">           [5. 6.]</span>
<span class="sd">           [8. 9.]]</span>
<span class="sd">          [[2. 3.]</span>
<span class="sd">           [5. 6.]</span>
<span class="sd">           [8. 9.]]</span>
<span class="sd">          [[2. 3.]</span>
<span class="sd">           [5. 6.]</span>
<span class="sd">           [8. 9.]]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 2: output_size=2</span>
<span class="sd">        &gt;&gt;&gt; output = ops.adaptive_max_pool2d(input_x, 2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[5. 6.]</span>
<span class="sd">           [8. 9.]]</span>
<span class="sd">          [[5. 6.]</span>
<span class="sd">           [8. 9.]]</span>
<span class="sd">          [[5. 6.]</span>
<span class="sd">           [8. 9.]]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: output_size=(1, 2)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.adaptive_max_pool2d(input_x, (1, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[8. 9.]]</span>
<span class="sd">          [[8. 9.]]</span>
<span class="sd">          [[8. 9.]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_adaptive_max_pool2d</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">AdaptiveMaxPool2D</span><span class="p">)(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">return_indices</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_adaptive_max_pool2d</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="adaptive_max_pool3d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.adaptive_max_pool3d.html#mindspore.ops.adaptive_max_pool3d">[docs]</a><span class="k">def</span> <span class="nf">adaptive_max_pool3d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a 3D adaptive max pooling over an input signal composed of several input planes.</span>

<span class="sd">    The output is of size :math:`(D, H, W)`, for any input size.</span>
<span class="sd">    The number of output features is equal to the number of input planes.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Tensor, with shape :math:`(C, D, H, W)` or :math:`(N, C, D, H, W)`, which support int8, int16,</span>
<span class="sd">            int32, int64, uint8, uint16, uint32, uint64, float16, float32 or float64 data type.</span>
<span class="sd">        output_size (Union[int, tuple]): The target output size. `ouput_size` can be a tuple :math:`(D, H, W)`,</span>
<span class="sd">            or an int D for :math:`(D, D, D)`. :math:`D`, :math:`H` and :math:`W` can be int or None</span>
<span class="sd">            which means the output size is the same as that of the input.</span>
<span class="sd">        return_indices (bool, optional): If `return_indices` is True, the indices of max value would be output,</span>
<span class="sd">            else would not be output. Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        - **y** (Tensor) - Tensor, with the same number of dims and data type as the `x`.</span>
<span class="sd">        - **argmax** (Tensor) - Tensor, the indices of max value, which has the same shape as the</span>
<span class="sd">          `y` and it&#39;s data type is int32. It will output only when `return_indices` is True.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        ValueError: If the dimensions number of `x` is not 4 or 5.</span>
<span class="sd">        TypeError: If dtype of `x` is not int8, int16, int32, int64, uint8, uint16, uint32, uint64,</span>
<span class="sd">                   float16, float32 or float64.</span>
<span class="sd">        ValueError: If `output_size` is neither an int nor a tuple with shape (3,).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(0,36).reshape((1, 3, 3, 4)).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output_size = (1, 1, 2)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.adaptive_max_pool3d(x, output_size, True)</span>
<span class="sd">        &gt;&gt;&gt; print(output[0].asnumpy())</span>
<span class="sd">        [[[[33. 35.]]]]</span>
<span class="sd">        &gt;&gt;&gt; print(output[1].asnumpy())</span>
<span class="sd">        [[[[33 35]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">adaptive_max_pool3d_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">AdaptiveMaxPool3D</span><span class="p">)()</span>
    <span class="n">output_size_</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">adaptive_max_pool3d_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">output_size_</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">out</span> <span class="k">if</span> <span class="n">return_indices</span> <span class="k">else</span> <span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">output</span></div>


<div class="viewcode-block" id="max_unpool1d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.max_unpool1d.html#mindspore.ops.max_unpool1d">[docs]</a><span class="k">def</span> <span class="nf">max_unpool1d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes a partial inverse of maxpool1d.</span>

<span class="sd">    maxpool1d is not fully invertible, since the non-maximal values are lost.</span>

<span class="sd">    max_unpool1d takes the output of maxpool1d as input including the indices of the maximal values</span>
<span class="sd">    and computes a partial inverse in which all non-maximal values are set to zero. Typically the input</span>
<span class="sd">    is of shape :math:`(N, C, H_{in})` or :math:`(C, H_{in})`, and the output is of shape :math:`(N, C, H_{out}`</span>
<span class="sd">    or :math:`(C, H_{out}`. The operation is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">        H_{out} = (H{in} - 1) \times stride[0] - 2 \times padding[0] + kernel\_size[0] \\</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input Tensor to invert. Tensor of shape :math:`(N, C, H_{in})` or :math:`(C, H_{in})`.</span>
<span class="sd">        indices (Tensor): Index of maximum value.</span>
<span class="sd">          Tensor of shape must be same with input &#39;x&#39;.</span>
<span class="sd">          Values of indices must belong to :math:`[0, H_{in} - 1]`.</span>
<span class="sd">          Data type must be in int32 or int64.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of kernel used to take the maximum value.</span>
<span class="sd">        stride (Union[int, tuple[int]]): The distance of kernel moving,</span>
<span class="sd">            If stride is 0, (0) or None, then stride equal to kernel_size. Default: None.</span>
<span class="sd">        padding (Union[int, tuple[int]]): The pad value to be filled. Default: 0.</span>
<span class="sd">        output_size (tuple[int], optional): The output shape. Default: None.</span>
<span class="sd">            If output_size == (), then the shape of output computed by `kernel_size`, `stride` and `padding`.</span>
<span class="sd">            If output_size != (), then output_size must be :math:`(N, C, H)` or</span>
<span class="sd">            :math:`(C, H)` and output_size must belong to</span>
<span class="sd">            :math:`[(N, C, H_{out} - stride[0]), (N, C, H_{out} + stride[0])]`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with shape :math:`(N, C, H_{out})` or :math:`(C, H_{out})`,</span>
<span class="sd">        with the same data type with `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If data type of `x` or `indices` is not supported.</span>
<span class="sd">        TypeError: If `kernel_size`, `stride` or `padding` is neither an int nor a tuple.</span>
<span class="sd">        ValueError: If numbers in `stride`, `padding` (also support 0 and (0)) or `kernel_size` is not positive.</span>
<span class="sd">        ValueError: If the shapes of `x` and `indices` are not equal.</span>
<span class="sd">        ValueError: If `x` whose length is not 2 or 3.</span>
<span class="sd">        ValueError: If type of `output_size` is not tuple.</span>
<span class="sd">        ValueError: If `output_size` whose length is not 0, 2 or 3.</span>
<span class="sd">        ValueError: If `output_size` is not close to output size computed by attr `kernel_size`, `stride`, `padding`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[2, 4, 6, 8]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[1, 3, 5, 7]]).astype(np.int64))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.max_unpool1d(x, indices, kernel_size =2, stride=2, padding=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output.asnumpy())</span>
<span class="sd">        [[0, 2, 0, 4, 0, 6, 0, 8]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">output_size</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For max_unpool1d, output_size must be tuple, but type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For max_unpool1d, length of output_size with tuple must be 0, 2, 3, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got type </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">output_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output_size</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">output_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output_size</span>

    <span class="n">shape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()</span>
    <span class="n">x_shape</span> <span class="o">=</span> <span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">indices_shape</span> <span class="o">=</span> <span class="n">shape</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
    <span class="n">x_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x_shape</span> <span class="o">!=</span> <span class="n">indices_shape</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For max_unpool1d, the x shape and indices shape must be equal, but got input &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;shape </span><span class="si">{</span><span class="n">x_shape</span><span class="si">}</span><span class="s2"> and indices shape </span><span class="si">{</span><span class="n">indices_shape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x_dim</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For max_unpool1d, the x shape must have 2 or 3 dims, but got </span><span class="si">{</span><span class="n">x_dim</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="n">max_unpool_2d</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">MaxUnpool2D</span><span class="p">)(</span><span class="n">ksize</span><span class="o">=</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                                                 <span class="n">pads</span><span class="o">=</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">output_shape</span><span class="o">=</span><span class="n">output_size</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x_dim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">max_unpool_2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">max_unpool_2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="max_unpool2d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.max_unpool2d.html#mindspore.ops.max_unpool2d">[docs]</a><span class="k">def</span> <span class="nf">max_unpool2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes a partial inverse of maxpool2d.</span>

<span class="sd">    maxpool2d is not fully invertible, since the non-maximal values are lost.</span>

<span class="sd">    max_unpool2d takes the output of maxpool2d as inputs including the indices of the maximal values</span>
<span class="sd">    and computes a partial inverse in which all non-maximal values are set to zero. Typically the input</span>
<span class="sd">    is of shape :math:`(N, C, H_{in}, W_{in})` or :math:`(C, H_{in}, W_{in})`, and the output is of</span>
<span class="sd">    shape :math:`(N, C, H_{out}, W_{out})` or :math:`(C, H_{out}, W_{out})`. The operation is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">        H_{out} = (H{in} - 1) \times stride[0] - 2 \times padding[0] + kernel\_size[0] \\</span>
<span class="sd">        W_{out} = (W{in} - 1) \times stride[1] - 2 \times padding[1] + kernel\_size[1] \\</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input Tensor to invert.</span>
<span class="sd">          Tensor of shape :math:`(N, C, H_{in}, W_{in})` or :math:`(C, H_{in}, W_{in})`.</span>
<span class="sd">        indices (Tensor): Max values&#39; index represented by the indices.</span>
<span class="sd">          Tensor of shape must be same with input &#39;x&#39;.</span>
<span class="sd">          Values of indices must belong to :math:`[0, H_{in} \times W_{in} - 1]`.</span>
<span class="sd">          Data type must be in int32 or int64.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of kernel used to take the maximum value,</span>
<span class="sd">            an int number that represents height and width of the kernel, or a tuple</span>
<span class="sd">            of two int numbers that represent height and width respectively.</span>
<span class="sd">        stride (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the height and width of movement are both stride, or a tuple of two int numbers that</span>
<span class="sd">            represent height and width of movement respectively.</span>
<span class="sd">            If stride is 0, (0, 0) or None, then stride equal to kernel_size. Default: None.</span>
<span class="sd">        padding (Union[int, tuple[int]]): The pad value to be filled. Default: 0. If `padding` is an integer,</span>
<span class="sd">            the paddings of height and width are the same, equal to padding. If `padding` is a tuple of two</span>
<span class="sd">            integers, the padding of height and width equal to padding[0] and padding[1] correspondingly.</span>
<span class="sd">        output_size (tuple[int], optional): The target output size. Default: None.</span>
<span class="sd">            If output_size == (), then the shape of output computed by `kernel_size`, `stride` and `padding`.</span>
<span class="sd">            If output_size != (), then output_size must be :math:`(N, C, H, W)` and output_size must belong to</span>
<span class="sd">            :math:`[(N, C, H_{out} - stride[0], W_{out} - stride[1]),</span>
<span class="sd">            (N, C, H_{out} + stride[0], W_{out} + stride[1])]`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with shape :math:`(N, C, H_{out}, W_{out})` or :math:`(C, H_{out}, W_{out})`,</span>
<span class="sd">        with the same data type with `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If data type of `x` or `indices` is not supported.</span>
<span class="sd">        TypeError: If `kernel_size`, `stride` or `padding` is neither an int nor a tuple.</span>
<span class="sd">        ValueError: If numbers in `stride`, `padding` (also support 0 and (0, 0)) or `kernel_size` is not positive.</span>
<span class="sd">        ValueError: If the shape of `x` and `indices` are not equal.</span>
<span class="sd">        ValueError: If `kernel_size`, `stride` or `padding` is a tuple whose length is not equal to 2.</span>
<span class="sd">        ValueError: If `x` whose length is not 3 or 4.</span>
<span class="sd">        ValueError: If `output_size` whose type is not tuple.</span>
<span class="sd">        ValueError: If `output_size` whose length is not 0, 3 or 4.</span>
<span class="sd">        ValueError: If `output_size` is not close to output size computed by attr `kernel_size`, `stride`, `padding`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[[0, 1], [8, 9]]]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[[[0, 1], [2, 3]]]]).astype(np.int64))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.max_unpool2d(x, indices, kernel_size=1, stride=1, padding=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output.asnumpy())</span>
<span class="sd">        [[[[0. 1.]</span>
<span class="sd">           [8. 9.]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">output_size</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For max_unpool2d, output_size must be tuple, but type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For max_unpool2d, length of output_size with tuple must be 0, 3, 4, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got type </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">output_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output_size</span>

    <span class="n">shape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()</span>
    <span class="n">x_shape</span> <span class="o">=</span> <span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">indices_shape</span> <span class="o">=</span> <span class="n">shape</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
    <span class="n">x_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x_shape</span> <span class="o">!=</span> <span class="n">indices_shape</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For max_unpool2d, the x shape and indices shape must be equal, but got input &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;shape </span><span class="si">{</span><span class="n">x_shape</span><span class="si">}</span><span class="s2"> and indices shape </span><span class="si">{</span><span class="n">indices_shape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x_dim</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For max_unpool2d, the x shape must have 3 or 4 dims, but got </span><span class="si">{</span><span class="n">x_dim</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="n">max_unpool_2d</span> <span class="o">=</span> <span class="n">MaxUnpool2D</span><span class="p">(</span><span class="n">ksize</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">pads</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span> <span class="n">output_shape</span><span class="o">=</span><span class="n">output_size</span><span class="p">,</span>
                                <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x_dim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">max_unpool_2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">max_unpool_2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="max_unpool3d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.max_unpool3d.html#mindspore.ops.max_unpool3d">[docs]</a><span class="k">def</span> <span class="nf">max_unpool3d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes a partial inverse of maxpool3d.</span>

<span class="sd">    maxpool3d is not fully invertible, since the non-maximal values are lost.</span>

<span class="sd">    max_unpool3d takes the output of maxpool3d as input including the indices of the maximal values and computes a</span>
<span class="sd">    partial inverse in which all non-maximal values are set to zero. Typically the input is of shape</span>
<span class="sd">    :math:`(N, C, D_{in}, H_{in}, W_{in})` or :math:`(C, D_{in}, H_{in}, W_{in})`, and the output is of shape</span>
<span class="sd">    :math:`(N, C, D_{out}, H_{out}, W_{out})` or :math:`(C, D_{out}, H_{out}, W_{out})`. The operation is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">        D_{out} = (D{in} - 1) \times stride[0] - 2 \times padding[0] + kernel\_size[0] \\</span>
<span class="sd">        H_{out} = (H{in} - 1) \times stride[1] - 2 \times padding[1] + kernel\_size[1] \\</span>
<span class="sd">        W_{out} = (W{in} - 1) \times stride[2] - 2 \times padding[2] + kernel\_size[2] \\</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input Tensor to invert.</span>
<span class="sd">          Tensor of shape :math:`(N, C, D_{in}, H_{in}, W_{in})` or :math:`(C, D_{in}, H_{in}, W_{in})`.</span>
<span class="sd">        indices (Tensor): Max values&#39; index represented by the indices. Tensor of shape must be same with input &#39;x&#39;.</span>
<span class="sd">          Values of indices must belong to :math:`[0, D_{in} \times H_{in} \times W_{in} - 1]`.</span>
<span class="sd">          Data type must be in int32 or int64.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of kernel used to take the maximum value,</span>
<span class="sd">            an int number that represents depth, height and width of the kernel, or a tuple</span>
<span class="sd">            of three int numbers that represent depth, height and width respectively.</span>
<span class="sd">        stride (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the depth, height and width of movement are both stride, or a tuple of three int numbers that</span>
<span class="sd">            represent depth, height and width of movement respectively.</span>
<span class="sd">            If stride is 0, (0, 0, 0) or None, then stride equal to kernel_size. Default: None.</span>
<span class="sd">        padding (Union[int, tuple[int]]): The pad value to be filled. Default: 0. If `padding` is an integer,</span>
<span class="sd">            the paddings of depth, height and width are the same, equal to padding. If `padding` is a tuple of three</span>
<span class="sd">            integers, the padding of depth, height and width equal to padding[0], padding[1] and padding[2]</span>
<span class="sd">            correspondingly.</span>
<span class="sd">        output_size (tuple[int], optional): The output size. Default: None. If output_size == (), then the shape of</span>
<span class="sd">            output computed by `kernel_size`, `stride` and `padding`. If output_size != (), then output_size must be</span>
<span class="sd">            :math:`(N, C, D, H, W)` or :math:`(C, D, H, W)` and output_size must belong to</span>
<span class="sd">            :math:`[(N, C, D_{out} - stride[0], H_{out} - stride[1], W_{out} - stride[2]),</span>
<span class="sd">            (N, C, D_{out} + stride[0], H_{out} + stride[1], W_{out} + stride[2])]`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with shape :math:`(N, C, D_{out}, H_{out}, W_{out})` or :math:`(C, D_{out}, H_{out}, W_{out})`,</span>
<span class="sd">        with the same data type with `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If data type of `x` or `indices` is not supported.</span>
<span class="sd">        TypeError: If `kernel_size`, `stride` or `padding` is neither an int nor a tuple.</span>
<span class="sd">        ValueError: If numbers in `stride` or `padding` (also support 0 and (0, 0, 0)) or `kernel_size` is not positive.</span>
<span class="sd">        ValueError: If the shape of `x` and `indices` are not equal.</span>
<span class="sd">        ValueError: If `kernel_size`, `stride` or `padding` is a tuple whose length is not equal to 3.</span>
<span class="sd">        ValueError: If `x` whose length is not 4 or 5.</span>
<span class="sd">        ValueError: If `output_size` whose length is not 0, 4 or 5.</span>
<span class="sd">        ValueError: If `output_size` whose type is not tuple.</span>
<span class="sd">        ValueError: If `output_size` is not close to output size computed by attr `kernel_size`, `stride`, `padding`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[[[0, 1], [8, 9]]]]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices= Tensor(np.array([[[[[0, 1], [2, 3]]]]]).astype(np.int64))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.max_unpool3d(x, indices, kernel_size=2, stride=1, padding=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[[0. 1. 8.]</span>
<span class="sd">            [9. 0. 0.]</span>
<span class="sd">            [0. 0. 0.]]</span>
<span class="sd">           [[0. 0. 0.]</span>
<span class="sd">            [0. 0. 0.]</span>
<span class="sd">            [0. 0. 0.]]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">output_size</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For max_unpool3d, output_size must be tuple, but type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For max_unpool3d, length of output_size with tuple must be 0, 4, 5, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got type </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">output_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output_size</span>
    <span class="n">max_unpool_3d</span> <span class="o">=</span> <span class="n">MaxUnpool3D</span><span class="p">(</span><span class="n">ksize</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">pads</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span> <span class="n">output_shape</span><span class="o">=</span><span class="n">output_size</span><span class="p">,</span>
                                <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCDHW&quot;</span><span class="p">)</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()</span>
    <span class="n">x_shape</span> <span class="o">=</span> <span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">indices_shape</span> <span class="o">=</span> <span class="n">shape</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
    <span class="n">x_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x_shape</span> <span class="o">!=</span> <span class="n">indices_shape</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For max_unpool3d, the x shape and indices shape must be equal, but got input &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;shape </span><span class="si">{</span><span class="n">x_shape</span><span class="si">}</span><span class="s2"> and indices shape </span><span class="si">{</span><span class="n">indices_shape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x_dim</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For max_unpool3d, the x shape must have 4 or 5 dims, but got </span><span class="si">{</span><span class="n">x_dim</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x_dim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">max_unpool_3d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">max_unpool_3d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="binary_cross_entropy_with_logits"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.binary_cross_entropy_with_logits.html#mindspore.ops.binary_cross_entropy_with_logits">[docs]</a><span class="k">def</span> <span class="nf">binary_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">pos_weight</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adds sigmoid activation function to input `logits`, and uses the given logits to compute binary cross entropy</span>
<span class="sd">    between the logits and the label.</span>

<span class="sd">    Sets input logits as :math:`X`, input label as :math:`Y`, input weight as :math:`W`, output as :math:`L`. Then,</span>

<span class="sd">    .. math::</span>

<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            p_{ij} = sigmoid(X_{ij}) = \frac{1}{1 + e^{-X_{ij}}} \\</span>
<span class="sd">            L_{ij} = -[Y_{ij} * log(p_{ij}) + (1 - Y_{ij})log(1 - p_{ij})]</span>
<span class="sd">        \end{array}</span>

<span class="sd">    :math:`i` indicates the :math:`i^{th}` sample, :math:`j` indicates the category. Then,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = \begin{cases}</span>
<span class="sd">        L, &amp; \text{if reduction} = \text{&#39;none&#39;;}\\</span>
<span class="sd">        \operatorname{mean}(L), &amp; \text{if reduction} = \text{&#39;mean&#39;;}\\</span>
<span class="sd">        \operatorname{sum}(L),  &amp; \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    :math:`\ell` indicates the method of calculating the loss. There are three methods:</span>
<span class="sd">    the first method is to provide the loss value directly,</span>
<span class="sd">    the second method is to calculate the average value of all losses,</span>
<span class="sd">    and the third method is to calculate the sum of all losses.</span>

<span class="sd">    This operator will multiply the output by the corresponding weight.</span>
<span class="sd">    The tensor weight assigns different weights to each piece of data in the batch,</span>
<span class="sd">    and the tensor pos_weight adds corresponding weights to the positive examples of each category.</span>

<span class="sd">    In addition, it can trade off recall and precision by adding weights to positive examples.</span>
<span class="sd">    In the case of multi-label classification the loss can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            p_{ij,c} = sigmoid(X_{ij,c}) = \frac{1}{1 + e^{-X_{ij,c}}} \\</span>
<span class="sd">            L_{ij,c} = -[P_{c}Y_{ij,c} * log(p_{ij,c}) + (1 - Y_{ij,c})log(1 - p_{ij,c})]</span>
<span class="sd">        \end{array}</span>

<span class="sd">    where c is the class number (c&gt;1 for multi-label binary classification, c=1 for single-label binary classification),</span>
<span class="sd">    n is the number of the sample in the batch and :math:`p_c` is the weight of the positive answer for the class c.</span>
<span class="sd">    :math:`p_c&gt;1` increases the recall, :math:`p_c&lt;1` increases the precision.</span>

<span class="sd">    Args:</span>
<span class="sd">        logits (Tensor): Input logits. Data type must be float16 or float32.</span>
<span class="sd">          Tensor of shape :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        label (Tensor): Ground truth label, has the same shape as `logits`.</span>
<span class="sd">          Data type must be float16 or float32.</span>
<span class="sd">        weight (Tensor): A rescaling weight applied to the loss of each batch element. It can be</span>
<span class="sd">          broadcast to a tensor with shape of `logits`. Data type must be float16 or float32.</span>
<span class="sd">        pos_weight (Tensor): A weight of positive examples. Must be a vector with length equal to the</span>
<span class="sd">          number of classes. It can be broadcast to a tensor with shape of `logits`.</span>
<span class="sd">          Data type must be float16 or float32.</span>
<span class="sd">        reduction (str): Type of reduction to be applied to loss. The optional values are &#39;mean&#39;, &#39;sum&#39;, and &#39;none&#39;,</span>
<span class="sd">             not case sensitive. If &#39;none&#39;, do not perform reduction. Default: &#39;mean&#39;.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Tensor or Scalar, if `reduction` is &#39;none&#39;, it&#39;s a tensor with the same shape and type as input `logits`.</span>
<span class="sd">        Otherwise, the output is a scalar.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If input `logits`, `label`, `weight`, `pos_weight` is not Tensor.</span>
<span class="sd">        TypeError: If data type of input `logits`, `label`, `weight`, `pos_weight` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If data type of input `reduction` is not string.</span>
<span class="sd">        ValueError: If `weight` or `pos_weight` can not be broadcast to a tensor with shape of `logits`.</span>
<span class="sd">        ValueError: If `reduction` is not one of &#39;none&#39;, &#39;mean&#39; or &#39;sum&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([[-0.8, 1.2, 0.7], [-0.1, -0.4, 0.7]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; label = Tensor(np.array([[0.3, 0.8, 1.2], [-0.6, 0.1, 2.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.array([1.0, 1.0, 1.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; pos_weight = Tensor(np.array([1.0, 1.0, 1.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.binary_cross_entropy_with_logits(logits, label, weight, pos_weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.3463612</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">bce_with_logits_loss_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">)(</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">bce_with_logits_loss_op</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">pos_weight</span><span class="p">)</span></div>


<div class="viewcode-block" id="dropout"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.dropout.html#mindspore.ops.dropout">[docs]</a><span class="k">def</span> <span class="nf">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">seed0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">seed1</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    During training, randomly zeroes some of the elements of the input tensor</span>
<span class="sd">    with probability `p` from a Bernoulli distribution. It plays the role of</span>
<span class="sd">    reducing neuron correlation and avoid overfitting.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input of Dropout, a Tensor of any shape with data type of float16 or float32.</span>
<span class="sd">        p (float, optional): The dropping rate, between 0 and 1, e.g. p = 0.1,</span>
<span class="sd">            means dropping out 10% of input units. Default: 0.5.</span>
<span class="sd">        seed0 (int, optional): seed0 value for random generating. Default: 0.</span>
<span class="sd">        seed1 (int, optional): seed1 value for random generating. Default: 0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        - **output** (Tensor) - With the same shape and data type as `x`.</span>
<span class="sd">        - **mask** (Tensor) - With the same shape as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `p` is not a float.</span>
<span class="sd">        TypeError: If `seed0` or `seed1` is not an int.</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(((20, 16), (50, 50)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output, mask = ops.dropout(x, p=0.5)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 2)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">keep_prob</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">p</span>
    <span class="n">dropout_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">keep_prob</span><span class="o">=</span><span class="n">keep_prob</span><span class="p">,</span> <span class="n">Seed0</span><span class="o">=</span><span class="n">seed0</span><span class="p">,</span> <span class="n">Seed1</span><span class="o">=</span><span class="n">seed1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dropout_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">celu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    celu activation function, computes celu (Continuously differentiable exponential</span>
<span class="sd">    linear units) of input tensors element-wise. The formula is defined as follows:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{CeLU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1))</span>

<span class="sd">    For more details, please refer to `celu &lt;https://arxiv.org/abs/1704.07483&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input of celu with data type of float16 or float32.</span>
<span class="sd">        alpha (float): The :math:`\alpha` value for the Celu formulation. Default: 1.0</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same data type and shape as the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `alpha` is not a float.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 nor float32.</span>
<span class="sd">        ValueError: If `alpha` has the value of 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-2.0, -1.0, 1.0, 2.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.celu(x, alpha=1.0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-0.86466473 -0.63212055  1.          2.        ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">celu_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">CeLU</span><span class="p">)(</span><span class="n">alpha</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">celu_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<div class="viewcode-block" id="dropout1d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.dropout1d.html#mindspore.ops.dropout1d">[docs]</a><span class="k">def</span> <span class="nf">dropout1d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    During training, randomly zeroes some channels of the input tensor with probability `p`</span>
<span class="sd">    from a Bernoulli distribution(For a 3-dimensional tensor with a shape of :math:`NCL`,</span>
<span class="sd">    the channel feature map refers to a 1-dimensional feature map with the shape of :math:`L`).</span>

<span class="sd">    For example, the :math:`j\_th` channel of the :math:`i\_th` sample in the batched input is a to-be-processed</span>
<span class="sd">    `1D` tensor input[i,j].</span>
<span class="sd">    Each channel will be zeroed out independently on every forward call which based on Bernoulli distribution</span>
<span class="sd">    probability `p`.</span>

<span class="sd">    The parper `Dropout: A Simple Way to Prevent Neural Networks from Overfitting</span>
<span class="sd">    &lt;http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf&gt;`_ mentioned this technology，And it is proved that</span>
<span class="sd">    it can effectively reduce over fitting and prevent neuronal coadaptation.</span>
<span class="sd">    For more details, refer to `Improving neural networks by preventing co-adaptation of feature detectors</span>
<span class="sd">    &lt;https://arxiv.org/pdf/1207.0580.pdf&gt;`_ .</span>

<span class="sd">    `dropout1d` can improve the independence between channel feature maps.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): A tensor with shape :math:`(N, C, L)` or :math:`(C, L)`, where `N` is the batch size, `C` is the</span>
<span class="sd">            number of channels, `L` is the feature length. The data type must be int8, int16, int32, int64, float16,</span>
<span class="sd">            float32 or float64.</span>
<span class="sd">        p (float): The dropping probability of a channel, between 0 and 1, e.g. `p` = 0.8,</span>
<span class="sd">            which means an 80% chance of clearing. Default: 0.5.</span>
<span class="sd">        training (bool): Apply dropout if is True. Default: True.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, output, with the same shape and data type as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not int8, int16, int32, int64, float16, float32 or float64.</span>
<span class="sd">        TypeError: If the data type of `p` is not float.</span>
<span class="sd">        ValueError: If `p` is out of the range `[0.0, 1.0]`.</span>
<span class="sd">        ValueError: If `x` shape is not `2D` or `3D`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.random.randn(4, 3), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.dropout1d(input_x, 0.5)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (4, 3)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">training</span><span class="p">:</span>
        <span class="n">p</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">dropout_2d_op</span> <span class="o">=</span> <span class="n">NN_OPS</span><span class="o">.</span><span class="n">Dropout2D</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">dropout_2d_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">dropout_2d_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="dropout2d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.dropout2d.html#mindspore.ops.dropout2d">[docs]</a><span class="k">def</span> <span class="nf">dropout2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    During training, randomly zeroes some channels of the input tensor with probability `p`</span>
<span class="sd">    from a Bernoulli distribution(For a 4-dimensional tensor with a shape of :math:`NCHW`,</span>
<span class="sd">    the channel feature map refers to a 2-dimensional feature map with the shape of :math:`HW`).</span>

<span class="sd">    For example, the :math:`j\_th` channel of the :math:`i\_th` sample in the batched input is a to-be-processed</span>
<span class="sd">    `2D` tensor input[i,j].</span>
<span class="sd">    Each channel will be zeroed out independently on every forward call which based on Bernoulli distribution</span>
<span class="sd">    probability `p`.</span>
<span class="sd">    The parper `Dropout: A Simple Way to Prevent Neural Networks from Overfitting</span>
<span class="sd">    &lt;http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf&gt;`_ mentioned this technology，And it is proved that</span>
<span class="sd">    it can effectively reduce over fitting and prevent neuronal coadaptation.</span>
<span class="sd">    For more details, refer to `Improving neural networks by preventing co-adaptation of feature detectors</span>
<span class="sd">    &lt;https://arxiv.org/pdf/1207.0580.pdf&gt;`_ .</span>

<span class="sd">    `dropout2d` can improve the independence between channel feature maps.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): A `4D` tensor with shape :math:`(N, C, H, W)`, where `N` is the batch size, `C` is the number</span>
<span class="sd">            of channels, `H` is the feature height, and `W` is the feature width. The data type must be int8,</span>
<span class="sd">            int16, int32, int64, float16, float32 or float64.</span>
<span class="sd">        p (float): The dropping probability of a channel, between 0 and 1, e.g. `p` = 0.8,</span>
<span class="sd">            which means dropping out 80% of channels. Default: 0.5.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, output, with the same shape and data type as `x`.</span>

<span class="sd">        Tensor, mask, with the same shape as `x` and the data type is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not int8, int16, int32, int64, float16, float32 or float64.</span>
<span class="sd">        TypeError: If the data type of `p` is not float.</span>
<span class="sd">        ValueError: If `p` is out of the range `[0.0, 1.0]`.</span>
<span class="sd">        ValueError: If `x` shape is not `4D`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones([2, 1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output, mask = ops.dropout2d(input_x, 0.5)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 1, 2, 3)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dropout_2d_op</span> <span class="o">=</span> <span class="n">NN_OPS</span><span class="o">.</span><span class="n">Dropout2D</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dropout_2d_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="dropout3d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.dropout3d.html#mindspore.ops.dropout3d">[docs]</a><span class="k">def</span> <span class="nf">dropout3d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    During training, randomly zeroes some channels of the input tensor</span>
<span class="sd">    with probability `p` from a Bernoulli distribution(For a 5-dimensional tensor</span>
<span class="sd">    with a shape of :math:`NCDHW`, the channel feature map refers to a 3-dimensional</span>
<span class="sd">    feature map with a shape of :math:`DHW`).</span>

<span class="sd">    For example, the :math:`j\_th` channel of the :math:`i\_th` sample in the batched input is a to-be-processed</span>
<span class="sd">    `3D` tensor input[i,j].</span>
<span class="sd">    Each channel will be zeroed out independently on every forward call which based on Bernoulli distribution</span>
<span class="sd">    probability `p`.</span>

<span class="sd">    `dropout3d` can improve the independence between channel feature maps.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): A `5D` tensor with shape :math:`(N, C, D, H, W)`, where `N` is the batch size, `C` is the number</span>
<span class="sd">            of channels, `D` is the feature depth, `H` is the feature height, and `W` is the feature width.</span>
<span class="sd">            The data type must be int8, int16, int32, int64, float16, float32 or float64.</span>
<span class="sd">        p (float): The dropping probability of a channel, between 0 and 1, e.g. `p` = 0.8,</span>
<span class="sd">            which means dropping out 80% of channels. Default: 0.5.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, output, with the same shape and data type as `x`.</span>

<span class="sd">        Tensor, mask, with the same shape as `x` and the data type is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not int8, int16, int32, int64, float16, float32 or float64.</span>
<span class="sd">        TypeError: If the data type of `p` is not float.</span>
<span class="sd">        ValueError: If `p` is out of the range `[0.0, 1.0]`.</span>
<span class="sd">        ValueError: If `x` shape is not 5D.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones([2, 1, 2, 1, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output, mask = ops.dropout3d(input_x, 0.5)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 1, 2, 1, 2)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dropout_3d_op</span> <span class="o">=</span> <span class="n">NN_OPS</span><span class="o">.</span><span class="n">Dropout3D</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dropout_3d_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="fast_gelu"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.fast_gelu.html#mindspore.ops.fast_gelu">[docs]</a><span class="k">def</span> <span class="nf">fast_gelu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fast Gaussian Error Linear Units activation function.</span>

<span class="sd">    FastGeLU is defined as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output} = \frac {x} {1 + \exp(-1.702 * \left| x \right|)} * \exp(0.851 * (x - \left| x \right|)),</span>

<span class="sd">    where :math:`x` is the element of the input.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input to compute the FastGeLU with data type of float16 or float32.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.fast_gelu(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-1.5418735e-01  3.9921875e+00 -9.7473649e-06]</span>
<span class="sd">         [ 1.9375000e+00 -1.0052517e-03  8.9824219e+00]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">fast_gelu_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_float_range_inc_right</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="n">lower_limit</span><span class="p">,</span> <span class="n">upper_limit</span><span class="p">,</span> <span class="n">arg_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prim_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Method for checking whether input value is in float range inc right.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="n">lower_limit</span><span class="p">,</span> <span class="n">upper_limit</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_RIGHT</span><span class="p">,</span> <span class="n">arg_name</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>


<div class="viewcode-block" id="fractional_max_pool2d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.fractional_max_pool2d.html#mindspore.ops.fractional_max_pool2d">[docs]</a><span class="k">def</span> <span class="nf">fractional_max_pool2d</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_ratio</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                          <span class="n">_random_samples</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a 2D fractional max pooling to an input signal.</span>
<span class="sd">    The input is composed of multiple input planes.</span>
<span class="sd">    The max-pooling operation is applied in kH × kW regions by a stochastic step size determined by</span>
<span class="sd">    the target output size. For any input size, the size of the specified output is H x W. The number</span>
<span class="sd">    of output features is equal to the number of input planes.</span>

<span class="sd">    Fractional MaxPooling is described in the paper `Fractional Max-Pooling &lt;https://arxiv.org/pdf/1412.6071&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Tensor of shape :math:`(N, C, H_{in}, W_{in})`,</span>
<span class="sd">            with float16, float32, float64, int32, int64 data type.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of kernel used to take the maximum value,</span>
<span class="sd">            is an int number that represents height and width of the kernel, or a tuple</span>
<span class="sd">            of two int numbers that represent height and width respectively.</span>
<span class="sd">            The value must be a positive integer.</span>
<span class="sd">        output_size (Union[int, tuple[int]], optional): The shape of the target `output_size`,</span>
<span class="sd">            is an int number that represents height and width, or a tuple</span>
<span class="sd">            of two int numbers that represent height and width respectively.</span>
<span class="sd">            The value must be a positive integer.</span>
<span class="sd">            Default: None.</span>
<span class="sd">        output_ratio (Union[float, tuple[float]], optional): The ratio of target output shape to input shape.</span>
<span class="sd">            Specifying the size of the output tensor by using a ratio of the input size.</span>
<span class="sd">            Data type: float16, float32, double, and value is between (0, 1).</span>
<span class="sd">            Default: None.</span>
<span class="sd">        return_indices (bool, optional): If `return_indices` is True, the indices of max value would be output.</span>
<span class="sd">            Default: False.</span>
<span class="sd">        _random_samples (Tensor, optional): The random step of FractionalMaxPool2d, which is a 3D tensor.</span>
<span class="sd">            Tensor of data type: float16, float32, double, and value is between (0, 1).</span>
<span class="sd">            Supported shape :math:`(N, C, 2)`.</span>
<span class="sd">            Default: None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        - **y** (Tensor) - Has the same type as the `input_x`.</span>
<span class="sd">          Has the shape :math:`(N, C, H, W)`.</span>

<span class="sd">        - **argmax** (Tensor) - The indices along with the outputs, which is a Tensor, with the same shape as the</span>
<span class="sd">          `y` and int64 data type. It will output only when `return_indices` is True.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If data type of `input_x` is not one of the following: float16, float32, float64, int32, int64.</span>
<span class="sd">        TypeError: If data type of `_random_samples` is not one of the following: float16, float32, float64.</span>
<span class="sd">        ValueError: If `kernel_size` is not a number and `kernel_size` is not a tuple of length 2.</span>
<span class="sd">        ValueError: If `output_size` is not a number and `output_size` is not a tuple of length 2.</span>
<span class="sd">        ValueError: If the sum of `kernel_size` , `output_size` and -1 is larger than the corresponding</span>
<span class="sd">                    dimension of `input_x`.</span>
<span class="sd">        ValueError: If the dimension of `_random_samples` is not 3.</span>
<span class="sd">        ValueError: if `output_size` and `output_ratio` are None at the same time.</span>
<span class="sd">        ValueError: If the first dimension size of `input_x` and `_random_samples` is not equal.</span>
<span class="sd">        ValueError: If the second dimension size of `input_x` and `_random_samples` is not equal.</span>
<span class="sd">        ValueError: If the third dimension size of `_random_samples` is not 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([0.3220, 0.9545, 0.7879, 0.0975, 0.3698,</span>
<span class="sd">        ...                            0.5135, 0.5740, 0.3435, 0.1895, 0.8764,</span>
<span class="sd">        ...                            0.9581, 0.4760, 0.9014, 0.8522, 0.3664,</span>
<span class="sd">        ...                            0.4980, 0.9673, 0.9879, 0.6988, 0.9022,</span>
<span class="sd">        ...                            0.9304, 0.1558, 0.0153, 0.1559, 0.9852]).reshape([1, 1, 5, 5]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; _random_samples = Tensor(np.array([[[0.8, 0.8]]]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; y, argmax = ops.fractional_max_pool2d(input_x, kernel_size=2, output_size=(2, 2),</span>
<span class="sd">        ...                                       _random_samples=_random_samples, return_indices=True)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[[[0.9545 0.8764]</span>
<span class="sd">           [0.9673 0.9852]]]]</span>
<span class="sd">        &gt;&gt;&gt; print(argmax)</span>
<span class="sd">        [[[[ 1  9]</span>
<span class="sd">           [16 24]]]]</span>
<span class="sd">        &gt;&gt;&gt; y, argmax = ops.fractional_max_pool2d(input_x, kernel_size=2, output_ratio=(0.5, 0.5),</span>
<span class="sd">        ...                                       _random_samples=_random_samples, return_indices=True)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[[[0.9545 0.8764]</span>
<span class="sd">           [0.9673 0.9852]]]]</span>
<span class="sd">        &gt;&gt;&gt; print(argmax)</span>
<span class="sd">        [[[[ 1  9]</span>
<span class="sd">           [16 24]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">output_ratio</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">output_ratio</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For fractional_max_pool2d, &#39;output_size&#39; and &#39;output_ratio&#39; can not be specified or None&quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;at the same time, but got </span><span class="si">{</span><span class="n">output_ratio</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">output_size</span><span class="si">}</span><span class="s2"> .&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="n">input_x</span><span class="o">.</span><span class="n">expend_dims</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_random_samples</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_random_samples</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">output_ratio</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_ratio</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
            <span class="n">output_ratio</span> <span class="o">=</span> <span class="p">(</span><span class="n">output_ratio</span><span class="p">,</span> <span class="n">output_ratio</span><span class="p">)</span>
        <span class="n">_check_float_range_inc_right</span><span class="p">(</span><span class="n">output_ratio</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="n">_check_float_range_inc_right</span><span class="p">(</span><span class="n">output_ratio</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="n">output_size</span> <span class="o">=</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">input_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">output_ratio</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">int</span><span class="p">(</span><span class="n">input_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">output_ratio</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">fractional_max_pool</span> <span class="o">=</span> <span class="n">FractionalMaxPoolWithFixedKsize</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">fractional_max_pool</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">_random_samples</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">return_indices</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">output</span>
    <span class="k">return</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></div>


<div class="viewcode-block" id="fractional_max_pool3d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.fractional_max_pool3d.html#mindspore.ops.fractional_max_pool3d">[docs]</a><span class="k">def</span> <span class="nf">fractional_max_pool3d</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_ratio</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                          <span class="n">_random_samples</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This operator applies a 3D fractional max pooling over an input signal.</span>
<span class="sd">    The input is composed of several input planes.</span>
<span class="sd">    The max-pooling operation is applied in kD x kH x kW regions by a stochastic step size determined</span>
<span class="sd">    by the target output size.The number of output features is equal to the number of input planes.</span>

<span class="sd">    Refer to the paper `Fractional MaxPooling by Ben Graham &lt;https://arxiv.org/abs/1412.6071&gt;`_  for more details.</span>

<span class="sd">    The input and output data format can be &quot;NCDHW&quot;. N is the batch size, C is the number of channels,</span>
<span class="sd">    D the feature depth, H is the feature height, and W is the feature width.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The input of FractionalMaxPool3d, which is a 4D or 5D tensor.</span>
<span class="sd">            Tensor of data type: float16, float32, double, int32, int64.</span>
<span class="sd">            Supported shape :math:`(N, C, D_{in}, H_{in}, W_{in})`.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of kernel used to take the maximum value,</span>
<span class="sd">            is an int number that represents depth, height and width of the kernel, or a tuple</span>
<span class="sd">            of three int numbers that represent depth, height and width respectively.</span>
<span class="sd">            The value must be a positive integer.</span>
<span class="sd">        output_size (Union[int, tuple[int]], optional): The Shape of the target `output_size`,</span>
<span class="sd">            is an int number that represents depth, height and width, or a tuple</span>
<span class="sd">            of three int numbers that represent depth, height and width respectively.</span>
<span class="sd">            The value must be a positive integer.</span>
<span class="sd">            Default: None.</span>
<span class="sd">        output_ratio (Union[float, tuple[float]], optional): The ratio of target output shape to input shape.</span>
<span class="sd">            Specifying the size of the output tensor by using a ratio of the input size.</span>
<span class="sd">            Data type: float16, float32, double, and value is between (0, 1).</span>
<span class="sd">            Default: None.</span>
<span class="sd">        return_indices (bool, optional): If `return_indices` is True, the indices of max value would be output.</span>
<span class="sd">            Default: False.</span>
<span class="sd">        _random_samples (Tensor, optional): The random step of FractionalMaxPool3d, which is a 3D tensor.</span>
<span class="sd">            Tensor of data type: float16, float32, double, and value is between (0, 1).</span>
<span class="sd">            Supported shape :math:`(N, C, 3)`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        - **y** (Tensor) - A tensor, the output of FractionalMaxPool3d.</span>
<span class="sd">          Has the same data type with `imput_x`.</span>
<span class="sd">          Tensor of shape :math:`(N, C, D, H, W)` .</span>

<span class="sd">        - **argmax** (Tensor) - The indices along with the outputs, which is a Tensor, with the same shape as the</span>
<span class="sd">          `y` and int32 data type. It will output only when `return_indices` is True.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a 4D or 5D tensor.</span>
<span class="sd">        TypeError: If `_random_samples` is not a 3D tensor.</span>
<span class="sd">        TypeError: If data type of `imput_x` is not float16, float32, double, int32, int64.</span>
<span class="sd">        TypeError: If dtype of `_random_samples` is not float16, float32, double.</span>
<span class="sd">        TypeError: If dtype of `argmax` is not int32, int64.</span>
<span class="sd">        ValueError: If `output_size` is a tuple and if `output_size` length is not 3.</span>
<span class="sd">        ValueError: If `kernel_size` is a tuple and if `kernel_size` length is not 3.</span>
<span class="sd">        ValueError: If numbers in `output_size` or `kernel_size` is not positive.</span>
<span class="sd">        ValueError: if `output_size` and `output_ratio` are None at the same time.</span>
<span class="sd">        ValueError: If the first dimension size of `input_x` and `_random_samples` is not equal.</span>
<span class="sd">        ValueError: If the second dimension size of `input_x` and `_random_samples` is not equal.</span>
<span class="sd">        ValueError: If the third dimension size of `_random_samples` is not 3.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16])</span>
<span class="sd">        ...            .reshape([1, 1, 2, 2, 4]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; _random_samples = Tensor(np.array([0.7, 0.7, 0.7]).reshape([1, 1, 3]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; output, argmax = ops.fractional_max_pool3d(x, kernel_size=(1.0, 1.0, 1.0), output_size=(1, 1, 3),</span>
<span class="sd">        ...                                            _random_samples=_random_samples, return_indices=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[[13. 14. 16.]]]]]</span>
<span class="sd">        &gt;&gt;&gt; print(argmax)</span>
<span class="sd">        [[[[[12 13 15]]]]]</span>
<span class="sd">        &gt;&gt;&gt; output, argmax = ops.fractional_max_pool3d(x, kernel_size=(1.0, 1.0, 1.0), output_ratio=(0.5, 0.5, 0.5),</span>
<span class="sd">        ...                                            _random_samples=_random_samples, return_indices=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[[13. 16.]]]]]</span>
<span class="sd">        &gt;&gt;&gt; print(argmax)</span>
<span class="sd">        [[[[[12 15]]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">output_ratio</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">output_ratio</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For fractional_max_pool2d, &#39;output_size&#39; and &#39;output_ratio&#39; can not be specified or None&quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;at the same time, but got </span><span class="si">{</span><span class="n">output_ratio</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">output_size</span><span class="si">}</span><span class="s2"> .&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="n">input_x</span><span class="o">.</span><span class="n">expend_dims</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_random_samples</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_random_samples</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">output_ratio</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_ratio</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
            <span class="n">output_ratio</span> <span class="o">=</span> <span class="p">(</span><span class="n">output_ratio</span><span class="p">,</span> <span class="n">output_ratio</span><span class="p">,</span> <span class="n">output_ratio</span><span class="p">)</span>
        <span class="n">_check_float_range_inc_right</span><span class="p">(</span><span class="n">output_ratio</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="n">_check_float_range_inc_right</span><span class="p">(</span><span class="n">output_ratio</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="n">_check_float_range_inc_right</span><span class="p">(</span><span class="n">output_ratio</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="n">output_size</span> <span class="o">=</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">input_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="n">output_ratio</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">int</span><span class="p">(</span><span class="n">input_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">output_ratio</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                       <span class="nb">int</span><span class="p">(</span><span class="n">input_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">output_ratio</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
    <span class="n">fractional_max_pool</span> <span class="o">=</span> <span class="n">FractionalMaxPool3DWithFixedKsize</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">fractional_max_pool</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">_random_samples</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">return_indices</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">output</span>
    <span class="k">return</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></div>


<div class="viewcode-block" id="kl_div"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.kl_div.html#mindspore.ops.kl_div">[docs]</a><span class="k">def</span> <span class="nf">kl_div</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Kullback-Leibler divergence between the logits and the labels.</span>

<span class="sd">    The updating formulas of KLDivLoss algorithm are as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">        l_n = target_n \cdot (\log target_n - x_n)</span>

<span class="sd">    Then,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, target) = \begin{cases}</span>
<span class="sd">        L, &amp; \text{if reduction} = \text{&#39;none&#39;;}\\</span>
<span class="sd">        \operatorname{mean}(L), &amp; \text{if reduction} = \text{&#39;mean&#39;;}\\</span>
<span class="sd">        \operatorname{batchmean}(L), &amp; \text{if reduction} = \text{&#39;batchmean&#39;;}\\</span>
<span class="sd">        \operatorname{sum}(L),  &amp; \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    where :math:`x` represents `logits`.</span>
<span class="sd">    :math:`target` represents `labels`.</span>
<span class="sd">    :math:`\ell(x, target)` represents `output`.</span>

<span class="sd">    Note:</span>
<span class="sd">        - Currently it does not support float64 input on `Ascend`.</span>
<span class="sd">        - The output aligns with the mathematical definition of Kullback-Leibler divergence</span>
<span class="sd">          only when `reduction` is set to &#39;batchmean&#39;.</span>

<span class="sd">    Args:</span>
<span class="sd">        logits (Tensor): The input Tensor. The data type must be float16, float32 or float64.</span>
<span class="sd">        labels (Tensor): The label Tensor which has the same shape and data type as `logits`.</span>
<span class="sd">        reduction (str): Specifies the reduction to be applied to the output.</span>
<span class="sd">            Its value must be one of &#39;none&#39;, &#39;mean&#39;, &#39;batchmean&#39; or &#39;sum&#39;. Default: &#39;mean&#39;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or Scalar, if `reduction` is &#39;none&#39;, then output is a tensor and has the same shape as `logits`.</span>
<span class="sd">        Otherwise, it is a scalar.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `reduction` is not a str.</span>
<span class="sd">        TypeError: If neither `logits` nor `labels` is a Tensor.</span>
<span class="sd">        TypeError: If dtype of `logits` or `labels` is not float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([0.2, 0.7, 0.1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([0., 1., 0.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = mindspore.ops.kl_div(logits, labels, &#39;mean&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        -0.23333333</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">reduction</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;kl_div&#39;, the &#39;reduction&#39; must be str and must be in &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;&#39;[&#39;none&#39;, &#39;mean&#39;, &#39;batchmean&#39;, &#39;sum&#39;]&#39;, but got &#39;</span><span class="si">{</span><span class="n">reduction</span><span class="si">}</span><span class="s2">&#39;.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;batchmean&#39;</span><span class="p">:</span>
        <span class="n">kl_div_sum</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">KLDivLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">)(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">()(</span><span class="n">logits</span><span class="p">)</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">kl_div_sum</span> <span class="o">/</span> <span class="n">batch_size</span>

    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
        <span class="n">kl_div_sum</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">KLDivLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">)(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">()(</span><span class="n">logits</span><span class="p">)</span>
        <span class="n">total_size</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="n">shape</span><span class="p">:</span>
            <span class="n">total_size</span> <span class="o">=</span> <span class="n">total_size</span> <span class="o">*</span> <span class="n">dim</span>
        <span class="k">return</span> <span class="n">kl_div_sum</span> <span class="o">/</span> <span class="n">total_size</span>

    <span class="k">return</span> <span class="n">P</span><span class="o">.</span><span class="n">KLDivLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span></div>


<div class="viewcode-block" id="hardshrink"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.hardshrink.html#mindspore.ops.hardshrink">[docs]</a><span class="k">def</span> <span class="nf">hardshrink</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hard Shrink activation function. Calculates the output according to the input elements.</span>

<span class="sd">    The formula is defined as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{HardShrink}(x) =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">        x, &amp; \text{ if } x &gt; \lambda \\</span>
<span class="sd">        x, &amp; \text{ if } x &lt; -\lambda \\</span>
<span class="sd">        0, &amp; \text{ otherwise }</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input of Hard Shrink with data type of float16 or float32.</span>
<span class="sd">        lambd (float): The threshold :math:`\lambda` defined by the Hard Shrink formula. Default: 0.5.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same data type and shape as the input `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `lambd` is not a float.</span>
<span class="sd">        TypeError: If `x` is not a tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[ 0.5,  1,  2.0], [0.0533,0.0776,-2.1233]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.hardshrink(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 0.      1.      2.    ]</span>
<span class="sd">        [ 0.      0.     -2.1233]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">hshrink_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">HShrink</span><span class="p">)(</span><span class="n">lambd</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">hshrink_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_axis_in_range</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">ndim</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Checks axes are with the bounds of ndim&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;The dims must be integers, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="o">-</span><span class="n">ndim</span> <span class="o">&lt;=</span> <span class="n">axis</span> <span class="o">&lt;</span> <span class="n">ndim</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The &#39;axis&#39; must be in the range of [-</span><span class="si">{</span><span class="n">ndim</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ndim</span><span class="si">}</span><span class="s2">), but got </span><span class="si">{</span><span class="n">axis</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">axis</span> <span class="o">%</span> <span class="n">ndim</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_axis_valid</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">ndim</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks axes are valid given ndim, and returns axes that can be passed</span>
<span class="sd">    to the built-in operator (non-negative, int or tuple)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The parameter dims can not be None.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
        <span class="n">axes</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">_check_axis_in_range</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ndim</span><span class="p">),</span> <span class="n">axes</span><span class="p">))</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">el</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">el</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The element of parameter &#39;dims&#39; can not be duplicate, but got </span><span class="si">{</span><span class="n">axes</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">axes</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The parameter dims must be tuple of ints, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_get_flip_start</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">axes</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate the start index of flip&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">([</span><span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">axes</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndim</span><span class="p">)])</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_get_flip_end</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">axes</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate the end index of flip&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">([</span><span class="o">-</span><span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">axes</span> <span class="k">else</span> <span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndim</span><span class="p">)])</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_get_flip_strides</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">axes</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate the strides of flip&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">axes</span> <span class="k">else</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndim</span><span class="p">)])</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_is_shape_empty</span><span class="p">(</span><span class="n">shp</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check whether shape contains zero&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shp</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">shp</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">shape_mul</span><span class="p">(</span><span class="n">shp</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>


<span class="k">def</span> <span class="nf">_check_input_tensor</span><span class="p">(</span><span class="n">arg_name</span><span class="p">,</span> <span class="o">*</span><span class="n">tensors</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check whether the input is tensor&quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="n">arg_name</span><span class="si">}</span><span class="s2">&#39;, the input must be Tensor, but got </span><span class="si">{</span><span class="n">ops</span><span class="o">.</span><span class="n">typeof</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">True</span>


<div class="viewcode-block" id="flip"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.flip.html#mindspore.ops.flip">[docs]</a><span class="k">def</span> <span class="nf">flip</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dims</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reverses the order of elements in a tensor along the given axis.</span>

<span class="sd">    The shape of the tensor is preserved, but the elements are reordered.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input tensor.</span>
<span class="sd">        dims (Union[list[int], tuple[int]]): Axis or axes along which to flip over.</span>
<span class="sd">            Flipping is performed on all of the axes specified in the tuple,</span>
<span class="sd">            If `dims` is a tuple of integers contains negative, it counts from the last to the first axis.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the entries of `dims` reversed.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input is not a tensor.</span>
<span class="sd">        ValueError: If `dims` is None.</span>
<span class="sd">        ValueError: If `dims` is not a tuple of ints.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = ms.Tensor(np.arange(8).reshape((2, 2, 2)))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.flip(x, (0, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[5. 4.]</span>
<span class="sd">        [7. 6.]]</span>
<span class="sd">        [[1. 0.]</span>
<span class="sd">        [3. 2.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="s2">&quot;flip&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">dims</span> <span class="o">=</span> <span class="n">_check_axis_valid</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_is_shape_empty</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">_get_flip_start</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dims</span><span class="p">)</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">_get_flip_end</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dims</span><span class="p">)</span>
    <span class="n">strides</span> <span class="o">=</span> <span class="n">_get_flip_strides</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">dims</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">strided_slice</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">strides</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span></div>


<div class="viewcode-block" id="flipud"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.flipud.html#mindspore.ops.flipud">[docs]</a><span class="k">def</span> <span class="nf">flipud</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Flips the entries in each column in the up/down direction.</span>
<span class="sd">    Rows are preserved, but appear in a different order than before.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input array.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input is not a tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = ms.Tensor(np.arange(8).reshape((2, 2, 2)))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.flipud(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[4. 5.]</span>
<span class="sd">        [6. 7.]]</span>
<span class="sd">        [[0. 1.]</span>
<span class="sd">        [2. 3.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">flip</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,))</span></div>


<div class="viewcode-block" id="fliplr"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.fliplr.html#mindspore.ops.fliplr">[docs]</a><span class="k">def</span> <span class="nf">fliplr</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Flips the entries in each row in the left/right direction.</span>
<span class="sd">    Columns are preserved, but appear in a different order than before.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input is not a tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = ms.Tensor(np.arange(8).reshape((2, 2, 2)))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.fliplr(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[2. 3.]</span>
<span class="sd">        [0. 1.]]</span>
<span class="sd">        [[6. 7.]</span>
<span class="sd">        [4. 5.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">flip</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span></div>


<div class="viewcode-block" id="is_floating_point"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.is_floating_point.html#mindspore.ops.is_floating_point">[docs]</a><span class="k">def</span> <span class="nf">is_floating_point</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Judge whether the data type of `x` is a floating point data type i.e., one of mindspore.flot64, mindspore.float32,</span>
<span class="sd">    mindspore.float16.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Bool. If the dtype of `x` is a floating point data type, return True. Otherwise, return False.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; x = ms.Tensor([1, 2, 3], ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = ms.Tensor([1, 2, 3], ms.int64)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.is_floating_point(x)</span>
<span class="sd">        &gt;&gt;&gt; output2 = ops.is_floating_point(y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        True</span>
<span class="sd">        &gt;&gt;&gt; print(output2)</span>
<span class="sd">        False</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">]</span></div>


<div class="viewcode-block" id="hardswish"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.hardswish.html#mindspore.ops.hardswish">[docs]</a><span class="k">def</span> <span class="nf">hardswish</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies hswish-type activation element-wise. The input is a Tensor with any valid shape.</span>

<span class="sd">    Hard swish is defined as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{hswish}(x_{i}) = x_{i} * \frac{ReLU6(x_{i} + 3)}{6},</span>

<span class="sd">    where :math:`x_i` is an element of the input Tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input to compute the Hard Swish with data type of float16 or float32.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same data type and shape as the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-1, -2, 0, 2, 1]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.hardswish(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-0.3333  -0.3333  0  1.666  0.6665]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">hardswish_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_interpolate_inputs</span><span class="p">(</span><span class="n">input_dims</span><span class="p">,</span> <span class="n">roi</span><span class="p">,</span> <span class="n">scales</span><span class="p">,</span> <span class="n">sizes</span><span class="p">,</span> <span class="n">coordinate_transformation_mode</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span>
                              <span class="n">prim_name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check input&quot;&quot;&quot;</span>
    <span class="n">msg_prefix</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="n">prim_name</span><span class="si">}</span><span class="s2">&#39;, the&quot;</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;coordinate_transformation_mode&quot;</span><span class="p">,</span> <span class="n">coordinate_transformation_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">prim_name</span><span class="p">)</span>
    <span class="n">support_coordinate_mode_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;align_corners&quot;</span><span class="p">,</span> <span class="s2">&quot;half_pixel&quot;</span><span class="p">,</span> <span class="s2">&quot;asymmetric&quot;</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">coordinate_transformation_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">support_coordinate_mode_list</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">msg_prefix</span><span class="si">}</span><span class="s2"> coordinate_transformation_mode must be in </span><span class="si">{</span><span class="n">support_coordinate_mode_list</span><span class="si">}</span><span class="s2">,&quot;</span>
                        <span class="s2">&quot; but got </span><span class="si">{coordinate_transformation_mode}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;mode&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">prim_name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;linear&quot;</span><span class="p">:</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">input_dims</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;input dims&quot;</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;bilinear&quot;</span><span class="p">:</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">input_dims</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;input dims&quot;</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">msg_prefix</span><span class="si">}</span><span class="s2"> mode must be &#39;linear&#39; or &#39;bilinear&#39;, but got </span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">sizes</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">scales</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">msg_prefix</span><span class="si">}</span><span class="s2"> &#39;sizes&#39; and &#39;scale&#39; both none.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">sizes</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">scales</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">msg_prefix</span><span class="si">}</span><span class="s2"> &#39;sizes&#39; and &#39;scale&#39; both not none.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">sizes</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sizes</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">msg_prefix</span><span class="si">}</span><span class="s2"> &#39;sizes&#39; must be tuple or None, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">sizes</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">sizes</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s1">&#39;sizes item&#39;</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;sizes item&quot;</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sizes</span><span class="p">),</span> <span class="n">input_dims</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;sizes&quot;</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>
        <span class="k">return</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scales</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">msg_prefix</span><span class="si">}</span><span class="s2"> &#39;scales&#39; must be tuple or None, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">scales</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">scales</span><span class="p">:</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_float</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s1">&#39;scales item&#39;</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;scales item&quot;</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>
    <span class="n">scales_dims</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">scales</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">scales_dims</span><span class="p">,</span> <span class="n">input_dims</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;scales dims&quot;</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_float</span><span class="p">(</span><span class="n">scales</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;scales[0]&quot;</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_float</span><span class="p">(</span><span class="n">scales</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;scales[1]&quot;</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_interpolate_output_shape</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">scales</span><span class="p">,</span> <span class="n">sizes</span><span class="p">,</span> <span class="n">mode</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;calculate output shape&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">sizes</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;bilinear&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">sizes</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">sizes</span><span class="p">)</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)):</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">ret</span> <span class="o">+</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">scales</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">]),)</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;bilinear&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">ret</span>
    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span>


<div class="viewcode-block" id="interpolate"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.interpolate.html#mindspore.ops.interpolate">[docs]</a><span class="k">def</span> <span class="nf">interpolate</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">roi</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scales</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sizes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">coordinate_transformation_mode</span><span class="o">=</span><span class="s2">&quot;align_corners&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Using the interpolate method specified by `mode` resize the input tensor `x`.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - This is an experimental prototype that is subject to change.</span>
<span class="sd">        - The `roi` is reserved interface for &#39;crop_and_resize&#39; coordinate transformation mode,</span>
<span class="sd">          which is not support now.</span>
<span class="sd">        - The Ascend platforms is currently not supported when `mode` is &quot;linear&quot;.</span>
<span class="sd">        - The &#39;half_pixel&#39; coordinate_transformation_mode is currently not supported on CPU device</span>
<span class="sd">          when mode is &quot;bilinear&quot;.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): a tensor which to resize. `x` is a 3-D tensor when `mode` is &quot;linear&quot;. `x` is a 4-D tensor when</span>
<span class="sd">            `mode` is &quot;bilinear&quot;.</span>
<span class="sd">        roi (tuple[float], optional): a tuple of float. Only takes effect when attr coordinate_transformation_mode is</span>
<span class="sd">            &#39;crop_and_resize&#39;.</span>
<span class="sd">        scales (tuple[float], optional): a tuple of float. Describe the scale along each dimension.</span>
<span class="sd">            Its length is the same as that of shape of `x`. The numbers in `scales` must all be positive. Only one of</span>
<span class="sd">            `scales` and `sizes` can be specified.</span>
<span class="sd">        sizes (tuple[int], optional): a tuple of int, describes the shape of the output tensor. The numbers in `sizes`</span>
<span class="sd">            must all be positive. Only one of `scales` and `sizes` can be specified.  If `sizes` is specified, then set</span>
<span class="sd">            `scales` to &#39;None&#39; in this operator&#39;s input list. It is 1 int elements :math:`(new\_width,)` when `mode`</span>
<span class="sd">            is &quot;linear&quot;. It is 2 int elements :math:`(new\_height, new\_width)` when `mode` is &quot;bilinear&quot;.</span>
<span class="sd">        coordinate_transformation_mode (str): Default is &#39;align_corners&#39;. Describes how to transform the coordinate</span>
<span class="sd">            in the resized tensor to the coordinate in the original tensor. Other optional: &#39;half_pixel&#39;, &#39;asymmetric&#39;.</span>
<span class="sd">            For example, we want to resize the original tensor along axis x. Let&#39;s denote `new_i` as the i-th coordinate</span>
<span class="sd">            of the resized tensor along axis x, `old_i` as the coordinate of the original tensor along axis x,</span>
<span class="sd">            `new_length` as the length of the resized tensor along axis x, `old_length` as the length of the original</span>
<span class="sd">            tensor along axis x. We compute the `old_i` via the following formula:</span>

<span class="sd">            .. code-block::</span>

<span class="sd">                old_i = new_length != 1 ? new_i * (old_length - 1) / (new_length - 1) : 0  # if set to &#39;align_corners&#39;</span>

<span class="sd">                old_i = new_length &gt; 1 ? (new_x + 0.5) * old_length / new_length - 0.5 : 0  # if set to &#39;half_pixel&#39;</span>

<span class="sd">                old_i = new_length != 0 ? new_i * old_length / new_length : 0  # if set to &#39;asymmetric&#39;</span>

<span class="sd">        mode (str): The method used to interpolate: &#39;linear&#39; | &#39;bilinear&#39;. Default is &#39;linear&#39;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Resized tensor, with the same data type as input `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If the data type of `x` is not supported.</span>
<span class="sd">        TypeError: If `scales` is not a float tuple.</span>
<span class="sd">        ValueError: If not all numbers in `scales` are positive.</span>
<span class="sd">        TypeError: If `sizes` is not an int tuple.</span>
<span class="sd">        ValueError: If not all numbers in `sizes` are positive.</span>
<span class="sd">        TypeError: If `coordinate_transformation_mode` is not a string.</span>
<span class="sd">        ValueError: If `coordinate_transformation_mode` is not in the support list.</span>
<span class="sd">        TypeError: If `mode` is not a string.</span>
<span class="sd">        ValueError: If `mode` is not in the support list.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # case 1: linear mode</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[[1, 2, 3], [4, 5, 6]]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.interpolate(x, None, None, (6,), &quot;align_corners&quot;)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1. 1.4 1.8 2.2 2.6 3.]</span>
<span class="sd">          [4. 4.4 4.8 5.2 5.6 6.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 2: bilinear mode</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[[[1, 2, 3, 4, 5], [1, 2, 3, 4, 5]]]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.interpolate(x, None, None, (5, 5), &quot;asymmetric&quot;, &quot;bilinear&quot;)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[1. 2. 3. 4. 5.]</span>
<span class="sd">           [1. 2. 3. 4. 5.]</span>
<span class="sd">           [1. 2. 3. 4. 5.]</span>
<span class="sd">           [1. 2. 3. 4. 5.]</span>
<span class="sd">           [1. 2. 3. 4. 5.]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For interpolate, the input x must be tensor&quot;</span><span class="p">)</span>
    <span class="n">input_shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">input_dims</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
    <span class="n">_check_interpolate_inputs</span><span class="p">(</span><span class="n">input_dims</span><span class="p">,</span> <span class="n">roi</span><span class="p">,</span> <span class="n">scales</span><span class="p">,</span> <span class="n">sizes</span><span class="p">,</span> <span class="n">coordinate_transformation_mode</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span>
                              <span class="s2">&quot;interpolate&quot;</span><span class="p">)</span>
    <span class="n">output_size</span> <span class="o">=</span> <span class="n">_interpolate_output_shape</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">scales</span><span class="p">,</span> <span class="n">sizes</span><span class="p">,</span> <span class="n">mode</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;linear&quot;</span><span class="p">:</span>
        <span class="n">resize_linear_inner</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">IMG</span><span class="o">.</span><span class="n">ResizeLinear1D</span><span class="p">)(</span>
            <span class="n">coordinate_transformation_mode</span><span class="o">=</span><span class="n">coordinate_transformation_mode</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">resize_linear_inner</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;bilinear&quot;</span><span class="p">:</span>
        <span class="n">align_corners</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">half_pixel_centers</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">coordinate_transformation_mode</span> <span class="o">==</span> <span class="s2">&quot;align_corners&quot;</span><span class="p">:</span>
            <span class="n">align_corners</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">elif</span> <span class="n">coordinate_transformation_mode</span> <span class="o">==</span> <span class="s2">&quot;half_pixel&quot;</span><span class="p">:</span>
            <span class="n">half_pixel_centers</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">resize_bilinear_inner</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">IMG</span><span class="o">.</span><span class="n">ResizeBilinearV2</span><span class="p">)(</span><span class="n">align_corners</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">resize_bilinear_inner</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>

    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
        <span class="s2">&quot;Input Error: For interpolate,  </span><span class="si">{}</span><span class="s2"> mode is not support now&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mode</span><span class="p">))</span></div>


<div class="viewcode-block" id="softsign"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.softsign.html#mindspore.ops.softsign">[docs]</a><span class="k">def</span> <span class="nf">softsign</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softsign activation function.</span>

<span class="sd">    The function is shown as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{SoftSign}(x) = \frac{x}{1 + |x|}</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Tensor of shape :math:`(N, *)`, where :math:`*` means, any number of</span>
<span class="sd">            additional dimensions, with float16 or float32 data type.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0, -1, 2, 30, -30]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.softsign(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0.        -0.5         0.6666667  0.9677419 -0.9677419]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">softsign_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="softmax"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.softmax.html#mindspore.ops.softmax">[docs]</a><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies the Softmax operation to the input tensor on the specified axis.</span>
<span class="sd">    Suppose a slice in the given axis :math:`x`, then for each element :math:`x_i`,</span>
<span class="sd">    the Softmax function is shown as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(x_i) = \frac{exp(x_i)}{\sum_{j = 0}^{N-1}\exp(x_j)},</span>

<span class="sd">    where :math:`N` is the length of the tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (Union[int, tuple[int]], optional): The axis to perform the Softmax operation. Default: -1.</span>
<span class="sd">        x (Tensor): Tensor of shape :math:`(N, *)`, where :math:`*` means, any number of</span>
<span class="sd">          additional dimensions, with float16 or float32 data type.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as the logits.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is not an int or a tuple.</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 nor float32.</span>
<span class="sd">        ValueError: If `axis` is a tuple whose length is less than 1.</span>
<span class="sd">        ValueError: If `axis` is a tuple whose elements are not all in range [-len(logits.shape), len(logits.shape))</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3, 4, 5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.softmax(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.01165623 0.03168492 0.08612854 0.23412167 0.6364086 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">type_axis</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot; the type of &#39;axis&#39; must be &#39;int&#39;, but got &#39;</span><span class="si">{</span><span class="n">axis</span><span class="si">}</span><span class="s2">&#39; with type &#39;</span><span class="si">{</span><span class="n">type_axis</span><span class="si">}</span><span class="s2">&#39;.&quot;</span><span class="p">)</span>
    <span class="n">softmax_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Softmax</span><span class="p">)(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">softmax_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="soft_shrink"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.soft_shrink.html#mindspore.ops.soft_shrink">[docs]</a><span class="k">def</span> <span class="nf">soft_shrink</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies the SoftShrink function element-wise.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{SoftShrink}(x) =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">        x - \lambda, &amp; \text{ if } x &gt; \lambda \\</span>
<span class="sd">        x + \lambda, &amp; \text{ if } x &lt; -\lambda \\</span>
<span class="sd">        0, &amp; \text{ otherwise }</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input of soft shrink with data type of float16 or float32.</span>
<span class="sd">        lambd(float): The :math:`\lambda` must be no less than zero. Default: 0.5.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and data type as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If lambd is not a float.</span>
<span class="sd">        TypeError: If input_x is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of input_x is neither float16 nor float32.</span>
<span class="sd">        ValueError: If lambd is less than 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[ 0.5297,  0.7871,  1.1754], [ 0.7836,  0.6218, -1.1542]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.soft_shrink(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 0.02979  0.287    0.676  ]</span>
<span class="sd">         [ 0.2837   0.1216  -0.6543 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">soft_shrink_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">SoftShrink</span><span class="p">)(</span><span class="n">lambd</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">soft_shrink_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="selu"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.selu.html#mindspore.ops.selu">[docs]</a><span class="k">def</span> <span class="nf">selu</span><span class="p">(</span><span class="n">input_x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Activation function SeLU (Scaled exponential Linear Unit).</span>

<span class="sd">    The activation function is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        E_{i} =</span>
<span class="sd">        scale *</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">        x_{i}, &amp;\text{if } x_{i} \geq 0; \cr</span>
<span class="sd">        \text{alpha} * (\exp(x_i) - 1), &amp;\text{otherwise.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    where :math:`alpha` and :math:`scale` are pre-defined constants(:math:`alpha=1.67326324`</span>
<span class="sd">    and :math:`scale=1.05070098`).</span>

<span class="sd">    See more details in `Self-Normalizing Neural Networks &lt;https://arxiv.org/abs/1706.02515&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Tensor of any dimension, the data type is float16 or float32.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as the `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `input_x` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.selu(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-1.1113307 4.202804 -1.7575096]</span>
<span class="sd">        [ 2.101402 -1.7462534 9.456309 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">selu_</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="sigmoid"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.sigmoid.html#mindspore.ops.sigmoid">[docs]</a><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">input_x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes Sigmoid of input element-wise. The Sigmoid function is defined as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{sigmoid}(x_i) = \frac{1}{1 + \exp(-x_i)}</span>

<span class="sd">    where :math:`x_i` is an element of the input_x.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Tensor of any dimension, the data type is float16, float32, float64, complex64 or complex128.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as the input_x.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `input_x` is not float16, float32, float64, complex64 or complex128.</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3, 4, 5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.sigmoid(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.7310586  0.880797   0.95257413 0.98201376 0.9933072 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">sigmoid_</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="deformable_conv2d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.deformable_conv2d.html#mindspore.ops.deformable_conv2d">[docs]</a><span class="k">def</span> <span class="nf">deformable_conv2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilations</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                      <span class="n">deformable_groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">modulated</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Given 4D tensor inputs `x`, `weight` and `offsets`, compute a 2D deformable convolution. The deformable convolution</span>
<span class="sd">    operation can be expressed as follow:</span>

<span class="sd">    Deformable Convolution v1:</span>

<span class="sd">    .. math::</span>
<span class="sd">        y(p)=\sum_{k=1}^{K}w_{k}\cdot x(p+p_{k}+\Delta{p_{k}})</span>

<span class="sd">    Deformable Convolution v2:</span>

<span class="sd">    .. math::</span>
<span class="sd">        y(p)=\sum_{k=1}^{K}w_{k}\cdot x(p+p_{k}+\Delta{p_{k}})\cdot \Delta{m_{k}}</span>

<span class="sd">    Where :math:`\Delta{p_{k}}` and :math:`\Delta{m_{k}}` are the learnable offset and modulation scalar for the k-th</span>
<span class="sd">    location. For details, please refer to `Deformable ConvNets v2: More Deformable, Better Results</span>
<span class="sd">    &lt;https://arxiv.org/abs/1811.11168&gt;`_ and `Deformable Convolutional Networks &lt;https://arxiv.org/abs/1703.06211&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): A 4D tensor of input image. With the format &quot;NCHW&quot;,</span>
<span class="sd">            the shape is :math:`(N, C_{in}, H_{in}, W_{in})`. Dtype: float16 or float32.</span>
<span class="sd">        weight (Tensor): A 4D tensor of learnable filters. Must have the same type as `x`.</span>
<span class="sd">            The shape is :math:`(C_{out}, C_{in} / groups, H_{f}, W_{f})`.</span>
<span class="sd">        offsets (Tensor): A 4D tensor of x-y coordinates offset and mask. With the format &quot;NCHW&quot;,</span>
<span class="sd">            the shape is :math:`(batch, 3 * deformable\_groups * H_{f} * W_{f}, H_{out}, W_{out})`. Note the C dimension</span>
<span class="sd">            is stored in the order of (offset_x, offset_y, mask). Must have the same type as `x`.</span>
<span class="sd">        kernel_size (tuple[int]): A tuple of 2 integers. The size of kernel.</span>
<span class="sd">        strides (tuple[int]): A tuple of 4 integers. The stride of the sliding window for each dimension of</span>
<span class="sd">            input. The dimension order is interpreted according to the data format of `x`. The N and C dimensions must</span>
<span class="sd">            be set to 1.</span>
<span class="sd">        padding (tuple[int]): A tuple of 4 integers. The number of pixels to add to each (top, bottom, left,</span>
<span class="sd">            right) side of the input.</span>
<span class="sd">        bias (Tensor, optional): An 1D tensor of additive biases to the filter outputs.</span>
<span class="sd">            The shape is :math:`(C_{out})`. Defaults to None.</span>
<span class="sd">        dilations (tuple[int], optional): A tuple of 4 integers. The dilation factor for each dimension of input. The</span>
<span class="sd">            dimension order is interpreted according to the data format of `x`. The N and C dimensions must be set</span>
<span class="sd">            to 1. Defaults to (1, 1, 1, 1).</span>
<span class="sd">        groups (int, optional): An integer of type int32. The number of blocked connections from input channels</span>
<span class="sd">            to output channels. In_channels and out_channels must both be divisible by `groups`. Defaults to 1.</span>
<span class="sd">        deformable_groups (int, optional): An integer of type int32. The number of deformable group partitions.</span>
<span class="sd">            In_channels must be divisible by `deformable_groups`. Defaults to 1.</span>
<span class="sd">        modulated (bool, optional): Specifies version of DeformableConv2D, True means v2, False means v1, currently</span>
<span class="sd">            only supports v2. Defaults to True.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, A 4D Tensor of output feature map. With the same type as `x`. With the format &quot;NCHW&quot;,</span>
<span class="sd">        the shape is :math:`(N, C_{out}, H_{out}, W_{out})`.</span>

<span class="sd">        .. math::</span>
<span class="sd">            \begin{array}{ll} \\</span>
<span class="sd">                H_{out} = \left \lfloor{\frac{H_{in} + padding[0] + padding[1] - (H_{f} - 1) \times</span>
<span class="sd">                \text{dilations[2]} - 1 }{\text{stride[0]}} + 1} \right \rfloor \\</span>
<span class="sd">                W_{out} = \left \lfloor{\frac{W_{in} + padding[2] + padding[3] - (W_{f} - 1) \times</span>
<span class="sd">                \text{dilations[3]} - 1 }{\text{stride[1]}} + 1} \right \rfloor \\</span>
<span class="sd">            \end{array}</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `strides`, `padding`, `kernel_size` or `dilations` is not a tuple with integer elements.</span>
<span class="sd">        TypeError: If `modulated` is not a bool.</span>
<span class="sd">        ValueError: If the tuple size of `strides`, `padding`, `kernel_size` or `dilations` is not expected.</span>
<span class="sd">        ValueError: The N or C dimensions of &#39;strides&#39; or `dilations` is not set to 1.</span>
<span class="sd">        ValueError: If `modulated` is not set to True.</span>

<span class="sd">    Note:</span>
<span class="sd">        - This is an experimental interface that is subject to change or deletion.</span>
<span class="sd">        - For Ascend platform, only AI-CORE kernel is implemented, which has the following limitations:</span>

<span class="sd">          - :math:`C_{in}` cannot be divisible by 8 is not supported, e.g. `x` is :math:`(N, 2, H_{in}, W_{in})`.</span>
<span class="sd">          - `deformable_groups` must equal to 1.</span>
<span class="sd">          - `offsets` value is float which does not contain a decimal part is not supported, e.g. `offsets` is assigned</span>
<span class="sd">            with &quot;numpy.ones()&quot;.</span>
<span class="sd">          - `kernel_size` should meet the requirement::math:`3 * kernel\_size[0] * kernel\_size[1] &gt; 8`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones((4, 3, 10, 10)), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; kh, kw = 3, 3</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones((5, 3, kh, kw)), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; offsets = Tensor(np.ones((4, 3 * kh * kw, 8, 8)), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.deformable_conv2d(x, weight, offsets, (kh, kw), (1, 1, 1, 1), (0, 0, 0, 0))</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (4, 5, 8, 8)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">deformable_offsets</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">DeformableOffsets</span><span class="p">)(</span><span class="n">strides</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">dilations</span><span class="p">,</span> <span class="s2">&quot;NCHW&quot;</span><span class="p">,</span>
                                                                   <span class="n">deformable_groups</span><span class="p">,</span>
                                                                   <span class="n">modulated</span><span class="p">)</span>
    <span class="n">fm_offset</span> <span class="o">=</span> <span class="n">deformable_offsets</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">offsets</span><span class="p">)</span>

    <span class="n">weight_shape</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">out_channel</span> <span class="o">=</span> <span class="n">weight_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">strides_conv</span> <span class="o">=</span> <span class="p">(</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">conv</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">)(</span><span class="n">out_channel</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">strides_conv</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="p">)</span>
    <span class="n">bias_add_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">BiasAdd</span><span class="p">)()</span>

    <span class="n">output</span> <span class="o">=</span> <span class="n">conv</span><span class="p">(</span><span class="n">fm_offset</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">bias_add_</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span></div>


<div class="viewcode-block" id="pdist"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.pdist.html#mindspore.ops.pdist">[docs]</a><span class="k">def</span> <span class="nf">pdist</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">2.0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the p-norm distance between each pair of row vectors in the input. If `x` is a 2D Tensor of</span>
<span class="sd">    shape :math:`(N, M)`, then `output` must be a 1D Tensor of shape :math:`(N * (N - 1) / 2,)`. If `x` is a</span>
<span class="sd">    Tensor of shape :math:`(*B, N, M)`, then `output` must be a Tensor of shape :math:`(*B, N * (N - 1) / 2)`.</span>

<span class="sd">    .. math::</span>
<span class="sd">        y[n] = \sqrt[p]{{\mid x_{i} - x_{j} \mid}^p}</span>

<span class="sd">    where :math:`x_{i}, x_{j}` are two different row vectors in the input.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input tensor of shape :math:`(*B, N, M)`. :math:`*B` is batch size, one-dim or multi-dim.</span>
<span class="sd">            dtype: float16, float32 or float64.</span>
<span class="sd">        p (float): p value for the p-norm distance to calculate between each vector pair. :math:`p∈[0,∞]`. Default: 2.0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64.</span>
<span class="sd">        TypeError: If `p` is not a float.</span>
<span class="sd">        ValueError: If `p` is a negative float.</span>
<span class="sd">        ValueError: If dimension of `x` is less than 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1.0, 1.0], [2.0, 2.0], [3.0, 3.0]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; y = ops.pdist(x, p=2.0)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [1.4142135 2.828427 1.4142135]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">pdist_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">Pdist</span><span class="p">)(</span><span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pdist_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_pad_inputs</span><span class="p">(</span><span class="n">padding</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;check the input of pad&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;pad&#39;, the size of padding must be divisible by 2, but got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;pad&#39;, the type of &#39;paddings&#39; must be a tuple of int or list of int or a Tensor,&quot;</span>
                        <span class="sa">f</span><span class="s2">&quot; but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">pd</span> <span class="ow">in</span> <span class="n">padding</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pd</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;pad&#39;, the paddings value must be tuple of int or list of int, but got </span><span class="si">{</span><span class="n">padding</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="pad"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.pad.html#mindspore.ops.pad">[docs]</a><span class="k">def</span> <span class="nf">pad</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;constant&#39;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Pads the input tensor according to the padding.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Tensor of shape :math:`(N, *)`, where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        padding (Union[tuple[int], list[int], Tensor]): Filling position of pad.</span>
<span class="sd">            :math:`\left\lfloor\frac{\text{len(padding)}}{2}\right\rfloor` dimensions</span>
<span class="sd">            of `input_x` will be padded.</span>

<span class="sd">            Example: to pad only the last dimension of the input tensor, then</span>
<span class="sd">            :attr:`padding` has the form</span>
<span class="sd">            :math:`(\text{padding_left}, \text{padding_right})`;</span>

<span class="sd">            Example: to pad the last 2 dimensions of the input tensor, then use</span>
<span class="sd">            :math:`(\text{padding_left}, \text{padding_right}`,</span>
<span class="sd">            :math:`\text{padding_top}, \text{padding_bottom})`;</span>

<span class="sd">            Example: to pad the last 3 dimensions, use</span>
<span class="sd">            :math:`(\text{padding_left}, \text{padding_right}`,</span>
<span class="sd">            :math:`\text{padding_top}, \text{padding_bottom}`,</span>
<span class="sd">            :math:`\text{padding_front}, \text{padding_back})` and so on.</span>

<span class="sd">        mode (str, optional): Pad filling mode, &quot;constant&quot;, &quot;reflect&quot; or &quot;replicate&quot;. Default: &quot;constant&quot;.</span>

<span class="sd">            For &quot;constant&quot; mode, please refer to :class:`mindspore.nn.ConstantPad1d` as an example to understand</span>
<span class="sd">            this filling pattern and extend the padding pattern to n dimensions.</span>

<span class="sd">            For &quot;reflect&quot; mode, please refer to :class:`mindspore.nn.ReflectionPad1d` as an example</span>
<span class="sd">            and extend the padding pattern to n dimensions.</span>

<span class="sd">            For &quot;replicate&quot; mode, please refer to :class:`mindspore.nn.ReplicationPad1d` as an example</span>
<span class="sd">            and extend the padding pattern to n dimensions.</span>

<span class="sd">        value (Union[int, float, None], optional): Valid only in &quot;constant&quot; mode.</span>
<span class="sd">            Set the padding value in &quot;constant&quot; mode. If the value is None, 0 is used as the default padding value.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the tensor after padding.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `paddings` is not an int of tuple or int of list.</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        ValueError: If padding.size is not equal to 2 * len(input_x).</span>
<span class="sd">        ValueError: If mode is not &quot;constant&quot; and value not None.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = ms.Tensor(np.arange(1 * 2 * 2 * 2).reshape((1, 2, 2, 2)), dtype=ms.float64)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.pad(x, [1, 0, 0, 1], mode=&#39;constant&#39;, value=6.0)</span>
<span class="sd">        &gt;&gt;&gt; print(x)</span>
<span class="sd">        [[[[6. 0. 1.]</span>
<span class="sd">           [6. 2. 3.]</span>
<span class="sd">           [6. 6. 6.]]</span>
<span class="sd">          [[6. 4. 5.]</span>
<span class="sd">           [6. 6. 7.]</span>
<span class="sd">           [6. 6. 6.]]]]</span>
<span class="sd">        &gt;&gt;&gt; output1 = ops.pad(x, (1, 0, 0, 1), mode=&#39;reflect&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(output1)</span>
<span class="sd">        [[[[1. 0. 1.]</span>
<span class="sd">           [3. 2. 3.]</span>
<span class="sd">           [1. 0. 1.]]</span>
<span class="sd">          [[5. 4. 5.]</span>
<span class="sd">           [7. 6. 7.]</span>
<span class="sd">           [5. 4. 5.]]]]</span>
<span class="sd">        &gt;&gt;&gt; output2 = ops.pad(x, (1, 1, 2, 1), mode=&#39;replicate&#39;)</span>
<span class="sd">        [[[[0. 0. 1. 1.]</span>
<span class="sd">           [0. 0. 1. 1.]</span>
<span class="sd">           [0. 0. 1. 1.]</span>
<span class="sd">           [2. 2. 3. 3.]</span>
<span class="sd">           [2. 2. 3. 3.]]</span>
<span class="sd">          [[4. 4. 5. 5.]</span>
<span class="sd">           [4. 4. 5. 5.]</span>
<span class="sd">           [4. 4. 5. 5.]</span>
<span class="sd">           [6. 6. 7. 7.]</span>
<span class="sd">           [6. 6. 7. 7.]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;pad&#39;, the type of &#39;input_x&#39; must be Tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">padding</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">padding</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)):</span>
        <span class="k">return</span> <span class="n">input_x</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">_check_pad_inputs</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;constant&quot;</span><span class="p">:</span>
        <span class="n">value</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">value</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">)):</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">scalar_to_tensor_</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">input_x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;pad&#39;, the padding mode &#39;</span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s2">&#39; can not set value, but got value </span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;replicate&quot;</span><span class="p">:</span>
            <span class="n">mode</span> <span class="o">=</span> <span class="s2">&quot;edge&quot;</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">PadV3</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span> <span class="n">paddings_contiguous</span><span class="o">=</span><span class="kc">True</span><span class="p">)(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="relu"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.relu.html#mindspore.ops.relu">[docs]</a><span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes ReLU (Rectified Linear Unit activation function) of input tensors element-wise.</span>

<span class="sd">    It returns :math:`\max(x,\  0)` element-wise. Specially, the neurons with the negative output</span>
<span class="sd">    will be suppressed and the active neurons will stay the same.</span>

<span class="sd">    .. math::</span>

<span class="sd">        ReLU(x) = (x)^+ = max(0, x)</span>

<span class="sd">    Note:</span>
<span class="sd">        In general, this operator is more commonly used. The difference from `ReLuV2` is that the `ReLuV2` will</span>
<span class="sd">        output one more Mask.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Tensor of shape :math:`(N, *)`, where :math:`*` means, any number of</span>
<span class="sd">          additional dimensions, data type is</span>
<span class="sd">          `number &lt;https://www.mindspore.cn/docs/en/r2.0.0-alpha/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor of shape :math:`(N, *)`, with the same dtype and shape as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x` is not a number.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.relu(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 4. 0.]</span>
<span class="sd">         [2. 0. 9.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">relu_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">ReLU</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">relu_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="relu6"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.relu6.html#mindspore.ops.relu6">[docs]</a><span class="k">def</span> <span class="nf">relu6</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes ReLU (Rectified Linear Unit) upper bounded by 6 of input tensors element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{ReLU6}(x) = \min(\max(0,x), 6)</span>

<span class="sd">    It returns :math:`\min(\max(0,x), 6)` element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Tensor of shape :math:`(N, *)`, where :math:`*` means, any number of</span>
<span class="sd">          additional dimensions, with float16 or float32 data type.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same dtype and shape as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; result = ops.relu6(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [[0. 4. 0.]</span>
<span class="sd">         [2. 0. 6.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">relu6_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">ReLU6</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">relu6_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="prelu"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.prelu.html#mindspore.ops.prelu">[docs]</a><span class="k">def</span> <span class="nf">prelu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Parametric Rectified Linear Unit activation function.</span>

<span class="sd">    PReLU is described in the paper `Delving Deep into Rectifiers: Surpassing Human-Level Performance on</span>
<span class="sd">    ImageNet Classification &lt;https://arxiv.org/abs/1502.01852&gt;`_. Defined as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        prelu(x_i)= \max(0, x_i) + \min(0, w * x_i),</span>

<span class="sd">    where :math:`x_i` is an element of a channel of the input, `w` is the weight of the channel.</span>

<span class="sd">    Note:</span>
<span class="sd">        Scalar or 1-D input x is not supported on Ascend.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input Tensor of the activation function. The data type is float16 or float32.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        weight (Tensor):  Weight Tensor. The data type is float16 or float32.</span>
<span class="sd">          The weight can only be a vector, and the length is the same as the number of channels C of the `input_x`.</span>
<span class="sd">          On GPU devices, when the input is a scalar, the shape is (1,).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same shape and dtype as `x`.</span>

<span class="sd">    For detailed information, please refer to :class:`mindspore.nn.PReLU`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x` or `weight` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If the `x` or the `weight` is not a Tensor.</span>
<span class="sd">        ValueError: If the `x` is a 0-D or 1-D Tensor on Ascend.</span>
<span class="sd">        ValueError: If the `weight` is not a 1-D Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(-6, 6).reshape((2, 3, 2)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.array([0.1, 0.6, -0.3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.prelu(x, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[-0.60 -0.50]</span>
<span class="sd">          [-2.40 -1.80]</span>
<span class="sd">          [ 0.60  0.30]]</span>
<span class="sd">         [[ 0.00  1.00]</span>
<span class="sd">          [ 2.00  3.00]</span>
<span class="sd">          [ 4.0   5.00]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">prelu_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">PReLU</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">prelu_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">mirror_pad</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">mode</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Pads the input tensor according to the paddings and mode.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Tensor of shape :math:`(N, *)`, where :math:`*` means, any number of</span>
<span class="sd">          additional dimensions.</span>
<span class="sd">        paddings (Tensor): Paddings requires constant tensor. The value of `paddings` is a</span>
<span class="sd">          matrix(list), and its shape is (N, 2). N is the rank of input data. All elements of paddings</span>
<span class="sd">          are int type. For the input in the `D` th dimension, paddings[D, 0] indicates how many sizes</span>
<span class="sd">          to be extended ahead of the input tensor in the `D` th dimension, and paddings[D, 1]</span>
<span class="sd">          indicates how many sizes to be extended behind the input tensor in the `D` th dimension. Both</span>
<span class="sd">          paddings[D, 0] and paddings[D, 1] must be no greater than input_x.dim_size(D)</span>
<span class="sd">          (or input_x.dim_size(D) - 1) if mode is SYMMETRIC (if REFLECT, respectively).</span>
<span class="sd">        mode (str): Specifies the padding mode. The optional values are &quot;REFLECT&quot; and &quot;SYMMETRIC&quot;.</span>
<span class="sd">            Default: &quot;REFLECT&quot;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the tensor after padding.</span>

<span class="sd">        - If `mode` is &quot;REFLECT&quot;, it uses a way of symmetrical copying through the axis of symmetry to fill in.</span>
<span class="sd">          If the `input_x` is [[1,2,3], [4,5,6], [7,8,9]] and `paddings` is [[1,1], [2,2]], then the</span>
<span class="sd">          `Outputs` is [[6,5,4,5,6,5,4], [3,2,1,2,3,2,1], [6,5,4,5,6,5,4], [9,8,7,8,9,8,7], [6,5,4,5,6,5,4]].</span>
<span class="sd">          For a more intuitive understanding, please see the example below.</span>
<span class="sd">        - If `mode` is &quot;SYMMETRIC&quot;, the filling method is similar to the &quot;REFLECT&quot;. It is also copied</span>
<span class="sd">          according to the symmetry axis, except that it includes the symmetry axis. If the `input_x`</span>
<span class="sd">          is [[1,2,3], [4,5,6], [7,8,9]] and `paddings` is [[1,1], [2,2]], then the `Outputs` is</span>
<span class="sd">          [[2,1,1,2,3,3,2], [2,1,1,2,3,3,2], [5,4,4,5,6,6,5], [8,7,7,8,9,9,8], [8,7,7,8,9,9,8]].</span>
<span class="sd">          For a more intuitive understanding, please see the example below.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` or `paddings` is not a Tensor.</span>
<span class="sd">        TypeError: If `mode` is not a str.</span>
<span class="sd">        ValueError: If paddings.size is not equal to 2 * rank of input_x.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor([[1,2,3], [4,5,6], [7,8,9]])</span>
<span class="sd">        &gt;&gt;&gt; mode = &quot;REFLECT&quot;</span>
<span class="sd">        &gt;&gt;&gt; paddings = Tensor([[1, 1], [2, 2]])</span>
<span class="sd">        &gt;&gt;&gt; output = ops.mirror_pad(input_x, paddings, mode)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[6 5 4 5 6 5 4]</span>
<span class="sd">         [3 2 1 2 3 2 1]</span>
<span class="sd">         [6 5 4 5 6 5 4]</span>
<span class="sd">         [9 8 7 8 9 8 7]</span>
<span class="sd">         [6 5 4 5 6 5 4]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_mirror_pad</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">MirrorPad</span><span class="p">)(</span><span class="n">mode</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_mirror_pad</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_innner_log_softmax</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;inner implementation of log_softmax, since the LogSoftmaxGrad op do not support inputs &gt; 2d&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">inputs</span> <span class="o">-</span> <span class="n">logsumexp</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>


<div class="viewcode-block" id="cross_entropy"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.cross_entropy.html#mindspore.ops.cross_entropy">[docs]</a><span class="k">def</span> <span class="nf">cross_entropy</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The cross entropy loss between input and target.</span>

<span class="sd">    The cross entropy support two kind of targets:</span>

<span class="sd">    - Class indices (int) in the range :math:`[0, C)` where :math:`C` is the number of classes,</span>
<span class="sd">      the loss with reduction=none can be described as:</span>

<span class="sd">      .. math::</span>

<span class="sd">          \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">          l_n = - w_{y_n} \log \frac{\exp(x_{n,y_n})}{\sum_{c=1}^C \exp(x_{n,c})}</span>
<span class="sd">          \cdot \mathbb{1}\{y_n \not= \text{ignore_index}\}</span>

<span class="sd">      where :math:`x` is the inputs, :math:`t` is the target, :math:`w` is the weight,</span>
<span class="sd">      N is the batch size, :math:`c` belonging to [0, C-1] is class index, where :math:`C` is the number of classes.</span>

<span class="sd">      If reduction is not &#39;none&#39; (default &#39;mean&#39;), then</span>

<span class="sd">      .. math::</span>

<span class="sd">          \ell(x, y) = \begin{cases}</span>
<span class="sd">              \sum_{n=1}^N \frac{1}{\sum_{n=1}^N w_{y_n} \cdot \mathbb{1}\{y_n \not= \text{ignore_index}\}} l_n, &amp;</span>
<span class="sd">              \text{if reduction} = \text{&#39;mean&#39;,}\\</span>
<span class="sd">              \sum_{n=1}^N l_n,  &amp;</span>
<span class="sd">              \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">              \end{cases}</span>

<span class="sd">    - Probabilities (float) for each class, useful when labels beyond a single class per minibatch item</span>
<span class="sd">      are required, the loss with reduction=none can be described as:</span>

<span class="sd">      .. math::</span>

<span class="sd">          \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">          l_n = - \sum_{c=1}^C w_c \log \frac{\exp(x_{n,c})}{\sum_{i=1}^C \exp(x_{n,i})} y_{n,c}</span>

<span class="sd">      where :math:`x` is the inputs, :math:`t` is the target, :math:`w` is the weight,</span>
<span class="sd">      N is the batch size, :math:`c` belonging to [0, C-1] is class index, where :math:`C` is the number of classes.</span>

<span class="sd">      If reduction is not &#39;none&#39; (default &#39;mean&#39;), then</span>

<span class="sd">      .. math::</span>

<span class="sd">          \ell(x, y) = \begin{cases}</span>
<span class="sd">              \frac{\sum_{n=1}^N l_n}{N}, &amp;</span>
<span class="sd">              \text{if reduction} = \text{&#39;mean&#39;,}\\</span>
<span class="sd">              \sum_{n=1}^N l_n,  &amp;</span>
<span class="sd">              \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">              \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs (Tensor): :math:`(N, C)` where `C = number of classes` or :math:`(N, C, H, W)`</span>
<span class="sd">            in case of 2D Loss, or :math:`(N, C, d_1, d_2, ..., d_K)`.</span>
<span class="sd">            `inputs` is expected to be log-probabilities, data type must be float16 or float32.</span>
<span class="sd">        target (Tensor): :math:`(N)` or :math:`(N, d_1, d_2, ..., d_K)` for</span>
<span class="sd">            high-dimensional loss.</span>
<span class="sd">        weight (Tensor): A rescaling weight applied to the loss of each batch element.</span>
<span class="sd">            If not None, the shape is :math:`(C,)`,</span>
<span class="sd">            data type must be float16 or float32. Default: None.</span>
<span class="sd">        ignore_index (int): Specifies a target value that is ignored</span>
<span class="sd">            and does not contribute to the input gradient. Default: -100</span>
<span class="sd">        reduction (str):  Apply specific reduction method to the output: &#39;none&#39;, &#39;mean&#39;, or &#39;sum&#39;.</span>
<span class="sd">            Default: &#39;mean&#39;.</span>
<span class="sd">        label_smoothing (float): Label smoothing values, a regularization tool used to prevent the model</span>
<span class="sd">            from overfitting when calculating Loss. The value range is [0.0, 1.0]. Default value: 0.0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the computed loss value.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # Case 1: Indices labels</span>
<span class="sd">        &gt;&gt;&gt; inputs = mindspore.Tensor(np.random.randn(3, 5), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; target = mindspore.Tensor(np.array([1, 0, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.cross_entropy(inputs, target)</span>
<span class="sd">        &gt;&gt;&gt; # Case 2: Probability labels</span>
<span class="sd">        &gt;&gt;&gt; inputs = mindspore.Tensor(np.random.randn(3, 5), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; target = mindspore.Tensor(np.random.randn(3, 5), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.cross_entropy(inputs, target)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">class_dim</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">inputs</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">inputs</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_cross_entropy</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">class_dim</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">nll_loss</span><span class="p">(</span><span class="n">_innner_log_softmax</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">class_dim</span><span class="p">),</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_cross_entropy</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">target_dim</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;cross entropy inner function&quot;&quot;&quot;</span>
    <span class="n">_ones_like</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">OnesLike</span><span class="p">)()</span>

    <span class="n">class_dim</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">inputs</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">1</span>
    <span class="n">n_classes</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">class_dim</span><span class="p">]</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">_innner_log_softmax</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">class_dim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">label_smoothing</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">:</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">label_smoothing</span><span class="p">)</span> <span class="o">+</span> <span class="n">label_smoothing</span> <span class="o">/</span> <span class="n">n_classes</span>

    <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">_ones_like</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">broadcast_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">ndim</span><span class="p">)]</span>
        <span class="n">broadcast_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">broadcast_shape</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="n">inputs</span> <span class="o">*</span> <span class="n">target</span> <span class="o">*</span> <span class="n">weight</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">size</span> <span class="o">/</span> <span class="n">n_classes</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;sum&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="n">inputs</span> <span class="o">*</span> <span class="n">target</span> <span class="o">*</span> <span class="n">weight</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="n">inputs</span> <span class="o">*</span> <span class="n">target</span> <span class="o">*</span> <span class="n">weight</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">class_dim</span><span class="p">)</span>


<div class="viewcode-block" id="nll_loss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.nll_loss.html#mindspore.ops.nll_loss">[docs]</a><span class="k">def</span> <span class="nf">nll_loss</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets the negative log likelihood loss between inputs and target.</span>

<span class="sd">    The nll loss with reduction=none can be described as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \ell(x, t)=L=\left\{l_{1}, \ldots, l_{N}\right\}^{\top},</span>
<span class="sd">        \quad l_{n}=-w_{t_{n}} x_{n, t_{n}},</span>
<span class="sd">        \quad w_{c}=\text { weight }[c] \cdot \mathbb{1}</span>
<span class="sd">        \{c \not= \text{ignore_index}\},</span>

<span class="sd">    where :math:`x` is the inputs, :math:`t` is the target, :math:`w` is the weight,</span>
<span class="sd">    N is the batch size, :math:`c` belonging to [0, C-1] is class index, where :math:`C` is the number of classes.</span>

<span class="sd">    If reduction is not &#39;none&#39; (default &#39;mean&#39;), then</span>

<span class="sd">    .. math::</span>

<span class="sd">        \ell(x, t)=\left\{\begin{array}{ll}</span>
<span class="sd">        \sum_{n=1}^{N} \frac{1}{\sum_{n=1}^{N} w_{t n}} l_{n}, &amp; \text { if reduction }=\text { &#39;mean&#39;, } \\</span>
<span class="sd">        \sum_{n=1}^{N} l_{n}, &amp; \text { if reduction }=\text { &#39;sum&#39; }</span>
<span class="sd">        \end{array}\right.</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs (Tensor): :math:`(N, C)` where `C = number of classes` or :math:`(N, C, H, W)`</span>
<span class="sd">            in case of 2D Loss, or :math:`(N, C, d_1, d_2, ..., d_K)`.</span>
<span class="sd">            `inputs` is expected to be log-probabilities, data type must be float16 or float32.</span>
<span class="sd">        target (Tensor): :math:`(N)` or :math:`(N, d_1, d_2, ..., d_K)` for</span>
<span class="sd">            high-dimensional loss, data type must be int32.</span>
<span class="sd">        weight (Tensor): A rescaling weight applied to the loss of each batch element.</span>
<span class="sd">            If not None, the shape is :math:`(C,)`.</span>
<span class="sd">            The data type must be float16 or float32. Default: None.</span>
<span class="sd">        ignore_index (int): Specifies a target value that is ignored</span>
<span class="sd">            and does not contribute to the input gradient. Default: -100</span>
<span class="sd">        reduction (str):  Apply specific reduction method to the output: &#39;none&#39;, &#39;mean&#39;, or &#39;sum&#39;.</span>
<span class="sd">            Default: &#39;mean&#39;.</span>
<span class="sd">        label_smoothing (float): Label smoothing values, a regularization tool used to prevent the model</span>
<span class="sd">            from overfitting when calculating Loss. The value range is [0.0, 1.0]. Default value: 0.0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the computed loss value.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>

<span class="sd">        &gt;&gt;&gt; inputs = mindspore.Tensor(np.random.randn(3, 5), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; target = mindspore.Tensor(np.array([1, 0, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.nll_loss(inputs, target)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">ndim</span>
    <span class="k">if</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">_nll_loss</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">_nll_loss</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">_nll_loss</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">n</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">out_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">n</span><span class="p">,)</span> <span class="o">+</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">reduction</span> <span class="o">!=</span> <span class="s1">&#39;none&#39;</span><span class="p">:</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">_nll_loss</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">_nll_loss</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="o">=</span><span class="n">label_smoothing</span><span class="p">)</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">ret</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">out_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span></div>


<span class="k">def</span> <span class="nf">_nll_loss</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">target_dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;nll loss inner function&quot;&quot;&quot;</span>
    <span class="n">_neg</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Neg</span><span class="p">)()</span>
    <span class="n">_gather_d</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">GatherD</span><span class="p">)()</span>
    <span class="n">_gather</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Gather</span><span class="p">)()</span>
    <span class="n">_ones_like</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">OnesLike</span><span class="p">)()</span>
    <span class="n">_equal</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Equal</span><span class="p">)()</span>

    <span class="k">if</span> <span class="n">target</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="n">inputs</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">target_dim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ignore_index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">non_pad_mask</span> <span class="o">=</span> <span class="n">_equal</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">)</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">non_pad_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">non_pad_mask</span> <span class="o">=</span> <span class="n">target</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">_neg</span><span class="p">(</span><span class="n">_gather_d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target_dim</span><span class="p">,</span> <span class="n">target</span><span class="p">))</span>
    <span class="n">smooth_loss</span> <span class="o">=</span> <span class="n">_neg</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">target_dim</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">loss_weights</span> <span class="o">=</span> <span class="n">_gather</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">*</span> <span class="n">loss_weights</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">loss_weights</span> <span class="o">=</span> <span class="n">_ones_like</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ignore_index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">non_pad_mask</span><span class="p">,</span> <span class="mf">0.</span><span class="p">)</span>
        <span class="n">loss_weights</span> <span class="o">=</span> <span class="n">loss_weights</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">non_pad_mask</span><span class="p">,</span> <span class="mf">0.</span><span class="p">)</span>
        <span class="n">smooth_loss</span> <span class="o">=</span> <span class="n">smooth_loss</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">non_pad_mask</span><span class="p">,</span> <span class="mf">0.</span><span class="p">)</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">target_dim</span><span class="p">)</span>
    <span class="n">smooth_loss</span> <span class="o">=</span> <span class="n">smooth_loss</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">target_dim</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;sum&#39;</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">smooth_loss</span> <span class="o">=</span> <span class="n">smooth_loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">loss_weights</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">smooth_loss</span> <span class="o">=</span> <span class="n">smooth_loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="n">eps_i</span> <span class="o">=</span> <span class="n">label_smoothing</span> <span class="o">/</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">target_dim</span><span class="p">]</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">label_smoothing</span><span class="p">)</span> <span class="o">*</span> <span class="n">loss</span> <span class="o">+</span> <span class="n">eps_i</span> <span class="o">*</span> <span class="n">smooth_loss</span>

    <span class="k">return</span> <span class="n">loss</span>


<div class="viewcode-block" id="smooth_l1_loss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.smooth_l1_loss.html#mindspore.ops.smooth_l1_loss">[docs]</a><span class="k">def</span> <span class="nf">smooth_l1_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes smooth L1 loss, a robust L1 loss.</span>

<span class="sd">    SmoothL1Loss is a Loss similar to MSELoss but less sensitive to outliers as described in the</span>
<span class="sd">    `Fast R-CNN &lt;https://arxiv.org/abs/1504.08083&gt;`_ by Ross Girshick.</span>

<span class="sd">    Given two input :math:`x,\  y` of length :math:`N`, the unreduced SmoothL1Loss can be described</span>
<span class="sd">    as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        L_{i} =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">        \frac{0.5 (x_i - y_i)^{2}}{\text{beta}}, &amp; \text{if } |x_i - y_i| &lt; \text{beta} \\</span>
<span class="sd">        |x_i - y_i| - 0.5 \text{beta}, &amp; \text{otherwise. }</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    If `reduction` is not `none`, then:</span>

<span class="sd">    .. math::</span>
<span class="sd">        L =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">            \operatorname{mean}(L_{i}), &amp;  \text{if reduction} = \text{&#39;mean&#39;;}\\</span>
<span class="sd">            \operatorname{sum}(L_{i}),  &amp;  \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Here :math:`\text{beta}` controls the point where the loss function changes from quadratic to linear.</span>
<span class="sd">    Its default value is 1.0. :math:`N` is the batch size.</span>

<span class="sd">    Note:</span>
<span class="sd">        For Ascend platform, the float64 data type of `logits` is not support now.</span>

<span class="sd">    Args:</span>
<span class="sd">        logits (Tensor): Tensor of shape :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        labels (Tensor): Ground truth data, tensor of shape :math:`(N, *)`, same shape and dtype as the `logits`.</span>
<span class="sd">        beta (float): A parameter used to control the point where the function will change from</span>
<span class="sd">            quadratic to linear. Default: 1.0.</span>
<span class="sd">        reduction (str): Apply specific reduction method to the output: &#39;none&#39;, &#39;mean&#39; or &#39;sum&#39;. Default: &#39;none&#39;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, if `reduction` is &#39;none&#39;, then output is a tensor with the same shape as `logits`.</span>
<span class="sd">        Otherwise the shape of output tensor is `(1,)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `beta` is not a float.</span>
<span class="sd">        ValueError: If `reduction` is not one of &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;.</span>
<span class="sd">        TypeError: If dtype of `logits` or `labels` is neither float16 nor float32.</span>
<span class="sd">        ValueError: If `beta` is less than or equal to 0.</span>
<span class="sd">        ValueError: If shape of `logits` is not the same as `labels`.</span>
<span class="sd">        TypeError: The float64 data type of `logits` is support on Ascend platform.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([1, 2, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.smooth_l1_loss(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.  0.  0.5]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_smooth_l1_loss</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">SmoothL1Loss</span><span class="p">)(</span><span class="n">beta</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_smooth_l1_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span></div>


<div class="viewcode-block" id="intopk"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.intopk.html#mindspore.ops.intopk">[docs]</a><span class="k">def</span> <span class="nf">intopk</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Determines whether the targets are in the top `k` predictions.</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Tensor): A 2D Tensor defines the predictions of a batch of samples with float16 or float32</span>
<span class="sd">          data type.</span>
<span class="sd">        x2 (Tensor): A 1D Tensor defines the labels of a batch of samples with int32 data type. The size of `x2`</span>
<span class="sd">          must be equal to the first dimension of `x1`. The values of `x2` can not be negative and</span>
<span class="sd">          must be equal to or less than index of x1&#39;s second dimension.</span>
<span class="sd">        k (int): Specifies the number of top elements to be used for computing precision along the last dimension.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor has 1 dimension of type bool and the same shape with `x2`. For labeling sample `i` in `x2`,</span>
<span class="sd">        if the label in the first `k` predictions for sample `i` is in `x1`, then the value is True, otherwise False.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `k` is not an int.</span>
<span class="sd">        TypeError: If `x1` or `x2` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x1` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([[1, 8, 5, 2, 7], [4, 9, 1, 3, 5]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([1, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.intopk(x1, x2, 3)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True  False]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_in_topk</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">InTopK</span><span class="p">)(</span><span class="n">k</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_in_topk</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span></div>


<div class="viewcode-block" id="log_softmax"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.log_softmax.html#mindspore.ops.log_softmax">[docs]</a><span class="k">def</span> <span class="nf">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies the Log Softmax function to the input tensor on the specified axis.</span>
<span class="sd">    Supposes a slice in the given axis, :math:`x` for each element :math:`x_i`,</span>
<span class="sd">    the Log Softmax function is shown as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(x_i) = \log \left(\frac{\exp(x_i)} {\sum_{j = 0}^{N-1}\exp(x_j)}\right),</span>

<span class="sd">    where :math:`N` is the length of the Tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        logits (Tensor): Tensor of shape :math:`(N, *)`, where :math:`*` means, any number of</span>
<span class="sd">          additional dimensions, with float16 or float32 data type.</span>
<span class="sd">        axis (int): The axis to perform the Log softmax operation. Default: -1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as the logits.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>
<span class="sd">        TypeError: If dtype of `logits` is neither float16 nor float32.</span>
<span class="sd">        ValueError: If `axis` is not in range [-len(logits.shape), len(logits.shape)).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([1, 2, 3, 4, 5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.log_softmax(logits)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-4.4519143 -3.4519143 -2.4519143 -1.4519144 -0.4519144]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_log_softmax</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">)(</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span></div>


<div class="viewcode-block" id="lrn"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.lrn.html#mindspore.ops.lrn">[docs]</a><span class="k">def</span> <span class="nf">lrn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">depth_radius</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">norm_region</span><span class="o">=</span><span class="s2">&quot;ACROSS_CHANNELS&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Local Response Normalization.</span>

<span class="sd">    .. math::</span>

<span class="sd">        b_{c} = a_{c}\left(k + \frac{\alpha}{n}</span>
<span class="sd">        \sum_{c&#39;=\max(0, c-n/2)}^{\min(N-1,c+n/2)}a_{c&#39;}^2\right)^{-\beta}</span>

<span class="sd">    where the :math:`a_{c}` indicates the specific value of the pixel corresponding to c in feature map;</span>
<span class="sd">    where the :math:`n/2` indicates the `depth_radius`; where the :math:`k` indicates the `bias`;</span>
<span class="sd">    where the :math:`\alpha` indicates the `alpha`; where the :math:`\beta` indicates the `beta`.</span>

<span class="sd">    Args:</span>
<span class="sd">        depth_radius (int): Half-width of the 1-D normalization window with the shape of 0-D. Default: 5.</span>
<span class="sd">        bias (float): An offset (usually positive to avoid dividing by 0). Default: 1.0.</span>
<span class="sd">        alpha (float): A scale factor, usually positive. Default: 1.0.</span>
<span class="sd">        beta (float): An exponent. Default: 0.5.</span>
<span class="sd">        norm_region (str): Specifies normalization region. Options: &quot;ACROSS_CHANNELS&quot;. Default: &quot;ACROSS_CHANNELS&quot;.</span>
<span class="sd">        x (Tensor): A 4-D Tensor with float16 or float32 data type.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same shape and data type as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `depth_radius` is not an int.</span>
<span class="sd">        TypeError: If `bias`, `alpha` or `beta` is not a float.</span>
<span class="sd">        TypeError: If `norm_region` is not a str.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[[0.1], [0.2]],</span>
<span class="sd">        ...                       [[0.3], [0.4]]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.lrn(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[0.09534626]</span>
<span class="sd">           [0.1825742 ]]</span>
<span class="sd">          [[0.2860388 ]</span>
<span class="sd">           [0.3651484 ]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">lrn_op</span> <span class="o">=</span> <span class="n">NN_OPS</span><span class="o">.</span><span class="n">LRN</span><span class="p">(</span><span class="n">depth_radius</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">norm_region</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">lrn_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="mish"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.mish.html#mindspore.ops.mish">[docs]</a><span class="k">def</span> <span class="nf">mish</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes MISH(A Self Regularized Non-Monotonic Neural Activation Function) of input tensors element-wise.</span>

<span class="sd">    The function is shown as follows:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{output} = x * \tanh(\log(1 + \exp(\text{x})))</span>

<span class="sd">    See more details in `A Self Regularized Non-Monotonic Neural Activation Function</span>
<span class="sd">    &lt;https://arxiv.org/abs/1908.08681&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Tensor of shape :math:`(N, *)`, where :math:`*` means, any number of</span>
<span class="sd">            additional dimensions, with float16 or float32 data type.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.mish(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-3.0340147e-01  3.9974129e+00 -2.68311895e-03]</span>
<span class="sd">         [ 1.9439590e+00  -3.3576239e-02 8.99999990e+00]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">mish_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_value_type</span><span class="p">(</span><span class="n">arg_name</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">,</span> <span class="n">valid_types</span><span class="p">,</span> <span class="n">prim_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Checks whether a value is instance of some types.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="n">arg_name</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">,</span> <span class="n">valid_types</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>


<span class="nd">@constexpr</span><span class="p">(</span><span class="n">check</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_check_is_tensor</span><span class="p">(</span><span class="n">param_name</span><span class="p">,</span> <span class="n">input_data</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Internal function, used to check whether the input data is Tensor.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">input_data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">typeof</span><span class="p">(</span><span class="n">input_data</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="n">cls_name</span><span class="si">}</span><span class="s2">&#39;, the &#39;</span><span class="si">{</span><span class="n">param_name</span><span class="si">}</span><span class="s2">&#39; must be &#39;</span><span class="si">{</span><span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="si">}</span><span class="s2">&#39;, &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;but got &#39;</span><span class="si">{</span><span class="n">ops</span><span class="o">.</span><span class="n">typeof</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_get_axis</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get a range of axis for input.&quot;&quot;&quot;</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">length</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">tuple_len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">perm</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">make_range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">length</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">perm</span>


<span class="k">def</span> <span class="nf">_get_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate the loss with reduction and weights.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="s1">&#39;none&#39;</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="n">cls_name</span><span class="si">}</span><span class="s2">&#39;, the &#39;reduction&#39; must be in [&#39;mean&#39;, &#39;sum&#39;, &#39;none&#39;], &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">reduction</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="n">reduce_mean</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceMean</span><span class="p">()</span>
    <span class="n">reduce_sum</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">()</span>
    <span class="n">mul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span>
    <span class="n">cast</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>

    <span class="n">input_dtype</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">mul</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">reduce_mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">_get_axis</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;sum&#39;</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">reduce_sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">_get_axis</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>


<div class="viewcode-block" id="margin_ranking_loss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.margin_ranking_loss.html#mindspore.ops.margin_ranking_loss">[docs]</a><span class="k">def</span> <span class="nf">margin_ranking_loss</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    MarginRankingLoss creates a criterion that measures the loss.</span>

<span class="sd">    For details, please refer to :class:`mindspore.nn.MarginRankingLoss`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">margin</span> <span class="o">=</span> <span class="n">_check_value_type</span><span class="p">(</span><span class="s2">&quot;margin&quot;</span><span class="p">,</span> <span class="n">margin</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="s2">&quot;margin_ranking_loss&quot;</span><span class="p">)</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;input1&#39;</span><span class="p">,</span> <span class="n">input1</span><span class="p">,</span> <span class="s2">&quot;margin_ranking_loss&quot;</span><span class="p">)</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;input2&#39;</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="s2">&quot;margin_ranking_loss&quot;</span><span class="p">)</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;target&#39;</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="s2">&quot;margin_ranking_loss&quot;</span><span class="p">)</span>
    <span class="n">maximum</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Maximum</span><span class="p">()</span>
    <span class="n">inner</span><span class="o">.</span><span class="n">same_type_shape_</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">)</span>
    <span class="n">inner</span><span class="o">.</span><span class="n">same_type_shape_</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">input1</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="n">target</span> <span class="o">*</span> <span class="p">(</span><span class="n">input1</span> <span class="o">-</span> <span class="n">input2</span><span class="p">)</span> <span class="o">+</span> <span class="n">margin</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_get_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="s2">&quot;margin_ranking_loss&quot;</span><span class="p">)</span></div>


<div class="viewcode-block" id="max_pool3d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.max_pool3d.html#mindspore.ops.max_pool3d">[docs]</a><span class="k">def</span> <span class="nf">max_pool3d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs a 3D max pooling on the input Tensor.</span>

<span class="sd">    Typically the input is a Tensor with shape :math:`(N_{in}, C_{in}, D_{in}, H_{in}, W_{in})`, outputs</span>
<span class="sd">    regional maximum in the :math:`(D_{in}, H_{in}, W_{in})`-dimension. Given `kernel_size`</span>
<span class="sd">    :math:`ks = (d_{ker}, h_{ker}, w_{ker})` and `stride` :math:`s = (s_0, s_1, s_2)`, the operation is as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(N_i, C_j, d, h, w) =</span>
<span class="sd">        \max_{l=0, \ldots, d_{ker}-1} \max_{m=0, \ldots, h_{ker}-1} \max_{n=0, \ldots, w_{ker}-1}</span>
<span class="sd">        \text{input}(N_i, C_j, s_0 \times d + l, s_1 \times h + m, s_2 \times w + n)</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Tensor of shape :math:`(N_{in}, C_{in}, D_{in}, H_{in}, W_{in})` with data type of int8,</span>
<span class="sd">            int16, int32, int64, uint8, uint16, uint32, uint64, float16, float32 or float64.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of kernel used to take the maximum value and arg</span>
<span class="sd">            value, is an int number that represents depth, height and width of the kernel, or a tuple of</span>
<span class="sd">            three int numbers that represent depth, height and width respectively.</span>
<span class="sd">        stride (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the depth, height and width of movement are both stride, or a tuple of three int numbers that</span>
<span class="sd">            represent depth, height and width of movement respectively. Default: `kernel_size`.</span>
<span class="sd">        padding (Union[int, tuple[int]]): An int number that represents the depth, height and width of movement are both</span>
<span class="sd">            strides, or a tuple of three int numbers that represent depth, height and width of movement respectively.</span>
<span class="sd">            Default: 0.</span>
<span class="sd">        dilation (Union[int, tuple[int]]): Control the stride of elements in the kernel. Default: 1.</span>
<span class="sd">        ceil_mode (bool): Whether to use ceil instead of floor to calculate output shape. Default: False.</span>
<span class="sd">        return_indices (bool): Whether to output the indices of max value. Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        If `return_indices` is False, return a Tensor `output`, else return a tuple (`output`, `argmax`).</span>

<span class="sd">        - **output** (Tensor) - Maxpooling result, with shape :math:`(N_{out}, C_{out}, D_{out}, H_{out}, W_{out})`.</span>
<span class="sd">          It has the same data type as `x`.</span>
<span class="sd">        - **argmax** (Tensor) - Index corresponding to the maximum value. Data type is int64. It will be return</span>
<span class="sd">          only when `return_indices` is True.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        ValueError: If length of shape of `x` is not equal to 5.</span>
<span class="sd">        TypeError: If `kernel_size` , `stride` , `padding` or `dilation` is not int or tuple.</span>
<span class="sd">        ValueError: If `kernel_size` or `stride` is less than 1.</span>
<span class="sd">        ValueError: If `padding` is less than 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(2 * 1 * 2 * 2 * 2).reshape((2, 1, 2, 2, 2)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output_tensor, argmax = ops.max_pool3d(x, kernel_size=2, stride=1, padding=1, return_indices=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output_tensor.shape)</span>
<span class="sd">        (2, 1, 3, 3, 3)</span>
<span class="sd">        &gt;&gt;&gt; print(argmax.shape)</span>
<span class="sd">        (2, 1, 3, 3, 3)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">strides</span> <span class="o">=</span> <span class="n">stride</span> <span class="k">if</span> <span class="p">(</span><span class="n">stride</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="k">else</span> <span class="n">kernel_size</span>
    <span class="n">max_pool3d_with_argmax_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">MaxPool3DWithArgmax</span><span class="p">)(</span>
        <span class="n">kernel_size</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">)</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">max_pool3d_with_argmax_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">return_indices</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">indices</span>
    <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="grid_sample"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.grid_sample.html#mindspore.ops.grid_sample">[docs]</a><span class="k">def</span> <span class="nf">grid_sample</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="n">interpolation_mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Given an `input_x` and a flow-field `grid`, computes the `output` using `input_x` values and pixel locations from</span>
<span class="sd">    `grid`. Only spatial (4-D) and volumetric (5-D) `input_x` is supported.</span>

<span class="sd">    In the spatial (4-D) case, for `input_x` with shape :math:`(N, C, H_{in}, W_{in})` and `grid` with shape</span>
<span class="sd">    :math:`(N, H_{out}, W_{out}, 2)`, the `output` will have shape :math:`(N, C, H_{out}, W_{out})`.</span>

<span class="sd">    For each output location `output[n, :, h, w]`, the size-2 vector `grid[n, h, w]` specifies `input_x` pixel</span>
<span class="sd">    locations `x` and `y`, which are used to interpolate the output value `output[n, :, h, w]`. In the case of 5D</span>
<span class="sd">    inputs, `grid[n, d, h, w]`, specifies the `x`, `y`, `z` pixel locations for interpolating</span>
<span class="sd">    `output[n, :, d, h, w]`. And `interpolation_mode` argument specifies &quot;nearest&quot; or &quot;bilinear&quot; or &quot;bicubic&quot;</span>
<span class="sd">    (supported in 4D case only) interpolation method to sample the input pixels.</span>

<span class="sd">    `grid` specifies the sampling pixel locations normalized by the `input_x` spatial dimensions. Therefore, it should</span>
<span class="sd">    have most values in the range of :math:`[-1, 1]`.</span>

<span class="sd">    If `grid` has values outside the range of :math:`[-1, 1]`, the corresponding outputs are handled as defined by</span>
<span class="sd">    `padding_mode`. If `padding_mode` is set to be &quot;zeros&quot;, use :math:`0` for out-of-bound grid locations. If</span>
<span class="sd">    `padding_mode` is set to be &quot;border&quot;, use border values for out-of-bound grid locations. If `padding_mode` is set</span>
<span class="sd">    to be &quot;reflection&quot;, use values at locations reflected by the border for out-of-bound grid locations. For location</span>
<span class="sd">    far away from the border, it will keep being reflected until becoming in bound.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): input with shape of :math:`(N, C, H_{in}, W_{in})` (4-D case) or :math:`(N, C, D_{in},</span>
<span class="sd">            H_{in}, W_{in})` (5-D case) and dtype of float32 or float64.</span>
<span class="sd">        grid (Tensor): flow-field with shape of :math:`(N, H_{out}, W_{out}, 2)` (4-D case) or :math:`(N, D_{out},</span>
<span class="sd">            H_{out}, W_{out}, 3)` (5-D case) and same dtype as `input_x`.</span>
<span class="sd">        interpolation_mode (str): An optional string specifying the interpolation method. The optional values are</span>
<span class="sd">            &quot;bilinear&quot;, &quot;nearest&quot; or &quot;bicubic&quot;. Default: &quot;bilinear&quot;. Note: `bicubic` supports only 4-D input. When</span>
<span class="sd">            `interpolation_mode=&quot;bilinear&quot;` and the input is 5-D, the interpolation mode used internally will actually</span>
<span class="sd">            be trilinear. However, when the input is 4-D, the interpolation mode will legistimately be bilinear.</span>
<span class="sd">        padding_mode (str): An optional string specifying the pad method. The optional values are &quot;zeros&quot;, &quot;border&quot; or</span>
<span class="sd">            &quot;reflection&quot;. Default: &quot;zeros&quot;.</span>
<span class="sd">        align_corners (bool): An optional bool. If set to `True`, the extrema (-1 and 1) are considered as referring to</span>
<span class="sd">            the center points of the input’s corner pixels. If set to `False`, they are instead considered as referring</span>
<span class="sd">            to the corner points of the input’s corner pixels, making the sampling more resolution agnostic. Default:</span>
<span class="sd">            `False`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, dtype is the same as `input_x` and whose shape is :math:`(N, C, H_{out}, W_{out})` (4-D) and</span>
<span class="sd">        :math:`(N, C, D_{out}, H_{out}, W_{out})` (5-D).</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` or `grid` is not a Tensor.</span>
<span class="sd">        TypeError: If the dtypes of `input_x` and `grid` are inconsistent.</span>
<span class="sd">        TypeError: If the dtype of `input_x` or `grid` is not a valid type.</span>
<span class="sd">        TypeError: If `align_corners` is not a boolean value.</span>
<span class="sd">        ValueError: If the rank of `input_x` or `grid` is not equal to 4(4-D case) or 5(5-D case).</span>
<span class="sd">        ValueError: If the first dimension of `input_x` is not equal to that of `grid`.</span>
<span class="sd">        ValueError: If the last dimension of `grid` is not equal to 2(4-D case) or 3(5-D case).</span>
<span class="sd">        ValueError: If `interpolation_mode` is not &quot;bilinear&quot;, &quot;nearest&quot;, &quot;bicubic&quot; or a string value.</span>
<span class="sd">        ValueError: If `padding_mode` is not &quot;zeros&quot;, &quot;border&quot;, &quot;reflection&quot; or a string value.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.arange(16).reshape((2, 2, 2, 2)).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; grid = Tensor(np.arange(0.2, 1, 0.1).reshape((2, 2, 1, 2)).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.grid_sample(input_x, grid, interpolation_mode=&#39;bilinear&#39;, padding_mode=&#39;zeros&#39;,</span>
<span class="sd">        ...                          align_corners=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[ 1.9      ]</span>
<span class="sd">           [ 2.1999998]]</span>
<span class="sd">          [[ 5.9      ]</span>
<span class="sd">           [ 6.2      ]]]</span>
<span class="sd">         [[[10.5      ]</span>
<span class="sd">           [10.8      ]]</span>
<span class="sd">          [[14.5      ]</span>
<span class="sd">           [14.8      ]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">input_x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="n">_grid_sampler_2d</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">GridSampler2D</span><span class="p">)(</span><span class="n">interpolation_mode</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">_grid_sampler_2d</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span>
    <span class="n">_grid_sampler_3d</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">GridSampler3D</span><span class="p">)(</span><span class="n">interpolation_mode</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_grid_sampler_3d</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_ctc_loss_inputs</span><span class="p">(</span><span class="n">blank</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">zero_infinity</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">):</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;blank&quot;</span><span class="p">,</span> <span class="n">blank</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">prim_name</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;reduction&#39;</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">prim_name</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">reduction</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">],</span> <span class="s1">&#39;reduction&#39;</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;zero_infinity&quot;</span><span class="p">,</span> <span class="n">zero_infinity</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="n">prim_name</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">ctc_loss</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">target_lengths</span><span class="p">,</span> <span class="n">blank</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="n">zero_infinity</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the CTC (Connectionist Temporal Classification) loss and the gradient.</span>

<span class="sd">    The CTC algorithm is proposed in `Connectionist Temporal Classification: Labeling Unsegmented Sequence Data with</span>
<span class="sd">    Recurrent Neural Networks &lt;http://www.cs.toronto.edu/~graves/icml_2006.pdf&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        log_probs (Tensor): A tensor of shape (T, N, C), where T is input length, N is batch size and C is</span>
<span class="sd">            number of classes (including blank).</span>
<span class="sd">        targets (Tensor): A tensor of shape (N, S), where S is max target length, means the target sequences.</span>
<span class="sd">        input_lengths (Union(Tuple, Tensor)): A tuple or Tensor of shape(N). It means the lengths of the input.</span>
<span class="sd">        target_lengths (Union(Tuple, Tensor)): A tuple or Tensor of shape(N). It means the lengths of the target.</span>
<span class="sd">        blank (int): The blank label. Default: 0.</span>
<span class="sd">        reduction (string): Apply specific reduction method to the output: &#39;none&#39;, &#39;mean&#39;, or &#39;sum&#39;. Default: &#39;mean&#39;.</span>
<span class="sd">        zero_infinity (bool): Whether to set infinite loss and correlation gradient to zero. Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        neg_log_likelihood (Tensor), A loss value with shape (N), which is differentiable with respect to</span>
<span class="sd">        each input node.</span>

<span class="sd">        log_alpha (Tensor), The probability of possible trace of input to target with shape (N, T, 2 * S + 1).</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `zero_infinity` is not a bool, reduction is not string.</span>
<span class="sd">        TypeError: If the dtype of `log_probs` or `grad_out` is not float or double.</span>
<span class="sd">        TypeError: If the dtype of `targets`, `input_lengths` or `target_lengths` is not int32 or int64.</span>
<span class="sd">        ValueError: If the rank of `log_probs` is not 3.</span>
<span class="sd">        ValueError: If the rank of `targets` is not 2.</span>
<span class="sd">        ValueError: If the shape of `input_lengths` does not match {batch_size|N}.</span>
<span class="sd">        ValueError: If the shape of `target_lengths` does not match {batch_size|N}.</span>
<span class="sd">        TypeError: If the types of `targets`, `input_lengths`, `grad_out` or `target_lengths` are different.</span>
<span class="sd">        ValueError: If the value of `blank` is not in range [0, num_labels|C).</span>
<span class="sd">        RuntimeError: If any value of `input_lengths` is larger than (num_labels|C).</span>
<span class="sd">        RuntimeError: If any target_lengths[i] is not in range [0, input_length[i]].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; log_probs = Tensor(np.array([[[0.3, 0.6, 0.6]],</span>
<span class="sd">        ...                              [[0.9, 0.4, 0.2]]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; targets = Tensor(np.array([[0, 1]]), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; input_lengths = Tensor(np.array([2]), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; target_lengths = Tensor(np.array([1]), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; loss, log_alpha = ops.ctc_loss(log_probs, targets, input_lengths,</span>
<span class="sd">        ...                                target_lengths, 0, &#39;mean&#39;, True)</span>
<span class="sd">        &gt;&gt;&gt; print(loss)</span>
<span class="sd">        -2.2986124</span>
<span class="sd">        &gt;&gt;&gt; print(log_alpha)</span>
<span class="sd">        [[[0.3       0.3            -inf      -inf      -inf]</span>
<span class="sd">          [1.2       1.8931472 1.2            -inf      -inf]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_ctc_loss_inputs</span><span class="p">(</span><span class="n">blank</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">zero_infinity</span><span class="p">,</span> <span class="s1">&#39;ctc_loss&#39;</span><span class="p">)</span>
    <span class="n">ctc_loss_op</span> <span class="o">=</span> <span class="n">NN_OPS</span><span class="o">.</span><span class="n">CTCLossV2</span><span class="p">(</span><span class="n">blank</span><span class="o">=</span><span class="n">blank</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span> <span class="n">zero_infinity</span><span class="o">=</span><span class="n">zero_infinity</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">log_alpha</span> <span class="o">=</span> <span class="n">ctc_loss_op</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">target_lengths</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;sum&#39;</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
        <span class="n">input_type</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">target_length_t</span> <span class="o">=</span> <span class="n">target_lengths</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">target_length_t</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">input_type</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">log_alpha</span><span class="p">)</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_gaussian_nll_loss</span><span class="p">(</span><span class="n">full</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">reduction</span><span class="p">):</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;full&#39;</span><span class="p">,</span> <span class="n">full</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="s1">&#39;gaussian_nll_loss&#39;</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_float</span><span class="p">(</span><span class="n">eps</span><span class="p">,</span> <span class="s1">&#39;eps&#39;</span><span class="p">,</span> <span class="s1">&#39;gaussian_nll_loss&#39;</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">reduction</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">],</span> <span class="s1">&#39;reduction&#39;</span><span class="p">,</span> <span class="s1">&#39;gaussian_nll_loss&#39;</span><span class="p">)</span>


<div class="viewcode-block" id="gaussian_nll_loss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.gaussian_nll_loss.html#mindspore.ops.gaussian_nll_loss">[docs]</a><span class="k">def</span> <span class="nf">gaussian_nll_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">full</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gaussian negative log likelihood loss.</span>

<span class="sd">    The targets are treated as samples from Gaussian distributions with expectations and variances predicted by the</span>
<span class="sd">    neural network. For a `target` tensor modelled as having Gaussian distribution with a tensor of expectations</span>
<span class="sd">    `x` and a tensor of positive variances `var` the loss is:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss} = \frac{1}{2}\left(\log\left(\text{max}\left(\text{var},</span>
<span class="sd">        \ \text{eps}\right)\right) + \frac{\left(\text{x} - \text{target}\right)^2}</span>
<span class="sd">        {\text{max}\left(\text{var}, \ \text{eps}\right)}\right) + \text{const.}</span>

<span class="sd">    where `eps` is used for stability of :math:`log`. By default, the constant term of the loss function is omitted</span>
<span class="sd">    unless :math:`full=True`. If the shape of :math:`var` is not the same as `x` (due to a</span>
<span class="sd">    homoscedastic assumption), it must either have a final dimension of 1 or have one fewer dimension</span>
<span class="sd">    (with all other sizes being the same) for correct broadcasting.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Tensor of shape :math:`(N, *)` or :math:`(*)` where :math:`*` means any number of</span>
<span class="sd">            additional dimensions.</span>
<span class="sd">        target (Tensor): Tensor of shape :math:`(N, *)` or :math:`(*)`, same shape as the x, or same shape</span>
<span class="sd">            as the x but with one dimension equal to 1 (to allow broadcasting).</span>
<span class="sd">        var (Tensor): Tensor of shape :math:`(N, *)` or :math:`(*)`, same shape as x, or same shape as the x</span>
<span class="sd">            but with one dimension equal to 1, or same shape as the x but with one fewer dimension</span>
<span class="sd">            (to allow for broadcasting).</span>
<span class="sd">        full (bool, optional): Include the constant term in the loss calculation. When :math:`full=True`,</span>
<span class="sd">            the constant term `const.` will be :math:`0.5 * log(2\pi)`. Default: False.</span>
<span class="sd">        eps (float, optional): Used to improve the stability of log function must be greater than 0. Default: 1e-6.</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: &#39;none&#39;, &#39;mean&#39;, or &#39;sum&#39;.</span>
<span class="sd">            Default: &#39;mean&#39;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or Tensor scalar, the computed loss depending on `reduction`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `target` is not a Tensor.</span>
<span class="sd">        TypeError: If `var` is not a Tensor.</span>
<span class="sd">        TypeError: If `full` is not a bool.</span>
<span class="sd">        TypeError: If `eps` is not a float.</span>
<span class="sd">        ValueError: If `eps` is not a float within [0, inf).</span>
<span class="sd">        ValueError: If `reduction` is not one of &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.common.dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; arr1 = np.arange(8).reshape((4, 2))</span>
<span class="sd">        &gt;&gt;&gt; arr2 = np.array([2, 3, 1, 4, 6, 4, 4, 9]).reshape((4, 2))</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(arr1, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; var = Tensor(np.ones((4, 1)), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; target = Tensor(arr2, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.gaussian_nll_loss(x, target, var)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        1.4374993</span>

<span class="sd">    Reference:</span>
<span class="sd">        Nix, D. A. and Weigend, A. S., &quot;Estimating the mean and variance of the</span>
<span class="sd">        target probability distribution&quot;, Proceedings of 1994 IEEE International</span>
<span class="sd">        Conference on Neural Networks (ICNN&#39;94), Orlando, FL, USA, 1994, pp. 55-60</span>
<span class="sd">        vol.1, doi: 10.1109/ICNN.1994.374138.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;gaussian_nll_loss&#39;, &#39;x&#39; must be a tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;gaussian_nll_loss&#39;, &#39;target&#39; must be a tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">target</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;gaussian_nll_loss&#39;, &#39;var&#39; must be a tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">var</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">_check_gaussian_nll_loss</span><span class="p">(</span><span class="n">full</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
    <span class="n">max_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Maximum</span><span class="p">()</span>
    <span class="n">log_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Log</span><span class="p">()</span>
    <span class="n">square_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Square</span><span class="p">()</span>
    <span class="n">maxima</span> <span class="o">=</span> <span class="n">max_op</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
    <span class="n">logarithm</span> <span class="o">=</span> <span class="n">log_op</span><span class="p">(</span><span class="n">maxima</span><span class="p">)</span>
    <span class="n">squared_loss</span> <span class="o">=</span> <span class="n">square_op</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">target</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">full</span> <span class="k">else</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">pi</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">logarithm</span> <span class="o">+</span> <span class="n">squared_loss</span> <span class="o">/</span> <span class="n">maxima</span><span class="p">)</span> <span class="o">+</span> <span class="n">c</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="k">elif</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;sum&#39;</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">loss</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_hinge_embedding_loss</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">shape2</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">shape2</span> <span class="o">!=</span> <span class="n">shape</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="n">prim_name</span><span class="si">}</span><span class="s2">&#39; the input tensor and the labels must have the same shape.&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="hinge_embedding_loss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.hinge_embedding_loss.html#mindspore.ops.hinge_embedding_loss">[docs]</a><span class="k">def</span> <span class="nf">hinge_embedding_loss</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hinge Embedding Loss. Compute the output according to the input elements. Measures the loss given an input tensor x</span>
<span class="sd">    and a labels tensor y (containing 1 or -1).</span>
<span class="sd">    This is usually used for measuring the similarity between two inputs.</span>

<span class="sd">    The loss function for :math:`n`-th sample in the mini-batch is</span>

<span class="sd">    .. math::</span>
<span class="sd">        l_n = \begin{cases}</span>
<span class="sd">        x_n, &amp; \text{if}\; y_n = 1,\\</span>
<span class="sd">        \max \{0, \Delta - x_n\}, &amp; \text{if}\; y_n = -1,</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    and the total loss functions is</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = \begin{cases}</span>
<span class="sd">        \operatorname{mean}(L), &amp; \text{if reduction} = \text{&#39;mean&#39;;}\\</span>
<span class="sd">        \operatorname{sum}(L),  &amp; \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    where :math:`L = \{l_1,\dots,l_N\}^\top`.</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs (Tensor): Tensor of shape :math:`(*)` where :math:`*` means any number of dimensions.</span>
<span class="sd">        targets (Tensor): Same shape as the logits, contains -1 or 1.</span>
<span class="sd">        margin (float): Threshold defined by Hinge Embedding Loss :math:`margin`.</span>
<span class="sd">            Represented as :math:`\Delta` in the formula. Default: 1.0.</span>
<span class="sd">        reduction (str): Specify the computing method to be applied to the outputs: &#39;none&#39;, &#39;mean&#39;, or &#39;sum&#39;.</span>
<span class="sd">            Default: &#39;mean&#39;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or Tensor scalar, the computed loss depending on `reduction`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `inputs` is not a Tensor.</span>
<span class="sd">        TypeError: If `targets` is not a Tensor.</span>
<span class="sd">        TypeError: If `margin` is not a float.</span>
<span class="sd">        ValueError: If `targets` does not have the same shape as `inputs`.</span>
<span class="sd">        ValueError: If `reduction` is not one of &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.common.dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; arr1 = np.array([0.9, -1.2, 2, 0.8, 3.9, 2, 1, 0, -1]).reshape((3, 3))</span>
<span class="sd">        &gt;&gt;&gt; arr2 = np.array([1, 1, -1, 1, -1, 1, -1, 1, 1]).reshape((3, 3))</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(arr1, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(arr2, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; loss = ops.hinge_embedding_loss(logits, labels, margin=1.0, reduction=&#39;mean&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(loss)</span>
<span class="sd">        0.16666666</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_shape</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">_dtype</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">_t_shape</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">_check_hinge_embedding_loss</span><span class="p">(</span><span class="n">_shape</span><span class="p">,</span> <span class="n">_t_shape</span><span class="p">,</span> <span class="s1">&#39;HingeEmbeddingLoss&#39;</span><span class="p">)</span>
    <span class="n">min_val</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">_dtype</span><span class="p">)</span>
    <span class="n">pos_index</span> <span class="o">=</span> <span class="n">targets</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="n">neg_index</span> <span class="o">=</span> <span class="n">targets</span> <span class="o">&lt;</span> <span class="mi">0</span>
    <span class="n">pos</span> <span class="o">=</span> <span class="n">pos_index</span> <span class="o">*</span> <span class="n">inputs</span>
    <span class="n">neg</span> <span class="o">=</span> <span class="n">neg_index</span> <span class="o">*</span> <span class="n">inputs</span>
    <span class="n">margin_matrix</span> <span class="o">=</span> <span class="n">margin</span> <span class="o">*</span> <span class="n">neg_index</span>
    <span class="n">neg</span> <span class="o">=</span> <span class="n">margin_matrix</span> <span class="o">-</span> <span class="n">neg</span>
    <span class="n">neg</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">neg</span><span class="p">,</span> <span class="n">min_val</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">pos</span> <span class="o">+</span> <span class="n">neg</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="k">elif</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;sum&#39;</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">loss</span></div>


<div class="viewcode-block" id="ctc_greedy_decoder"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ctc_greedy_decoder.html#mindspore.ops.ctc_greedy_decoder">[docs]</a><span class="k">def</span> <span class="nf">ctc_greedy_decoder</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">merge_repeated</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs greedy decoding on the logits given in inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs (Tensor): The input Tensor must be a 3-D tensor whose shape is</span>
<span class="sd">            :math:`(max\_time, batch\_size, num\_classes)`. `num_classes` must be `num_labels + 1` classes,</span>
<span class="sd">            `num_labels` indicates the number of actual labels. Blank labels are reserved.</span>
<span class="sd">            Default blank label is `num_classes - 1`. Data type must be float32 or float64.</span>
<span class="sd">        sequence_length (Tensor): A tensor containing sequence lengths with the shape of :math:`(batch\_size, )`.</span>
<span class="sd">            The type must be int32. Each value in the tensor must be equal to or less than `max_time`.</span>
<span class="sd">        merge_repeated (bool): If true, merge repeated classes in output. Default: True.</span>

<span class="sd">    Returns:</span>
<span class="sd">        decoded_indices (Tensor), A tensor with shape of :math:`(total\_decoded\_outputs, 2)`.</span>
<span class="sd">        Data type is int64.</span>

<span class="sd">        decoded_values (Tensor), A tensor with shape of :math:`(total\_decoded\_outputs, )`,</span>
<span class="sd">        it stores the decoded classes. Data type is int64.</span>

<span class="sd">        decoded_shape (Tensor), A tensor with shape of :math:`(batch\_size, max\_decoded\_length)`.</span>
<span class="sd">        Data type is int64.</span>

<span class="sd">        log_probability (Tensor), A tensor with shape of :math:`(batch\_size, 1)`,</span>
<span class="sd">        containing sequence log-probability, has the same type as `inputs`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `merge_repeated` is not a bool.</span>
<span class="sd">        ValueError: If length of shape of `inputs` is not equal to 3.</span>
<span class="sd">        ValueError: If length of shape of `sequence_length` is not equal to 1.</span>
<span class="sd">        ValueError: If value in the `sequence_length` is larger than `max_time`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; inputs = Tensor(np.array([[[0.6, 0.4, 0.2], [0.8, 0.6, 0.3]],</span>
<span class="sd">        ...                           [[0.0, 0.6, 0.0], [0.5, 0.4, 0.5]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; sequence_length = Tensor(np.array([2, 2]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; decoded_indices, decoded_values, decoded_shape, log_probability = ops.ctc_greedy_decoder(inputs,</span>
<span class="sd">        ...                                                                                          sequence_length)</span>
<span class="sd">        &gt;&gt;&gt; print(decoded_indices)</span>
<span class="sd">        [[0 0]</span>
<span class="sd">         [0 1]</span>
<span class="sd">         [1 0]]</span>
<span class="sd">        &gt;&gt;&gt; print(decoded_values)</span>
<span class="sd">        [0 1 0]</span>
<span class="sd">        &gt;&gt;&gt; print(decoded_shape)</span>
<span class="sd">        [2 2]</span>
<span class="sd">        &gt;&gt;&gt; print(log_probability)</span>
<span class="sd">        [[-1.2]</span>
<span class="sd">         [-1.3]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_ctc_greedy_decoder</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">CTCGreedyDecoder</span><span class="p">)(</span><span class="n">merge_repeated</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_ctc_greedy_decoder</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">conv3d_transpose</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                     <span class="n">output_padding</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes a 3D transposed convolution, which is also known as a deconvolution</span>
<span class="sd">    (although it is not an actual deconvolution).</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs (Tensor): The gradients with respect to the output of the convolution.</span>
<span class="sd">           The shape conforms to the default.</span>
<span class="sd">           data_format :math:`(N, C_{in}, D_{out}, H_{out}, W_{out})`. Currently dout data type only supports float16</span>
<span class="sd">           and float32.</span>
<span class="sd">        weight (Tensor): Set size of kernel is :math:`(K_d, K_h, K_w)`, then the shape is</span>
<span class="sd">           :math:`(C_{in}, C_{out}//group, K_d, K_h, K_w)`. Where :math:`group` is the Args parameter,</span>
<span class="sd">           :math:`//` is the symbol for integer division.</span>
<span class="sd">           Currently weight data type only supports float16 and float32.</span>
<span class="sd">        pad_mode (str): Specifies padding mode. The optional values are</span>
<span class="sd">            &quot;same&quot;, &quot;valid&quot;, &quot;pad&quot;. Default: &quot;valid&quot;.</span>

<span class="sd">            - same: Adopts the way of completion. The depth, height and width of the output will be equal to</span>
<span class="sd">              the input `x` divided by stride. The padding will be evenly calculated in head and tail, top and bottom,</span>
<span class="sd">              left and right directions possibility.</span>
<span class="sd">              Otherwise, the last extra padding will be calculated from the tail, bottom and the right side.</span>
<span class="sd">              If this mode is set, `pad` must be 0.</span>

<span class="sd">            - valid: Adopts the way of discarding. The possible largest depth, height and width of output</span>
<span class="sd">              will be returned without padding. Extra pixels will be discarded. If this mode is set, `pad`</span>
<span class="sd">              and `output_padding` must be 0.</span>

<span class="sd">            - pad: Implicit paddings on both sides of the input in depth, height and width. The number of `pad` will</span>
<span class="sd">              be padded to the input Tensor borders. `pad` must be greater than or equal to 0.</span>

<span class="sd">        padding (Union(int, tuple[int])): The padding value to be filled. Default: 0. If `padding` is an integer, the</span>
<span class="sd">            paddings of head, tail, top, bottom, left and right are the same, equal to pad. If `padding` is a tuple of</span>
<span class="sd">            six integers, the padding of head, tail, top, bottom, left and right equal to padding[0], padding[1],</span>
<span class="sd">            padding[2], padding[3], padding[4] and padding[5] correspondingly.</span>
<span class="sd">        stride (Union(int, tuple[int])): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the depth, height and width of movement are both strides, or a tuple of three int numbers that</span>
<span class="sd">            represent depth, height and width of movement respectively. Default: 1.</span>
<span class="sd">        dilation (Union(int, tuple[int])): Specifies the space to use between kernel elements. Default: 1.</span>
<span class="sd">        group (int): Splits input into groups. Default: 1. Only 1 is currently supported.</span>
<span class="sd">        output_padding (Union(int, tuple[int])): Add extra size to each dimension of the output. Default: 0.</span>


<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the gradients with respect to the input of convolution 3D.</span>
<span class="sd">        Tensor of shape :math:`(N, C_{out}//group, D_{out}, H_{out}, W_{out})`,</span>
<span class="sd">        where :math:`group` is the Args parameter.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `group` is not an int.</span>
<span class="sd">        TypeError: If `stride`, `padding` , `dilation` or `output_padding` is neither an int not a tuple.</span>
<span class="sd">        ValueError: If the rank of `inputs`, `weight` is not equal to 5.</span>
<span class="sd">        ValueError: If `stride` or `dilation` is less than 1.</span>
<span class="sd">        ValueError: if inputs[1], weight[1] and weight[2:5] i.e. `in_channel`, `out_channel` and `kernel_size` is less</span>
<span class="sd">                    than 1.</span>
<span class="sd">        ValueError: If `padding` is less than 0.</span>
<span class="sd">        ValueError: If `pad_mode` is not one of &#39;same&#39;, &#39;valid&#39; nor &#39;pad&#39;.</span>
<span class="sd">        ValueError: If `padding` is a tuple whose length is not equal to 6.</span>
<span class="sd">        ValueError: If `pad_mode` is not equal to &#39;padding&#39; and `padding` is not equal to (0, 0, 0, 0, 0, 0).</span>
<span class="sd">        ValueError: If `data_format` is not &#39;NCDHW&#39;.</span>
<span class="sd">        TypeError: If data type of dout and weight is not float16.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; dout = Tensor(np.ones([32, 16, 10, 32, 32]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([16, 3, 4, 6, 2]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; output = conv3d_transpose(dout, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (32, 3, 13, 37, 33)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">5</span><span class="p">:</span>
        <span class="n">raise_value_error</span><span class="p">(</span><span class="s2">&quot;the rank of inputs tensor should be 5.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">5</span><span class="p">:</span>
        <span class="n">raise_value_error</span><span class="p">(</span><span class="s2">&quot;the rank of weight tensor should be 5.&quot;</span><span class="p">)</span>
    <span class="n">in_channel</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">out_channel</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">5</span><span class="p">]</span>
    <span class="n">_conv_3d_transpose</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">Conv3DTranspose</span><span class="p">)(</span><span class="n">in_channel</span><span class="p">,</span> <span class="n">out_channel</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span>
                                                                 <span class="n">padding</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="n">output_padding</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_conv_3d_transpose</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>


<div class="viewcode-block" id="conv2d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.conv2d.html#mindspore.ops.conv2d">[docs]</a><span class="k">def</span> <span class="nf">conv2d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a 2D convolution over an input tensor.</span>
<span class="sd">    The input tensor is typically of shape :math:`(N, C_{in}, H_{in}, W_{in})`,</span>
<span class="sd">    where :math:`N` is batch size, :math:`C` is channel number, :math:`H` is height, :math:`W` is width, :math:`X_i` is</span>
<span class="sd">    the :math:`i^{th}` input value and :math:`b_i` indicates the deviation value of the :math:`i^{th}` input value.</span>
<span class="sd">    For each batch of shape :math:`(C_{in}, H_{in}, W_{in})`, the formula is defined as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_j = \sum_{i=0}^{C_{in} - 1} ccor(W_{ij}, X_i) + b_j,</span>

<span class="sd">    where :math:`ccor` is the `cross-correlation &lt;https://en.wikipedia.org/wiki/Cross-correlation&gt;`_  operator,</span>
<span class="sd">    :math:`C_{in}` is the input channel number, :math:`j` ranges</span>
<span class="sd">    from :math:`0` to :math:`C_{out} - 1`, :math:`W_{ij}` corresponds to the :math:`i`-th channel of the :math:`j`-th</span>
<span class="sd">    filter and :math:`out_{j}` corresponds to the :math:`j`-th channel of the output. :math:`W_{ij}` is a slice</span>
<span class="sd">    of kernel and it has shape :math:`(\text{kernel_size[0]}, \text{kernel_size[1]})`, where :math:`\text{</span>
<span class="sd">    kernel_size[0]}` and :math:`\text{kernel_size[1]}` are the height and width of the convolution kernel.</span>
<span class="sd">    The full kernel has shape :math:`(C_{out}, C_{in} / \text{group}, \text{kernel_size[0]}, \text{kernel_size[1]})`,</span>
<span class="sd">    where `group` is the group number to split the input in the channel dimension.</span>

<span class="sd">    If the `pad_mode` is set to be &quot;valid&quot;, the output height and width will be :math:`\left \lfloor{</span>
<span class="sd">    1 + \frac{H_{in} + \text{padding[0]} + \text{padding[1]} - \text{kernel_size[0]} -</span>
<span class="sd">    (\text{kernel_size[0]} - 1) \times(\text{dilation[0]} - 1)} {\text { stride[0] }}} \right \rfloor` and</span>

<span class="sd">    :math:`\left \lfloor{1 + \frac{W_{in} + \text{padding[2]} + \text{padding[3]} - \text{kernel_size[1]} -</span>
<span class="sd">    (\text{kernel_size[1]} - 1) \times(\text{dilation[1]} - 1)} {\text { stride[1] }}} \right \rfloor` respectively.</span>

<span class="sd">    Where :math:`dilation` is Spacing between kernel elements, :math:`stride` is The step length of each step,</span>
<span class="sd">    :math:`padding` is zero-padding added to both sides of the input.</span>
<span class="sd">    For output height and width on other `pad_mode`, please refer to formula on `mindspore.nn.Conv2d</span>
<span class="sd">    &lt;https://www.mindspore.cn/docs/en/r2.0.0-alpha/api_python/nn/mindspore.nn.Conv2d.html&gt;`_.</span>

<span class="sd">    The first introduction can be found in paper `Gradient Based Learning Applied to Document Recognition</span>
<span class="sd">    &lt;http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf&gt;`_. More detailed introduction can be found here:</span>
<span class="sd">    `ConvNets &lt;http://cs231n.github.io/convolutional-networks/&gt;`_ .</span>

<span class="sd">    Note:</span>
<span class="sd">        On Ascend platform, only group convolution in depthwise convolution scenarios is supported.</span>
<span class="sd">        That is, when `group&gt;1`, condition `C_{in}` = `C_{out}` = `group` must be satisfied.</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs (Tensor): Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.</span>
<span class="sd">        weight (Tensor): Set size of kernel is :math:`(\text{kernel_size[0]}, \text{kernel_size[1]})`,</span>
<span class="sd">            then the shape is :math:`(C_{out}, C_{in}, \text{kernel_size[0]}, \text{kernel_size[1]})`.</span>
<span class="sd">        pad_mode (str, optional): Specifies padding mode. The optional values are</span>
<span class="sd">            &quot;same&quot;, &quot;valid&quot; and &quot;pad&quot;. Default: &quot;valid&quot;.</span>

<span class="sd">            - same: Adopts the way of completion. The height and width of the output will be equal to</span>
<span class="sd">              the input `x` divided by stride. The padding will be evenly calculated in top and bottom,</span>
<span class="sd">              left and right possiblily. Otherwise, the last extra padding will be calculated from the bottom</span>
<span class="sd">              and the right side. If this mode is set, `padding` must be 0.</span>

<span class="sd">            - valid: Adopts the way of discarding. The possible largest height and width of output will be returned</span>
<span class="sd">              without padding. Extra pixels will be discarded. If this mode is set, `padding` must be 0.</span>

<span class="sd">            - pad: Implicit paddings on both sides of the input `x`. The number of `padding` will be padded to the input</span>
<span class="sd">              Tensor borders. `padding` must be greater than or equal to 0.</span>
<span class="sd">        padding (Union(int, tuple[int]), optional): Implicit paddings on both sides of the input `x`.</span>
<span class="sd">            If `padding` is one integer, the paddings of top, bottom, left and right are the same, equal to padding.</span>
<span class="sd">            If `padding` is a tuple with four integers, the paddings of top, bottom, left and right will be equal</span>
<span class="sd">            to padding[0], padding[1], padding[2], and padding[3] accordingly. Default: 0.</span>
<span class="sd">        stride (Union(int, tuple[int]), optional): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the height and width of movement are both strides, or a tuple of two int numbers that</span>
<span class="sd">            represent height and width of movement respectively. Default: 1.</span>
<span class="sd">        dilation (Union(int, tuple[int]), optional): The data type is int or a tuple of 2 integers.</span>
<span class="sd">            Specifies the dilation rate to use for dilated convolution. If set to be :math:`k &gt; 1`, there will</span>
<span class="sd">            be :math:`k - 1` pixels skipped for each sampling location. Its value must</span>
<span class="sd">            be greater than or equal to 1 and bounded by the height and width of the input `x`. Default: 1.</span>
<span class="sd">        group (int, optional): Splits inputs into groups. Default: 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the value that applied 2D convolution. The shape is :math:`(N, C_{out}, H_{out}, W_{out})`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `stride`, `padding` or `dilation` is neither an int nor a tuple.</span>
<span class="sd">        TypeError: If `out_channel` or `group` is not an int.</span>
<span class="sd">        ValueError: If `stride` or `dilation` is less than 1.</span>
<span class="sd">        ValueError: If `pad_mode` is not one of &#39;same&#39;, &#39;valid&#39; or &#39;pad&#39;.</span>
<span class="sd">        ValueError: If `padding` is a tuple whose length is not equal to 4.</span>
<span class="sd">        ValueError: If `pad_mode` it not equal to &#39;pad&#39; and `padding` is not equal to (0, 0, 0, 0).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([10, 32, 32, 32]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([32, 32, 3, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.conv2d(x, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (10, 32, 30, 30)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">weight_shape</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">out_channel</span> <span class="o">=</span> <span class="n">weight_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">weight_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">4</span><span class="p">]</span>
    <span class="n">conv</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">)(</span><span class="n">out_channel</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="s2">&quot;NCHW&quot;</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">conv</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span></div>


<span class="k">def</span> <span class="nf">hardsigmoid</span><span class="p">(</span><span class="n">input_x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hard sigmoid activation function.</span>

<span class="sd">    Applies hard sigmoid activation element-wise. The input is a Tensor with any valid shape.</span>

<span class="sd">    Hard sigmoid is defined as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{hsigmoid}(x_{i}) = max(0, min(1, \frac{x_{i} + 3}{6})),</span>

<span class="sd">    where :math:`x_i` is an element of the input Tensor.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor of shape :math:`(*)`, where :math:`*` means any number of</span>
<span class="sd">          dimensions, with float16, float32 or float64 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        A Tensor whose dtype and shape are the same as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input_x` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([ -3.5,  0,  4.3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = F.hardsigmoid(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.  0.5 1. ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">hardsigmoid_</span> <span class="o">=</span> <span class="n">NN_OPS</span><span class="o">.</span><span class="n">HSigmoid</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">hardsigmoid_</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">adaptive_avg_pool1d</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a 1D adaptive average pooling over an input Tensor which can be regarded as a composition of 1D input</span>
<span class="sd">    planes.</span>

<span class="sd">    Typically, the input is of shape :math:`(N_{in}, C_{in}, L_{in})`, adaptive_avg_pool1d outputs regional average</span>
<span class="sd">    in the :math:`L_{in}`-dimension. The output is of shape :math:`(N_{in}, C_{in}, L_{out})`, where :math:`L_{out}`</span>
<span class="sd">    is defined by `output_size`.</span>

<span class="sd">    Note:</span>
<span class="sd">        :math:`L_{in}` must be divisible by `output_size`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Tensor of shape :math:`(N, C_{in}, L_{in})`, with float16 or float32 data type.</span>
<span class="sd">        output_size (int): the target output size :math:`L_{out}`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor of shape :math:`(N, C_{in}, L_{out})`, has the same type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `output_size` is not an int.</span>
<span class="sd">        TypeError: If `input_x` is neither float16 nor float32.</span>
<span class="sd">        ValueError: If `output_size` is less than 1.</span>
<span class="sd">        ValueError: If length of shape of `input_x` is not equal to 3.</span>
<span class="sd">        ValueError: If the last dimension of `input_x` is smaller than `output_size`.</span>
<span class="sd">        ValueError: If the last dimension of `input_x` is not divisible by `output_size`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.random.randint(0, 10, [1, 3, 6]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.adaptive_avg_pool1d(input_x, output_size=2)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 3, 2)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For adaptive_avg_pool1d, the input input_x must be tensor&quot;</span><span class="p">)</span>

    <span class="n">x_in_shape</span> <span class="o">=</span> <span class="n">input_x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">x_dtype</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">)()(</span><span class="n">input_x</span><span class="p">)</span>

    <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s2">&quot;output_size&quot;</span><span class="p">,</span> <span class="s1">&#39;adaptive_avg_pool1d&#39;</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;output_size&#39;</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="s1">&#39;adaptive_avg_pool1d&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_in_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For adaptive_avg_pool1d input must have 3 dim, but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_in_shape</span><span class="p">)))</span>
    <span class="k">if</span> <span class="n">x_in_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">output_size</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For adaptive_avg_pool1d input&#39;s last dimension must be greater or equal to &quot;</span>
                         <span class="s2">&quot;output size </span><span class="si">{}</span><span class="s2">, but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">x_in_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
    <span class="k">if</span> <span class="n">x_in_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">%</span> <span class="n">output_size</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For adaptive_avg_pool1d input&#39;s last dimension must be divisible by &quot;</span>
                         <span class="s2">&quot;output size </span><span class="si">{}</span><span class="s2">, but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">x_in_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
    <span class="k">if</span> <span class="n">x_dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For adaptive_avg_pool1d, the input_x dtype must be float16 or float32, &quot;</span>
                        <span class="s2">&quot;but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x_dtype</span><span class="p">))</span>

    <span class="n">expand_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">)()</span>
    <span class="n">squeeze_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Squeeze</span><span class="p">)(</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">width</span> <span class="o">=</span> <span class="n">x_in_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">width</span> <span class="o">//</span> <span class="n">output_size</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">width</span> <span class="o">-</span> <span class="p">(</span><span class="n">output_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">width</span> <span class="o">//</span> <span class="n">output_size</span><span class="p">)</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">)</span>

    <span class="n">avg_pool_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">AvgPool</span><span class="p">)(</span><span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="n">stride</span><span class="p">)</span>

    <span class="n">input_x</span> <span class="o">=</span> <span class="n">expand_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">input_x</span> <span class="o">=</span> <span class="n">avg_pool_</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
    <span class="n">input_x</span> <span class="o">=</span> <span class="n">squeeze_</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">input_x</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_adaptive_max_pool1d_output_size</span><span class="p">(</span><span class="n">output_size</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check the output_size value in adaptive_max_pool1d op.&quot;&quot;&quot;</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s2">&quot;output_size&quot;</span><span class="p">,</span> <span class="s1">&#39;adaptive_max_pool1d&#39;</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;output_size&#39;</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="s1">&#39;adaptive_max_pool1d&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">adaptive_max_pool1d</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a 1D adaptive maximum pooling over an input Tensor which can be regarded as</span>
<span class="sd">    a composition of 1D input planes.</span>

<span class="sd">    Typically, the input is of shape :math:`(N_{in}, C_{in}, L_{in})`,</span>
<span class="sd">    adaptive_max_pool1d outputs regional maximum in the :math:`L_{in}`-dimension. The output is of</span>
<span class="sd">    shape :math:`(N_{in}, C_{in}, L_{out})`, where :math:`L_{out}` is defined by `output_size`.</span>

<span class="sd">    Note:</span>
<span class="sd">        :math:`L_{in}` must be divisible by `output_size`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Tensor of shape :math:`(N, C_{in}, L_{in})`, with float16 or float32 data type.</span>
<span class="sd">        output_size (int): the target output size :math:`L_{out}`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor of shape :math:`(N, C_{in}, L_{out})`, has the same type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `output_size` is not an int.</span>
<span class="sd">        ValueError: If `output_size` is less than 1.</span>
<span class="sd">        ValueError: If the last dimension of `input_x` is smaller than `output_size`.</span>
<span class="sd">        ValueError: If the last dimension of `input_x` is not divisible by `output_size`.</span>
<span class="sd">        ValueError: If length of shape of `input_x` is not equal to 3.</span>


<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.random.randint(0, 10, [1, 3, 6]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.adaptive_max_pool1d(input_x, output_size=2)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 3, 2)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For adaptive_max_pool1d, the input input_x must be tensor&quot;</span><span class="p">)</span>

    <span class="n">_check_adaptive_max_pool1d_output_size</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span>

    <span class="n">x_in_shape</span> <span class="o">=</span> <span class="n">input_x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">x_dtype</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">)()(</span><span class="n">input_x</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_in_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For adaptive_max_pool1d input must have 3 dim, but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_in_shape</span><span class="p">)))</span>
    <span class="k">if</span> <span class="n">x_in_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">output_size</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For adaptive_max_pool1d input&#39;s last dimension must be greater or equal to &quot;</span>
                         <span class="s2">&quot;output size </span><span class="si">{}</span><span class="s2">, but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">x_in_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
    <span class="k">if</span> <span class="n">x_in_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">%</span> <span class="n">output_size</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For adaptive_max_pool1d input&#39;s last dimension must be divisible by &quot;</span>
                         <span class="s2">&quot;output size </span><span class="si">{}</span><span class="s2">, but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">x_in_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
    <span class="k">if</span> <span class="n">x_dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For adaptive_max_pool1d, the input_x dtype must be float16 or float32, &quot;</span>
                        <span class="s2">&quot;but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x_dtype</span><span class="p">))</span>

    <span class="n">expand_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">)()</span>
    <span class="n">squeeze_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Squeeze</span><span class="p">)(</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">width</span> <span class="o">=</span> <span class="n">x_in_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">width</span> <span class="o">//</span> <span class="n">output_size</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">width</span> <span class="o">-</span> <span class="p">(</span><span class="n">output_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride</span>

    <span class="n">stride</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">width</span> <span class="o">//</span> <span class="n">output_size</span><span class="p">)</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">)</span>

    <span class="n">max_pool_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">MaxPool</span><span class="p">)(</span><span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="n">stride</span><span class="p">)</span>
    <span class="n">input_x</span> <span class="o">=</span> <span class="n">expand_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">input_x</span> <span class="o">=</span> <span class="n">max_pool_</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
    <span class="n">input_x</span> <span class="o">=</span> <span class="n">squeeze_</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">input_x</span>


<div class="viewcode-block" id="batch_norm"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.batch_norm.html#mindspore.ops.batch_norm">[docs]</a><span class="k">def</span> <span class="nf">batch_norm</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">running_mean</span><span class="p">,</span> <span class="n">running_var</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Batch Normalization for input data and updated parameters.</span>

<span class="sd">    Batch Normalization is widely used in convolutional neural networks. This operation</span>
<span class="sd">    applies Batch Normalization over inputs to avoid internal covariate shift as described</span>
<span class="sd">    in the paper `Batch Normalization: Accelerating Deep Network Training by Reducing Internal</span>
<span class="sd">    Covariate Shift &lt;https://arxiv.org/abs/1502.03167&gt;`_. It rescales and recenters the</span>
<span class="sd">    features using a mini-batch of data and the learned parameters can be described</span>
<span class="sd">    in the following formula,</span>

<span class="sd">    .. math::</span>

<span class="sd">        y = \frac{x - mean}{\sqrt{variance + \epsilon}} * \gamma + \beta</span>

<span class="sd">    where :math:`\gamma` is `weight`, :math:`\beta` is `bias`, :math:`\epsilon` is `eps`, :math:`mean` is the</span>
<span class="sd">    mean of `input_x`, :math:`variance` is the variance of `input_x`.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - If this operation is used for inferring and output &quot;reserve_space_1&quot; and &quot;reserve_space_2&quot; are usable,</span>
<span class="sd">            then &quot;reserve_space_1&quot; and &quot;reserve_space_2&quot; have the same value as &quot;mean&quot; and &quot;variance&quot; respectively.</span>
<span class="sd">        - For Ascend 310, the result accuracy fails to reach 1‰ due to the square root instruction.</span>

<span class="sd">    Note:</span>
<span class="sd">        - If `training` is `False`, `weight`, `bias`, `running_mean` and `running_var` are Tensors.</span>
<span class="sd">        - If `training` is `True`, `weight`, `bias`, `running_mean` and `running_var` are Parameters.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Tensor of shape :math:`(N, C)`, with float16 or float32 data type.</span>
<span class="sd">        running_mean (Union[Tensor, Parameter]): The shape :math:`(C,)`, has the same data type with `weight`.</span>
<span class="sd">        running_var (Union[Tensor, Parameter]): The shape :math:`(C,)`, has the same data type with `weight`.</span>
<span class="sd">        weight (Union[Tensor, Parameter]): The shape :math:`(C,)`, with float16 or float32 data type.</span>
<span class="sd">        bias (Union[Tensor, Parameter]): The shape :math:`(C,)`, has the same data type with `weight`.</span>
<span class="sd">        training (bool): If `training` is `True`, `mean` and `variance` are computed during training.</span>
<span class="sd">            If `training` is `False`, they&#39;re loaded from checkpoint during inference. Default: False.</span>
<span class="sd">        momentum (float): The hyper parameter to compute moving average for `running_mean` and `running_var`</span>
<span class="sd">            (e.g. :math:`new\_running\_mean = (1 - momentum) * running\_mean + momentum * current\_mean`).</span>
<span class="sd">            Momentum value must be `[0, 1]`. Default: 0.1.</span>
<span class="sd">        eps (float): A small value added for numerical stability. Default: 1e-5.</span>

<span class="sd">    Returns:</span>
<span class="sd">        output_x (Tensor) - The same type and shape as the `input_x`. The shape is :math:`(N, C)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `training` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `eps` or `momentum` is not float.</span>
<span class="sd">        TypeError: If `input_x`, `weight`, `bias`, `running_mean` or `running_var` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input_x`, `weight` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones([2, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; running_mean = Tensor(np.ones([2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; running_var = Tensor(np.ones([2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; bias = Tensor(np.ones([2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.batch_norm(input_x, running_mean, running_var, weight, bias)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 1.]</span>
<span class="sd">         [1. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch_norm_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">)(</span><span class="n">is_training</span><span class="o">=</span><span class="n">training</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">batch_norm_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">running_mean</span><span class="p">,</span> <span class="n">running_var</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></div>


<div class="viewcode-block" id="bias_add"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bias_add.html#mindspore.ops.bias_add">[docs]</a><span class="k">def</span> <span class="nf">bias_add</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the sum of the `input_x` and the `bias` Tensor. Before adding, the `bias` Tensor will be broadcasted to be</span>
<span class="sd">    consistent with the shape of the `input_x` Tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The input tensor. The shape can be 2-5 dimensions.</span>
<span class="sd">        bias (Tensor): The bias tensor, with shape :math:`(C)`. C must be the same as channel dimension C of `input_x`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same shape and data type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` or `bias` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input_x` or `bias` is inconsistent.</span>
<span class="sd">        TypeError: If dimension of `input_x` is not in the range [2, 5].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.arange(6).reshape((2, 3)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; bias = Tensor(np.random.random(3).reshape((3)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bias_add(input_x, bias)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 3)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">bias_add_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">BiasAdd</span><span class="p">)(</span><span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">bias_add_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span></div>


<div class="viewcode-block" id="binary_cross_entropy"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.binary_cross_entropy.html#mindspore.ops.binary_cross_entropy">[docs]</a><span class="k">def</span> <span class="nf">binary_cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the binary cross entropy between predictive value `logits` and target value `labels`.</span>

<span class="sd">    Set `logits` as :math:`x`, `labels` as :math:`y`, output as :math:`\ell(x, y)`, the</span>
<span class="sd">    weight of nth batch of binary cross entropy is :math:`w_n`.</span>
<span class="sd">    Let,</span>

<span class="sd">    .. math::</span>
<span class="sd">        L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">        l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right]</span>

<span class="sd">    In which, :math:`L` indicates the loss of all `batch_size`, :math:`l` indicates the loss of one `batch_size`,</span>
<span class="sd">    and :math:`n` indicates one `batch_size` in the :math:`1-N` range. Then,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = \begin{cases}</span>
<span class="sd">        L, &amp; \text{if reduction} = \text{&#39;none&#39;;}\\</span>
<span class="sd">        \operatorname{mean}(L), &amp; \text{if reduction} = \text{&#39;mean&#39;;}\\</span>
<span class="sd">        \operatorname{sum}(L),  &amp; \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - The value of `logits` must range from `0` to `l`.</span>
<span class="sd">        - The value of `labels` must be `0` or `l`.</span>

<span class="sd">    Args:</span>
<span class="sd">        logits (Tensor): The predictive value whose data type must be float16 or float32.</span>
<span class="sd">            The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        labels (Tensor): The target value which has the same shape and data type as `logits`.</span>
<span class="sd">        weight (Tensor, optional): A rescaling weight applied to the loss of each batch element.</span>
<span class="sd">            Its shape must be able to broadcast to that of `logits` and `labels`.</span>
<span class="sd">            And it must have the same shape and data type as `logits`. Default: None.</span>
<span class="sd">        reduction (str): Specifies the reduction to be applied to the output.</span>
<span class="sd">            Its value must be one of &#39;none&#39;, &#39;mean&#39; or &#39;sum&#39;, not case-sensitive. Default: &#39;mean&#39;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or Scalar. Returns Tensor that has the same dtype and shape as `logits` if `reduction` is &#39;none&#39;.</span>
<span class="sd">        Otherwise, returns a scalar Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `logits`, `labels` or `weight` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `logits`, `labels` or `weight` (if given) is neither float16 nor float32.</span>
<span class="sd">        ValueError: If `reduction` is not one of &#39;none&#39;, &#39;mean&#39; or &#39;sum&#39;.</span>
<span class="sd">        ValueError: If shape of `labels` is not the same as `logits` or `weight` (if given).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([0.2, 0.7, 0.1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([0., 1., 0.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.array([1, 2, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.binary_cross_entropy(logits, labels, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.38240486</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">binary_cross_entropy_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">BinaryCrossEntropy</span><span class="p">)(</span><span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">binary_cross_entropy_op</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span></div>


<div class="viewcode-block" id="conv3d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.conv3d.html#mindspore.ops.conv3d">[docs]</a><span class="k">def</span> <span class="nf">conv3d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a 3D convolution over an input tensor. The input tensor is typically of shape</span>
<span class="sd">    :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})` and output shape</span>
<span class="sd">    :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})`. Where :math:`N` is batch size, :math:`C` is channel number,</span>
<span class="sd">    :math:`D` is depth, :math:`H` is height, :math:`W` is width.</span>
<span class="sd">    the formula is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \operatorname{out}\left(N_{i}, C_{\text {out}_j}\right)=\operatorname{bias}\left(C_{\text {out}_j}\right)+</span>
<span class="sd">        \sum_{k=0}^{C_{in}-1} ccor(\text {weight}\left(C_{\text {out}_j}, k\right),</span>
<span class="sd">        \operatorname{input}\left(N_{i}, k\right))</span>

<span class="sd">    where :math:`k` is kernel,</span>
<span class="sd">    :math:`ccor` is the `cross-correlation &lt;https://en.wikipedia.org/wiki/Cross-correlation&gt;`_ ,</span>
<span class="sd">    :math:`C_{in}` is the channel number of the input, :math:`out_{j}` corresponds to the jth channel of</span>
<span class="sd">    the output and :math:`j` is in the range of :math:`[0，C_{out}-1]`. :math:`\text{weight}(C_{\text{out}_j}, k)`</span>
<span class="sd">    is a convolution kernel slice with shape</span>
<span class="sd">    :math:`(\text{kernel_size[0]}, \text{kernel_size[1]}, \text{kernel_size[2]})`,</span>
<span class="sd">    where :math:`\text{kernel_size[0]}`, :math:`\text{kernel_size[1]}` and :math:`\text{kernel_size[2]}` are</span>
<span class="sd">    the depth, height and width of the convolution kernel respectively. :math:`\text{bias}` is the bias parameter</span>
<span class="sd">    and :math:`\text{X}` is the input tensor.</span>
<span class="sd">    The shape of full convolution kernel is</span>
<span class="sd">    :math:`(C_{out}, C_{in} / \text{group}, \text{kernel_size[0]}, \text{kernel_size[1]}, \text{kernel_size[2]})`,</span>
<span class="sd">    where `group` is the number of groups to split the input `x` in the channel dimension.</span>

<span class="sd">    For more details, please refers to the paper `Gradient Based Learning Applied to Document</span>
<span class="sd">    Recognition &lt;http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf&gt;`_ .</span>

<span class="sd">    Note:</span>
<span class="sd">        On Ascend platform, only group convolution in depthwise convolution scenarios is supported.</span>
<span class="sd">        That is, when `group&gt;1`, condition `C_{in}` = `C_{out}` = `group` must be satisfied.</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs (Tensor): Tensor of shape :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})`.</span>
<span class="sd">        weight (Tensor): Set size of kernel is :math:`(\text{kernel_size[0]}, \text{kernel_size[1]},</span>
<span class="sd">            \text{kernel_size[2]})`, then the shape is :math:`(C_{out}, C_{in}, \text{kernel_size[0]},</span>
<span class="sd">            \text{kernel_size[1]}, \text{kernel_size[1]})`.</span>
<span class="sd">        pad_mode (str, optional): Specifies padding mode. The optional values are</span>
<span class="sd">            &quot;same&quot;, &quot;valid&quot; and &quot;pad&quot;. Default: &quot;valid&quot;.</span>

<span class="sd">            - same: Adopts the way of completion. The depth, height and width of the output will be equal to</span>
<span class="sd">              the input `x` divided by stride. The padding will be evenly calculated in head and tail, top and bottom,</span>
<span class="sd">              left and right directions possiblily.</span>
<span class="sd">              Otherwise, the last extra padding will be calculated from the tail, bottom and the right side.</span>
<span class="sd">              If this mode is set, `pad` must be 0.</span>

<span class="sd">            - valid: Adopts the way of discarding. The possible largest depth, height and width of output</span>
<span class="sd">              will be returned without padding. Extra pixels will be discarded. If this mode is set, `pad`</span>
<span class="sd">              must be 0.</span>

<span class="sd">            - pad: Implicit paddings on both sides of the input in depth, height and width. The number of `pad` will</span>
<span class="sd">              be padded to the input Tensor borders. `pad` must be greater than or equal to 0.</span>

<span class="sd">        padding (Union[int, tuple[int]], optional): The pad value to be filled. Default: 0. If `pad` is an integer,</span>
<span class="sd">            the paddings of head, tail, top, bottom, left and right are the same, equal to pad.</span>
<span class="sd">            If `pad` is a tuple of six integers, the padding of head, tail, top, bottom,</span>
<span class="sd">            left and right equal to pad[0], pad[1], pad[2], pad[3], pad[4] and pad[5] correspondingly.</span>
<span class="sd">        stride (Union[int, tuple[int]], optional): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the height and width of movement are both strides, or a tuple of two int numbers that</span>
<span class="sd">            represent height and width of movement respectively. Default: 1.</span>
<span class="sd">        dilation (Union[int, tuple[int]], optional): The data type is int or a tuple of 3 integers</span>
<span class="sd">            :math:`(dilation_d, dilation_h, dilation_w)`. Currently, dilation on depth only supports the case of 1</span>
<span class="sd">            on Ascend backend. Specifies the dilation rate to use for dilated convolution. If set :math:`k &gt; 1`,</span>
<span class="sd">            there will be :math:`k - 1` pixels skipped for each sampling location.</span>
<span class="sd">            Its value must be greater than or equal to 1 and bounded by the height and width of the input. Default: 1.</span>
<span class="sd">        group (int, optional): Splits filter into groups. Default: 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the value that applied 3D convolution. The shape is :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})`.</span>

<span class="sd">        `pad_mode` is &#39;same&#39;:</span>

<span class="sd">        .. math::</span>
<span class="sd">            \begin{array}{ll} \\</span>
<span class="sd">                D_{out} ＝ \left \lceil{\frac{D_{in}}{\text{stride[0]}}} \right \rceil \\</span>
<span class="sd">                H_{out} ＝ \left \lceil{\frac{H_{in}}{\text{stride[1]}}} \right \rceil \\</span>
<span class="sd">                W_{out} ＝ \left \lceil{\frac{W_{in}}{\text{stride[2]}}} \right \rceil \\</span>
<span class="sd">            \end{array}</span>

<span class="sd">        `pad_mode` is &#39;valid&#39;:</span>

<span class="sd">        .. math::</span>
<span class="sd">            \begin{array}{ll} \\</span>
<span class="sd">                D_{out} ＝ \left \lfloor{\frac{D_{in} - \text{dilation[0]} \times (\text{kernel_size[0]} - 1) }</span>
<span class="sd">                {\text{stride[0]}} + 1} \right \rfloor \\</span>
<span class="sd">                H_{out} ＝ \left \lfloor{\frac{H_{in} - \text{dilation[1]} \times (\text{kernel_size[1]} - 1) }</span>
<span class="sd">                {\text{stride[1]}} + 1} \right \rfloor \\</span>
<span class="sd">                W_{out} ＝ \left \lfloor{\frac{W_{in} - \text{dilation[2]} \times (\text{kernel_size[2]} - 1) }</span>
<span class="sd">                {\text{stride[2]}} + 1} \right \rfloor \\</span>
<span class="sd">            \end{array}</span>

<span class="sd">        `pad_mode` is &#39;pad&#39;:</span>

<span class="sd">        .. math::</span>
<span class="sd">            \begin{array}{ll} \\</span>
<span class="sd">                D_{out} ＝ \left \lfloor{\frac{D_{in} + padding[0] + padding[1] - (\text{dilation[0]} - 1) \times</span>
<span class="sd">                \text{kernel_size[0]} - 1 }{\text{stride[0]}} + 1} \right \rfloor \\</span>
<span class="sd">                H_{out} ＝ \left \lfloor{\frac{H_{in} + padding[2] + padding[3] - (\text{dilation[1]} - 1) \times</span>
<span class="sd">                \text{kernel_size[1]} - 1 }{\text{stride[1]}} + 1} \right \rfloor \\</span>
<span class="sd">                W_{out} ＝ \left \lfloor{\frac{W_{in} + padding[4] + padding[5] - (\text{dilation[2]} - 1) \times</span>
<span class="sd">                \text{kernel_size[2]} - 1 }{\text{stride[2]}} + 1} \right \rfloor \\</span>
<span class="sd">            \end{array}</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `out_channel` or `group` is not an int.</span>
<span class="sd">        TypeError: If `stride`, `padding` or `dilation` is neither an int nor a tuple.</span>
<span class="sd">        ValueError: If `stride` or `dilation` is less than 1.</span>
<span class="sd">        ValueError: If `pad_mode` is not one of &#39;same&#39;, &#39;valid&#39; or &#39;pad&#39;.</span>
<span class="sd">        ValueError: If `padding` is a tuple whose length is not equal to 4.</span>
<span class="sd">        ValueError: If `pad_mode` is not equal to &#39;pad&#39; and `pad` is not equal to (0, 0, 0, 0, 0, 0).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([16, 3, 10, 32, 32]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([32, 3, 4, 3, 3]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.conv3d(x, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (16, 32, 7, 30, 30)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">weight_shape</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">out_channel</span> <span class="o">=</span> <span class="n">weight_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">weight_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">5</span><span class="p">]</span>
    <span class="n">conv</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Conv3D</span><span class="p">)(</span><span class="n">out_channel</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="s2">&quot;NCDHW&quot;</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">conv</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_positive_int</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="n">arg_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prim_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="n">arg_name</span><span class="o">=</span><span class="n">arg_name</span><span class="p">,</span> <span class="n">prim_name</span><span class="o">=</span><span class="n">prim_name</span><span class="p">)</span>


<div class="viewcode-block" id="pixel_shuffle"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.pixel_shuffle.html#mindspore.ops.pixel_shuffle">[docs]</a><span class="k">def</span> <span class="nf">pixel_shuffle</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">upscale_factor</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a pixel_shuffle operation over an input signal composed of several input planes. This is useful for</span>
<span class="sd">    implementiong efficient sub-pixel convolution with a stride of :math:`1/r`. For more details, refer to</span>
<span class="sd">    `Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network</span>
<span class="sd">    &lt;https://arxiv.org/abs/1609.05158&gt;`_ .</span>

<span class="sd">    Typically, the `x` is of shape :math:`(*, C \times r^2, H, W)` , and the output is of shape</span>
<span class="sd">    :math:`(*, C, H \times r, W \times r)`, where `r` is an upscale factor and `*` is zero or more batch dimensions.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Tensor of shape :math:`(*, C \times r^2, H, W)` . The dimension of `x` is larger than 2, and the</span>
<span class="sd">            length of third to last dimension can be divisible by `upscale_factor` squared.</span>
<span class="sd">        upscale_factor (int):  factor to increase spatial resolution by, and is a positive integer.</span>

<span class="sd">    Returns:</span>
<span class="sd">        - **output** (Tensor) - Tensor of shape :math:`(*, C, H \times r, W \times r)` .</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If `upscale_factor` is not a positive integer.</span>
<span class="sd">        ValueError: If the length of third to last dimension is not divisible by `upscale_factor` squared.</span>
<span class="sd">        TypeError: If the dimension of `x` is less than 3.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = np.arange(3 * 2 * 9 * 4 * 4).reshape((3, 2, 9, 4, 4))</span>
<span class="sd">        &gt;&gt;&gt; input_x = mindspore.Tensor(input_x, mindspore.dtype.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.pixel_shuffle(input_x, 3)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (3, 2, 1, 12, 12)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_positive_int</span><span class="p">(</span><span class="n">upscale_factor</span><span class="p">,</span> <span class="s2">&quot;upscale_factor&quot;</span><span class="p">)</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">length</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">length</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For pixel_shuffle, the dimension of `x` should be larger than 2, but got </span><span class="si">{</span><span class="n">length</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">pre</span> <span class="o">=</span> <span class="n">idx</span><span class="p">[:</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span>
    <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">idx</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">:]</span>
    <span class="k">if</span> <span class="n">c</span> <span class="o">%</span> <span class="n">upscale_factor</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;pixel_shuffle&#39;, the length of third to last dimension is not divisible&quot;</span>
                         <span class="s2">&quot;by `upscale_factor` squared.&quot;</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">c</span> <span class="o">//</span> <span class="n">upscale_factor</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="n">input_perm</span> <span class="o">=</span> <span class="p">(</span><span class="n">pre</span> <span class="o">+</span> <span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">upscale_factor</span><span class="p">,</span> <span class="n">upscale_factor</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">))</span>
    <span class="n">reshape</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">input_perm</span><span class="p">)</span>
    <span class="n">input_perm</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">length</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)]</span>
    <span class="n">input_perm</span> <span class="o">=</span> <span class="n">input_perm</span> <span class="o">+</span> <span class="p">[</span><span class="n">length</span><span class="p">,</span> <span class="n">length</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span> <span class="n">length</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">input_perm</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">input_perm</span><span class="p">)</span>
    <span class="n">transpose</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Transpose</span><span class="p">()</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">input_perm</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">pre</span> <span class="o">+</span> <span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">upscale_factor</span> <span class="o">*</span> <span class="n">h</span><span class="p">,</span> <span class="n">upscale_factor</span> <span class="o">*</span> <span class="n">w</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">x</span></div>


<div class="viewcode-block" id="pixel_unshuffle"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.pixel_unshuffle.html#mindspore.ops.pixel_unshuffle">[docs]</a><span class="k">def</span> <span class="nf">pixel_unshuffle</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">downscale_factor</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a pixel_unshuffle operation over an input signal composed of several input planes. For more details, refer</span>
<span class="sd">    to `Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network</span>
<span class="sd">    &lt;https://arxiv.org/abs/1609.05158&gt;`_ .</span>

<span class="sd">    Typically, the input is of shape :math:`(*, C, H \times r, W \times r)` , and the output is of shape</span>
<span class="sd">    :math:`(*, C \times r^2, H, W)` , where `r` is a downscale factor and `*` is zero or more batch dimensions.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Tensor of shape :math:`(*, C, H \times r, W \times r)` . The dimension of `x` is larger than 2,</span>
<span class="sd">            and the length of second to last dimension or last dimension can be divisible by `downscale_factor` .</span>
<span class="sd">        downscale_factor (int): factor to decrease spatial resolution by, and is a positive integer.</span>

<span class="sd">    Returns:</span>
<span class="sd">        - **output** (Tensor) - Tensor of shape :math:`(*, C \times r^2, H, W)` .</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If `downscale_factor` is not a positive integer.</span>
<span class="sd">        ValueError: If the length of second to last dimension or last dimension is not divisible by `downscale_factor` .</span>
<span class="sd">        TypeError: If the dimension of `x` is less than 3.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = np.arange(12 * 12).reshape((1, 1, 12, 12))</span>
<span class="sd">        &gt;&gt;&gt; input_x = mindspore.Tensor(input_x, mindspore.dtype.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.pixel_unshuffle(input_x, 3)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 9, 4, 4)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_positive_int</span><span class="p">(</span><span class="n">downscale_factor</span><span class="p">,</span> <span class="s2">&quot;downscale_factor&quot;</span><span class="p">)</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">length</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">length</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For pixel_unshuffle, the dimension of `x` should be larger than 2, but got </span><span class="si">{</span><span class="n">length</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">pre</span> <span class="o">=</span> <span class="n">idx</span><span class="p">[:</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span>
    <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">idx</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">:]</span>
    <span class="k">if</span> <span class="n">h</span> <span class="o">%</span> <span class="n">downscale_factor</span> <span class="o">!=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">w</span> <span class="o">%</span> <span class="n">downscale_factor</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;pixel_unshuffle&#39;, the length of second to last 2 dimension should be divisible &quot;</span>
                         <span class="s2">&quot;by downscale_factor.&quot;</span><span class="p">)</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">h</span> <span class="o">//</span> <span class="n">downscale_factor</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">//</span> <span class="n">downscale_factor</span>
    <span class="n">input_perm</span> <span class="o">=</span> <span class="p">(</span><span class="n">pre</span> <span class="o">+</span> <span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">downscale_factor</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">downscale_factor</span><span class="p">))</span>
    <span class="n">reshape</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">input_perm</span><span class="p">)</span>
    <span class="n">input_perm</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">length</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)]</span>
    <span class="n">input_perm</span> <span class="o">=</span> <span class="n">input_perm</span> <span class="o">+</span> <span class="p">[</span><span class="n">length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">length</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">length</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span> <span class="n">length</span><span class="p">]</span>
    <span class="n">input_perm</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">input_perm</span><span class="p">)</span>
    <span class="n">transpose</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Transpose</span><span class="p">()</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">input_perm</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">pre</span> <span class="o">+</span> <span class="p">(</span><span class="n">c</span> <span class="o">*</span> <span class="n">downscale_factor</span> <span class="o">*</span> <span class="n">downscale_factor</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">x</span></div>


<div class="viewcode-block" id="glu"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.glu.html#mindspore.ops.glu">[docs]</a><span class="k">def</span> <span class="nf">glu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes GLU (Gated Linear Unit activation function) of input tensors .</span>

<span class="sd">    .. math::</span>
<span class="sd">        {GLU}(a, b)= a \otimes \sigma(b)</span>

<span class="sd">    where :math:`a` is the first half of the input matrices and :math:`b` is the second half.</span>

<span class="sd">    Here :math:`\sigma` is the sigmoid function, and :math:`*` is the Hadamard product.</span>
<span class="sd">    See `Language Modeling with Gated Convluational Networks &lt;https://arxiv.org/abs/1612.08083&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Tensor to be splited. Its dtype is number.Number, and shape is :math:`(\ast_1, N, \ast_2)`</span>
<span class="sd">            where `*` means, any number of additional dimensions.</span>
<span class="sd">        axis (int, optional): the dimension on which to split the input. It must be int. Default: -1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the same dtype as the `x`, with the shape :math:`(\ast_1, M, \ast_2)` where :math:`M=N/2`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x` is not number.Number.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor([[0.1,0.2,0.3,0.4],[0.5,0.6,0.7,0.8]])</span>
<span class="sd">        &gt;&gt;&gt; output = ops.glu(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0.05744425 0.11973753]</span>
<span class="sd">         [0.33409387 0.41398472]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">or</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;glu does not support scalars because halving size must be even&quot;</span><span class="p">)</span>

    <span class="n">spilt</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Split</span><span class="p">)(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">output_num</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">spilt</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">sigmoid_</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span></div>


<span class="k">def</span> <span class="nf">multi_margin_loss</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hinge loss for optimizing a multi-class classification.</span>

<span class="sd">    Creates a criterion that optimizes a multi-class classification hinge</span>
<span class="sd">    loss (margin-based loss) between input :math:`x` (a 2D mini-batch `Tensor`) and</span>
<span class="sd">    output :math:`y` (which is a 1D tensor of target class indices,</span>
<span class="sd">    :math:`0 \leq y \leq \text{x.size}(1)-1`):</span>
<span class="sd">    For each mini-batch sample, the loss in terms of the 1D input :math:`x` and scalar</span>
<span class="sd">    output :math:`y` is:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss}(x, y) = \frac{\sum_i \max(0, w[y] * (\text{margin} - x[y] + x[i]))^p)}{\text{x.size}(0)}</span>

<span class="sd">    where :math:`x \in \left\{0, \; \cdots , \; \text{x.size}(0) - 1\right\}`</span>
<span class="sd">    and :math:`i \neq y`.</span>
<span class="sd">    Optionally, you can give non-equal weighting on the classes by passing</span>
<span class="sd">    a 1D input `weight` tensor into the constructor.</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs (Tensor): Input , with shape :math:`(N, C)`. Data type only support float32, float16 or float64.</span>
<span class="sd">        target (Tensor): Ground truth labels, with shape :math:`(N,)`. Data type only support int64. The</span>
<span class="sd">            value of target should be non-negative, less than C.</span>
<span class="sd">        p (int, optional): The norm degree for pairwise distance. Should be 1 or 2. Default: 1.</span>
<span class="sd">        margin (int, optional): A parameter to change pairwise distance. Default: 1.</span>
<span class="sd">        weight (Tensor, optional): The rescaling weight to each class with shape :math:`(C,)`. Data type only</span>
<span class="sd">            support float16, float32 or float64. Default: None.</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: &#39;none&#39;, &#39;mean&#39;,</span>
<span class="sd">            &#39;sum&#39;. Default: &#39;mean&#39;.</span>

<span class="sd">            - &#39;none&#39;: no reduction will be applied.</span>
<span class="sd">            - &#39;mean&#39;: the sum of the output will be divided by the number of elements in the output.</span>
<span class="sd">            - &#39;sum&#39;: the output will be summed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, When `reduction` is &#39;none&#39;, the shape is :math:`(N,)`.</span>
<span class="sd">        Otherwise, it is a scalar. Has the same data type with `inputs`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `p` or `target` is not int.</span>
<span class="sd">        TypeError: If dtype of `margin` is not int.</span>
<span class="sd">        TypeError: If dtype of `reduction` is not str.</span>
<span class="sd">        TypeError: If dtype of `inputs` is not float16, float or float64.</span>
<span class="sd">        TypeError: If dtype of `weight` and `inputs` is not the same.</span>
<span class="sd">        ValueError: If `p` is not 1 or 2.</span>
<span class="sd">        ValueError: If `reduction` is not one of {&#39;none&#39;,&#39;sum&#39;,&#39;mean&#39;}.</span>
<span class="sd">        ValueError: If shape[0] of `inputs` is not equal to shape[0] of `target`.</span>
<span class="sd">        ValueError: If shape[1] of `inputs` is not equal to shape[0] of `weight`.</span>
<span class="sd">        ValueError: If rank of `weight` is not 1 or  rank of `target` is not 1 or `inputs` is not 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``  ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; inputs = Tensor(np.ones(shape=[3, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; target = Tensor(np.array([1, 2, 1]), mindspore.int64)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.array([1, 1, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.multi_margin_loss(inputs, target, weight=weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.6666667</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">margin</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;multi_margin_loss&#39;, the type of &#39;margin&#39; must be int, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">margin</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">margin_</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">margin</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">MultiMarginLoss</span><span class="p">)(</span><span class="n">p</span><span class="p">,</span> <span class="n">margin_</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">outputs</span>


<div class="viewcode-block" id="multi_label_margin_loss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.multi_label_margin_loss.html#mindspore.ops.multi_label_margin_loss">[docs]</a><span class="k">def</span> <span class="nf">multi_label_margin_loss</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hinge loss for optimizing a multi-label classification.</span>

<span class="sd">    Creates a criterion that optimizes a multi-label multi-classification</span>
<span class="sd">    hinge loss (margin-based loss) between input :math:`x` (a 2D mini-batch `Tensor`)</span>
<span class="sd">    and output :math:`y` (which is a 2D `Tensor` of target class indices).</span>
<span class="sd">    For each sample in the mini-batch:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss}(x, y) = \sum_{ij}\frac{\max(0, 1 - (x[y[j]] - x[i]))}{\text{x.size}(0)}</span>

<span class="sd">    where :math:`x \in \left\{0, \; \cdots , \; \text{x.size}(0) - 1\right\}`, \</span>
<span class="sd">    :math:`y \in \left\{0, \; \cdots , \; \text{y.size}(0) - 1\right\}`, \</span>
<span class="sd">    :math:`0 \leq y[j] \leq \text{x.size}(0)-1`, \</span>
<span class="sd">    and :math:`i \neq y[j]` for all :math:`i` and :math:`j`.</span>
<span class="sd">    :math:`y` and :math:`x` must have the same size.</span>
<span class="sd">    The criterion only considers a contiguous block of non-negative targets that</span>
<span class="sd">    starts at the front.</span>
<span class="sd">    This allows for different samples to have variable amounts of target classes.</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs (Tensor): Predict data. Tensor of shape :math:`(C)` or :math:`(N, C)`, where :math:`N`</span>
<span class="sd">            is the batch size and :math:`C` is the number of classes. Data type must be float16 or float32.</span>
<span class="sd">        target (Tensor): Ground truth data, with the same shape as `inputs`, data type must be int32 and</span>
<span class="sd">            label targets padded by -1.</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: &#39;none&#39;, &#39;mean&#39;,</span>
<span class="sd">            &#39;sum&#39;. Default: &#39;mean&#39;.</span>

<span class="sd">            - &#39;none&#39;: no reduction will be applied.</span>
<span class="sd">            - &#39;mean&#39;: the sum of the output will be divided by the number of elements in the output.</span>
<span class="sd">            - &#39;sum&#39;: the output will be summed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        - **outputs** (Union[Tensor, Scalar]) - The loss of MultilabelMarginLoss. If `reduction` is &quot;none&quot;, its shape</span>
<span class="sd">          is :math:`(N)`. Otherwise, a scalar value will be returned.</span>


<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `inputs` or `target` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `inputs` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `target` is not int32.</span>
<span class="sd">        ValueError: If length of shape of `inputs` is neither 1 nor 2.</span>
<span class="sd">        ValueError: If shape of `inputs` is not the same as `target`.</span>
<span class="sd">        ValueError: If `reduction` is not one of &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">       &gt;&gt;&gt; inputs = Tensor(np.array([[0.1, 0.2, 0.4, 0.8], [0.2, 0.3, 0.5, 0.7]]), mindspore.float32)</span>
<span class="sd">       &gt;&gt;&gt; target = Tensor(np.array([[1, 2, 0, 3], [2, 3, -1, 1]]), mindspore.int32)</span>
<span class="sd">       &gt;&gt;&gt; output, _ = ops.multi_label_margin_loss(inputs, target)</span>
<span class="sd">       &gt;&gt;&gt; print(output)</span>
<span class="sd">       (Tensor(shape=[], dtype=Float32, value= 0.325), Tensor(shape=[2, 4], dtype=Int32, value=</span>
<span class="sd">       [[1, 1, 1, 1], [0, 0, 1, 1]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">MultilabelMarginLoss</span><span class="p">)(</span><span class="n">reduction</span><span class="p">)</span>
    <span class="n">outputs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">outputs</span></div>


<div class="viewcode-block" id="elu"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.elu.html#mindspore.ops.elu">[docs]</a><span class="k">def</span> <span class="nf">elu</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential Linear Unit activation function.</span>

<span class="sd">    Applies the exponential linear unit function element-wise.</span>
<span class="sd">    The activation function is defined as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{ELU}(x)= \left\{</span>
<span class="sd">        \begin{array}{align}</span>
<span class="sd">            \alpha(e^{x}  - 1) &amp; \text{if } x \le 0\\</span>
<span class="sd">            x &amp; \text{if } x \gt 0\\</span>
<span class="sd">        \end{array}\right.</span>

<span class="sd">    Where :math:`x` is the element of input Tensor `input_x`, :math:`\alpha` is param `alpha`,</span>
<span class="sd">    it determines the smoothness of ELU.</span>
<span class="sd">    The picture about ELU looks like this `ELU &lt;https://en.wikipedia.org/wiki/</span>
<span class="sd">    Activation_function#/media/File:Activation_elu.svg&gt;`_ .</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The input of ELU is a Tensor of any dimension with data type of float16 or float32.</span>
<span class="sd">        alpha (float): The alpha value of ELU, the data type is float. Only support &#39;1.0&#39; currently. Default: 1.0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and data type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `alpha` is not a float.</span>
<span class="sd">        TypeError: If dtype of `input_x` is neither float16 nor float32.</span>
<span class="sd">        ValueError: If `alpha` is not equal to 1.0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.elu(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-0.63212055  4.         -0.99966455]</span>
<span class="sd">         [ 2.         -0.99326205  9.        ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Elu</span><span class="p">)(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)(</span><span class="n">input_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="gelu"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.gelu.html#mindspore.ops.gelu">[docs]</a><span class="k">def</span> <span class="nf">gelu</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">approximate</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gaussian Error Linear Units activation function.</span>

<span class="sd">    GeLU is described in the paper `Gaussian Error Linear Units (GELUs) &lt;https://arxiv.org/abs/1606.08415&gt;`_.</span>
<span class="sd">    And also please refer to `BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</span>
<span class="sd">    &lt;https://arxiv.org/abs/1810.04805&gt;`_.</span>

<span class="sd">    When `approximate` argument is `none`, GeLU is defined as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        GELU(x_i) = x_i*P(X &lt; x_i)</span>

<span class="sd">    where :math:`P` is the cumulative distribution function of the standard Gaussian distribution,</span>
<span class="sd">    :math:`x_i` is the input element.</span>

<span class="sd">    When `approximate` argument is `tanh`, GeLU is estimated with:</span>

<span class="sd">    .. math::</span>
<span class="sd">        GELU(x_i) = 0.5 * x_i * (1 + tanh(\sqrt(2 / \pi) * (x_i + 0.044715 * x_i^3)))</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The input of the activation function GeLU, the data type is float16, float32 or float64.</span>
<span class="sd">        approximate (str): the gelu approximation algorithm to use. Acceptable vaslues are &#39;none&#39; and &#39;tanh&#39;.</span>
<span class="sd">            Default: &#39;none&#39;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input_x` is not float16, float32 or float64.</span>
<span class="sd">        ValueError: If `approximate` value is neither `none` or `tanh`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([1.0, 2.0, 3.0], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; result = ops.gelu(x)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [0.841192 1.9545976 2.9963627]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">approximate</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;tanh&#39;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For ops.gelu, approximate value should be either &#39;none&#39; or &#39;tanh&#39;.&quot;</span><span class="p">)</span>

    <span class="n">output</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">GeLU</span><span class="p">)()(</span><span class="n">input_x</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">approximate</span> <span class="o">==</span> <span class="s1">&#39;tanh&#39;</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Pow</span><span class="p">)()(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">]))</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span> <span class="o">*</span> <span class="n">Tensor</span><span class="p">([</span><span class="mf">0.044715</span><span class="p">])</span> <span class="o">+</span> <span class="n">input_x</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span> <span class="o">*</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Sqrt</span><span class="p">)()(</span><span class="n">Tensor</span><span class="p">(</span><span class="mf">2.0</span> <span class="o">/</span> <span class="n">pi</span><span class="p">))</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Tanh</span><span class="p">)()(</span><span class="n">output</span><span class="p">)</span> <span class="o">+</span> <span class="n">Tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">])</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span> <span class="o">*</span> <span class="n">input_x</span> <span class="o">*</span> <span class="n">Tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">output</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_shape_check</span><span class="p">(</span><span class="n">in_shape</span><span class="p">,</span> <span class="n">dim_list</span><span class="p">,</span> <span class="n">prim_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">msg_prefix</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="n">prim_name</span><span class="si">}</span><span class="s2">&#39;, the&quot;</span> <span class="k">if</span> <span class="n">prim_name</span> <span class="k">else</span> <span class="s2">&quot;The&quot;</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">in_shape</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">dim_list</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">msg_prefix</span><span class="si">}</span><span class="s2"> input must has dim in </span><span class="si">{</span><span class="n">dim_list</span><span class="si">}</span><span class="s2">, but got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">in_shape</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="lp_pool1d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.lp_pool1d.html#mindspore.ops.lp_pool1d">[docs]</a><span class="k">def</span> <span class="nf">lp_pool1d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a 1D power lp pooling over an input signal composed of several input planes.</span>

<span class="sd">    Typically the input is of shape :math:`(N, C, L_{in})` or :math:`(C, L_{in})`, the output is of shape</span>
<span class="sd">    :math:`(N, C, L_{in})` or :math:`(C, L_{in})`, with the same shape as input, the operation is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        f(X) = \sqrt[p]{\sum_{x \in X} x^{p}}</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Tensor of shape :math:`(N, C, L_{in})` or :math:`(C, L_{in})`.</span>
<span class="sd">        norm_type (Union[int, float]): Type of normalization, represents p in the formula, can not be 0,</span>

<span class="sd">            - if p = 1, one gets Sum Pooling (which is proportional to Average Pooling),</span>
<span class="sd">            - if p = :math:`\infty`, one gets Max Pooling.</span>

<span class="sd">        kernel_size (int): The size of kernel window.</span>
<span class="sd">        stride (int): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the width of movement is stride, if the value is None, the default value `kernel_size` is used;</span>
<span class="sd">        ceil_mode (bool): Whether to use ceil or floor to calculate output shape. Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        - **output** (Tensor) - LPPool1d result, with shape :math:`(N, C, L_{in})` or :math:`(C, L_{in})`,</span>
<span class="sd">          It has the same data type as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not an Tensor.</span>
<span class="sd">        TypeError: If `kernel_size` or `stride` is not an int.</span>
<span class="sd">        TypeError: If `ceil_mode` is not a bool.</span>
<span class="sd">        TypeError: If `norm_type` is neither float nor int.</span>
<span class="sd">        ValueError: If `norm_type` is equal to 0.</span>
<span class="sd">        ValueError: If `kernel_size` or `stride` is less than 1.</span>
<span class="sd">        ValueError: If length of shape of `x` is not equal to 2 or 3.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(2 * 3 * 4).reshape((2, 3, 4)), dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; out = ops.lp_pool1d(x, norm_type=1, kernel_size=3, stride=1, ceil_mode=False)</span>
<span class="sd">        &gt;&gt;&gt; print(out)</span>
<span class="sd">        [[[ 3.  6.]</span>
<span class="sd">          [15. 18.]</span>
<span class="sd">          [27. 30.]]</span>
<span class="sd">         [[39. 42.]</span>
<span class="sd">          [51. 54.]</span>
<span class="sd">          [63. 66.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_shape_check</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="s2">&quot;lp_pool1d&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">norm_type</span><span class="p">,</span> <span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">)):</span>
        <span class="n">norm_type</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">norm_type</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For lp_pool1d, the type of &#39;norm_type&#39; must be float or int, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">norm_type</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For lp_pool1d, the value of &#39;norm_type&#39; can not be 0.&quot;</span><span class="p">)</span>
    <span class="n">sign</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">Sign</span><span class="p">)()</span>
    <span class="n">squeeze</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">Squeeze</span><span class="p">)(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">expand_dims</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">)()</span>
    <span class="n">_is_squeeze</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">_is_squeeze</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">avg_pool1d</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">norm_type</span><span class="p">),</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="n">ceil_mode</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">avg_pool1d</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">norm_type</span><span class="p">),</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                             <span class="n">ceil_mode</span><span class="o">=</span><span class="n">ceil_mode</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_is_squeeze</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">squeeze</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">((</span><span class="n">sign</span><span class="p">(</span><span class="n">out</span><span class="p">)</span> <span class="o">*</span> <span class="n">ops</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">out</span><span class="p">)))</span> <span class="o">*</span> <span class="n">kernel_size</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">norm_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="lp_pool2d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.lp_pool2d.html#mindspore.ops.lp_pool2d">[docs]</a><span class="k">def</span> <span class="nf">lp_pool2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a 2D power lp pooling over an input signal composed of several input planes.</span>

<span class="sd">    Typically the input is of shape :math:`(N, C, H_{in}, W_{in})`, the output is of shape</span>
<span class="sd">    :math:`(N, C, H_{in}, W_{in})`, with the same shape as input, the operation is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        f(X) = \sqrt[p]{\sum_{x \in X} x^{p}}</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Tensor of shape :math:`(N, C, H_{in}, W_{in})`.</span>
<span class="sd">        norm_type (Union[int, float]): Type of normalization, represents p in the formula, can not be 0,</span>

<span class="sd">            - if p = 1, one gets Sum Pooling (which is proportional to Average Pooling),</span>
<span class="sd">            - if p = :math:`\infty`, one gets Max Pooling.</span>

<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of kernel window.</span>
<span class="sd">            The data type of kernel_size must be int and the value represents the height and width,</span>
<span class="sd">            or a tuple of two int numbers that represent height and width respectively.</span>
<span class="sd">        stride (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the height and width of movement are both strides, or a tuple of two int numbers that</span>
<span class="sd">            represent height and width of movement respectively, if the value is None,</span>
<span class="sd">            the default value `kernel_size` is used;</span>
<span class="sd">        ceil_mode (bool): Whether to use ceil or floor to calculate output shape. Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        - **output** (Tensor) - LPPool2d result, with shape :math:`(N, C, H_{in}, W_{in})`,</span>
<span class="sd">          It has the same data type as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not an Tensor.</span>
<span class="sd">        TypeError: If `kernel_size` or `stride` is neither int nor tuple.</span>
<span class="sd">        TypeError: If `ceil_mode` is not a bool.</span>
<span class="sd">        TypeError: If `norm_type` is neither float nor int.</span>
<span class="sd">        ValueError: If `norm_type` is equal to 0.</span>
<span class="sd">        ValueError: If `kernel_size` or `stride` is less than 1.</span>
<span class="sd">        ValueError: If `kernel_size` or `stride` is a tuple whose length is not equal to `2`.</span>
<span class="sd">        ValueError: If length of shape of `x` is not equal to 4.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(2 * 3 * 4 * 5).reshape((2, 3, 4, 5)), dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; out = ops.lp_pool2d(x, norm_type=1, kernel_size=3, stride=1, ceil_mode=False)</span>
<span class="sd">        &gt;&gt;&gt; print(out)</span>
<span class="sd">        [[[[  54.   63.   72.]</span>
<span class="sd">           [  99.  108.  117.]]</span>
<span class="sd">          [[ 234.  243.  252.]</span>
<span class="sd">           [ 279.  288.  297.]]</span>
<span class="sd">          [[ 414.  423.  432.]</span>
<span class="sd">           [ 459.  468.  477.]]]</span>
<span class="sd">         [[[ 594.  603.  612.]</span>
<span class="sd">           [ 639.  648.  657.]]</span>
<span class="sd">          [[ 774.  783.  792.]</span>
<span class="sd">           [ 819.  828.  837.]]</span>
<span class="sd">          [[ 954.  963.  972.]</span>
<span class="sd">           [ 999. 1008. 1017.]]]]</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_shape_check</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="s2">&quot;lp_pool2d&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">norm_type</span><span class="p">,</span> <span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">)):</span>
        <span class="n">norm_type</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">norm_type</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For lp_pool2d, the type of &#39;norm_type&#39; must be float or int, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">norm_type</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For lp_pool2d, the value of &#39;norm_type&#39; can not be 0.&quot;</span><span class="p">)</span>
    <span class="n">sign</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">Sign</span><span class="p">)()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">((</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">))</span>
    <span class="n">kw</span><span class="p">,</span> <span class="n">kh</span> <span class="o">=</span> <span class="n">kernel_size</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">avg_pool2d</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">norm_type</span><span class="p">),</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="n">ceil_mode</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">avg_pool2d</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">norm_type</span><span class="p">),</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                             <span class="n">ceil_mode</span><span class="o">=</span><span class="n">ceil_mode</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">((</span><span class="n">sign</span><span class="p">(</span><span class="n">out</span><span class="p">)</span> <span class="o">*</span> <span class="n">ops</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">out</span><span class="p">)))</span> <span class="o">*</span> <span class="p">(</span><span class="n">kw</span> <span class="o">*</span> <span class="n">kh</span><span class="p">))</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">norm_type</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">mse_loss</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the mean squared error between the predicted value and the label value.</span>

<span class="sd">    For detailed information, please refer to :class:`mindspore.nn.MSELoss`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Tensor of any dimension.</span>
<span class="sd">        target (Tensor): The input label. Tensor of any dimension, same shape as the `input_x` in common cases.</span>
<span class="sd">            However, it supports that the shape of `input_x` is different from the shape of `target`</span>
<span class="sd">            and they should be broadcasted to each other.</span>
<span class="sd">        reduction (str, optional): Type of reduction to be applied to loss.</span>
<span class="sd">            The optional values are &quot;mean&quot;, &quot;none&quot; and &quot;sum&quot;. Default: &quot;mean&quot;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, loss of type float, the shape is zero if `reduction` is &#39;mean&#39; or &#39;sum&#39;,</span>
<span class="sd">        while the shape of output is the broadcasted shape if `reduction` is &#39;none&#39;.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If `reduction` is not one of &#39;none&#39;, &#39;mean&#39; or &#39;sum&#39;.</span>
<span class="sd">        ValueError: If `input_x` and `target` have different shapes and cannot be broadcasted.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([[1, 1, 1], [1, 2, 2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.mse_loss(logits, labels, reduction=&#39;none&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 1. 4.]</span>
<span class="sd">         [0. 0. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For ops.mse_loss, the `input_x` must be tensor&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For ops.mse_loss, the `target` must be tensor&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For ops.mse_loss, `reduction` value should be either &#39;mean&#39;, &#39;none&#39; or &#39;sum&#39;.&quot;</span><span class="p">)</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Square</span><span class="p">)()(</span><span class="n">input_x</span> <span class="o">-</span> <span class="n">target</span><span class="p">)</span>
    <span class="n">input_dtype</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">)()(</span><span class="n">x</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="n">average_flag</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">reduce_flag</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;sum&#39;</span><span class="p">:</span>
        <span class="n">average_flag</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;none&#39;</span><span class="p">:</span>
        <span class="n">reduce_flag</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="n">perm</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Range</span><span class="p">)()(</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
                                      <span class="n">Tensor</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
                                      <span class="n">Tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">reduce_flag</span> <span class="ow">and</span> <span class="n">average_flag</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ReduceMean</span><span class="p">)()(</span><span class="n">x</span><span class="p">,</span> <span class="n">perm</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">reduce_flag</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">average_flag</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">)()(</span><span class="n">x</span><span class="p">,</span> <span class="n">perm</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">)()(</span><span class="n">x</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">)</span>


<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;adaptive_avg_pool1d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;adaptive_avg_pool2d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;adaptive_avg_pool3d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;adaptive_max_pool1d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;adaptive_max_pool2d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;adaptive_max_pool3d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;avg_pool1d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;avg_pool2d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;avg_pool3d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;batch_norm&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bias_add&#39;</span><span class="p">,</span>
    <span class="s1">&#39;binary_cross_entropy&#39;</span><span class="p">,</span>
    <span class="s1">&#39;binary_cross_entropy_with_logits&#39;</span><span class="p">,</span>
    <span class="s1">&#39;max_pool3d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;kl_div&#39;</span><span class="p">,</span>
    <span class="s1">&#39;celu&#39;</span><span class="p">,</span>
    <span class="s1">&#39;deformable_conv2d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;dropout1d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;dropout2d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;dropout3d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;fast_gelu&#39;</span><span class="p">,</span>
    <span class="s1">&#39;fractional_max_pool2d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;fractional_max_pool3d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;pixel_shuffle&#39;</span><span class="p">,</span>
    <span class="s1">&#39;pixel_unshuffle&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hardshrink&#39;</span><span class="p">,</span>
    <span class="s1">&#39;soft_shrink&#39;</span><span class="p">,</span>
    <span class="s1">&#39;is_floating_point&#39;</span><span class="p">,</span>
    <span class="s1">&#39;flip&#39;</span><span class="p">,</span>
    <span class="s1">&#39;fliplr&#39;</span><span class="p">,</span>
    <span class="s1">&#39;flipud&#39;</span><span class="p">,</span>
    <span class="s1">&#39;intopk&#39;</span><span class="p">,</span>
    <span class="s1">&#39;interpolate&#39;</span><span class="p">,</span>
    <span class="s1">&#39;log_softmax&#39;</span><span class="p">,</span>
    <span class="s1">&#39;mish&#39;</span><span class="p">,</span>
    <span class="s1">&#39;lrn&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hardswish&#39;</span><span class="p">,</span>
    <span class="s1">&#39;softsign&#39;</span><span class="p">,</span>
    <span class="s1">&#39;selu&#39;</span><span class="p">,</span>
    <span class="s1">&#39;softmax&#39;</span><span class="p">,</span>
    <span class="s1">&#39;pdist&#39;</span><span class="p">,</span>
    <span class="s1">&#39;pad&#39;</span><span class="p">,</span>
    <span class="s1">&#39;prelu&#39;</span><span class="p">,</span>
    <span class="s1">&#39;mirror_pad&#39;</span><span class="p">,</span>
    <span class="s1">&#39;cross_entropy&#39;</span><span class="p">,</span>
    <span class="s1">&#39;grid_sample&#39;</span><span class="p">,</span>
    <span class="s1">&#39;smooth_l1_loss&#39;</span><span class="p">,</span>
    <span class="s1">&#39;nll_loss&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ctc_loss&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ctc_greedy_decoder&#39;</span><span class="p">,</span>
    <span class="s1">&#39;dropout&#39;</span><span class="p">,</span>
    <span class="s1">&#39;conv3d_transpose&#39;</span><span class="p">,</span>
    <span class="s1">&#39;conv2d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;sigmoid&#39;</span><span class="p">,</span>
    <span class="s1">&#39;relu&#39;</span><span class="p">,</span>
    <span class="s1">&#39;relu6&#39;</span><span class="p">,</span>
    <span class="s1">&#39;conv3d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;glu&#39;</span><span class="p">,</span>
    <span class="s1">&#39;margin_ranking_loss&#39;</span><span class="p">,</span>
    <span class="s1">&#39;multi_margin_loss&#39;</span><span class="p">,</span>
    <span class="s1">&#39;multi_label_margin_loss&#39;</span><span class="p">,</span>
    <span class="s1">&#39;elu&#39;</span><span class="p">,</span>
    <span class="s1">&#39;gelu&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hinge_embedding_loss&#39;</span><span class="p">,</span>
    <span class="s1">&#39;gaussian_nll_loss&#39;</span><span class="p">,</span>
    <span class="s1">&#39;lp_pool1d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;lp_pool2d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;max_unpool1d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;max_unpool2d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;max_unpool3d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;mse_loss&#39;</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">__all__</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>
</pre></div>

           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2022, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>