<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Distributed Configuration &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script>
      
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Inference" href="inference.html" />
    <link rel="prev" title="Precision Tuning" href="precision_tuning.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../design/overview.html">MindSpore Design Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/auto_gradient.html">Functional Differential Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/distributed_training_design.html">Distributed Training Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/dynamic_graph_and_static_graph.html">Combination of Dynamic and Static Graphs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/graph_fusion_engine.html">Graph-Kernel Fusion Acceleration Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/mindir.html">MindSpore IR (MindIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/all_scenarios.html">Full-scenarios Unification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/thor.html">Second Order Optimizer</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r2.0.0-alpha/training_visual_design.html">Design of Visualization↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/data_engine.html">High Performance Data Processing Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Specification</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/benchmark.html">Benchmarks</a></li>
<li class="toctree-l1"><a class="reference external" href="https://gitee.com/mindspore/models/blob/r2.0.0-alpha/README.md#table-of-contents">Network List↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/operator_list.html">API List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/syntax_list.html">Syntax Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.transformer.html">mindspore.nn.transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/api_cpp/mindspore.html">C++ API↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Mapping</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/pytorch_api_mapping.html">PyTorch and MindSpore API Mapping Table</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/tensorflow_api_mapping.html">TensorFlow and MindSpore API Mapping Table</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Migration Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/overview.html">Overview of Migration Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/enveriment_preparation.html">Environment Preparation and Information Acquisition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/analysis_and_preparation.html">Model Analysis and Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/model_development/model_development.html">Constructing MindSpore Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/debug_and_tune.html">Debugging and Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/sample_code.html">Network Migration Debugging Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/faq.html">FAQs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/typical_api_comparision.html">Differences Between MindSpore and PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/use_third_party_op.html">Using Third-party Operator Libraries Based on Customized Interfaces</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_processing.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="implement_problem.html">Implement Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="network_compilation.html">Network Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="operators_compile.html">Operators Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="usage_migrate_3rd.html">Migration from a Third-party Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="precision_tuning.html">Precision Tuning</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Distributed Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="feature_advice.html">Feature Advice</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Distributed Configuration</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/faq/distributed_configure.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="distributed-configuration">
<h1>Distributed Configuration<a class="headerlink" href="#distributed-configuration" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r2.0.0-alpha/docs/mindspore/source_en/faq/distributed_configure.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.0.0-alpha/resource/_static/logo_source_en.png"></a></p>
<p><font size=3><strong>Q: What should I do if the error message <code class="docutils literal notranslate"><span class="pre">Init</span> <span class="pre">plugin</span> <span class="pre">so</span> <span class="pre">failed,</span> <span class="pre">ret</span> <span class="pre">=</span> <span class="pre">1343225860</span></code> is displayed during the HCCL distributed training?</strong></font></p>
<p>A: When the user starts distributed training on the Ascend and meets the error that HCCL fails to be initialized, the possible cause is that <code class="docutils literal notranslate"><span class="pre">rank_table.json</span></code> is incorrect. You can use the tool in <a class="reference external" href="https://gitee.com/mindspore/models/blob/r2.0.0-alpha/utils/hccl_tools/hccl_tools.py">hccl_tools.py</a> to generate new <code class="docutils literal notranslate"><span class="pre">rank_table.json</span></code>. Alternatively, set the environment variable <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">ASCEND_SLOG_PRINT_TO_STDOUT=1</span></code> to enable the log printing function of HCCL and check the ERROR log information.</p>
<br/>
<p><font size=3><strong>Q: How to fix the error below when running MindSpore distributed training with GPU:</strong></font></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Loading libgpu_collective.so failed. Many reasons could cause this:
1.libgpu_collective.so is not installed.
2.nccl is not installed or found.
3.mpi is not installed or found
</pre></div>
</div>
<p>A: This message means that MindSpore is failed to dynamically load the collection communication library. The Possible causes are:</p>
<ul class="simple">
<li><p>OpenMPI or NCCL relied by the diatributed training is not installed in this environment.</p></li>
<li><p>NCCL version is not updated to <code class="docutils literal notranslate"><span class="pre">v2.7.6</span></code>: MindSpore <code class="docutils literal notranslate"><span class="pre">v1.1.0</span></code> adds GPU P2P communication operator which relies on NCCL <code class="docutils literal notranslate"><span class="pre">v2.7.6</span></code>. The loading failure is caused if NCCL is not updated to this version.</p></li>
</ul>
<br/>
<p><font size=3><strong>Q: In the GPU distributed training scenario, if the number of environment variables CUDA_VISIBLE_DEVICES set incorrectly is less than the number of processes executed, the process blocking problem may occur.</strong></font></p>
<p>A: In this scenario, some training processes will prompt the following error:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[ERROR] DEVICE [mindspore/ccsrc/runtime/device/gpu/cuda_driver.cc:245] SetDevice] SetDevice for id:7 failed, ret[101], invalid device ordinal. Please make sure that the &#39;device_id&#39; set in context is in the range:[0, total number of GPU). If the environment variable &#39;CUDA_VISIBLE_DEVICES&#39; is set, the total number of GPU will be the number set in the environment variable &#39;CUDA_VISIBLE_DEVICES&#39;. For example, if export CUDA_VISIBLE_DEVICES=4,5,6, the &#39;device_id&#39; can be 0,1,2 at the moment, &#39;device_id&#39; starts from 0, and &#39;device_id&#39;=0 means using GPU of number 4.
[ERROR] DEVICE [mindspore/ccsrc/runtime/device/gpu/gpu_device_manager.cc:27] InitDevice] Op Error: Failed to set current device id | Error Number: 0
</pre></div>
</div>
<p>The remaining processes may normally execute to the initialization <code class="docutils literal notranslate"><span class="pre">NCCL</span></code> step due to the successful allocation of GPU resources, and the log is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[INFO] DEVICE [mindspore/ccsrc/runtime/hardware/gpu/gpu_device_context.cc:90] Initialize] Start initializing NCCL communicator for device 1
</pre></div>
</div>
<p>In this step, the <code class="docutils literal notranslate"><span class="pre">NCCL</span></code> interface <code class="docutils literal notranslate"><span class="pre">ncclCommInitRank</span></code> is called, which blocks until all processes agree. So if a process doesn’t call <code class="docutils literal notranslate"><span class="pre">ncclCommInitRank</span></code>, it will cause the process to block.</p>
<p>We have reported this issue to the <code class="docutils literal notranslate"><span class="pre">NCCL</span></code> community, and the community developers are designing a solution. The latest version has not been fixed, see <a class="reference external" href="https://github.com/NVIDIA/nccl/issues/593#issuecomment-965939279">issue link</a>.</p>
<p>Solution: Manually <code class="docutils literal notranslate"><span class="pre">kill</span></code> the training process. According to the error log, set the correct card number, and then restart the training task.</p>
<br/>
<p><font size=3><strong>Q: What can we do when in the GPU distributed training scenario, if a process exits abnormally, it may cause other processes to block?</strong></font></p>
<p>A: In this scenario, the abnormal process exits due to various problems, and the remaining processes are executed normally to the initialization <code class="docutils literal notranslate"><span class="pre">NCCL</span></code> step due to the successful allocation of GPU resources. The log is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[INFO] DEVICE [mindspore/ccsrc/runtime/hardware/gpu/gpu_device_context.cc:90] Initialize] Start initializing NCCL communicator for device 1
</pre></div>
</div>
<p>In this step, the <code class="docutils literal notranslate"><span class="pre">NCCL</span></code> interface <code class="docutils literal notranslate"><span class="pre">ncclCommInitRank</span></code> is called, which blocks until all processes agree. So if a process doesn’t call <code class="docutils literal notranslate"><span class="pre">ncclCommInitRank</span></code>, it will cause the process to block.</p>
<p>We have reported this issue to the <code class="docutils literal notranslate"><span class="pre">NCCL</span></code> community, and the community developers are designing a solution. The latest version has not been fixed, see <a class="reference external" href="https://github.com/NVIDIA/nccl/issues/593#issuecomment-965939279">issue link</a>.</p>
<p>Solution: Manually <code class="docutils literal notranslate"><span class="pre">kill</span></code> the training process. According to the error log, set the correct card number, and then restart the training task.</p>
<br/>
<p><font size=3><strong>Q: When executing a GPU stand-alone single-card script, when the process is started without mpirun, calling the mindspore.communication.init method may report an error, resulting in execution failure, how to deal with it?</strong></font></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[CRITICAL] DISTRIBUTED [mindspore/ccsrc/distributed/cluster/cluster_context.cc:130] InitNodeRole] Role name is invalid...
</pre></div>
</div>
<p>A: In the case where the user does not start the process using <code class="docutils literal notranslate"><span class="pre">mpirun</span></code> but still calls the <code class="docutils literal notranslate"><span class="pre">init()</span></code> method, MindSpore requires the user to configure several environment variables and verify according to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/r2.0.0-alpha/parallel/train_gpu.html#%E4%B8%8D%E4%BE%9D%E8%B5%96openmpi%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83">training and do not rely on OpenMPI for training</a>. If without configuring, MindSpore may display the above error message. Therefore, it is suggested that only when performing distributed training, <code class="docutils literal notranslate"><span class="pre">mindspore.communication.init</span></code> is called, and in the case of not using <code class="docutils literal notranslate"><span class="pre">mpirun</span></code>, it is configured the correct environment variables according to the documentation to start distributed training.</p>
<br/>
<p><font size=3><strong>Q: What can we do when performing multi-machine multi-card training via OpenMPI, the prompt fails due to MPI_Allgather?</strong></font></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>pml_ucx.c:175 Error: Failed to receive UCX worker address: Not found (-13)
pml_ucx.c:452 Error: Failed to resolve UCX endpoint for rank X
</pre></div>
</div>
<p>A: This problem is that <code class="docutils literal notranslate"><span class="pre">OpenMPI</span></code> cannot communicate with the peer address when communicating on the Host side, which is generally caused by the different configuration of the NIC between the machines, and can be solved by manually setting the NIC name or subnet:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mpirun -n process_num --mca btl tcp --mca btl_tcp_if_include eth0 ./run.sh
</pre></div>
</div>
<p>The above instruction starts the <code class="docutils literal notranslate"><span class="pre">process_num</span></code> of <code class="docutils literal notranslate"><span class="pre">run.sh</span></code> processes, and selects the Host side communication mode as <code class="docutils literal notranslate"><span class="pre">tcp</span></code>. The network card selects <code class="docutils literal notranslate"><span class="pre">eth0</span></code>, so that the network card used on each machine is the same, and then the communication abnormal problem is solved.</p>
<p>You can also select subnets for matching:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mpirun -n process_num --mca btl tcp --mca btl_tcp_if_include 192.168.1.0/24 ./run.sh
</pre></div>
</div>
<p>The subnet range needs to include the IP addresses used by all machines.</p>
<br/>
<p><font size=3><strong>Q: What can we do when performing distributed training via OpenMPI, stand-alone multi-card training is normal, but during the multi-machine multi-card training, some machines prompt the GPU device id setting to fail?</strong></font></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[ERROR] DEVICE [mindspore/ccsrc/runtime/device/gpu/cuda_driver.cc:245] SetDevice] SetDevice for id:7 failed, ret[101], invalid device ordinal. Please make sure that the &#39;device_id&#39; set in context is in the range:[0, total number of GPU). If the environment variable &#39;CUDA_VISIBLE_DEVICES&#39; is set, the total number of GPU will be the number set in the environment variable &#39;CUDA_VISIBLE_DEVICES&#39;. For example, if export CUDA_VISIBLE_DEVICES=4,5,6, the &#39;device_id&#39; can be 0,1,2 at the moment, &#39;device_id&#39; starts from 0, and &#39;device_id&#39;=0 means using GPU of number 4.
[ERROR] DEVICE [mindspore/ccsrc/runtime/device/gpu/gpu_device_manager.cc:27] InitDevice] Op Error: Failed to set current device id | Error Number: 0
</pre></div>
</div>
<p>A: In the multi-machine scenario, each process card number needs to be calculated after the host side <code class="docutils literal notranslate"><span class="pre">AllGather</span></code>and <code class="docutils literal notranslate"><span class="pre">HOSTNAME</span></code>. If the same <code class="docutils literal notranslate"><span class="pre">HOSTNAME</span></code> is used between machines, the process card number will be calculated incorrectly, causing the card number to cross the boundary and the setting to fail. This can be resolved by setting the HOSTNAME of each machine to its respective IP address in the execution script:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>export HOSTNAME=node_ip_address
</pre></div>
</div>
<br/>
<p><font size=3><strong>Q: What can we do when performing multi-machine multi-card training via OpenMPI, the NCCL error message displays that the network is not working.?</strong></font></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>include/socket.h:403 NCCL WARN Connect to XXX failed: Network is unreachable
</pre></div>
</div>
<p>A: This problem is that <code class="docutils literal notranslate"><span class="pre">NCCL</span></code> cannot communicate with the peer address when synchronizing process information or initializing the communication domain on the Host side, which is generally caused by the different configuration of the network card between the machines, and the network card can be selected by setting the <code class="docutils literal notranslate"><span class="pre">NCCL</span></code> environment variable <code class="docutils literal notranslate"><span class="pre">NCCL_SOCKET_IFNAME</span></code>:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>export NCCL_SOCKET_IFNAME=eth
</pre></div>
</div>
<p>The above command sets the <code class="docutils literal notranslate"><span class="pre">NCCL</span></code> to select the network card name with <code class="docutils literal notranslate"><span class="pre">eth</span></code> in the Host side to communicate.</p>
<br/>
<p><font size=3><strong>Q: Performing the distributed training via OpenMPI on Ascend, got <code class="docutils literal notranslate"><span class="pre">HcclCommInitRootInfo</span></code> error message:</strong></font></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Ascend collective Error: &quot;HcclCommInitRootInfo failed. | Error Number 2
</pre></div>
</div>
<p>A: Currently, when training via OpenMPI, hccl needs to allocate about 300M device memory for each card within a communicator. The more communicators one card involved in, the more extra device memory needed. This probably cause memory issue.
You can set <code class="docutils literal notranslate"><span class="pre">variable_memory_max_size</span></code> in <code class="docutils literal notranslate"><span class="pre">context</span></code>to reduce variable memory for Ascend processes, so that hccl will have enough memory to create communicators.</p>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="precision_tuning.html" class="btn btn-neutral float-left" title="Precision Tuning" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="inference.html" class="btn btn-neutral float-right" title="Inference" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>