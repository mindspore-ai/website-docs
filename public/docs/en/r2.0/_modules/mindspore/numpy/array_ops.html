

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>mindspore.numpy.array_ops &mdash; MindSpore master documentation</title>
  

  
  <link rel="stylesheet" href="../../../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/training.css" type="text/css" />
   
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/language_data.js"></script>
        <script src="../../../_static/js/training.js"></script>
        
        
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../design/overview.html">MindSpore Design Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/programming_paradigm.html">Programming Paradigm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/auto_gradient.html">Functional Differential Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/mindir.html">MindSpore IR (MindIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/all_scenarios.html">Full-scenarios Unification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/dynamic_graph_and_static_graph.html">Combination of Dynamic and Static Graphs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/pluggable_device.html">Third-Party Hardware Interconnection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/distributed_training_design.html">Distributed Training Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/graph_fusion_engine.html">Graph-Kernel Fusion Acceleration Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/data_engine.html">High Performance Data Processing Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/glossary.html">Glossary</a></li>
</ul>
<p class="caption"><span class="caption-text">Specification</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../note/benchmark.html">Benchmarks</a></li>
<li class="toctree-l1"><a class="reference external" href="https://gitee.com/mindspore/models/blob/r2.0/README.md#table-of-contents">Network List↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../note/operator_list.html">API List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../note/syntax_list.html">Syntax Support</a></li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0/api_cpp/mindspore.html">C++ API↗</a></li>
</ul>
<p class="caption"><span class="caption-text">API Mapping</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../note/api_mapping/pytorch_api_mapping.html">PyTorch and MindSpore API Mapping Table</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../note/api_mapping/tensorflow_api_mapping.html">TensorFlow and MindSpore API Mapping Table</a></li>
</ul>
<p class="caption"><span class="caption-text">Migration Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../migration_guide/overview.html">Overview of Migration Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../migration_guide/enveriment_preparation.html">Environment Preparation and Information Acquisition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../migration_guide/analysis_and_preparation.html">Model Analysis and Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../migration_guide/model_development/model_development.html">Constructing MindSpore Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../migration_guide/debug_and_tune.html">Debugging and Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../migration_guide/sample_code.html">Network Migration Debugging Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../migration_guide/faq.html">FAQs</a></li>
</ul>
<p class="caption"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/data_processing.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/implement_problem.html">Implement Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/network_compilation.html">Network Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/operators_compile.html">Operators Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/usage_migrate_3rd.html">Migration from a Third-party Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/precision_tuning.html">Precision Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/distributed_parallel.html">Distributed Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/feature_advice.html">Feature Advice</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
      <li>mindspore.numpy.array_ops</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for mindspore.numpy.array_ops</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2020-2021 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>
<span class="sd">&quot;&quot;&quot;array operations, the function docs are adapted from Numpy API.&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span>

<span class="kn">import</span> <span class="nn">operator</span>

<span class="kn">from</span> <span class="nn">mindspore.common</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="kn">from</span> <span class="nn">mindspore.common</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mutable</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.primitive</span> <span class="kn">import</span> <span class="n">constexpr</span><span class="p">,</span> <span class="n">_primexpr</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">Cell</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span>

<span class="kn">from</span> <span class="nn">mindspore.numpy.utils</span> <span class="kn">import</span> <span class="n">_convert_list_tensor_to_tuple_tensor</span><span class="p">,</span> <span class="n">_expand</span><span class="p">,</span> <span class="n">_broadcast_to_shape</span><span class="p">,</span> \
    <span class="n">_check_input_tensor</span><span class="p">,</span> <span class="n">_broadcast_to</span><span class="p">,</span> <span class="n">_to_tensor</span><span class="p">,</span> <span class="n">_callable</span>
<span class="kn">from</span> <span class="nn">mindspore.numpy.utils_const</span> <span class="kn">import</span> <span class="n">_check_axes_range</span><span class="p">,</span> <span class="n">_check_start_normalize</span><span class="p">,</span> \
    <span class="n">_raise_type_error</span><span class="p">,</span> <span class="n">_raise_value_error</span><span class="p">,</span> <span class="n">_infer_out_shape</span><span class="p">,</span> <span class="n">_empty</span><span class="p">,</span> <span class="n">_promote</span><span class="p">,</span> \
    <span class="n">_check_same_type</span><span class="p">,</span> <span class="n">_check_axis_valid</span><span class="p">,</span> <span class="n">_add_unit_axes</span><span class="p">,</span> <span class="n">_broadcast_tuples</span><span class="p">,</span> \
    <span class="n">_check_is_float</span><span class="p">,</span> <span class="n">_check_axis_in_range</span><span class="p">,</span> <span class="n">_check_axis_type</span><span class="p">,</span> <span class="n">_canonicalize_axis</span><span class="p">,</span> \
    <span class="n">_list_comprehensions</span><span class="p">,</span> <span class="n">_check_element_int</span><span class="p">,</span> <span class="n">_is_shape_empty</span><span class="p">,</span> <span class="n">_type_convert</span><span class="p">,</span> \
    <span class="n">_tuple_slice</span><span class="p">,</span> <span class="n">_expanded_shape</span><span class="p">,</span> <span class="n">_seq_prod</span><span class="p">,</span> <span class="n">_tuple_setitem</span><span class="p">,</span> <span class="n">_iota</span><span class="p">,</span> \
    <span class="n">_raise_unimplemented_error</span><span class="p">,</span> <span class="n">_cumprod</span><span class="p">,</span> <span class="n">_get_device</span><span class="p">,</span> <span class="n">_check_is_int</span>


<span class="c1"># According to official numpy reference, the dimension of a numpy array must be less</span>
<span class="c1"># than 32</span>
<span class="n">MAX_NUMPY_DIMS</span> <span class="o">=</span> <span class="mi">32</span>


<div class="viewcode-block" id="expand_dims"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.expand_dims.html#mindspore.numpy.expand_dims">[docs]</a><span class="k">def</span> <span class="nf">expand_dims</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Expands the shape of a tensor.</span>

<span class="sd">    Inserts a new axis that will appear at the axis position in the expanded tensor shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): Input tensor array.</span>
<span class="sd">        axis (Union[int, list(int), tuple(int)]): Position in the expanded axes where</span>
<span class="sd">            the new axis is placed,</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the number of dimensions increased at specified axis.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If input arguments have types not specified above.</span>
<span class="sd">        ValueError: If axis exceeds a.ndim.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.ones((2,2))</span>
<span class="sd">        &gt;&gt;&gt; x = np.expand_dims(x,0)</span>
<span class="sd">        &gt;&gt;&gt; print(x.shape)</span>
<span class="sd">        (1, 2, 2)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;axis must be tuple, list or int, but got &quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">ndim</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="n">_canonicalize_axis</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">axis</span><span class="p">:</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">ax</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">a</span></div>


<div class="viewcode-block" id="squeeze"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.squeeze.html#mindspore.numpy.squeeze">[docs]</a><span class="k">def</span> <span class="nf">squeeze</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Removes single-dimensional entries from the shape of a tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): Input tensor array.</span>
<span class="sd">        axis (Union[None, int, list(int), tuple(list)]): The axis(axes) to squeeze,</span>
<span class="sd">            default is None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with all or a subset of the dimensions of length :math:`1` removed.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If input arguments have types not specified above.</span>
<span class="sd">        ValueError: If specified axis has shape entry :math:`&gt; 1`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.ones((1,2,2,1))</span>
<span class="sd">        &gt;&gt;&gt; x = np.squeeze(x)</span>
<span class="sd">        &gt;&gt;&gt; print(x.shape)</span>
<span class="sd">        (2, 2)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="transpose"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.transpose.html#mindspore.numpy.transpose">[docs]</a><span class="k">def</span> <span class="nf">transpose</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reverses or permutes the axes of a tensor; returns the modified tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): a tensor to be transposed</span>
<span class="sd">        axes (Union[None, tuple, list]): the axes order, if `axes` is `None`, transpose</span>
<span class="sd">            the entire tensor. Default is `None`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the transposed tensor array.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If input arguments have types not specified above.</span>
<span class="sd">        ValueError: If the number of `axes` is not equal to a.ndim.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.ones((1,2,3))</span>
<span class="sd">        &gt;&gt;&gt; x = np.transpose(x)</span>
<span class="sd">        &gt;&gt;&gt; print(x.shape)</span>
<span class="sd">        (3, 2, 1)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span></div>


<div class="viewcode-block" id="rollaxis"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.rollaxis.html#mindspore.numpy.rollaxis">[docs]</a><span class="k">def</span> <span class="nf">rollaxis</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Rolls the specified axis backwards, until it lies in the given position.</span>
<span class="sd">    The positions of the other axes do not change relative to one another.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): A Tensor to be transposed.</span>
<span class="sd">        axis (int): The axis to be rolled.</span>
<span class="sd">        start (int): Default: 0.</span>
<span class="sd">            If :math:`start &lt;= axis`, the axis is rolled back until it lies in this position (`start`).</span>
<span class="sd">            If :math:`start &gt; axis`: the axis is rolled until it lies before this position (`start`).</span>
<span class="sd">            If :math:`start &lt; 0`, the start will be normalized as a non-negative number (more details</span>
<span class="sd">            can be seen in the source code.)</span>

<span class="sd">            .. table</span>
<span class="sd">                +===========+=================+</span>
<span class="sd">                |start      |Normalized start |</span>
<span class="sd">                +===========+=================+</span>
<span class="sd">                |-(x.ndim+1)| raise ValueError|</span>
<span class="sd">                +-----------+-----------------+</span>
<span class="sd">                |-x.ndim    |0                |</span>
<span class="sd">                +-----------+-----------------+</span>
<span class="sd">                |...        |...              |</span>
<span class="sd">                +-----------+-----------------+</span>
<span class="sd">                |-1         |x.ndim-1         |</span>
<span class="sd">                +-----------+-----------------+</span>
<span class="sd">                |...        |...              |</span>
<span class="sd">                +-----------+-----------------+</span>
<span class="sd">                |x.ndim     |x.ndim           |</span>
<span class="sd">                +-----------+-----------------+</span>
<span class="sd">                |x.ndim+1   |raise ValueError |</span>
<span class="sd">                +===========+=================+</span>
<span class="sd">            ..</span>

<span class="sd">    Returns:</span>
<span class="sd">        Transposed Tensor. Has the same data type as the original tensor `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` or `start` is not integer, or `x` is not tensor.</span>
<span class="sd">        ValueError: If `axis` is not in the range of :math:`[-ndim, ndim-1]` or</span>
<span class="sd">            `start` is not in the range of :math:`[-ndim, ndim]`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.ones((2,3,4))</span>
<span class="sd">        &gt;&gt;&gt; output = np.rollaxis(x, 0, 2)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (3, 2, 4)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;integer argument expected, but got &quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;integer argument expected, but got &quot;</span><span class="p">,</span> <span class="n">start</span><span class="p">)</span>

    <span class="n">shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tuple_len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>

    <span class="n">axis</span> <span class="o">=</span> <span class="n">_check_axes_range</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">_check_start_normalize</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">start</span> <span class="o">-</span> <span class="n">axis</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">start</span> <span class="o">-</span> <span class="n">axis</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span>
    <span class="n">perm</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">make_range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
    <span class="n">new_perm</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">start</span> <span class="o">&lt;</span> <span class="n">axis</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">&lt;</span> <span class="n">ndim</span><span class="p">:</span>
            <span class="n">new_perm</span> <span class="o">=</span> <span class="n">perm</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">start</span><span class="p">]</span> <span class="o">+</span> <span class="n">perm</span><span class="p">[</span><span class="n">axis</span><span class="p">:</span><span class="n">axis</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> \
                <span class="n">perm</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">axis</span><span class="p">]</span> <span class="o">+</span> <span class="n">perm</span><span class="p">[</span><span class="n">axis</span><span class="o">+</span><span class="mi">1</span><span class="p">:]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">new_perm</span> <span class="o">=</span> <span class="n">perm</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">start</span><span class="p">]</span> <span class="o">+</span> <span class="n">perm</span><span class="p">[</span><span class="n">axis</span><span class="p">:</span><span class="n">axis</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">perm</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">axis</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">start</span> <span class="o">&gt;</span> <span class="n">axis</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">start</span> <span class="o">&lt;</span> <span class="n">ndim</span><span class="p">:</span>
            <span class="n">new_perm</span> <span class="o">=</span> <span class="n">perm</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">axis</span><span class="p">]</span> <span class="o">+</span> <span class="n">perm</span><span class="p">[</span><span class="n">axis</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">start</span><span class="p">]</span> <span class="o">+</span> \
                <span class="n">perm</span><span class="p">[</span><span class="n">axis</span><span class="p">:</span><span class="n">axis</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">perm</span><span class="p">[</span><span class="n">start</span><span class="p">:]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">new_perm</span> <span class="o">=</span> <span class="n">perm</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">axis</span><span class="p">]</span> <span class="o">+</span> <span class="n">perm</span><span class="p">[</span><span class="n">axis</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">start</span><span class="p">]</span> <span class="o">+</span> \
                <span class="n">perm</span><span class="p">[</span><span class="n">axis</span><span class="p">:</span><span class="n">axis</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">new_perm</span><span class="p">)</span></div>


<div class="viewcode-block" id="swapaxes"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.swapaxes.html#mindspore.numpy.swapaxes">[docs]</a><span class="k">def</span> <span class="nf">swapaxes</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis1</span><span class="p">,</span> <span class="n">axis2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Interchanges two axes of a tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): A tensor to be transposed.</span>
<span class="sd">        axis1 (int): First axis.</span>
<span class="sd">        axis2 (int): Second axis.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Transposed tensor, has the same data type as the original tensor `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis1` or `axis2` is not integer, or `x` is not tensor.</span>
<span class="sd">        ValueError: If `axis1` or `axis2` is not in the range of :math:`[-ndim, ndim-1]`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.ones((2,3,4))</span>
<span class="sd">        &gt;&gt;&gt; output = np.swapaxes(x, 0, 2)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (4,3,2)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">axis1</span><span class="p">,</span> <span class="n">axis2</span><span class="p">)</span></div>


<div class="viewcode-block" id="reshape"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.reshape.html#mindspore.numpy.reshape">[docs]</a><span class="k">def</span> <span class="nf">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">new_shape</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reshapes a tensor without changing its data.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): A tensor to be reshaped.</span>
<span class="sd">        new_shape (Union[int, list(int), tuple(int)]): The new shape should be</span>
<span class="sd">            compatible with the original shape. If the tuple has only one element,</span>
<span class="sd">            the result will be a 1-D tensor of that length. One shape dimension</span>
<span class="sd">            can be :math:`-1`. In this case, the value is inferred from the length of</span>
<span class="sd">            the tensor and remaining dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Reshaped Tensor. Has the same data type as the original tensor `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If new_shape is not integer, list or tuple, or `x` is not tensor.</span>
<span class="sd">        ValueError: If new_shape is not compatible with the original shape.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.asarray([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]])</span>
<span class="sd">        &gt;&gt;&gt; output = np.reshape(x, (3, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-0.1  0.3]</span>
<span class="sd">         [ 3.6  0.4]</span>
<span class="sd">         [ 0.5 -3.2]]</span>
<span class="sd">        &gt;&gt;&gt; output = np.reshape(x, (3, -1))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-0.1  0.3]</span>
<span class="sd">         [ 3.6  0.4]</span>
<span class="sd">         [ 0.5 -3.2]]</span>
<span class="sd">        &gt;&gt;&gt; output = np.reshape(x, (6, ))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-0.1  0.3  3.6  0.4  0.5 -3.2]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">new_shape</span><span class="p">)</span></div>


<div class="viewcode-block" id="ravel"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.ravel.html#mindspore.numpy.ravel">[docs]</a><span class="k">def</span> <span class="nf">ravel</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a contiguous flattened tensor.</span>

<span class="sd">    A 1-D tensor, containing the elements of the input, is returned.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): A tensor to be flattened.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Flattened tensor, has the same data type as the original tensor `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.ones((2,3,4))</span>
<span class="sd">        &gt;&gt;&gt; output = np.ravel(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (24,)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span></div>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_move_axes_for_concatenate</span><span class="p">(</span><span class="n">arr_shape</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Moves axis 0 to the desiganated position, while keeps other axes&#39; relative</span>
<span class="sd">    positions unchanged, only used if a single tensor is concatenated.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">original_axes</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">arr_shape</span><span class="p">)))</span>
    <span class="n">new_axes</span> <span class="o">=</span> <span class="n">original_axes</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="n">axis</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">+</span> <span class="n">original_axes</span><span class="p">[</span><span class="n">axis</span><span class="o">+</span><span class="mi">1</span><span class="p">:]</span>
    <span class="n">new_shape</span> <span class="o">=</span> <span class="n">arr_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="n">axis</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">arr_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">arr_shape</span><span class="p">[</span><span class="n">axis</span><span class="o">+</span><span class="mi">1</span><span class="p">],)</span> <span class="o">+</span> \
        <span class="n">arr_shape</span><span class="p">[</span><span class="n">axis</span><span class="o">+</span><span class="mi">2</span><span class="p">:]</span>
    <span class="k">return</span> <span class="n">new_axes</span><span class="p">,</span> <span class="n">new_shape</span>


<span class="k">def</span> <span class="nf">_promote_type_for_concatenate</span><span class="p">(</span><span class="n">tuple_of_tensors</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks dtype for all tensors in the tuple. If dtypes are not the same, promote</span>
<span class="sd">    them to the `highest` dtype in the tuple, so that they are ready for the concat</span>
<span class="sd">    operator.</span>

<span class="sd">    Args:</span>
<span class="sd">        tuple_of_tensors(tuple(tensor)): A tuple of tensors</span>

<span class="sd">    Returns:</span>
<span class="sd">        tuple of tensors, with each tensor promoted to ths same dtype.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">need_cast</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">final_type</span> <span class="o">=</span> <span class="n">tuple_of_tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span>

    <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tuple_of_tensors</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">final_type</span><span class="p">,</span> <span class="n">tensor</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
            <span class="n">need_cast</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">final_type</span> <span class="o">=</span> <span class="n">_promote</span><span class="p">(</span><span class="n">final_type</span><span class="p">,</span> <span class="n">tensor</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">need_cast</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">tuple_of_tensors</span>
    <span class="n">tuple_of_casted_tensors</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tuple_of_tensors</span><span class="p">:</span>
        <span class="n">tuple_of_casted_tensors</span> <span class="o">+=</span> <span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">final_type</span><span class="p">,</span> <span class="kc">False</span><span class="p">),)</span>
    <span class="k">return</span> <span class="n">tuple_of_casted_tensors</span>


<div class="viewcode-block" id="concatenate"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.concatenate.html#mindspore.numpy.concatenate">[docs]</a><span class="k">def</span> <span class="nf">concatenate</span><span class="p">(</span><span class="n">arrays</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Joins a sequence of tensors along an existing axis.</span>

<span class="sd">    Note:</span>
<span class="sd">        To match Numpy behaviour, :math:`axis &gt;= 32` will not cause value error, the</span>
<span class="sd">        `axis` will be treated as :class:`None` instead.</span>

<span class="sd">    Args:</span>
<span class="sd">        arrays (Union[Tensor, tuple(Tensor), list(Tensor)]): a tensor or a list</span>
<span class="sd">            of tensors to be concatenated.</span>
<span class="sd">        axis (Union[None, int], optional): The axis along which the tensors will be joined,</span>
<span class="sd">            if `axis` is :class:`None`, tensors are flattened before use. Default is 0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tensor concatenated from a tensor or a list of tensors.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If input arguments have types not specified above.</span>
<span class="sd">        ValueError: If `axis` is not in the range of :math:`[-ndim, ndim-1]`, and less than 32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x1 = np.ones((1,2,3))</span>
<span class="sd">        &gt;&gt;&gt; x2 = np.ones((1,2,1))</span>
<span class="sd">        &gt;&gt;&gt; x = np.concatenate((x1, x2), axis=-1)</span>
<span class="sd">        &gt;&gt;&gt; print(x.shape)</span>
<span class="sd">        (1, 2, 4)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arrays</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="c1"># if only one tensor is provided, it is treated as a tuple along the</span>
        <span class="c1"># first dimension. For example, a tensor of shape (3,4,5) will be treated</span>
        <span class="c1"># as: tuple(tensor_1(4,5), tensor_2(4,5), tensor_3(4,5))</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">axis</span> <span class="o">&gt;=</span> <span class="n">MAX_NUMPY_DIMS</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">ravel</span><span class="p">(</span><span class="n">arrays</span><span class="p">)</span>
        <span class="n">arr_shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">arrays</span><span class="p">)</span>
        <span class="n">_check_axes_range</span><span class="p">((</span><span class="n">axis</span><span class="p">,),</span> <span class="nb">len</span><span class="p">(</span><span class="n">arr_shape</span><span class="p">))</span>
        <span class="c1"># move axis 0 to the disiganated position, while keep other axes&#39; relative</span>
        <span class="c1"># positions unchanged</span>
        <span class="n">new_axes</span><span class="p">,</span> <span class="n">new_shape</span> <span class="o">=</span> <span class="n">_move_axes_for_concatenate</span><span class="p">(</span><span class="n">arr_shape</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
        <span class="n">arrays</span> <span class="o">=</span> <span class="n">transpose</span><span class="p">(</span><span class="n">arrays</span><span class="p">,</span> <span class="n">new_axes</span><span class="p">)</span>
        <span class="n">arrays</span> <span class="o">=</span> <span class="n">reshape</span><span class="p">(</span><span class="n">arrays</span><span class="p">,</span> <span class="n">new_shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">arrays</span>

    <span class="n">flattened_arrays</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">axis</span> <span class="o">&gt;=</span> <span class="n">MAX_NUMPY_DIMS</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">arr</span> <span class="ow">in</span> <span class="n">arrays</span><span class="p">:</span>
            <span class="n">flattened_arrays</span> <span class="o">+=</span> <span class="p">(</span><span class="n">ravel</span><span class="p">(</span><span class="n">arr</span><span class="p">),)</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="n">flattened_arrays</span> <span class="o">=</span> <span class="n">_promote_type_for_concatenate</span><span class="p">(</span><span class="n">flattened_arrays</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="n">axis</span><span class="p">)(</span><span class="n">flattened_arrays</span><span class="p">)</span>

    <span class="c1"># convert a list of tensor to a tuple of tensor</span>
    <span class="n">arrays</span> <span class="o">=</span> <span class="n">_convert_list_tensor_to_tuple_tensor</span><span class="p">(</span><span class="n">arrays</span><span class="p">)</span>

    <span class="n">arr_shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">arrays</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">_check_axes_range</span><span class="p">((</span><span class="n">axis</span><span class="p">,),</span> <span class="nb">len</span><span class="p">(</span><span class="n">arr_shape</span><span class="p">))</span>

    <span class="c1"># if only one tensor in the tuple/list, return the tensor itself</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">arrays</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">arrays</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">arrays</span> <span class="o">=</span> <span class="n">_promote_type_for_concatenate</span><span class="p">(</span><span class="n">arrays</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="n">axis</span><span class="p">)(</span><span class="n">arrays</span><span class="p">)</span></div>


<div class="viewcode-block" id="append"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.append.html#mindspore.numpy.append">[docs]</a><span class="k">def</span> <span class="nf">append</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Appends values to the end of a tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        arr (Tensor): Values are appended to a copy of this tensor.</span>
<span class="sd">        values (Tensor): These values are appended to a copy of `arr`. It must be of</span>
<span class="sd">            the correct shape (the same shape as `arr`, excluding `axis`). If `axis` is</span>
<span class="sd">            not specified, `values` can be any shape and will be flattened before use.</span>
<span class="sd">        axis (None, int, optional): The `axis` along which values are appended. If `axis` is not</span>
<span class="sd">            given, both `arr` and `values` are flattened before use, default is :class:`None`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, a copy of tensor with values appended to axis.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If input arguments have types not specified above.</span>
<span class="sd">        ValueError: If specified axis exceeds `arr.ndim`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.ones((2, 3))</span>
<span class="sd">        &gt;&gt;&gt; b = np.ones((2, 1))</span>
<span class="sd">        &gt;&gt;&gt; print(np.append(a, b, axis=1).shape)</span>
<span class="sd">        (2, 4)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">arr</span> <span class="o">=</span> <span class="n">arr</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">_check_axis_in_range</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">arr</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span> <span class="o">!=</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">values</span><span class="p">):</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;all tensors must have same number of dimensions&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">concatenate</span><span class="p">((</span><span class="n">arr</span><span class="p">,</span> <span class="n">values</span><span class="p">),</span> <span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="column_stack"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.column_stack.html#mindspore.numpy.column_stack">[docs]</a><span class="k">def</span> <span class="nf">column_stack</span><span class="p">(</span><span class="n">tup</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Stacks 1-D tensors as columns into a 2-D tensor. 2-D tensors are stacked as-is,</span>
<span class="sd">    like np.hstack.</span>

<span class="sd">    Args:</span>
<span class="sd">        tup (Union[Tensor, tuple, list]): A sequence of 1-D or 2-D tensors. All</span>
<span class="sd">            of them must have the same shape except the axis to be concatenated.</span>

<span class="sd">    Returns:</span>
<span class="sd">        2-D Tensor, formed by stacking the given tensors.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `tup` is not Tensor, list or tuple.</span>
<span class="sd">        ValueError: If `tup` is empty.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x1 = np.array([1, 2, 3]).astype(&#39;int32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; x2 = np.array([4, 5, 6]).astype(&#39;int32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = np.column_stack((x1, x2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1 4]</span>
<span class="sd">         [2 5]</span>
<span class="sd">         [3 6]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tup</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tup</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tup</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;Tensor or, list or tuple of tensors are required, but got &quot;</span><span class="p">,</span> <span class="n">tup</span><span class="p">)</span>

    <span class="n">trans_tup</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tup</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">trans_tup</span> <span class="o">+=</span> <span class="p">(</span><span class="n">tensor</span><span class="p">,)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">trans_tup</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;Need at least one tensor to concatenate.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">trans_tup</span><span class="p">)</span></div>


<div class="viewcode-block" id="vstack"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.vstack.html#mindspore.numpy.vstack">[docs]</a><span class="k">def</span> <span class="nf">vstack</span><span class="p">(</span><span class="n">tup</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Stacks tensors in sequence vertically.</span>
<span class="sd">    This is equivalent to concatenation along the first axis. 1-D tensors should firstly be reshaped to `(1, N)`,</span>
<span class="sd">    and then be concatenated along the first axis.</span>

<span class="sd">    Args:</span>
<span class="sd">        tup (Union[Tensor, tuple, list]): A sequence of 1-D or 2-D tensors. The tensors must have the same shape</span>
<span class="sd">            along all but the first axis. 1-D tensors must have the same shape.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Stacked Tensor, formed by stacking the given tensors.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `tup` is not Tensor, list or tuple.</span>
<span class="sd">        ValueError: If `tup` is empty.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x1 = np.array([1, 2, 3]).astype(&#39;int32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; x2 = np.array([4, 5, 6]).astype(&#39;int32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = np.vstack((x1, x2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1 2 3]</span>
<span class="sd">         [4 5 6]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tup</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tup</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tup</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;Tensor or, list or tuple of tensors are required, but got&quot;</span><span class="p">,</span> <span class="n">tup</span><span class="p">)</span>

    <span class="n">trans_tup</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tup</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">_expand</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">trans_tup</span> <span class="o">+=</span> <span class="p">(</span><span class="n">tensor</span><span class="p">,)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">trans_tup</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;Need at least one tensor to concatenate.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="mi">0</span><span class="p">)(</span><span class="n">trans_tup</span><span class="p">)</span></div>


<div class="viewcode-block" id="hstack"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.hstack.html#mindspore.numpy.hstack">[docs]</a><span class="k">def</span> <span class="nf">hstack</span><span class="p">(</span><span class="n">tup</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Stacks tensors in sequence horizontally.</span>
<span class="sd">    This is equivalent to concatenation along the second axis, except for 1-D tensors</span>
<span class="sd">    where it concatenates along the first axis.</span>

<span class="sd">    Args:</span>
<span class="sd">        tup (Union[Tensor, tuple, list]): A sequence of 1-D or 2-D tensors. The</span>
<span class="sd">            tensors must have the same shape along all but the second axis, except</span>
<span class="sd">            1-D tensors which can be any length.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Stacked Tensor, formed by stacking the given tensors.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `tup` is not Tensor, list or tuple.</span>
<span class="sd">        ValueError: If `tup` is empty.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x1 = np.array([1, 2, 3]).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; x2 = np.array([4, 5, 6]).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = np.hstack((x1, x2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 2. 3. 4. 5. 6.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tup</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tup</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tup</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;Tensor or, list or tuple of tensors are required, but got&quot;</span><span class="p">,</span> <span class="n">tup</span><span class="p">)</span>

    <span class="n">tuple_of_tensor</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tup</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">tuple_of_tensor</span> <span class="o">+=</span> <span class="p">(</span><span class="n">tensor</span><span class="p">,)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">tuple_of_tensor</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;Need at least one tensor to concatenate.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">tuple_of_tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="mi">0</span><span class="p">)(</span><span class="n">tuple_of_tensor</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">tuple_of_tensor</span><span class="p">)</span></div>


<div class="viewcode-block" id="dstack"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.dstack.html#mindspore.numpy.dstack">[docs]</a><span class="k">def</span> <span class="nf">dstack</span><span class="p">(</span><span class="n">tup</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Stacks tensors in sequence depth wise (along the third axis).</span>
<span class="sd">    This is equivalent to concatenation along the third axis. 1-D tensors :math:`(N,)` should be</span>
<span class="sd">    reshaped to :math:`(1,N,1)`.</span>
<span class="sd">    2-D tensors :math:`(M,N)` should be reshaped to :math:`(M,N,1)` before concatenation.</span>

<span class="sd">    Args:</span>
<span class="sd">        tup (Union[Tensor, tuple, list]): A sequence of tensors. The tensors must have the same shape along all but</span>
<span class="sd">            the third axis. 1-D or 2-D tensors must have the same shape.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Stacked Tensor, formed by stacking the given tensors.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `tup` is not Tensor, list or tuple.</span>
<span class="sd">        ValueError: If `tup` is empty.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x1 = np.array([1, 2, 3]).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; x2 = np.array([4, 5, 6]).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = np.dstack((x1, x2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1. 4.]</span>
<span class="sd">          [2. 5.]</span>
<span class="sd">          [3. 6.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tup</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tup</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tup</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;Tensor or list or tuple of tensors are required, but got&quot;</span><span class="p">,</span> <span class="n">tup</span><span class="p">)</span>

    <span class="n">trans_tup</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tup</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">_expand</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">trans_tup</span> <span class="o">+=</span> <span class="p">(</span><span class="n">tensor</span><span class="p">,)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">trans_tup</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;Need at least one tensor to concatenate.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="mi">2</span><span class="p">)(</span><span class="n">trans_tup</span><span class="p">)</span></div>


<div class="viewcode-block" id="where"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.where.html#mindspore.numpy.where">[docs]</a><span class="k">def</span> <span class="nf">where</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns elements chosen from `x` or `y` depending on `condition`.</span>

<span class="sd">    Note:</span>
<span class="sd">        As nonzero is not supported, both `x` and `y` must be provided Tensor</span>
<span class="sd">    input.</span>

<span class="sd">    Args:</span>
<span class="sd">        condition (Tensor): where True, yield `x`, otherwise yield `y`.</span>
<span class="sd">        x (Tensor): Values from which to choose. Defaults to None.</span>
<span class="sd">        y (Tensor): Values from which to choose. `x`, `y` and `condition` need</span>
<span class="sd">            to be broadcastable to some shape. Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, with elements from `x` where `condition` is True, and</span>
<span class="sd">        elements from `y` elsewhere.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If operands cannot be broadcast.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; condition = np.full((1, 1, 2), [False, True])</span>
<span class="sd">        &gt;&gt;&gt; x = np.full((1, 3, 2), 5)</span>
<span class="sd">        &gt;&gt;&gt; y = np.full((2, 1, 1), 7)</span>
<span class="sd">        &gt;&gt;&gt; output = np.where(condition, x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[7 5]</span>
<span class="sd">        [7 5]</span>
<span class="sd">        [7 5]]</span>
<span class="sd">        [[7 5]</span>
<span class="sd">        [7 5]</span>
<span class="sd">        [7 5]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">condition</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="c1"># type promotes input tensors</span>
    <span class="n">dtype1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">dtype2</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">_promote</span><span class="p">(</span><span class="n">dtype1</span><span class="p">,</span> <span class="n">dtype2</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">dtype1</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">dtype2</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="n">is_bool</span> <span class="o">=</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">dtype1</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span> <span class="ow">and</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">dtype2</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_bool</span><span class="p">:</span>
        <span class="c1"># select does not support bool type for x or y</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">condition</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
        <span class="c1"># tiling with bool is not supported on GPU</span>
        <span class="n">condition</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">dynamic</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">is_sequence_value_unknown</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">condition</span><span class="p">))</span> <span class="ow">or</span> <span class="n">F</span><span class="o">.</span><span class="n">is_sequence_value_unknown</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>\
              <span class="ow">or</span> <span class="n">F</span><span class="o">.</span><span class="n">is_sequence_value_unknown</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
    <span class="c1"># As select op currently does not support broadcast, broadcasts input tensors</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">dynamic</span><span class="p">:</span>
        <span class="n">shape_out</span> <span class="o">=</span> <span class="n">_infer_out_shape</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">condition</span><span class="p">),</span>
                                     <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
        <span class="n">condition</span> <span class="o">=</span> <span class="n">_broadcast_to_shape</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">shape_out</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">_broadcast_to_shape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">shape_out</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">_broadcast_to_shape</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">shape_out</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Get the broadcast shape through broadcast calculation</span>
        <span class="n">add_x_y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
        <span class="n">add_out</span> <span class="o">=</span> <span class="n">condition</span> <span class="o">+</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">add_x_y</span><span class="p">,</span> <span class="n">condition</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">shape_out</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">()(</span><span class="n">add_out</span><span class="p">)</span>
        <span class="n">condition</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">shape_out</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">shape_out</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">shape_out</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">condition</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">):</span>
        <span class="n">condition</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_bool</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span></div>


<span class="k">def</span> <span class="nf">_atleast_xd</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">arys</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns arys with at least ndim.&quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="o">*</span><span class="n">arys</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">arr</span> <span class="ow">in</span> <span class="n">arys</span><span class="p">:</span>
        <span class="n">arr</span> <span class="o">=</span> <span class="n">_expand</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
        <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">res</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">res</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">res</span>


<div class="viewcode-block" id="atleast_1d"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.atleast_1d.html#mindspore.numpy.atleast_1d">[docs]</a><span class="k">def</span> <span class="nf">atleast_1d</span><span class="p">(</span><span class="o">*</span><span class="n">arys</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts inputs to arrays with at least one dimension.</span>

<span class="sd">    Scalar inputs are converted to 1-dimensional arrays, whilst</span>
<span class="sd">    higher-dimensional inputs are preserved.</span>

<span class="sd">    Note:</span>
<span class="sd">        In graph mode, returns a tuple of tensor instead of a list of</span>
<span class="sd">        tensors.</span>

<span class="sd">    Args:</span>
<span class="sd">        *arys (Tensor): one or more input tensors.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, or list of tensors, each with ``a.ndim &gt;= 1``.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input is not a tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.ones((2, 3))</span>
<span class="sd">        &gt;&gt;&gt; b = np.ones(())</span>
<span class="sd">        &gt;&gt;&gt; c = np.ones(5)</span>
<span class="sd">        &gt;&gt;&gt; output = np.atleast_1d(a, b, c)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">            [Tensor(shape=[2, 3], dtype=Float32, value=</span>
<span class="sd">            [[1.00000000e+00, 1.00000000e+00, 1.00000000e+00],</span>
<span class="sd">            [1.00000000e+00, 1.00000000e+00, 1.00000000e+00]]),</span>
<span class="sd">            Tensor(shape=[1], dtype=Float32, value= [1.00000000e+00]),</span>
<span class="sd">            Tensor(shape=[5], dtype=Float32,</span>
<span class="sd">            value= [1.00000000e+00, 1.00000000e+00, 1.00000000e+00,</span>
<span class="sd">            1.00000000e+00, 1.00000000e+00])]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_atleast_xd</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">arys</span><span class="p">)</span></div>


<div class="viewcode-block" id="atleast_2d"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.atleast_2d.html#mindspore.numpy.atleast_2d">[docs]</a><span class="k">def</span> <span class="nf">atleast_2d</span><span class="p">(</span><span class="o">*</span><span class="n">arys</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reshapes inputs as arrays with at least two dimensions.</span>

<span class="sd">    Note:</span>
<span class="sd">        In graph mode, returns a tuple of tensor instead of a list of</span>
<span class="sd">        tensors.</span>
<span class="sd">    Args:</span>
<span class="sd">        *arys (Tensor): one or more input tensors.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, or list of tensors, each with ``a.ndim &gt;= 2``.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input is not a tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.ones((2, 3))</span>
<span class="sd">        &gt;&gt;&gt; b = np.ones(())</span>
<span class="sd">        &gt;&gt;&gt; c = np.ones(5)</span>
<span class="sd">        &gt;&gt;&gt; output = np.atleast_2d(a, b, c)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">            [Tensor(shape=[2, 3], dtype=Float32, value=</span>
<span class="sd">            [[1.00000000e+00, 1.00000000e+00, 1.00000000e+00],</span>
<span class="sd">            [1.00000000e+00, 1.00000000e+00, 1.00000000e+00]]),</span>
<span class="sd">            Tensor(shape=[1, 1], dtype=Float32, value= [[1.00000000e+00]]),</span>
<span class="sd">            Tensor(shape=[1, 5], dtype=Float32,</span>
<span class="sd">            value= [[1.00000000e+00, 1.00000000e+00, 1.00000000e+00,</span>
<span class="sd">            1.00000000e+00, 1.00000000e+00]])]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_atleast_xd</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">arys</span><span class="p">)</span></div>


<div class="viewcode-block" id="atleast_3d"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.atleast_3d.html#mindspore.numpy.atleast_3d">[docs]</a><span class="k">def</span> <span class="nf">atleast_3d</span><span class="p">(</span><span class="o">*</span><span class="n">arys</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reshapes inputs as arrays with at least three dimensions.</span>

<span class="sd">    Note:</span>
<span class="sd">        In graph mode, returns a tuple of tensor instead of a list of</span>
<span class="sd">        tensors.</span>

<span class="sd">    Args:</span>
<span class="sd">        *arys (Tensor): one or more input tensors.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, or list of tensors, each with ``a.ndim &gt;= 3``. For example,</span>
<span class="sd">        a 1-D array of shape `(N,)` becomes a tensor of shape `(1, N, 1)`, and</span>
<span class="sd">        a 2-D array of shape `(M, N)` becomes a tensor of shape `(M, N, 1)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input is not a tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.ones((2, 3))</span>
<span class="sd">        &gt;&gt;&gt; b = np.ones(())</span>
<span class="sd">        &gt;&gt;&gt; c = np.ones(5)</span>
<span class="sd">        &gt;&gt;&gt; output = np.atleast_3d(a, b, c)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">            [Tensor(shape=[2, 3, 1], dtype=Float32, value=</span>
<span class="sd">            [[[1.00000000e+00], [1.00000000e+00], [1.00000000e+00]],</span>
<span class="sd">            [[1.00000000e+00], [1.00000000e+00], [1.00000000e+00]]]),</span>
<span class="sd">            Tensor(shape=[1, 1, 1], dtype=Float32, value= [[[1.00000000e+00]]]),</span>
<span class="sd">            Tensor(shape=[1, 5, 1], dtype=Float32,</span>
<span class="sd">            value= [[[1.00000000e+00], [1.00000000e+00], [1.00000000e+00],</span>
<span class="sd">            [1.00000000e+00], [1.00000000e+00]]])]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">res</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">arr</span> <span class="ow">in</span> <span class="n">arys</span><span class="p">:</span>
        <span class="n">ndim</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">arr</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="k">elif</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">arr</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">arr</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span>
        <span class="k">elif</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">arr</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
        <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">res</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">res</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">res</span></div>


<div class="viewcode-block" id="stack"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.stack.html#mindspore.numpy.stack">[docs]</a><span class="k">def</span> <span class="nf">stack</span><span class="p">(</span><span class="n">arrays</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Joins a sequence of arrays along a new axis.</span>

<span class="sd">    The `axis` parameter specifies the index of the new axis in the</span>
<span class="sd">    dimensions of the result. For example, if ``axis=0`` it will be the</span>
<span class="sd">    first dimension and if ``axis=-1`` it will be the last dimension.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy argument out is not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        arrays (sequence of Tensor): Each array must have the same shape.</span>
<span class="sd">        axis (int, optional): The axis in the result array along which the</span>
<span class="sd">            input arrays are stacked. Default: 0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, The stacked array has one more dimension than the input</span>
<span class="sd">        arrays.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If input is not Tensor, tuple, or list.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; arrays = [np.ones((3, 4)) for _ in range(10)]</span>
<span class="sd">        &gt;&gt;&gt; output = np.stack(arrays, axis=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (10, 3, 4)</span>
<span class="sd">        &gt;&gt;&gt; output = np.stack(arrays, axis=1)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (3, 10, 4)</span>
<span class="sd">        &gt;&gt;&gt; output = np.stack(arrays, axis=2)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (3, 4, 10)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arrays</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">arrays</span><span class="p">)</span>
        <span class="n">ndim</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">arrays</span><span class="p">)</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span> <span class="o">%</span> <span class="n">ndim</span>
        <span class="n">axes</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">make_range</span><span class="p">(</span><span class="n">ndim</span><span class="p">)</span>
        <span class="n">perm</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="n">axis</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">+</span> <span class="n">axes</span><span class="p">[</span><span class="n">axis</span><span class="o">+</span><span class="mi">1</span><span class="p">:]</span>
        <span class="k">if</span> <span class="n">_is_shape_empty</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">_empty</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="n">axis</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],)</span> <span class="o">+</span> <span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="o">+</span><span class="mi">1</span><span class="p">:])</span>
        <span class="k">return</span> <span class="n">transpose</span><span class="p">(</span><span class="n">arrays</span><span class="p">,</span> <span class="n">perm</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arrays</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">arrays</span><span class="p">),)</span> <span class="o">+</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">arrays</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">ndim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span> <span class="o">%</span> <span class="n">ndim</span>
        <span class="k">if</span> <span class="n">_is_shape_empty</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">_empty</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="n">axis</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],)</span> <span class="o">+</span> <span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="o">+</span><span class="mi">1</span><span class="p">:])</span>
        <span class="n">seq</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">for</span> <span class="n">arr</span> <span class="ow">in</span> <span class="n">arrays</span><span class="p">:</span>
            <span class="n">seq</span> <span class="o">+=</span> <span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">axis</span><span class="p">),)</span>
        <span class="k">return</span> <span class="n">concatenate</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;input arrays must be Tensor, tuple, or list&#39;</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">UniqueNet</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The operation is wrapped inside a model. &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">UniqueNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">unique</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Unique</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<div class="viewcode-block" id="unique"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.unique.html#mindspore.numpy.unique">[docs]</a><span class="k">def</span> <span class="nf">unique</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">return_inverse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Finds the unique elements of a tensor. The input tensor will be flattened first</span>
<span class="sd">    when it has more than one dimension.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `axis`, `return_index` and `return_counts` are not supported.</span>
<span class="sd">        On CPU, this operator must be executed in graph mode.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor to be processed.</span>
<span class="sd">        return_inverse (bool): If `True`, also return the indices of the unique tensor.</span>
<span class="sd">            Default: `False`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or tuple of Tensors.</span>
<span class="sd">        If `return_inverse` is `False`, return the unique tensor, otherwise return tuple of tensors.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not tensor.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(mode=ms.GRAPH_MODE)</span>
<span class="sd">        &gt;&gt;&gt; input_x = np.asarray([1, 2, 2, 2, 3, 4, 5]).astype(&#39;int32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output_x = np.unique(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output_x)</span>
<span class="sd">        [1 2 3 4 5]</span>
<span class="sd">        &gt;&gt;&gt; output_x = np.unique(input_x, return_inverse=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output_x)</span>
<span class="sd">        (Tensor(shape=[5], dtype=Int32, value= [ 1, 2, 3, 4, 5]), Tensor(shape=[7], dtype=Int32,</span>
<span class="sd">            value= [0, 1, 1, 1, 2, 3, 4]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">F</span><span class="o">.</span><span class="n">tuple_len</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">ravel</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">uniq</span> <span class="o">=</span> <span class="n">UniqueNet</span><span class="p">()</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">uniq</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">return_inverse</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">res</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">res</span></div>


<span class="k">def</span> <span class="nf">roll_along_axis</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">shift</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Rolls a tensor along a given axis. This is a helper function of np.roll.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): Input tensor.</span>
<span class="sd">        shift (int): The number of places the tensor is shifted.</span>
<span class="sd">        axis (int): The designated axis for shifting.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Shifted tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_axis_in_range</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="n">_check_element_int</span><span class="p">((</span><span class="n">shift</span><span class="p">,</span> <span class="n">axis</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">axis</span> <span class="o">+=</span> <span class="n">a</span><span class="o">.</span><span class="n">ndim</span>
    <span class="n">shift</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">shift</span> <span class="o">%</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">])</span>
    <span class="c1"># if shift is 0, we do not need to roll at all</span>
    <span class="k">if</span> <span class="n">shift</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">a</span>
    <span class="n">begin1</span> <span class="o">=</span> <span class="p">()</span>
    <span class="n">begin2</span> <span class="o">=</span> <span class="p">()</span>
    <span class="n">end1</span> <span class="o">=</span> <span class="p">()</span>
    <span class="n">end2</span> <span class="o">=</span> <span class="p">()</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">F</span><span class="o">.</span><span class="n">make_range</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">ndim</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">axis</span><span class="p">:</span>
            <span class="n">begin1</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span>
            <span class="n">end1</span> <span class="o">+=</span> <span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">],)</span>
            <span class="n">begin2</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span>
            <span class="n">end2</span> <span class="o">+=</span> <span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">],)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">begin1</span> <span class="o">+=</span> <span class="p">(</span><span class="n">shift</span><span class="p">,)</span>
            <span class="n">end1</span> <span class="o">+=</span> <span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">],)</span>
            <span class="n">begin2</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span>
            <span class="n">end2</span> <span class="o">+=</span> <span class="p">(</span><span class="n">shift</span><span class="p">,)</span>
    <span class="k">return</span> <span class="n">append</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">strided_slice</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">begin1</span><span class="p">,</span> <span class="n">end1</span><span class="p">,</span> <span class="n">stride</span><span class="p">),</span>
                  <span class="n">F</span><span class="o">.</span><span class="n">strided_slice</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">begin2</span><span class="p">,</span> <span class="n">end2</span><span class="p">,</span> <span class="n">stride</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>


<div class="viewcode-block" id="roll"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.roll.html#mindspore.numpy.roll">[docs]</a><span class="k">def</span> <span class="nf">roll</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">shift</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Rolls a tensor along given axes.</span>

<span class="sd">    Elements that rolls beyond the last position are re-introduced at the first.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): Input tensor.</span>
<span class="sd">        shift (Union[int, tuple(int)]): The number of places by which elements are</span>
<span class="sd">            shifted. If a tuple, then axis must be a tuple of the same size, and</span>
<span class="sd">            each of the given axes is shifted by the corresponding number. If shift</span>
<span class="sd">            is an int while axis is a tuple of integers, then the same value is used</span>
<span class="sd">            for all given axes.</span>
<span class="sd">        axis (Union[int, tuple(int)], optional): Axis or axes along which elements</span>
<span class="sd">            are shifted. By default, the array is flattened before shifting, after</span>
<span class="sd">            which the original shape is restored. Default: None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same shape as a.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If input arguments have types not specified above.</span>
<span class="sd">        ValueError: If axis exceeds `a.ndim`, or `shift` and `axis` cannot broadcast.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.reshape(np.arange(12), (3, 4))</span>
<span class="sd">        &gt;&gt;&gt; print(np.roll(a, [2,-3], [0,-1]))</span>
<span class="sd">            [[ 7  4  5  6]</span>
<span class="sd">             [11  8  9 10]</span>
<span class="sd">             [ 3  0  1  2]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">original_shape</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">original_dtype</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">restore_shape</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="c1"># F.strided_slice only supports float on cpu, this will change once more supports</span>
    <span class="c1"># are added.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_is_float</span><span class="p">(</span><span class="n">original_dtype</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">original_dtype</span> <span class="ow">in</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">complex64</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">complex128</span><span class="p">):</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">restore_shape</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="c1"># Broadcast shift and axis to the same length</span>
    <span class="n">shift</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="n">_broadcast_tuples</span><span class="p">(</span><span class="n">shift</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">shift_each</span><span class="p">,</span> <span class="n">axis_each</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">shift</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">roll_along_axis</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">shift_each</span><span class="p">,</span> <span class="n">axis_each</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">restore_shape</span><span class="p">:</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">original_shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_is_float</span><span class="p">(</span><span class="n">original_dtype</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">original_dtype</span> <span class="ow">in</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">complex64</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">complex128</span><span class="p">):</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">original_dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">a</span></div>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_get_moved_perm</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">destination</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper function for moveaxis, returns permutation after moving axes</span>
<span class="sd">    from source to destination.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dest_sorted_idx</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">destination</span><span class="p">),</span> <span class="n">key</span><span class="o">=</span><span class="n">operator</span><span class="o">.</span><span class="n">itemgetter</span><span class="p">(</span><span class="mi">1</span><span class="p">))]</span>
    <span class="n">axes_orig</span> <span class="o">=</span> <span class="n">mutable</span><span class="p">([],</span> <span class="kc">True</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndim</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">source</span><span class="p">:</span>
            <span class="n">axes_orig</span> <span class="o">=</span> <span class="n">axes_orig</span> <span class="o">+</span> <span class="p">[</span><span class="n">i</span><span class="p">]</span>

    <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">m</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">perm</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">dest_sorted_idx</span><span class="p">:</span>
        <span class="c1"># inserts an axis that has been moved, denoted by n, and axes that remain</span>
        <span class="c1"># in their original position, indexed from k to k + n - m, into index m in</span>
        <span class="c1"># the list of permuted axes</span>
        <span class="n">n</span> <span class="o">=</span> <span class="n">destination</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">j</span> <span class="o">=</span> <span class="n">k</span> <span class="o">+</span> <span class="n">n</span> <span class="o">-</span> <span class="n">m</span>
        <span class="n">perm</span> <span class="o">+=</span> <span class="n">axes_orig</span><span class="p">[</span><span class="n">k</span><span class="p">:</span><span class="n">j</span><span class="p">]</span>
        <span class="n">perm</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">source</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="n">k</span> <span class="o">+=</span> <span class="n">n</span> <span class="o">-</span> <span class="n">m</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">perm</span> <span class="o">+=</span> <span class="n">axes_orig</span><span class="p">[</span><span class="n">k</span><span class="p">:]</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">perm</span><span class="p">)</span>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_get_moved_shape</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">perm</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper function for moveaxis, returns the permuated shape after</span>
<span class="sd">    applying perm.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">([</span><span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">perm</span><span class="p">])</span>


<div class="viewcode-block" id="moveaxis"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.moveaxis.html#mindspore.numpy.moveaxis">[docs]</a><span class="k">def</span> <span class="nf">moveaxis</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">destination</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Moves axes of an array to new positions.</span>

<span class="sd">    Other axes remain in their original order.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): The array whose axes should be reordered.</span>
<span class="sd">        source (int or sequence of ints): Original positions of the</span>
<span class="sd">            axes to move. These must be unique.</span>
<span class="sd">        destination (int or sequence of ints): Destination positions</span>
<span class="sd">            for each of the original axes. These must also be unique.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, array with moved axes.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If axes are out of the range of ``[-a.ndim, a.ndim)``, or</span>
<span class="sd">            if the axes contain duplicates.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.zeros((3, 4, 5))</span>
<span class="sd">        &gt;&gt;&gt; output = np.moveaxis(x, 0, -1)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (4, 5, 3)</span>
<span class="sd">        &gt;&gt;&gt; output = np.moveaxis(x, -1, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (5, 3, 4)</span>
<span class="sd">        &gt;&gt;&gt; output = np.moveaxis(x, [0, 1, 2], [-1, -2, -3])</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (5, 4, 3)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">source</span> <span class="o">=</span> <span class="n">_check_axis_valid</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
    <span class="n">destination</span> <span class="o">=</span> <span class="n">_check_axis_valid</span><span class="p">(</span><span class="n">destination</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">source</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">destination</span><span class="p">):</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;`source` and `destination` arguments must have the same number of elements&#39;</span><span class="p">)</span>
    <span class="n">perm</span> <span class="o">=</span> <span class="n">_get_moved_perm</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">destination</span><span class="p">)</span>

    <span class="n">shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_is_shape_empty</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_empty</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">_get_moved_shape</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">perm</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">perm</span><span class="p">)</span></div>


<div class="viewcode-block" id="tile"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.tile.html#mindspore.numpy.tile">[docs]</a><span class="k">def</span> <span class="nf">tile</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">reps</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs an array by repeating `a` the number of times given by `reps`.</span>

<span class="sd">    If `reps` has length `d`, the result will have dimension of ``max(d, a.ndim)``.</span>
<span class="sd">    If ``a.ndim &lt; d``, `a` is promoted to be d-dimensional by prepending new axes.</span>
<span class="sd">    So a shape (3,) array is promoted to (1, 3) for 2-D replication, or</span>
<span class="sd">    shape (1, 1, 3) for 3-D replication. If this is not the desired behavior,</span>
<span class="sd">    promote `a` to d-dimensions manually before calling this function.</span>
<span class="sd">    If ``a.ndim &gt; d``, `reps` is promoted to ``a.ndim`` by pre-pending 1&#39;s to it. Thus</span>
<span class="sd">    for an `a` of shape (2, 3, 4, 5), a `reps` of (2, 2) is treated as (1, 1, 2, 2).</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): The input array.</span>
<span class="sd">        reps (int or sequence of ints): The number of repetitions of `a` along</span>
<span class="sd">            each axis.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the tiled output array.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input is not a tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.array([0, 1, 2])</span>
<span class="sd">        &gt;&gt;&gt; output = np.tile(a, 2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0 1 2 0 1 2]</span>
<span class="sd">        &gt;&gt;&gt; output = np.tile(a, (2, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0 1 2 0 1 2]</span>
<span class="sd">        [0 1 2 0 1 2]]</span>
<span class="sd">        &gt;&gt;&gt; output = np.tile(a, (2, 1, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[0 1 2 0 1 2]]</span>
<span class="sd">        [[0 1 2 0 1 2]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">reps</span> <span class="o">=</span> <span class="n">_add_unit_axes</span><span class="p">(</span><span class="n">reps</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_is_shape_empty</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="ow">or</span> <span class="n">_is_shape_empty</span><span class="p">(</span><span class="n">reps</span><span class="p">):</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">_add_unit_axes</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">reps</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">_empty</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">_seq_prod</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">reps</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">reps</span><span class="p">)</span></div>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_check_can_broadcast_to</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">target_shape</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Determines if shape can be broadcast to target_shape.&quot;&quot;&quot;</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">ndim_target</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">target_shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ndim</span> <span class="o">&gt;</span> <span class="n">ndim_target</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">shape</span><span class="p">),</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">target_shape</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">j</span><span class="p">):</span>
            <span class="k">return</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="kc">True</span>


<div class="viewcode-block" id="broadcast_to"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.broadcast_to.html#mindspore.numpy.broadcast_to">[docs]</a><span class="k">def</span> <span class="nf">broadcast_to</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Broadcasts an array to a new shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        array (Tensor): The array to broadcast.</span>
<span class="sd">        shape (tuple): The shape of the desired array.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, original array broadcast to the given shape.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If array cannot be broadcast to shape.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.array([1, 2, 3])</span>
<span class="sd">        &gt;&gt;&gt; output = np.broadcast_to(x, (3, 3))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1 2 3]</span>
<span class="sd">        [1 2 3]</span>
<span class="sd">        [1 2 3]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">_check</span><span class="p">(</span><span class="n">shape_a</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_can_broadcast_to</span><span class="p">(</span><span class="n">shape_a</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
            <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;cannot broadcast with &#39;</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>
    <span class="n">shape_a</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">array</span><span class="p">)</span>
    <span class="n">_check</span><span class="p">(</span><span class="n">shape_a</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_broadcast_to_shape</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span></div>


<div class="viewcode-block" id="broadcast_arrays"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.broadcast_arrays.html#mindspore.numpy.broadcast_arrays">[docs]</a><span class="k">def</span> <span class="nf">broadcast_arrays</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Broadcasts any number of arrays against each other.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy argument `subok` is not supported.</span>
<span class="sd">        In graph mode, returns a tuple of Tensor instead of a list</span>
<span class="sd">        of Tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        *args (Tensor): The arrays to broadcast.</span>

<span class="sd">    Returns:</span>
<span class="sd">        List of Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If arrays cannot be broadcast.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.array([[1,2,3]])</span>
<span class="sd">        &gt;&gt;&gt; y = np.array([[4],[5]])</span>
<span class="sd">        &gt;&gt;&gt; output = np.broadcast_arrays(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [Tensor(shape=[2, 3], dtype=Int32, value=</span>
<span class="sd">        [[1, 2, 3],</span>
<span class="sd">        [1, 2, 3]]), Tensor(shape=[2, 3], dtype=Int32, value=</span>
<span class="sd">        [[4, 4, 4],</span>
<span class="sd">        [5, 5, 5]])]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">shapes</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
    <span class="n">out_shape</span> <span class="o">=</span> <span class="n">_infer_out_shape</span><span class="p">(</span><span class="o">*</span><span class="n">shapes</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">arr</span> <span class="ow">in</span> <span class="n">args</span><span class="p">:</span>
        <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">out_shape</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">res</span></div>


<div class="viewcode-block" id="array_split"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.array_split.html#mindspore.numpy.array_split">[docs]</a><span class="k">def</span> <span class="nf">array_split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Splits a tensor into multiple sub-tensors.</span>

<span class="sd">    Note:</span>
<span class="sd">        Currently, array_split only supports :class:`mindspore.float32` on ``CPU``.</span>

<span class="sd">    The only difference between ``np.split`` and ``np.array_split`` is that</span>
<span class="sd">    ``np.array_split`` allows indices_or_sections to be an integer that does not</span>
<span class="sd">    equally divide the axis. For a tensor of length l that should be split into</span>
<span class="sd">    n sections, it returns :math:`l % n` sub-arrays of size :math:`l//n + 1` and</span>
<span class="sd">    the rest of size :math:`l//n`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): A Tensor to be divided.</span>
<span class="sd">        indices_or_sections (Union[int, tuple(int), list(int)]):</span>
<span class="sd">            If integer, :math:`N`, the tensor will be divided into</span>
<span class="sd">            :math:`N` tensors along axis.</span>
<span class="sd">            If tuple(int), list(int) or of sorted integers,</span>
<span class="sd">            the entries indicate where along axis the array is split.</span>
<span class="sd">            For example, :math:`[2, 3]` would, for :math:`axis=0`, result in</span>
<span class="sd">            three sub-tensors :math:`x[:2]`, :math:`x[2:3]`and :math:`x[3:]`.</span>
<span class="sd">            If an index exceeds the dimension of the array along axis,</span>
<span class="sd">            an empty sub-array is returned correspondingly.</span>
<span class="sd">        axis (int): The axis along which to split. Default: 0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A list of sub-tensors.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If argument `indices_or_sections` is not integer,</span>
<span class="sd">            tuple(int) or list(int) or argument `axis` is not integer.</span>
<span class="sd">        ValueError: If argument `axis` is out of range of :math:`[-x.ndim, x.ndim)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; input_x = np.arange(9).astype(&quot;float32&quot;)</span>
<span class="sd">        &gt;&gt;&gt; output = np.array_split(input_x, 4)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[3], dtype=Float32,</span>
<span class="sd">            value= [ 0.00000000e+00,  1.00000000e+00,  2.00000000e+00]),</span>
<span class="sd">        Tensor(shape=[2], dtype=Float32,</span>
<span class="sd">            value= [ 3.00000000e+00,  4.00000000e+00]),</span>
<span class="sd">        Tensor(shape=[2], dtype=Float32,</span>
<span class="sd">            value= [ 5.00000000e+00,  6.00000000e+00]),</span>
<span class="sd">        Tensor(shape=[2], dtype=Float32,</span>
<span class="sd">            value= [ 7.00000000e+00,  8.00000000e+00]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">,</span> <span class="n">opname</span><span class="o">=</span><span class="s2">&quot;array_split&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="split"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.split.html#mindspore.numpy.split">[docs]</a><span class="k">def</span> <span class="nf">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Splits a tensor into multiple sub-tensors along the given axis.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): A Tensor to be divided.</span>
<span class="sd">        indices_or_sections (Union[int, tuple(int), list(int)]):</span>
<span class="sd">            If integer, :math:`N`, the tensor will be divided into</span>
<span class="sd">            :math:`N` equal tensors along axis.</span>
<span class="sd">            If tuple(int), list(int) or of sorted integers,</span>
<span class="sd">            the entries indicate where along axis the array is split.</span>
<span class="sd">            For example, :math:`[2, 3]` would, for :math:`axis=0`, result in</span>
<span class="sd">            three sub-tensors :math:`x[:2]`, :math:`x[2:3]`and :math:`x[3:]`.</span>
<span class="sd">            If an index exceeds the dimension of the array along axis,</span>
<span class="sd">            an empty sub-array is returned correspondingly.</span>
<span class="sd">        axis (int): The axis along which to split. Default: 0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tuple of sub-tensors.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If argument `indices_or_sections` is not integer,</span>
<span class="sd">            tuple(int) or list(int) or argument `axis` is not integer.</span>
<span class="sd">        ValueError: If argument `axis` is out of range of :math:`[-x.ndim, x.ndim)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; input_x = np.arange(9).astype(&quot;float32&quot;)</span>
<span class="sd">        &gt;&gt;&gt; output = np.split(input_x, 3)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[3], dtype=Float32,</span>
<span class="sd">          value= [ 0.00000000e+00,  1.00000000e+00,  2.00000000e+00]),</span>
<span class="sd">         Tensor(shape=[3], dtype=Float32,</span>
<span class="sd">          value= [ 3.00000000e+00,  4.00000000e+00,  5.00000000e+00]),</span>
<span class="sd">         Tensor(shape=[3], dtype=Float32,</span>
<span class="sd">          value= [ 6.00000000e+00,  7.00000000e+00,  8.00000000e+00]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">,</span> <span class="n">opname</span><span class="o">=</span><span class="s2">&quot;split&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">,</span> <span class="n">opname</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Splits a tensor based on ``np.split`` or ``np.array_split``.&quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">_check_axis_type</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">axis_new</span> <span class="o">=</span> <span class="n">_canonicalize_axis</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">arr_shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">length_along_dim</span> <span class="o">=</span> <span class="n">arr_shape</span><span class="p">[</span><span class="n">axis_new</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">indices_or_sections</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">indices_or_sections</span> <span class="o">&gt;</span> <span class="n">length_along_dim</span><span class="p">:</span>
            <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;empty tensor encountered.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">opname</span> <span class="o">==</span> <span class="s2">&quot;split&quot;</span> <span class="ow">or</span> <span class="n">length_along_dim</span> <span class="o">%</span> <span class="n">indices_or_sections</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="n">axis_new</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">num_long_tensor</span> <span class="o">=</span> <span class="n">length_along_dim</span> <span class="o">%</span> <span class="n">indices_or_sections</span>
            <span class="n">num_short_tensor</span> <span class="o">=</span> <span class="n">indices_or_sections</span> <span class="o">-</span> <span class="n">num_long_tensor</span>
            <span class="n">length1</span> <span class="o">=</span> <span class="n">num_long_tensor</span> <span class="o">*</span> <span class="p">(</span><span class="n">length_along_dim</span> <span class="o">//</span> <span class="n">indices_or_sections</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">length2</span> <span class="o">=</span> <span class="n">length_along_dim</span> <span class="o">-</span> <span class="n">length1</span>
            <span class="n">start1</span> <span class="o">=</span> <span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
            <span class="n">size1</span> <span class="o">=</span> <span class="n">_tuple_setitem</span><span class="p">(</span><span class="n">arr_shape</span><span class="p">,</span> <span class="n">axis_new</span><span class="p">,</span> <span class="n">length1</span><span class="p">)</span>
            <span class="n">start2</span> <span class="o">=</span> <span class="n">_tuple_setitem</span><span class="p">(</span><span class="n">start1</span><span class="p">,</span> <span class="n">axis_new</span><span class="p">,</span> <span class="n">length1</span><span class="p">)</span>
            <span class="n">size2</span> <span class="o">=</span> <span class="n">_tuple_setitem</span><span class="p">(</span><span class="n">arr_shape</span><span class="p">,</span> <span class="n">axis_new</span><span class="p">,</span> <span class="n">length2</span><span class="p">)</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="n">axis_new</span><span class="p">,</span> <span class="n">num_long_tensor</span><span class="p">)(</span><span class="n">F</span><span class="o">.</span><span class="n">tensor_slice</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">start1</span><span class="p">,</span> <span class="n">size1</span><span class="p">))</span> <span class="o">+</span> \
                <span class="n">P</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="n">axis_new</span><span class="p">,</span> <span class="n">num_short_tensor</span><span class="p">)(</span><span class="n">F</span><span class="o">.</span><span class="n">tensor_slice</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">start2</span><span class="p">,</span> <span class="n">size2</span><span class="p">))</span>

    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">indices_or_sections</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span> <span class="ow">and</span> <span class="n">_check_element_int</span><span class="p">(</span><span class="n">indices_or_sections</span><span class="p">):</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">_split_sub_tensors</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">,</span> <span class="n">axis_new</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;Argument `indices_or_sections` in `mindspore.numpy.split`</span><span class="se">\</span>
<span class="s2">            should be integer, tuple(int) or list(int), but got&quot;</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">convert_neg_indices</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">ndim</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;converts negative values in tuple/list indices&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">canonicalizer</span><span class="p">(</span><span class="n">ax</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">ax</span> <span class="o">+</span> <span class="n">ndim</span> <span class="k">if</span> <span class="n">ax</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">ax</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">([</span><span class="n">canonicalizer</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span> <span class="k">for</span> <span class="n">axis</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">indices</span>


<span class="k">def</span> <span class="nf">_split_sub_tensors</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Splits the input tensor `x` into multiple sub-tensors</span>
<span class="sd">    along the axis according to the given indices.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">length_along_dim</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">convert_neg_indices</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">length_along_dim</span><span class="p">)</span>
    <span class="n">indices</span> <span class="o">+=</span> <span class="p">(</span><span class="n">length_along_dim</span><span class="p">,)</span>

    <span class="n">sub_tensors</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">strides</span> <span class="o">=</span> <span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">begin</span> <span class="o">=</span> <span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">indices</span><span class="p">):</span>
        <span class="n">begin</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">indices</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">end</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span>
        <span class="k">if</span> <span class="n">end</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">begin</span><span class="p">[</span><span class="n">axis</span><span class="p">]:</span>
            <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;empty sub-tensor encountered.&quot;</span><span class="p">)</span>
        <span class="n">sliced_tensor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">strided_slice</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">_type_convert</span><span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="n">begin</span><span class="p">),</span> <span class="n">_type_convert</span><span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="n">end</span><span class="p">),</span> <span class="n">strides</span><span class="p">)</span>
        <span class="n">sub_tensors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sliced_tensor</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sub_tensors</span>


<div class="viewcode-block" id="vsplit"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.vsplit.html#mindspore.numpy.vsplit">[docs]</a><span class="k">def</span> <span class="nf">vsplit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Splits a tensor into multiple sub-tensors vertically (row-wise).</span>
<span class="sd">    It is equivalent to split with :math:`axis=0` (default), the array is always</span>
<span class="sd">    split along the first axis regardless of the array dimension.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): A Tensor to be divided.</span>
<span class="sd">        indices_or_sections (Union[int, tuple(int), list(int)]):</span>
<span class="sd">            If integer, :math:`N`, the tensor will be divided into</span>
<span class="sd">            :math:`N` equal tensors along axis.</span>
<span class="sd">            If tuple(int), list(int) or of sorted integers,</span>
<span class="sd">            the entries indicate where along axis the array is split.</span>
<span class="sd">            For example, :math:`[2, 3]` would, for :math:`axis=0`, result in</span>
<span class="sd">            three sub-tensors :math:`x[:2]`, :math:`x[2:3]`and :math:`x[3:]`.</span>
<span class="sd">            If an index exceeds the dimension of the array along axis,</span>
<span class="sd">            an empty sub-array is returned correspondingly.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A list of sub-tensors.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If argument `indices_or_sections` is not integer.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; input_x = np.arange(9).reshape((3, 3)).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = np.vsplit(input_x, 3)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[1, 3], dtype=Float32,</span>
<span class="sd">          value=[[ 0.00000000e+00,  1.00000000e+00,  2.00000000e+00]]),</span>
<span class="sd">         Tensor(shape=[1, 3], dtype=Float32,</span>
<span class="sd">          value=[[ 3.00000000e+00,  4.00000000e+00,  5.00000000e+00]]),</span>
<span class="sd">         Tensor(shape=[1, 3], dtype=Float32,</span>
<span class="sd">          value=[[ 6.00000000e+00,  7.00000000e+00,  8.00000000e+00]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span></div>


<div class="viewcode-block" id="hsplit"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.hsplit.html#mindspore.numpy.hsplit">[docs]</a><span class="k">def</span> <span class="nf">hsplit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Splits a tensor into multiple sub-tensors horizontally (column-wise).</span>
<span class="sd">    It is equivalent to split with :math:`axis=1` (default), the array is always</span>
<span class="sd">    split along the second axis regardless of the array dimension.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): A Tensor to be divided.</span>
<span class="sd">        indices_or_sections (Union[int, tuple(int), list(int)]):</span>
<span class="sd">            If integer, :math:`N`, the tensor will be divided into</span>
<span class="sd">            :math:`N` equal tensors along axis.</span>
<span class="sd">            If tuple(int), list(int) or of sorted integers,</span>
<span class="sd">            the entries indicate where along axis the array is split.</span>
<span class="sd">            For example, :math:`[2, 3]` would, for :math:`axis=0`, result in</span>
<span class="sd">            three sub-tensors :math:`x[:2]`, :math:`x[2:3]`and :math:`x[3:]`.</span>
<span class="sd">            If an index exceeds the dimension of the array along axis,</span>
<span class="sd">            an empty sub-array is returned correspondingly.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A list of sub-tensors.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If argument `indices_or_sections` is not integer.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; input_x = np.arange(6).reshape((2, 3)).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = np.hsplit(input_x, 3)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 1], dtype=Float32,</span>
<span class="sd">        value=[[ 0.00000000e+00],</span>
<span class="sd">               [ 3.00000000e+00]]),</span>
<span class="sd">        Tensor(shape=[2, 1], dtype=Float32,</span>
<span class="sd">        value=[[ 1.00000000e+00],</span>
<span class="sd">               [ 4.00000000e+00]]),</span>
<span class="sd">        Tensor(shape=[2, 1], dtype=Float32,</span>
<span class="sd">        value=[[ 2.00000000e+00],</span>
<span class="sd">               [ 5.00000000e+00]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span></div>


<div class="viewcode-block" id="dsplit"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.dsplit.html#mindspore.numpy.dsplit">[docs]</a><span class="k">def</span> <span class="nf">dsplit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Splits a tensor into multiple sub-tensors along the 3rd axis (depth).</span>
<span class="sd">    It is equivalent to split with :math:`axis=2` (default), the array is always</span>
<span class="sd">    split along the third axis regardless of the array dimension.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): A Tensor to be divided.</span>
<span class="sd">        indices_or_sections (Union[int, tuple(int), list(int)]):</span>
<span class="sd">            If integer, :math:`N`, the tensor will be divided into</span>
<span class="sd">            :math:`N` equal tensors along axis.</span>
<span class="sd">            If tuple(int), list(int) or of sorted integers,</span>
<span class="sd">            the entries indicate where along axis the array is split.</span>
<span class="sd">            For example, :math:`[2, 3]` would, for :math:`axis=0`, result in</span>
<span class="sd">            three sub-tensors :math:`x[:2]`, :math:`x[2:3]`and :math:`x[3:]`.</span>
<span class="sd">            If an index exceeds the dimension of the array along axis,</span>
<span class="sd">            an empty sub-array is returned correspondingly.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A list of sub-tensors.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If argument `indices_or_sections` is not integer.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; input_x = np.arange(6).reshape((1, 2, 3)).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = np.dsplit(input_x, 3)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[1, 2, 1], dtype=Float32,</span>
<span class="sd">        value=[[[ 0.00000000e+00],</span>
<span class="sd">                [ 3.00000000e+00]]]),</span>
<span class="sd">        Tensor(shape=[1, 2, 1], dtype=Float32,</span>
<span class="sd">        value=[[[ 1.00000000e+00],</span>
<span class="sd">                [ 4.00000000e+00]]]),</span>
<span class="sd">        Tensor(shape=[1, 2, 1], dtype=Float32,</span>
<span class="sd">        value=[[[ 2.00000000e+00],</span>
<span class="sd">                [ 5.00000000e+00]]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span></div>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_get_flip_start</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">axes</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">([</span><span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">axes</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndim</span><span class="p">)])</span>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_get_flip_end</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">axes</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">([</span><span class="o">-</span><span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">axes</span> <span class="k">else</span> <span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndim</span><span class="p">)])</span>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_get_flip_strides</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">axes</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">axes</span> <span class="k">else</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndim</span><span class="p">)])</span>


<div class="viewcode-block" id="flip"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.flip.html#mindspore.numpy.flip">[docs]</a><span class="k">def</span> <span class="nf">flip</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reverses the order of elements in an array along the given axis.</span>

<span class="sd">    The shape of the array is preserved, but the elements are reordered.</span>

<span class="sd">    Args:</span>
<span class="sd">        m (Tensor): Input array.</span>
<span class="sd">        axis (None or int or tuple of integers, optional): Axis or axes along which</span>
<span class="sd">            to flip over. The default, ``axis=None``, will flip over all of the axes</span>
<span class="sd">            of the input array. If `axis` is negative it counts from the last to</span>
<span class="sd">            the first axis. If `axis` is a tuple of integers, flipping is performed on</span>
<span class="sd">            all of the axes specified in the tuple.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the entries of `axis` reversed.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input is not a tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; A = np.arange(8.0).reshape((2,2,2))</span>
<span class="sd">        &gt;&gt;&gt; output = np.flip(A)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[7. 6.]</span>
<span class="sd">        [5. 4.]]</span>
<span class="sd">        [[3. 2.]</span>
<span class="sd">        [1. 0.]]]</span>
<span class="sd">        &gt;&gt;&gt; output = np.flip(A, (0, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[5. 4.]</span>
<span class="sd">        [7. 6.]]</span>
<span class="sd">        [[1. 0.]</span>
<span class="sd">        [3. 2.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="n">axes</span> <span class="o">=</span> <span class="n">_check_axis_valid</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_is_shape_empty</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">m</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_is_float</span><span class="p">(</span><span class="n">dtype</span><span class="p">):</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">_get_flip_start</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">axes</span><span class="p">)</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">_get_flip_end</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">axes</span><span class="p">)</span>
    <span class="n">strides</span> <span class="o">=</span> <span class="n">_get_flip_strides</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">axes</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">strided_slice</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">strides</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">res</span><span class="p">),</span> <span class="n">dtype</span><span class="p">):</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span></div>


<div class="viewcode-block" id="flipud"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.flipud.html#mindspore.numpy.flipud">[docs]</a><span class="k">def</span> <span class="nf">flipud</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Flips the entries in each column in the up/down direction.</span>
<span class="sd">    Rows are preserved, but appear in a different order than before.</span>

<span class="sd">    Args:</span>
<span class="sd">        m (Tensor): Input array.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input is not a tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; A = np.arange(8.0).reshape((2,2,2))</span>
<span class="sd">        &gt;&gt;&gt; output = np.flipud(A)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[4. 5.]</span>
<span class="sd">        [6. 7.]]</span>
<span class="sd">        [[0. 1.]</span>
<span class="sd">        [2. 3.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">flip</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span></div>


<div class="viewcode-block" id="fliplr"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.fliplr.html#mindspore.numpy.fliplr">[docs]</a><span class="k">def</span> <span class="nf">fliplr</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Flips the entries in each row in the left/right direction.</span>
<span class="sd">    Columns are preserved, but appear in a different order than before.</span>

<span class="sd">    Args:</span>
<span class="sd">        m (Tensor): Input array.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input is not a tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; A = np.arange(8.0).reshape((2,2,2))</span>
<span class="sd">        &gt;&gt;&gt; output = np.fliplr(A)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[2. 3.]</span>
<span class="sd">        [0. 1.]]</span>
<span class="sd">        [[6. 7.]</span>
<span class="sd">        [4. 5.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">flip</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span></div>


<div class="viewcode-block" id="take_along_axis"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.take_along_axis.html#mindspore.numpy.take_along_axis">[docs]</a><span class="k">def</span> <span class="nf">take_along_axis</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Takes values from the input array by matching 1d index and data slices.</span>

<span class="sd">    This iterates over matching 1d slices oriented along the specified axis in the</span>
<span class="sd">    index and data arrays, and uses the former to look up values in the latter.</span>
<span class="sd">    These slices can be different lengths.</span>

<span class="sd">    Args:</span>
<span class="sd">        arr (Tensor): Source array with shape `(Ni…, M, Nk…)`.</span>
<span class="sd">        indices (Tensor): Indices with shape `(Ni…, J, Nk…)` to take along each 1d</span>
<span class="sd">            slice of `arr`. This must match the dimension of `arr`, but dimensions `Ni`</span>
<span class="sd">            and `Nj` only need to broadcast against `arr`.</span>
<span class="sd">        axis (int): The axis to take 1d slices along. If `axis` is None, the input</span>
<span class="sd">            array is treated as if it had first been flattened to 1d.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the indexed result, with shape `(Ni…, J, Nk…)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If input array and indices have different number of dimensions.</span>
<span class="sd">        TypeError: If the input is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.arange(12).reshape(3, 4)</span>
<span class="sd">        &gt;&gt;&gt; indices = np.arange(3).reshape(1, 3)</span>
<span class="sd">        &gt;&gt;&gt; output = np.take_along_axis(x, indices, 1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 0  1  2]</span>
<span class="sd">        [ 4  5  6]</span>
<span class="sd">        [ 8  9 10]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">arr</span> <span class="o">=</span> <span class="n">ravel</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ndim</span> <span class="o">!=</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">indices</span><span class="p">):</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;`indices` and `arr` must have the same number of dimensions&#39;</span><span class="p">)</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="n">_check_axis_in_range</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>

    <span class="n">shape_arr</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
    <span class="n">shape_indices</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
    <span class="c1"># broadcasts indices against the shape of arr except at axis</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">_broadcast_to</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">_tuple_slice</span><span class="p">(</span><span class="n">shape_indices</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">axis</span><span class="p">),</span>
                            <span class="n">_tuple_slice</span><span class="p">(</span><span class="n">shape_arr</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">axis</span><span class="p">),</span> <span class="n">ndim</span><span class="p">)</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">_broadcast_to</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">_tuple_slice</span><span class="p">(</span><span class="n">shape_arr</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span>
                            <span class="n">_tuple_slice</span><span class="p">(</span><span class="n">shape_indices</span><span class="p">,</span> <span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="n">shape_arr</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
    <span class="n">arr</span> <span class="o">=</span> <span class="n">_broadcast_to</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">shape_arr</span><span class="p">,</span> <span class="n">indices</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">gather_d</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_mod</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Computes x mod y.&quot;&quot;&quot;</span>
    <span class="n">quotient</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_floordiv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">prod</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_mul</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">quotient</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_sub</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">prod</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_indices</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">allow_negative_index</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Checks whether indices are out of bounds.&quot;&quot;&quot;</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">allow_negative_index</span><span class="p">:</span>
        <span class="n">lowerbounds</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">lowerbounds</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="o">-</span><span class="n">dims</span><span class="p">)</span>
    <span class="n">upperbounds</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dims</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">out_of_lowerbounds</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_lt</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">lowerbounds</span><span class="p">)</span>
    <span class="n">out_of_upperbounds</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_gt</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">upperbounds</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;raise&#39;</span><span class="p">:</span>
        <span class="n">_raise_unimplemented_error</span><span class="p">(</span><span class="s1">&#39;&quot;raise&quot; mode is not implemented&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;wrap&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_mod</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dims</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="o">!=</span> <span class="s1">&#39;clip&#39;</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;invalid mode. Expected &quot;raise&quot;, &quot;wrap&quot;, or &quot;clip&quot;&#39;</span><span class="p">)</span>
    <span class="n">zeros</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">clipped</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">out_of_lowerbounds</span><span class="p">,</span> <span class="n">zeros</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
    <span class="n">clipped</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">out_of_upperbounds</span><span class="p">,</span> <span class="n">upperbounds</span><span class="p">,</span> <span class="n">clipped</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">clipped</span>


<div class="viewcode-block" id="take"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.take.html#mindspore.numpy.take">[docs]</a><span class="k">def</span> <span class="nf">take</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;clip&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Takes elements from an array along an axis.</span>

<span class="sd">    When axis is not None, this function does the same thing as &quot;fancy&quot; indexing</span>
<span class="sd">    (indexing arrays using arrays); however, it can be easier to use if you need</span>
<span class="sd">    elements along a given axis. A call such as ``np.take(arr, indices, axis=3)`` is</span>
<span class="sd">    equivalent to ``arr[:,:,:,indices,...]``.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy argument out is not supported.</span>
<span class="sd">        ``mode = &#39;raise&#39;`` is not supported, and the default mode is &#39;clip&#39; instead.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): Source array with shape `(Ni…, M, Nk…)`.</span>
<span class="sd">        indices (Tensor): The indices with shape `(Nj...)` of the values to extract.</span>
<span class="sd">        axis (int, optional): The axis over which to select values. By default,</span>
<span class="sd">            the flattened input array is used. Defaults to None.</span>
<span class="sd">        mode (&#39;raise&#39;, &#39;wrap&#39;, &#39;clip&#39;, optional): Specifies how out-of-bounds</span>
<span class="sd">            indices will behave. Defaults to &quot;clip&quot;.</span>

<span class="sd">            &#39;raise&#39; – raise an error;</span>

<span class="sd">            &#39;wrap&#39; – wrap around;</span>

<span class="sd">            &#39;clip&#39; – clip to the range. &#39;clip&#39; mode means that all indices that are</span>
<span class="sd">            too large are replaced by the index that addresses the last element</span>
<span class="sd">            along that axis. Note that this disables indexing with negative numbers.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the indexed result.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If axis is out of range.</span>
<span class="sd">        TypeError: If the input is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.array([4, 3, 5, 7, 6, 8])</span>
<span class="sd">        &gt;&gt;&gt; indices = np.array([0, 1, 4])</span>
<span class="sd">        &gt;&gt;&gt; output = np.take(a, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [4 3 6]</span>
<span class="sd">        &gt;&gt;&gt; indices = np.array([[0, 1], [2, 3]])</span>
<span class="sd">        &gt;&gt;&gt; output = np.take(a, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[4 3]</span>
<span class="sd">        [5 7]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">mode</span><span class="p">)</span></div>


<div class="viewcode-block" id="repeat"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.repeat.html#mindspore.numpy.repeat">[docs]</a><span class="k">def</span> <span class="nf">repeat</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">repeats</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Repeats elements of an array.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): Input array.</span>
<span class="sd">        repeats (int or sequence of ints): The number of repetitions for each element.</span>
<span class="sd">            `repeats` is broadcasted to fit the shape of the given axis.</span>
<span class="sd">        axis (int, optional): The axis along which to repeat values. By default,</span>
<span class="sd">            use the flattened input array, and return a flat output array. Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, output array which has the same shape as `a`, except along the given</span>
<span class="sd">        axis.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If axis is out of range.</span>
<span class="sd">        TypeError: If input `a` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; output = np.repeat(np.array(3), 4)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [3 3 3 3]</span>
<span class="sd">        &gt;&gt;&gt; x = np.array([[1,2],[3,4]])</span>
<span class="sd">        &gt;&gt;&gt; output = np.repeat(x, 2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1 1 2 2 3 3 4 4]</span>
<span class="sd">        &gt;&gt;&gt; output = np.repeat(x, 3, axis=1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1 1 1 2 2 2]</span>
<span class="sd">        [3 3 3 4 4 4]]</span>
<span class="sd">        &gt;&gt;&gt; output = np.repeat(x, [1, 2], axis=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1 2]</span>
<span class="sd">        [3 4]</span>
<span class="sd">        [3 4]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">repeats</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="rot90"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.rot90.html#mindspore.numpy.rot90">[docs]</a><span class="k">def</span> <span class="nf">rot90</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Rotates a tensor by 90 degrees in the plane specified by axes.</span>
<span class="sd">    Rotation direction is from the first towards the second axis.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): Input tensor of two or more dimensions.</span>
<span class="sd">        k (int): Number of times the tensor is rotated by 90 degrees. Default: 1.</span>
<span class="sd">        axes (Union[tuple(int), list(int)]): The tensor is rotated in the plane</span>
<span class="sd">            defined by the axes. Default: `(0, 1)`.</span>
<span class="sd">            Axes must be different and with the shape of `(2,)`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If input `a` is not a Tensor or</span>
<span class="sd">            the argument `k` is not integer or</span>
<span class="sd">            the argument `axes` is not tuple of integers or list of ints.</span>
<span class="sd">        ValueError: If any axis is out of range or</span>
<span class="sd">            the length of `axes` is not `2`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.arange(24).reshape((2, 3, 4))</span>
<span class="sd">        &gt;&gt;&gt; output = np.rot90(a)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[ 8  9 10 11]</span>
<span class="sd">          [20 21 22 23]]</span>
<span class="sd">         [[ 4  5  6  7]</span>
<span class="sd">          [16 17 18 19]]</span>
<span class="sd">         [[ 0  1  2  3]</span>
<span class="sd">          [12 13 14 15]]]</span>
<span class="sd">        &gt;&gt;&gt; output = np.rot90(a, 3, (1, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[ 8  4  0]</span>
<span class="sd">          [ 9  5  1]</span>
<span class="sd">          [10  6  2]</span>
<span class="sd">          [11  7  3]]</span>
<span class="sd">         [[20 16 12]</span>
<span class="sd">          [21 17 13]</span>
<span class="sd">          [22 18 14]</span>
<span class="sd">          [23 19 15]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;integer argument expected, but got &quot;</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">k</span> <span class="o">%</span> <span class="mi">4</span> <span class="k">if</span> <span class="n">k</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">4</span> <span class="o">-</span> <span class="p">(</span><span class="o">-</span><span class="n">k</span> <span class="o">%</span> <span class="mi">4</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;tuple(ints) or list(ints) expected, but got &quot;</span><span class="p">,</span> <span class="n">axes</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;len(axes) must be 2.&quot;</span><span class="p">)</span>
    <span class="n">axis1_tmp</span><span class="p">,</span> <span class="n">axis2_tmp</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">axis1</span> <span class="o">=</span> <span class="n">_canonicalize_axis</span><span class="p">(</span><span class="n">axis1_tmp</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="n">axis2</span> <span class="o">=</span> <span class="n">_canonicalize_axis</span><span class="p">(</span><span class="n">axis2_tmp</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">axis1</span> <span class="o">==</span> <span class="n">axis2</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;Axes must be different.&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">a</span>
    <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">flip</span><span class="p">(</span><span class="n">flip</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis1</span><span class="p">),</span> <span class="n">axis2</span><span class="p">)</span>
    <span class="n">perm</span> <span class="o">=</span> <span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="n">perm</span><span class="p">[</span><span class="n">axis1</span><span class="p">],</span> <span class="n">perm</span><span class="p">[</span><span class="n">axis2</span><span class="p">]</span> <span class="o">=</span> <span class="n">perm</span><span class="p">[</span><span class="n">axis2</span><span class="p">],</span> <span class="n">perm</span><span class="p">[</span><span class="n">axis1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">flip</span><span class="p">(</span><span class="n">transpose</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">perm</span><span class="p">),</span> <span class="n">axis1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">flip</span><span class="p">(</span><span class="n">transpose</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">perm</span><span class="p">),</span> <span class="n">axis2</span><span class="p">)</span></div>


<div class="viewcode-block" id="select"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.select.html#mindspore.numpy.select">[docs]</a><span class="k">def</span> <span class="nf">select</span><span class="p">(</span><span class="n">condlist</span><span class="p">,</span> <span class="n">choicelist</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns an array drawn from elements in `choicelist`, depending on conditions.</span>

<span class="sd">    Args:</span>
<span class="sd">        condlist (Union[int, float, bool, list, tuple, Tensor]): The list of conditions</span>
<span class="sd">            which determine from which array in `choicelist` the output elements are</span>
<span class="sd">            taken. When multiple conditions are satisfied, the first one encountered in</span>
<span class="sd">            `condlist` is used.</span>
<span class="sd">        choicelist (Union[int, float, bool, list, tuple, Tensor]): The list of arrays</span>
<span class="sd">            from which the output elements are taken. It has to be of the same length as</span>
<span class="sd">            `condlist`.</span>
<span class="sd">        default (scalar, optional): The element inserted in output when all conditions</span>
<span class="sd">            evaluate to `False`. Defaults to 0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the output at position `m` is the `m-th` element of the array in</span>
<span class="sd">        `choicelist` where the `m-th` element of the corresponding array in `condlist`</span>
<span class="sd">        is `True`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If ``len(condlist) != len(choicelist)``.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; condlist = [[True, True, True, False, False], [False, False, True, False, True]]</span>
<span class="sd">        &gt;&gt;&gt; choicelist = [[0, 1, 2, 3, 4], [0, 1, 4, 9, 16]]</span>
<span class="sd">        &gt;&gt;&gt; output = np.select(condlist, choicelist)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0  1  2  0 16]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">condlist</span><span class="p">,</span> <span class="n">choicelist</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">condlist</span><span class="p">,</span> <span class="n">choicelist</span><span class="p">)</span>
    <span class="n">shape_cond</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">condlist</span><span class="p">)</span>
    <span class="n">shape_choice</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">choicelist</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">condlist</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">choicelist</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;input cannot be scalars&#39;</span><span class="p">)</span>
    <span class="n">case_num</span> <span class="o">=</span> <span class="n">shape_cond</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">shape_choice</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">case_num</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;list of cases must be same length as list of conditions&#39;</span><span class="p">)</span>

    <span class="n">case_size_cond</span> <span class="o">=</span> <span class="n">_tuple_slice</span><span class="p">(</span><span class="n">shape_cond</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">case_size_choice</span> <span class="o">=</span> <span class="n">_tuple_slice</span><span class="p">(</span><span class="n">shape_choice</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="c1"># performs broadcast over the cases in condlist and choicelist</span>
    <span class="n">case_size</span> <span class="o">=</span> <span class="n">_infer_out_shape</span><span class="p">(</span><span class="n">case_size_cond</span><span class="p">,</span> <span class="n">case_size_choice</span><span class="p">)</span>
    <span class="n">shape_broadcasted</span> <span class="o">=</span> <span class="p">(</span><span class="n">case_num</span><span class="p">,)</span> <span class="o">+</span> <span class="n">case_size</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape_broadcasted</span><span class="p">)</span>
    <span class="n">shape_cond_expanded</span> <span class="o">=</span> <span class="p">((</span><span class="n">case_num</span><span class="p">,)</span> <span class="o">+</span> <span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">ndim</span> <span class="o">-</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">condlist</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span> <span class="o">+</span>
                           <span class="n">case_size_cond</span><span class="p">)</span>
    <span class="n">condlist</span> <span class="o">=</span> <span class="n">_broadcast_to_shape</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">condlist</span><span class="p">,</span> <span class="n">shape_cond_expanded</span><span class="p">),</span> <span class="n">shape_broadcasted</span><span class="p">)</span>
    <span class="n">shape_choice_expanded</span> <span class="o">=</span> <span class="p">((</span><span class="n">case_num</span><span class="p">,)</span> <span class="o">+</span> <span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">ndim</span> <span class="o">-</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">choicelist</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span> <span class="o">+</span>
                             <span class="n">case_size_choice</span><span class="p">)</span>
    <span class="n">choicelist</span> <span class="o">=</span> <span class="n">_broadcast_to_shape</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">choicelist</span><span class="p">,</span> <span class="n">shape_choice_expanded</span><span class="p">),</span> <span class="n">shape_broadcasted</span><span class="p">)</span>

    <span class="n">slice_start</span> <span class="o">=</span> <span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">slice_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="n">case_size</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">choicelist</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">default</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">default_slice</span> <span class="o">=</span> <span class="n">default</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">choicelist</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">slice_size</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">default_slice</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">choicelist</span><span class="p">),</span> <span class="n">slice_size</span><span class="p">,</span> <span class="n">default</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">case_num</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">cond_slice</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_slice</span><span class="p">(</span><span class="n">condlist</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="p">(</span><span class="n">i</span><span class="p">,)</span> <span class="o">+</span> <span class="n">slice_start</span><span class="p">,</span> <span class="n">slice_size</span><span class="p">)</span>
        <span class="n">choice_slice</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_slice</span><span class="p">(</span><span class="n">choicelist</span><span class="p">,</span> <span class="p">(</span><span class="n">i</span><span class="p">,)</span> <span class="o">+</span> <span class="n">slice_start</span><span class="p">,</span> <span class="n">slice_size</span><span class="p">)</span>
        <span class="n">default_slice</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">cond_slice</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">),</span> <span class="n">choice_slice</span><span class="p">,</span> <span class="n">default_slice</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">default_slice</span><span class="p">,</span> <span class="p">(</span><span class="n">case_size</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span></div>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_get_grid</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns a grid representing all the indices for an array with the given shape.&quot;&quot;&quot;</span>
    <span class="n">grids</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndim</span><span class="p">):</span>
        <span class="n">dim_grid</span> <span class="o">=</span> <span class="n">_iota</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="n">dim_shape</span> <span class="o">=</span> <span class="n">_expanded_shape</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">i</span><span class="p">)</span>
        <span class="n">dim_grid</span> <span class="o">=</span> <span class="n">_broadcast_to_shape</span><span class="p">(</span><span class="n">dim_grid</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">dim_shape</span><span class="p">),</span> <span class="n">shape</span><span class="p">)</span>
        <span class="n">grids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dim_grid</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">stack</span><span class="p">(</span><span class="n">grids</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>


<div class="viewcode-block" id="choose"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.choose.html#mindspore.numpy.choose">[docs]</a><span class="k">def</span> <span class="nf">choose</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">choices</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;clip&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Construct an array from an index array and a list of arrays to choose from.</span>
<span class="sd">    Given an &quot;index&quot; array `a` of integers and a sequence of n arrays (choices),</span>
<span class="sd">    `a` and each choice array are first broadcast, as necessary, to arrays of a</span>
<span class="sd">    common shape; calling these `Ba` and `Bchoices[i], i = 0,…,n-1` we have that,</span>
<span class="sd">    necessarily, ``Ba.shape == Bchoices[i].shape`` for each `i`. Then, a new array</span>
<span class="sd">    with ``shape Ba.shape`` is created as follows:</span>

<span class="sd">    - if ``mode=&#39;raise&#39;`` (the default), then, first of all, each element of `a`</span>
<span class="sd">      (and thus `Ba`) must be in the range `[0, n-1]`; now, suppose that `i`</span>
<span class="sd">      (in that range) is the value at the `(j0, j1, ..., jm)` position in</span>
<span class="sd">      `Ba` - then the value at the same position in the new array is the</span>
<span class="sd">      value in ``Bchoices[i]`` at that same position;</span>

<span class="sd">    - if ``mode=&#39;wrap&#39;``, values in `a` (and thus `Ba`) may be any (signed)</span>
<span class="sd">      integer; modular arithmetic is used to map integers outside the</span>
<span class="sd">      range ``[0, n-1]`` back into that range; and then the new array is</span>
<span class="sd">      constructed as above;</span>

<span class="sd">    - if ``mode=&#39;clip&#39;``, values in `a` (and thus `Ba`) may be any (signed) integer;</span>
<span class="sd">      negative integers are mapped to 0; values greater than `n-1` are mapped to</span>
<span class="sd">      `n-1`; and then the new array is constructed as above.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy argument `out` is not supported.</span>
<span class="sd">        ``mode = &#39;raise&#39;`` is not supported, and the default mode is &#39;clip&#39; instead.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (int array): This array must contain integers in ``[0, n-1]``, where `n` is</span>
<span class="sd">            the number of choices, unless ``mode=wrap`` or ``mode=clip``, in which</span>
<span class="sd">            cases any integers are permissible.</span>
<span class="sd">        choices (sequence of arrays): Choice arrays. `a` and all of the `choices` must</span>
<span class="sd">            be broadcastable to the same shape. If `choices` is itself an array, then</span>
<span class="sd">            its outermost dimension (i.e., the one corresponding to ``choices.shape[0]``)</span>
<span class="sd">            is taken as defining the &quot;sequence&quot;.</span>
<span class="sd">        mode (&#39;raise&#39;, &#39;wrap&#39;, &#39;clip&#39;, optional): Specifies how indices outside</span>
<span class="sd">            ``[0, n-1]`` will be treated:</span>

<span class="sd">            &#39;raise&#39; – raise an error;</span>

<span class="sd">            &#39;wrap&#39; – wrap around;</span>

<span class="sd">            &#39;clip&#39; – clip to the range. &#39;clip&#39; mode means that all indices that are</span>
<span class="sd">            too large are replaced by the index that addresses the last element</span>
<span class="sd">            along that axis. Note that this disables indexing with negative numbers.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the merged result.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If `a` and any of the `choices` cannot be broadcast.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; choices = [[0, 1, 2, 3], [10, 11, 12, 13], [20, 21, 22, 23], [30, 31, 32, 33]]</span>
<span class="sd">        &gt;&gt;&gt; print(np.choose([2, 3, 1, 0], choices))</span>
<span class="sd">        [20 31 12  3]</span>
<span class="sd">        &gt;&gt;&gt; print(np.choose([2, 4, 1, 0], choices, mode=&#39;clip&#39;))</span>
<span class="sd">        [20 31 12  3]</span>
<span class="sd">        &gt;&gt;&gt; print(np.choose([2, 4, 1, 0], choices, mode=&#39;wrap&#39;))</span>
<span class="sd">        [20  1 12  3]</span>
<span class="sd">        &gt;&gt;&gt; a = [[1, 0, 1], [0, 1, 0], [1, 0, 1]]</span>
<span class="sd">        &gt;&gt;&gt; choices = [-10, 10]</span>
<span class="sd">        &gt;&gt;&gt; print(np.choose(a, choices))</span>
<span class="sd">        [[ 10 -10  10]</span>
<span class="sd">         [-10  10 -10]</span>
<span class="sd">         [ 10 -10  10]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_is_int</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">a</span><span class="p">)):</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;`a` should be an int array&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">choices</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
        <span class="c1"># broadcasts choices to the same shape if choices is a sequence</span>
        <span class="n">choices</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="o">*</span><span class="n">choices</span><span class="p">)</span>
        <span class="n">shapes</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">for</span> <span class="n">choice</span> <span class="ow">in</span> <span class="n">choices</span><span class="p">:</span>
            <span class="n">shapes</span> <span class="o">+=</span> <span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">choice</span><span class="p">),)</span>
        <span class="n">shape_choice</span> <span class="o">=</span> <span class="n">_infer_out_shape</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="o">*</span><span class="n">shapes</span><span class="p">)</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">choice</span> <span class="ow">in</span> <span class="n">choices</span><span class="p">:</span>
            <span class="n">tmp</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">choice</span><span class="p">,</span> <span class="n">shape_choice</span><span class="p">))</span>
        <span class="n">choices</span> <span class="o">=</span> <span class="n">stack</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">choices</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">choices</span><span class="p">)</span>
        <span class="n">shape_choice</span> <span class="o">=</span> <span class="n">_infer_out_shape</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">choices</span><span class="p">)[</span><span class="mi">1</span><span class="p">:])</span>
        <span class="n">choices</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">choices</span><span class="p">,</span> <span class="n">choices</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">_add_unit_axes</span><span class="p">(</span><span class="n">choices</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape_choice</span><span class="p">)))</span>
        <span class="n">choices</span> <span class="o">=</span> <span class="n">broadcast_to</span><span class="p">(</span><span class="n">choices</span><span class="p">,</span> <span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">choices</span><span class="p">)[</span><span class="mi">0</span><span class="p">],)</span> <span class="o">+</span> <span class="n">shape_choice</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">choices</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;input cannot be scalars&#39;</span><span class="p">)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">broadcast_to</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">shape_choice</span><span class="p">)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">_check_indices</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">choices</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">a</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">allow_negative_index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">_get_grid</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">concatenate</span><span class="p">((</span><span class="n">a</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)),</span> <span class="n">grid</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">gather_nd</span><span class="p">(</span><span class="n">choices</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span></div>


<div class="viewcode-block" id="size"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.size.html#mindspore.numpy.size">[docs]</a><span class="k">def</span> <span class="nf">size</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the number of elements along a given axis.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Union[int, float, bool, list, tuple, Tensor]): Input data.</span>
<span class="sd">        axis (int): Axis along which the elements are counted. Default: None.</span>
<span class="sd">            If None, give the total number of elements.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Number of elements along the specified axis.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If input is not array_like or `axis` is not int.</span>
<span class="sd">        ValueError: If any axis is out of range or duplicate axes exist.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.arange(10).reshape(2, 5).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(np.size(x))</span>
<span class="sd">        10</span>
<span class="sd">        &gt;&gt;&gt; print(np.size(x, axis=1))</span>
<span class="sd">        5</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">size</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;axis argument should be integer.&quot;</span><span class="p">)</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="n">_canonicalize_axis</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span></div>


<div class="viewcode-block" id="array_str"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.array_str.html#mindspore.numpy.array_str">[docs]</a><span class="k">def</span> <span class="nf">array_str</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a string representation of the data in an array.</span>

<span class="sd">    The data in the array is returned as a single string.</span>
<span class="sd">    This function is similar to array_repr, the difference being that array_repr also</span>
<span class="sd">    returns information on the kind of array and its data type.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy argument `max_line_width`, `precision` and `suppress_small` are not supported.</span>
<span class="sd">        Graph mode does not support the function.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): Input data.</span>

<span class="sd">    Returns:</span>
<span class="sd">        String.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If input is not tensor.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.arange(5)</span>
<span class="sd">        &gt;&gt;&gt; np.array_str(x)</span>
<span class="sd">        &#39;[0 1 2 3 4]&#39;</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;Expect input to be tensor.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="fm">__str__</span><span class="p">()</span></div>


<div class="viewcode-block" id="apply_along_axis"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.apply_along_axis.html#mindspore.numpy.apply_along_axis">[docs]</a><span class="k">def</span> <span class="nf">apply_along_axis</span><span class="p">(</span><span class="n">func1d</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">arr</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a function to 1-D slices along the given axis.</span>
<span class="sd">    Executes ``func1d(a, *args, **kwargs)`` where `func1d` operates on 1-D arrays and `a` is a</span>
<span class="sd">    1-D slice of arr along axis.</span>

<span class="sd">    Args:</span>
<span class="sd">        func1d (function): Maps `(M,) -&gt; (Nj…)`. This function should accept 1-D arrays. It is</span>
<span class="sd">            applied to 1-D slices of arr along the specified axis.</span>
<span class="sd">        axis (int): Axis along which arr is sliced.</span>
<span class="sd">        arr (Tensor): Input array with shape `(Ni…, M, Nk…)`.</span>
<span class="sd">        args (any): Additional arguments to `func1d`.</span>
<span class="sd">        kwargs (any): Additional named arguments to `func1d`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor with shape `(Ni…, Nj…, Nk…)`, the output array. Its shape is identical to the</span>
<span class="sd">        shape of `arr`, except along the `axis` dimension. This axis is removed, and replaced</span>
<span class="sd">        with new dimensions equal to the shape of the return value of `func1d`. So if `func1d`</span>
<span class="sd">        returns a scalar, the output will have one fewer dimensions than `arr`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If axis is out of the range.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; b = np.array([[1,2,3], [4,5,6], [7,8,9]])</span>
<span class="sd">        &gt;&gt;&gt; print(np.apply_along_axis(np.diag, -1, b))</span>
<span class="sd">        [[[1 0 0]</span>
<span class="sd">        [0 2 0]</span>
<span class="sd">        [0 0 3]]</span>
<span class="sd">        [[4 0 0]</span>
<span class="sd">        [0 5 0]</span>
<span class="sd">        [0 0 6]]</span>
<span class="sd">        [[7 0 0]</span>
<span class="sd">        [0 8 0]</span>
<span class="sd">        [0 0 9]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="n">_check_axis_in_range</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
    <span class="n">arr</span> <span class="o">=</span> <span class="n">moveaxis</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">arr</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">arr</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">slices</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">arr</span><span class="p">)[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">slices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">func1d</span><span class="p">(</span><span class="n">arr</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">))</span>
    <span class="n">stacked_slices</span> <span class="o">=</span> <span class="n">stack</span><span class="p">(</span><span class="n">slices</span><span class="p">)</span>
    <span class="n">shape_stacked</span> <span class="o">=</span> <span class="p">(</span><span class="n">_tuple_slice</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span> <span class="o">+</span> <span class="n">_tuple_slice</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="o">+</span>
                     <span class="n">_tuple_slice</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">stacked_slices</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">stacked_slices</span><span class="p">,</span> <span class="n">shape_stacked</span><span class="p">)</span>

    <span class="c1"># moves the dimensions returned by `func1d` back to `axis`</span>
    <span class="n">ndim_func</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">res</span><span class="p">)</span> <span class="o">-</span> <span class="n">ndim</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">ndim_func</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">moveaxis</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">make_range</span><span class="p">(</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">res</span><span class="p">)),</span>
                       <span class="n">F</span><span class="o">.</span><span class="n">make_range</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">axis</span> <span class="o">+</span> <span class="n">ndim_func</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">res</span></div>


<span class="k">def</span> <span class="nf">_stack_arrays</span><span class="p">(</span><span class="n">arrs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Stacks a sequence of Tensor&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arrs</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
        <span class="n">tensor_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">arr</span> <span class="ow">in</span> <span class="n">arrs</span><span class="p">:</span>
            <span class="n">tensor_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">_to_tensor</span><span class="p">(</span><span class="n">arr</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">stack</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">atleast_1d</span><span class="p">(</span><span class="n">_to_tensor</span><span class="p">(</span><span class="n">arrs</span><span class="p">))</span>


<div class="viewcode-block" id="piecewise"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.piecewise.html#mindspore.numpy.piecewise">[docs]</a><span class="k">def</span> <span class="nf">piecewise</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">condlist</span><span class="p">,</span> <span class="n">funclist</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Evaluates a piecewise-defined function.</span>
<span class="sd">    Given a set of conditions and corresponding functions, evaluate each function on the input</span>
<span class="sd">    data wherever its condition is true.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[int, float, bool, list, tuple, Tensor]): The input domain.</span>
<span class="sd">        condlist (Union[bool, list of bool Tensor]): Each boolean array corresponds to a</span>
<span class="sd">            function in `funclist`. Wherever `condlist[i]` is True, `funclist[i](x)` is used as</span>
<span class="sd">            the output value. Each boolean array in `condlist` selects a piece of `x`, and</span>
<span class="sd">            should therefore be of the same shape as `x`. The length of `condlist` must</span>
<span class="sd">            correspond to that of `funclist`. If one extra function is given, i.e. if</span>
<span class="sd">            ``len(funclist) == len(condlist) + 1``, then that extra function is the default</span>
<span class="sd">            value, used wherever all conditions are false.</span>
<span class="sd">        funclist (Union[list of callables, list of scalars]): Each function is evaluated over</span>
<span class="sd">            `x` wherever its corresponding condition is True. It should take a 1d array as input</span>
<span class="sd">            and give an 1d array or a scalar value as output. If, instead of a callable, a scalar</span>
<span class="sd">            is provided then a constant function ``(lambda x: scalar)`` is assumed.</span>
<span class="sd">        args (any): Any further arguments given to `piecewise` are passed to the functions upon</span>
<span class="sd">            execution, i.e., if called ``piecewise(..., ..., 1, &#39;a&#39;)``, then each function is</span>
<span class="sd">            called as ``f(x, 1, &#39;a&#39;)``.</span>
<span class="sd">        kw (any): Keyword arguments used in calling `piecewise` are passed to the functions upon</span>
<span class="sd">            execution, i.e., if called ``piecewise(..., ..., alpha=1)``, then each function is</span>
<span class="sd">            called as ``f(x, alpha=1)``.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the output is the same shape and type as `x` and is found by calling the</span>
<span class="sd">        functions in `funclist` on the appropriate portions of `x`, as defined by the boolean</span>
<span class="sd">        arrays in `condlist`. Portions not covered by any condition have a default value of 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If length of `funclist` is not in ``(len(condlist), len(condlist) + 1)``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.linspace(-2.5, 2.5, 6)</span>
<span class="sd">        &gt;&gt;&gt; print(np.piecewise(x, [x &lt; 0, x &gt;= 0], [-1, 1]))</span>
<span class="sd">        [-1 -1 -1  1  1  1]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">choicelist</span> <span class="o">=</span> <span class="n">funclist</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">funclist</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">_callable</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">funclist</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">choicelist</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">func</span> <span class="ow">in</span> <span class="n">funclist</span><span class="p">:</span>
                <span class="n">choicelist</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">))</span>
    <span class="n">condlist</span> <span class="o">=</span> <span class="n">_stack_arrays</span><span class="p">(</span><span class="n">condlist</span><span class="p">)</span>
    <span class="n">choicelist</span> <span class="o">=</span> <span class="n">_stack_arrays</span><span class="p">(</span><span class="n">choicelist</span><span class="p">)</span>

    <span class="n">default</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">n1</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">condlist</span><span class="p">)</span>
    <span class="n">n2</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">funclist</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">n1</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">==</span> <span class="n">n2</span><span class="p">:</span>
        <span class="n">default</span> <span class="o">=</span> <span class="n">choicelist</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">choicelist</span> <span class="o">=</span> <span class="n">choicelist</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">elif</span> <span class="n">n1</span> <span class="o">!=</span> <span class="n">n2</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;the number of choices should be either equal to conditions or &#39;</span><span class="p">,</span> <span class="n">n1</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">select</span><span class="p">(</span><span class="n">condlist</span><span class="p">,</span> <span class="n">choicelist</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="n">default</span><span class="p">)</span></div>


<div class="viewcode-block" id="unravel_index"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.unravel_index.html#mindspore.numpy.unravel_index">[docs]</a><span class="k">def</span> <span class="nf">unravel_index</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s1">&#39;C&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts a flat index or array of flat indices into a tuple of coordinate arrays.</span>

<span class="sd">    Note:</span>
<span class="sd">        Out-of-bound indices are clipped by the boundaries of `shape` instead of raising</span>
<span class="sd">        an error.</span>

<span class="sd">    Args:</span>
<span class="sd">        indices (Union[int, float, bool, list, tuple, Tensor]): An integer array whose elements</span>
<span class="sd">            are indices into the flattened version of an array of dimensions shape.</span>
<span class="sd">        shape (tuple of integers): The shape of the array to use for unraveling indices.</span>
<span class="sd">        order (Union[&#39;C&#39;, &#39;F&#39;], optional): Determines whether the indices should be viewed as</span>
<span class="sd">            indexing in row-major (C-style) or column-major (Fortran-style) order. Defaults to &quot;C&quot;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, each array in the tuple has the same shape as the indices array.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If `order` is not &#39;C&#39; or &#39;F&#39;.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; print(np.unravel_index([22, 41, 37], (7,6)))</span>
<span class="sd">        (Tensor(shape=[3], dtype=Int32, value= [3, 6, 6]),</span>
<span class="sd">        Tensor(shape=[3], dtype=Int32, value= [4, 5, 1]))</span>
<span class="sd">        &gt;&gt;&gt; print(np.unravel_index([31, 41, 13], (7,6), order=&#39;F&#39;))</span>
<span class="sd">        (Tensor(shape=[3], dtype=Int32, value= [3, 6, 6]),</span>
<span class="sd">        Tensor(shape=[3], dtype=Int32, value= [4, 5, 1]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">order</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;C&#39;</span><span class="p">,</span> <span class="s1">&#39;F&#39;</span><span class="p">):</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;invalid order. Expected &quot;C&quot; or &quot;F&quot;&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">shape</span><span class="p">,)</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">order</span> <span class="o">==</span> <span class="s1">&#39;F&#39;</span><span class="p">:</span>
        <span class="n">sizes</span> <span class="o">=</span> <span class="n">_cumprod</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">sizes</span> <span class="o">=</span> <span class="n">_cumprod</span><span class="p">(</span><span class="n">shape</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">sizes</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">sizes</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
    <span class="n">sizes</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">sizes</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">))</span>
    <span class="n">total_size</span> <span class="o">=</span> <span class="n">sizes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">where</span><span class="p">(</span><span class="n">indices</span> <span class="o">&gt;</span> <span class="n">total_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">total_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_get_device</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;GPU&#39;</span><span class="p">:</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">total_size</span><span class="p">)</span>
        <span class="n">lowerbounds</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">total_size</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">lowerbounds</span> <span class="o">=</span> <span class="o">-</span><span class="n">total_size</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">where</span><span class="p">(</span><span class="n">indices</span> <span class="o">&lt;</span> <span class="n">lowerbounds</span><span class="p">,</span> <span class="n">lowerbounds</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">_mod</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">sizes</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">//</span><span class="n">sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>

    <span class="n">num</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">num</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">res</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">order</span> <span class="o">==</span> <span class="s1">&#39;F&#39;</span><span class="p">:</span>
        <span class="n">r</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">num</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">r</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">num</span><span class="p">)</span>
    <span class="n">subs</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">r</span><span class="p">:</span>
        <span class="n">subs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="n">i</span><span class="p">],)</span>
    <span class="k">return</span> <span class="n">subs</span></div>


<div class="viewcode-block" id="apply_over_axes"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.apply_over_axes.html#mindspore.numpy.apply_over_axes">[docs]</a><span class="k">def</span> <span class="nf">apply_over_axes</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">axes</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a function repeatedly over multiple axes.</span>

<span class="sd">    `func` is called as `res = func(a, axis)`, where `axis` is the first element of `axes`.</span>
<span class="sd">    The result `res` of the function call must have either the same dimensions as `a` or</span>
<span class="sd">    one less dimension. If `res` has one less dimension than `a`, a dimension is inserted before `axis`.</span>
<span class="sd">    The call to `func` is then repeated for each axis in `axes`, with `res` as the first argument.</span>

<span class="sd">    Args:</span>
<span class="sd">        func (function): This function must take two arguments, `func(a, axis)`.</span>
<span class="sd">        a (Union[int, float, bool, list, tuple, Tensor]): Input tensor.</span>
<span class="sd">        axes (Union[int, list, tuple]): Axes over which `func` is applied; the elements must be integers.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor. The number of dimensions is the same as `a`, but the shape can be different.</span>
<span class="sd">        This depends on whether `func` changes the shape of its output with respect to its input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If input `a` is not array_like or `axes` is not int or sequence of ints.</span>
<span class="sd">        ValueError: If any axis is out of range or duplicate axes exist.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.arange(10).reshape(2, 5).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(x)</span>
<span class="sd">        [[0. 1. 2. 3. 4.]</span>
<span class="sd">         [5. 6. 7. 8. 9.]]</span>
<span class="sd">        &gt;&gt;&gt; print(np.apply_over_axes(np.sum, x, axes=0))</span>
<span class="sd">        [[ 5.  7.  9. 11. 13.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">axes</span> <span class="o">=</span> <span class="p">(</span><span class="n">axes</span><span class="p">,)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">a</span>
    <span class="k">for</span> <span class="n">axis</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span> <span class="k">if</span> <span class="n">res</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="n">a</span><span class="o">.</span><span class="n">ndim</span> <span class="k">else</span> <span class="n">res</span>
        <span class="k">if</span> <span class="n">res</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="n">a</span><span class="o">.</span><span class="n">ndim</span><span class="p">:</span>
            <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;function is not returning a tensor of the correct shape&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span></div>
</pre></div>

           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2022, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>