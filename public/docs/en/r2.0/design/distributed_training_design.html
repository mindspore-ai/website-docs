

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Distributed Training Design &mdash; MindSpore master documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/training.css" type="text/css" />
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script src="../_static/js/training.js"></script>
        
        
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Graph-Kernel Fusion Acceleration Engine" href="graph_fusion_engine.html" />
    <link rel="prev" title="Third-Party Hardware Interconnection" href="pluggable_device.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Design</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">MindSpore Design Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="programming_paradigm.html">Programming Paradigm</a></li>
<li class="toctree-l1"><a class="reference internal" href="auto_gradient.html">Functional Differential Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindir.html">MindSpore IR (MindIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="all_scenarios.html">Full-scenarios Unification</a></li>
<li class="toctree-l1"><a class="reference internal" href="dynamic_graph_and_static_graph.html">Combination of Dynamic and Static Graphs</a></li>
<li class="toctree-l1"><a class="reference internal" href="pluggable_device.html">Third-Party Hardware Interconnection</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Distributed Training Design</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#background">Background</a></li>
<li class="toctree-l2"><a class="reference internal" href="#concepts">Concepts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#collective-communication">Collective Communication</a></li>
<li class="toctree-l3"><a class="reference internal" href="#synchronization-mode">Synchronization Mode</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#data-parallelism">Data Parallelism</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#principle-of-data-parallelism">Principle of Data Parallelism</a></li>
<li class="toctree-l3"><a class="reference internal" href="#data-parallel-code">Data Parallel Code</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#automatic-parallelism">Automatic Parallelism</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#principle-of-automatic-parallelism">Principle of Automatic Parallelism</a></li>
<li class="toctree-l3"><a class="reference internal" href="#automatic-parallel-code">Automatic Parallel Code</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#heterogeneous-parallelism">Heterogeneous Parallelism</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#computational-process">Computational Process</a></li>
<li class="toctree-l3"><a class="reference internal" href="#optimizer-heterogeneity">Optimizer Heterogeneity</a></li>
<li class="toctree-l3"><a class="reference internal" href="#embedding-heterogeneity">Embedding Heterogeneity</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ps-heterogeneity">PS Heterogeneity</a></li>
<li class="toctree-l3"><a class="reference internal" href="#constraints">Constraints</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="graph_fusion_engine.html">Graph-Kernel Fusion Acceleration Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_engine.html">High Performance Data Processing Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="glossary.html">Glossary</a></li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/official_models.html">Official Models</a></li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0/api_cpp/mindspore.html">C++ API↗</a></li>
</ul>
<p class="caption"><span class="caption-text">API Mapping</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/pytorch_api_mapping.html">PyTorch and MindSpore API Mapping Table</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/tensorflow_api_mapping.html">TensorFlow and MindSpore API Mapping Table</a></li>
</ul>
<p class="caption"><span class="caption-text">Migration Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/overview.html">Overview of Migration Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/enveriment_preparation.html">Environment Preparation and Information Acquisition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/analysis_and_preparation.html">Model Analysis and Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/model_development/model_development.html">Constructing MindSpore Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/debug_and_tune.html">Debugging and Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/sample_code.html">Network Migration Debugging Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/faq.html">FAQs</a></li>
</ul>
<p class="caption"><span class="caption-text">Syntax Support</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax_support.html">Static Graph Syntax Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/index_support.html">Tensor Index Support</a></li>
</ul>
<p class="caption"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/data_processing.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/implement_problem.html">Implement Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/network_compilation.html">Network Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/operators_compile.html">Operators Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/usage_migrate_3rd.html">Migration from a Third-party Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/precision_tuning.html">Precision Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/distributed_parallel.html">Distributed Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/feature_advice.html">Feature Advice</a></li>
</ul>
<p class="caption"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Distributed Training Design</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/design/distributed_training_design.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="distributed-training-design">
<h1>Distributed Training Design<a class="headerlink" href="#distributed-training-design" title="Permalink to this headline">¶</a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r2.0/docs/mindspore/source_en/design/distributed_training_design.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.0/resource/_static/logo_source_en.png"></a></p>
<div class="section" id="background">
<h2>Background<a class="headerlink" href="#background" title="Permalink to this headline">¶</a></h2>
<p>With the rapid development of deep learning, the number of datasets and parameters are growing exponentially to improve the accuracy and generalization capability of neural networks. Parallel distributed training has become a development trend to resolve the performance bottleneck of ultra-large scale networks. MindSpore supports the mainstream distributed training paradigm and develops an automatic hybrid parallel solution. The following describes the design principles of several parallel training modes and provides guidance for users to perform custom development.</p>
</div>
<div class="section" id="concepts">
<h2>Concepts<a class="headerlink" href="#concepts" title="Permalink to this headline">¶</a></h2>
<div class="section" id="collective-communication">
<h3>Collective Communication<a class="headerlink" href="#collective-communication" title="Permalink to this headline">¶</a></h3>
<p>Collective communication is defined as communication that involves a group of processes. All processes in the group send and receive data after meeting certain conditions. MindSpore implements data transmission during parallel training through collective communication. On Ascend chips, MindSpore depends on the Huawei Collective Communication Library (<code class="docutils literal notranslate"><span class="pre">HCCL</span></code>) to implement the task. On GPU, MindSpore depends on the NVIDIA Collective Communication Library (<code class="docutils literal notranslate"><span class="pre">NCCL</span></code>) to implement the task.</p>
</div>
<div class="section" id="synchronization-mode">
<h3>Synchronization Mode<a class="headerlink" href="#synchronization-mode" title="Permalink to this headline">¶</a></h3>
<p>In synchronous mode, all devices strart training at the same time and update parameter values synchronously after the backward propagation algorithm is executed. Currently, MindSpore uses the synchronous training mode.</p>
</div>
</div>
<div class="section" id="data-parallelism">
<h2>Data Parallelism<a class="headerlink" href="#data-parallelism" title="Permalink to this headline">¶</a></h2>
<p>This section describes how the data parallel mode <code class="docutils literal notranslate"><span class="pre">ParallelMode.DATA_PARALLEL</span></code> works in MindSpore.</p>
<div class="section" id="principle-of-data-parallelism">
<h3>Principle of Data Parallelism<a class="headerlink" href="#principle-of-data-parallelism" title="Permalink to this headline">¶</a></h3>
<p><img alt="Data Parallel Description" src="../_images/data_parallel.png" /></p>
<ol>
<li><p>Environment dependencies</p>
<p>Each time before parallel training starts, the <code class="docutils literal notranslate"><span class="pre">mindspore.communication.init</span></code> API is called to initialize communication resources and the global communication group <code class="docutils literal notranslate"><span class="pre">WORLD_COMM_GROUP</span></code> is automatically created.</p>
</li>
<li><p>Data distribution</p>
<p>The key of data parallelism is to split datasets based on the sample dimension and deliver the split datasets to different devices. Each dataset loading API provided by the <code class="docutils literal notranslate"><span class="pre">mindspore.dataset</span></code> module has the <code class="docutils literal notranslate"><span class="pre">num_shards</span></code> and <code class="docutils literal notranslate"><span class="pre">shard_id</span></code> parameters. The parameters are used to split a dataset into multiple datasets, perform cyclic sampling, and collect data of the <code class="docutils literal notranslate"><span class="pre">batch</span></code> size to each device. When the data volume is insufficient, the sampling restarts from the beginning.</p>
</li>
<li><p>Network structure</p>
<p>The scripting method of data parallel network is the same as that of standalone network. This is because, although models of each device are executed independently during the forward and backward propagation processes, the same network structure is maintained. To ensure the synchronous training between devices, the initial values of corresponding network parameters must be the same. You are advised to enable <code class="docutils literal notranslate"><span class="pre">parameter_broadcast</span></code> to broadcast the values of weights in <code class="docutils literal notranslate"><span class="pre">DATA_PARALLEL</span></code> and <code class="docutils literal notranslate"><span class="pre">HYBRID_PARALLEL</span></code> modes. And in <code class="docutils literal notranslate"><span class="pre">AUTO_PARALLEL</span></code> and <code class="docutils literal notranslate"><span class="pre">SEMI_AUTO_PARALLEL</span></code> modes, the sharded dimensions of weights will be processed automatically by setting random seeds to ensure the initialization of weights are consistent on the devices which belongs to the same data parallel dimension.</p>
</li>
<li><p>Gradient aggregation</p>
<p>Theoretically, the training effect of data parallel network should be the same as that of the standalone network. To ensure the consistency of the calculation logic, the <code class="docutils literal notranslate"><span class="pre">AllReduce</span></code> operator is inserted after gradient calculation to implement the gradient aggregation operation between devices. You can enable <code class="docutils literal notranslate"><span class="pre">mean</span></code> to average the sum of gradient values, or regard <code class="docutils literal notranslate"><span class="pre">mean</span></code> as a hyperparameter. Enabling <code class="docutils literal notranslate"><span class="pre">mean</span></code> is equivalent to reducing the learning rate by multiple times.</p>
</li>
<li><p>Parameter update</p>
<p>Because the gradient aggregation operation is introduced, the models of each device perform parameter update with the same gradient value. Therefore, MindSpore implements a synchronous data parallel training mode. Theoretically, models trained by each device are the same. If the reduce operation on samples is involved on the network, the network output may be different. This is determined by the sharding attribute of data parallelism.</p>
</li>
</ol>
</div>
<div class="section" id="data-parallel-code">
<h3>Data Parallel Code<a class="headerlink" href="#data-parallel-code" title="Permalink to this headline">¶</a></h3>
<ol class="simple">
<li><p>Collective communication</p>
<ul class="simple">
<li><p><a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.0/mindspore/python/mindspore/communication/management.py">management.py</a>: This file covers the <code class="docutils literal notranslate"><span class="pre">helper</span></code> function APIs commonly used during the collective communication process, for example, the APIs for obtaining the number of clusters and device ID. When collective communication is executed on the Ascend chip, the framework loads the <code class="docutils literal notranslate"><span class="pre">libhccl.so</span></code> library file in the environment and uses it to call the communication APIs from the Python layer to the underlying layer.</p></li>
<li><p><a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.0/mindspore/python/mindspore/ops/operations/comm_ops.py">comm_ops.py</a>: MindSpore encapsulates supported collective communication operations as operators and stores the operators in this file. The operators include <code class="docutils literal notranslate"><span class="pre">AllReduce</span></code>, <code class="docutils literal notranslate"><span class="pre">AllGather</span></code>, <code class="docutils literal notranslate"><span class="pre">ReduceScatter</span></code>, and <code class="docutils literal notranslate"><span class="pre">Broadcast</span></code>. <code class="docutils literal notranslate"><span class="pre">PrimitiveWithInfer</span></code> defines the attributes required by the operators, as well as the <code class="docutils literal notranslate"><span class="pre">shape</span></code> and <code class="docutils literal notranslate"><span class="pre">dtype</span></code> inference methods from the input to the output during graph composition.</p></li>
</ul>
</li>
<li><p>Gradient aggregation</p>
<ul class="simple">
<li><p><a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.0/mindspore/python/mindspore/nn/wrap/grad_reducer.py">grad_reducer.py</a>: This file implements the gradient aggregation process. After the input parameter <code class="docutils literal notranslate"><span class="pre">grads</span></code> is expanded by using <code class="docutils literal notranslate"><span class="pre">HyperMap</span></code>, the <code class="docutils literal notranslate"><span class="pre">AllReduce</span></code> operator is inserted. The global communication group is used. You can also perform custom development by referring to this section based on your network requirements. In MindSpore, standalone and distributed execution shares a set of network encapsulation APIs. In the <code class="docutils literal notranslate"><span class="pre">Cell</span></code>, <code class="docutils literal notranslate"><span class="pre">ParallelMode</span></code> is used to determine whether to perform gradient aggregation. For details about the network encapsulation APIs, see the <code class="docutils literal notranslate"><span class="pre">TrainOneStepCell</span></code> code implementation.</p></li>
</ul>
</li>
</ol>
</div>
</div>
<div class="section" id="automatic-parallelism">
<h2>Automatic Parallelism<a class="headerlink" href="#automatic-parallelism" title="Permalink to this headline">¶</a></h2>
<p>As a key feature of MindSpore, automatic parallelism is used to implement hybrid parallel training that combines automatic data parallelism and model parallelism. It aims to help users express the parallel algorithm logic using standalone scripts, reduce the difficulty of distributed training, improve the algorithm R&amp;D efficiency, and maintain the high performance of training. This section describes how the automatic parallel mode <code class="docutils literal notranslate"><span class="pre">ParallelMode.AUTO_PARALLEL</span></code> and semi-automatic parallel mode <code class="docutils literal notranslate"><span class="pre">ParallelMode.SEMI_AUTO_PARALLEL</span></code> work in MindSpore.</p>
<div class="section" id="principle-of-automatic-parallelism">
<h3>Principle of Automatic Parallelism<a class="headerlink" href="#principle-of-automatic-parallelism" title="Permalink to this headline">¶</a></h3>
<p><img alt="Automatic Parallel Description" src="../_images/auto_parallel.png" /></p>
<ol>
<li><p>Distributed operator and tensor layout</p>
<p>As shown in the preceding figure, the automatic parallel process traverses the standalone forward ANF graphs and performs shard modeling on tensors in the unit of distributed operator, indicating how the input and output tensors of an operator are distributed to each device of the cluster, that is, the tensor layout. Users do not need to know which device runs which slice of a model. The framework automatically schedules and allocates model slices.</p>
<p>To obtain the tensor layout model, each operator has a shard strategy, which indicates the shard status of each input of the operator in the corresponding dimension. Generally, tensors can be sharded in any dimension as long as the value is a multiple of 2, and the even distribution principle is met. The following figure shows an example of the three-dimensional <code class="docutils literal notranslate"><span class="pre">BatchMatmul</span></code> operation. The parallel strategy consists of two tuples, indicating the sharding of <code class="docutils literal notranslate"><span class="pre">input</span></code> and <code class="docutils literal notranslate"><span class="pre">weight</span></code>, respectively. Elements in a tuple correspond to tensor dimensions one by one. <code class="docutils literal notranslate"><span class="pre">2^N</span></code> indicates the shard unit, and <code class="docutils literal notranslate"><span class="pre">1</span></code> indicates that the tuple is not sharded. If you want to express a parallel data shard strategy, that is, only data in the <code class="docutils literal notranslate"><span class="pre">batch</span></code> dimension of <code class="docutils literal notranslate"><span class="pre">input</span></code> is sharded, and data in other dimensions are not sharded, you can use <code class="docutils literal notranslate"><span class="pre">strategy=((2^N,</span> <span class="pre">1,</span> <span class="pre">1),(1,</span> <span class="pre">1,</span> <span class="pre">1))</span></code>. If you want to express a parallel model shard strategy, that is, only model in the non-<code class="docutils literal notranslate"><span class="pre">batch</span></code> dimension of <code class="docutils literal notranslate"><span class="pre">weight</span></code> is sharded, for example, only the <code class="docutils literal notranslate"><span class="pre">channel</span></code> dimension is sharded, you can use <code class="docutils literal notranslate"><span class="pre">strategy=((1,</span> <span class="pre">1,</span> <span class="pre">1),(1,</span> <span class="pre">1,</span> <span class="pre">2^N))</span></code>. If you want to express a hybrid parallel shard strategy, one of which is <code class="docutils literal notranslate"><span class="pre">strategy=((2^N,</span> <span class="pre">1,</span> <span class="pre">1),(1,</span> <span class="pre">1,</span> <span class="pre">2^N))</span></code>.</p>
<p><img alt="Operator Sharding Definition" src="../_images/operator_split.png" /></p>
<p>Based on the shard strategy of an operator, the framework automatically derives the distribution model of input tensors and output tensors of the operator. This distribution model consists of <code class="docutils literal notranslate"><span class="pre">device_matrix</span></code>, <code class="docutils literal notranslate"><span class="pre">tensor_shape</span></code>, and <code class="docutils literal notranslate"><span class="pre">tensor</span> <span class="pre">map</span></code>, which indicate the device matrix shape, tensor shape, and mapping between devices and tensor dimensions, respectively. Based on the tensor layout model, distributed operator determines whether to insert extra computation and communication operations in the graph to ensure that the operator computing logic is correct.</p>
</li>
<li><p>Tensor Redistribution</p>
<p>When the output tensor model of an operator is inconsistent with the input tensor model of the next operator, computation and communication operations need to be introduced to implement the change between tensor layouts. The automatic parallel process introduces the tensor redistribution algorithm, which can be used to derive the communication conversion operations between random tensor layouts. The following three examples represent a parallel computing process of the formula <code class="docutils literal notranslate"><span class="pre">Z=(X×W)×V</span></code>, that is, a <code class="docutils literal notranslate"><span class="pre">MatMul</span></code> operation of two two-dimensional matrices, and show how to perform conversion between different parallel modes.</p>
<p>In example 1, the output of the first data parallel matrix multiplication is sharded in the row rection, and the input of the second model parallel matrix multiplication requires full tensors. The framework automatically inserts the <code class="docutils literal notranslate"><span class="pre">AllGather</span></code> operator to implement redistribution.</p>
<p><img alt="Tensor Redistribution" src="../_images/tensor_redistribution1.png" /></p>
<p>In example 2, the output of parallel matrix multiplication of the first model is sharded in the column direction, and the input of parallel matrix multiplication of the second model is sharded in the row direction. The framework automatically inserts a communication operator equivalent to the <code class="docutils literal notranslate"><span class="pre">AlltoAll</span></code> operation in collective communication to implement redistribution.</p>
<p><img alt="Tensor Redistribution" src="../_images/tensor_redistribution2.png" /></p>
<p>In example 3, an output shard mode of the first hybrid parallel matrix multiplication is the same as an input shard mode of the second hybrid parallel matrix multiplication. Therefore, redistribution does not need to be introduced. In the second matrix multiplication operation, the related dimensions of the two inputs are sharded. Therefore, the <code class="docutils literal notranslate"><span class="pre">AllReduce</span></code> operator needs to be inserted to ensure the operation correctness.</p>
<p><img alt="Tensor Redistribution" src="../_images/tensor_redistribution3.png" /></p>
<p>In general, this distributed representation breaks the boundary between data parallelism and model parallelism, making it easy to implement hybrid parallelism. From the perspective of scripts, users only need to construct a standalone network to express the parallel algorithm logic. Framework automatically shards the entire graph.</p>
</li>
<li><p>Efficient parallel strategy search algorithm</p>
<p>The <code class="docutils literal notranslate"><span class="pre">SEMI_AUTO_PARALLEL</span></code> semi-automatic parallel mode indicates that you manually configure the parallel strategy for operators when you are familiar with the operator sharding representation. This mode is helpful for manual optimization, with certain commissioning difficulty. You need to master the parallel principle and obtain a high-performance parallel solution based on the network structure and cluster topology. <code class="docutils literal notranslate"><span class="pre">SEMI_AUTO_PARALLEL</span></code> requires users to configure every operator with sharding strategy. To reduce the users’ burden in configuring sharding strategies, the automatic parallel mode supports Sharding Propagation, which propagates sharding strategies from configured ops to non-configured ops. To completely liberate users from manual configuration, <code class="docutils literal notranslate"><span class="pre">AUTO_PARALLEL</span></code> introduces the automatic search feature of the parallel strategy, which builds cost models based on the hardware platform, and calculates the computation cost, memory cost, and communication cost of a certain amount of data and specific operators based on different parallel strategies. Using the dynamic programming algorithm or recursive programming algorithm and taking the memory capacity of a single device as a constraint condition, a parallel strategy with optimal performance is efficiently searched out.</p>
<p>Strategy search replaces manual model sharding and provides a high-performance sharding solution within a short period of time, greatly reducing the threshold for parallel training.</p>
</li>
<li><p>Convenient distributed automatic differentiation</p>
<p>In addition to forward network communication, the traditional manual model sharding needs to consider backward parallel computing. MindSpore encapsulates communication operations into operators and automatically generates backward propagation of communication operators based on the original automatic differentiation operations of the framework. Therefore, even during distributed training, users only need to pay attention to the forward propagation of the network to implement actual automatic parallel training.</p>
</li>
</ol>
</div>
<div class="section" id="automatic-parallel-code">
<h3>Automatic Parallel Code<a class="headerlink" href="#automatic-parallel-code" title="Permalink to this headline">¶</a></h3>
<ol class="simple">
<li><p>Tensor layout model</p>
<ul class="simple">
<li><p><a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r2.0/mindspore/ccsrc/frontend/parallel/tensor_layout">tensor_layout</a>: This directory contains the definitions and implementation of functions related to the tensor distribution model. <code class="docutils literal notranslate"><span class="pre">tensor_layout.h</span></code> declares the member variables <code class="docutils literal notranslate"><span class="pre">tensor_map_origin_</span></code>, <code class="docutils literal notranslate"><span class="pre">tensor_shape_</span></code>, and <code class="docutils literal notranslate"><span class="pre">device_arrangement_</span></code> required by a tensor distribution model. In <code class="docutils literal notranslate"><span class="pre">tensor_redistribution.h</span></code>, the related methods for implementing the <code class="docutils literal notranslate"><span class="pre">from_origin_</span></code> and <code class="docutils literal notranslate"><span class="pre">to_origin_</span></code> transformation between tensor distributions are declared. The deduced redistribution operation is stored in <code class="docutils literal notranslate"><span class="pre">operator_list_</span></code> and returned, in addition, the communication cost <code class="docutils literal notranslate"><span class="pre">comm_cost_</span></code>,, memory cost <code class="docutils literal notranslate"><span class="pre">memory_cost_</span></code>, and calculation cost <code class="docutils literal notranslate"><span class="pre">computation_cost_</span></code> required for redistribution are calculated.</p></li>
</ul>
</li>
<li><p>Distributed operators</p>
<ul class="simple">
<li><p><a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r2.0/mindspore/ccsrc/frontend/parallel/ops_info">ops_info</a>: This directory contains the implementation of distributed operators. In <code class="docutils literal notranslate"><span class="pre">operator_info.h</span></code>, the base class <code class="docutils literal notranslate"><span class="pre">OperatorInfo</span></code> of distributed operator implementation is defined. A distributed operator to be developed shall inherit the base class and explicitly implement related imaginary functions. The <code class="docutils literal notranslate"><span class="pre">InferTensorInfo</span></code>, <code class="docutils literal notranslate"><span class="pre">InferTensorMap</span></code>, and <code class="docutils literal notranslate"><span class="pre">InferDevMatrixShape</span></code> functions define the algorithms for deriving the input and output tensor distribution model of the operator. The <code class="docutils literal notranslate"><span class="pre">InferForwardCommunication</span></code> and <code class="docutils literal notranslate"><span class="pre">InferMirrorOps</span></code> functions define the extra calculation and communication operations to be inserted for operator sharding. The <code class="docutils literal notranslate"><span class="pre">CheckStrategy</span></code> and <code class="docutils literal notranslate"><span class="pre">GenerateStrategies</span></code> functions define the parallel strategy validation and generation for the operator. According to the parallel strategy <code class="docutils literal notranslate"><span class="pre">SetCostUnderStrategy</span></code>, the parallel cost <code class="docutils literal notranslate"><span class="pre">operator_cost_</span></code> of the distributed operator is generated.</p></li>
</ul>
</li>
<li><p>Strategy search algorithm</p>
<ul class="simple">
<li><p><a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r2.0/mindspore/ccsrc/frontend/parallel/auto_parallel">auto_parallel</a>: The shard strategy search algorithm is implemented in this directory. <code class="docutils literal notranslate"><span class="pre">graph_costmodel.h</span></code> defines the graph composition information. Each point indicates an operator <code class="docutils literal notranslate"><span class="pre">OperatorInfo</span></code>. The directed edge <code class="docutils literal notranslate"><span class="pre">edge_costmodel.h</span></code> indicates the input and output relationship of operators and the redistribution cost. <code class="docutils literal notranslate"><span class="pre">operator_costmodel.h</span></code> defines the cost model of each operator, including the calculation cost, communication cost, and memory cost. <code class="docutils literal notranslate"><span class="pre">dp_algorithm_costmodel.h</span></code> describes the main process of the dynamic planning algorithm, which consists of a series of graph operations. <code class="docutils literal notranslate"><span class="pre">costmodel.h</span></code> defines the data structures of cost and graph operations.</p></li>
</ul>
</li>
<li><p>Device management</p>
<ul class="simple">
<li><p><a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.0/mindspore/ccsrc/frontend/parallel/device_manager.h">device_manager.h</a>: This file is used to create and manage cluster device communication groups. The device matrix model is defined by <code class="docutils literal notranslate"><span class="pre">device_matrix.h</span></code>, and the communication domain is managed by <code class="docutils literal notranslate"><span class="pre">group_manager.h</span></code>.</p></li>
</ul>
</li>
<li><p>Entire graph sharding</p>
<ul class="simple">
<li><p><a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.0/mindspore/ccsrc/frontend/parallel/step_auto_parallel.h">step_auto_parallel.h</a>, and <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.0/mindspore/ccsrc/frontend/parallel/step_parallel.h">step_parallel.h</a>: The two files contain the core implementation of the automatic parallel process. <code class="docutils literal notranslate"><span class="pre">step_auto_parallel.h</span></code> calls the strategy search process and generates the <code class="docutils literal notranslate"><span class="pre">OperatorInfo</span></code> of the distributed operator. Then in <code class="docutils literal notranslate"><span class="pre">step_parallel.h</span></code>, processes such as operator sharding and tensor redistribution are processed to reconstruct the standalone computing graph in distributed mode.</p></li>
</ul>
</li>
<li><p>Backward propagation of communication operators</p>
<ul class="simple">
<li><p><a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.0/mindspore/python/mindspore/ops/_grad/grad_comm_ops.py">grad_comm_ops.py</a>: This file defines the backward propagation of communication operators, such as <code class="docutils literal notranslate"><span class="pre">AllReduce</span></code> and <code class="docutils literal notranslate"><span class="pre">AllGather</span></code>.</p></li>
</ul>
</li>
</ol>
</div>
</div>
<div class="section" id="heterogeneous-parallelism">
<h2>Heterogeneous Parallelism<a class="headerlink" href="#heterogeneous-parallelism" title="Permalink to this headline">¶</a></h2>
<p>The heterogeneous parallel training method is to analyze the memory occupation and computational intensity of the operators on the graph, and slice the operators with huge memory consumption or suitable for CPU logic processing to the CPU subgraph, and slice the computationally intensive operators with less memory consumption to the hardware accelerator subgraph. The framework cooperates with different subgraphs for network training, so that subgraphs in different hardware and without dependencies can perform the execution process in parallel.</p>
<div class="section" id="computational-process">
<h3>Computational Process<a class="headerlink" href="#computational-process" title="Permalink to this headline">¶</a></h3>
<p>A typical computational process for MindSpore heterogeneous parallel training is shown in the following figure:</p>
<ol>
<li><p>Users set backend for network execution</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Users set execution backend of specific operators</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span>

<span class="n">prim</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span>

<span class="n">prim</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>The framework is sliced according to the computational graph operator flag.</p></li>
<li><p>The framework schedules different back-end execution subgraphs.</p></li>
</ol>
<p>Current scenarios that typically use heterogeneous parallel computing are: optimizer heterogeneity, Embedding heterogeneity, and PS heterogeneity.</p>
</div>
<div class="section" id="optimizer-heterogeneity">
<h3>Optimizer Heterogeneity<a class="headerlink" href="#optimizer-heterogeneity" title="Permalink to this headline">¶</a></h3>
<p>During the training of a large model in PanGu or GPT3, the optimizer state takes up a large amount of memory, which in turn limits the size of the model that can be trained. Using optimizer heterogeneity, assigning optimizers to CPUs for execution can greatly scale the trainable models:</p>
<p><img alt="heterogeneous-heter-opt" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.0/docs/mindspore/source_zh_cn/design/images/heter-opt.png" /></p>
<p>As shown in the figure, configuring the Adam operator to CPU execution while specifying an accelerator for FP16 computation reduces the parameter memory footprint to 1/3 of the original.</p>
<ol class="simple">
<li><p>Configure the optimizer operators to CPU execution</p></li>
<li><p>Initialize weight parameters of FP16 and optimizer state variables of FP32</p></li>
<li><p>Convert the gradient of the input optimizer to FP16 (if the gradient is FP16, you can ignore this step)</p></li>
<li><p>The weights and gradients are converted to FP32 to participate in the optimizer operation</p></li>
<li><p>The updated FP32 weights are assigned to the FP16 weights</p></li>
</ol>
<p>Sample code of the optimizer heterogeneity is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">mindspore.common.initializer</span> <span class="kn">import</span> <span class="n">initializer</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">Optimizer</span>
<span class="n">_adam_opt</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MultitypeFuncGraph</span><span class="p">(</span><span class="s2">&quot;adam_opt&quot;</span><span class="p">)</span>
<span class="n">host_assign</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Assign</span><span class="p">()</span>
<span class="n">host_assign</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>
<span class="n">host_cast</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>
<span class="n">host_cast</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>
<span class="n">device_cast</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>

<span class="nd">@_adam_opt</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;Function&quot;</span><span class="p">,</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;Number&quot;</span><span class="p">,</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;Bool&quot;</span><span class="p">,</span> <span class="s2">&quot;Bool&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_update_run_kernel</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">weight_decay</span><span class="p">,</span> <span class="n">param</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">decay_flags</span><span class="p">,</span> <span class="n">optim_filter</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Update parameters by AdamWeightDecay op.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">success</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">if</span> <span class="n">optim_filter</span><span class="p">:</span>
        <span class="n">param32</span> <span class="o">=</span> <span class="n">host_cast</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">gradient</span> <span class="o">=</span> <span class="n">device_cast</span><span class="p">(</span><span class="n">gradient</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">decay_flags</span><span class="p">:</span>
            <span class="n">next_param</span> <span class="o">=</span> <span class="n">opt</span><span class="p">(</span><span class="n">param32</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">weight_decay</span><span class="p">,</span> <span class="n">gradient</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">next_param</span> <span class="o">=</span> <span class="n">opt</span><span class="p">(</span><span class="n">param32</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">gradient</span><span class="p">)</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">host_assign</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">host_cast</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">param32</span><span class="p">,</span> <span class="n">next_param</span><span class="p">),</span> <span class="n">ops</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">param</span><span class="p">)))</span>
        <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">success</span><span class="p">,</span> <span class="n">ret</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">success</span>

<span class="k">class</span> <span class="nc">AdamWeightDecayOp</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AdamWeightDecayOp</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">weight_decay</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">beta1</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta2</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">beta2</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">eps</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">moments1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clone_param32</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;adam_m&quot;</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">moments2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clone_param32</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;adam_v&quot;</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">opt</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">AdamWeightDecay</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hyper_map</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">HyperMap</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">opt</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gradients</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;AdamWeightDecayOp&quot;&quot;&quot;</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_lr</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_group</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_group_lr</span><span class="p">:</span>
                <span class="n">optim_result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">map_reverse</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_adam_opt</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">opt</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">),</span>
                                                <span class="n">lr</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">moments1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">moments2</span><span class="p">,</span>
                                                <span class="n">gradients</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">decay_flags</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optim_filter</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">optim_result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">map_reverse</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_adam_opt</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">opt</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span> <span class="n">lr</span><span class="p">),</span>
                                                <span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">moments1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">moments2</span><span class="p">,</span>
                                                <span class="n">gradients</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">decay_flags</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optim_filter</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">optim_result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">map_reverse</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_adam_opt</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">opt</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span>
                                                        <span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">moments1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">moments2</span><span class="p">,</span>
                                            <span class="n">gradients</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">decay_flags</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optim_filter</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">optim_result</span>

    <span class="k">def</span> <span class="nf">clone_param32</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">new</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">old_param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">:</span>
            <span class="n">param_init</span> <span class="o">=</span> <span class="n">init</span>
            <span class="k">if</span> <span class="n">init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">param_init</span> <span class="o">=</span> <span class="n">old_param</span><span class="o">.</span><span class="n">init</span>
            <span class="n">new_state</span> <span class="o">=</span> <span class="n">old_param</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
            <span class="n">new_state</span><span class="o">.</span><span class="n">set_dtype</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
            <span class="n">new_state</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="n">param_init</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">old_param</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
            <span class="n">new_state</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">prefix</span> <span class="o">+</span> <span class="s1">&#39;.&#39;</span> <span class="o">+</span> <span class="n">new_state</span><span class="o">.</span><span class="n">name</span>
            <span class="n">new</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_state</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ms</span><span class="o">.</span><span class="n">ParameterTuple</span><span class="p">(</span><span class="n">new</span><span class="p">)</span>
</pre></div>
</div>
<p>Steps 4 and 5 can also be directly fused into the optimizer operator for further optimization. The complete optimizer heterogeneous training process can be found at: <a class="reference external" href="https://gitee.com/mindspore/models/tree/r2.0/official/nlp/Pangu_alpha">https://gitee.com/mindspore/models/tree/r2.0/official/nlp/Pangu_alpha</a>.</p>
</div>
<div class="section" id="embedding-heterogeneity">
<h3>Embedding Heterogeneity<a class="headerlink" href="#embedding-heterogeneity" title="Permalink to this headline">¶</a></h3>
<p>In some networks where large Embedding tables need to be checked, the Embedding tables are often hundreds of gigabytes in size, which is limited by the accelerator memory size and cannot be executed by loading the entire table directly onto the accelerator. By putting the operators connected to the weight table on the CPU for execution, we avoid the problem that the accelerator cannot train the network due to memory limitation.</p>
<p><img alt="heterogeneous-heter-embed" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.0/docs/mindspore/source_zh_cn/design/images/heter-embed.png" /></p>
<ol>
<li><p>Configure EmbeddingLookup operator to CPU execution</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.common.initializer</span> <span class="kn">import</span> <span class="n">initializer</span>

<span class="k">class</span> <span class="nc">EmbeddingLookupNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="n">param_init</span><span class="o">=</span><span class="s1">&#39;normal&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EmbeddingLookupNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embeddinglookup</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">EmbeddingLookup</span><span class="p">()</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="s1">&#39;CPU&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_table</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="n">param_init</span><span class="p">,</span> <span class="p">[</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;embedding_table&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddinglookup</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_table</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
</li>
<li><p>Configure related sparse optimizer of EmbeddingLookup to CPU execution</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore.nn.optim</span> <span class="kn">import</span> <span class="n">LazyAdam</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">EmbeddingLookupNet</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">LazyAdam</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="s2">&quot;CPU&quot;</span>
</pre></div>
</div>
</li>
</ol>
<p>A sample code for setting up the EmbeddingLookup operator is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.common.initializer</span> <span class="kn">import</span> <span class="n">initializer</span>

<span class="k">class</span> <span class="nc">EmbeddingLookup</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="n">param_init</span><span class="o">=</span><span class="s1">&#39;normal&#39;</span><span class="p">,</span>
                 <span class="n">target</span><span class="o">=</span><span class="s1">&#39;CPU&#39;</span><span class="p">,</span> <span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize EmbeddingLookup.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EmbeddingLookup</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;sparse&#39;</span><span class="p">,</span> <span class="n">sparse</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="s1">&#39;vocab_size&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="n">target</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sparse</span> <span class="o">=</span> <span class="n">sparse</span>
        <span class="k">if</span> <span class="n">target</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;CPU&#39;</span><span class="p">,</span> <span class="s1">&#39;DEVICE&#39;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Attr </span><span class="se">\&#39;</span><span class="s1">target</span><span class="se">\&#39;</span><span class="s1"> of </span><span class="se">\&#39;</span><span class="s1">EmbeddingLookup</span><span class="se">\&#39;</span><span class="s1"> Op passed &#39;</span>
                             <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">target</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;, should be one of values in </span><span class="se">\&#39;</span><span class="s1">CPU</span><span class="se">\&#39;</span><span class="s1">, </span><span class="se">\&#39;</span><span class="s1">DEVICE</span><span class="se">\&#39;</span><span class="s1">.&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">sparse</span> <span class="ow">and</span> <span class="n">target</span> <span class="o">==</span> <span class="s1">&#39;CPU&#39;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;When target is CPU, embedding_lookup must be sparse.&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">sparse</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gatherv2</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">SparseGatherV2</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gatherv2</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Gather</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embeddinglookup</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">EmbeddingLookup</span><span class="p">()</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="s1">&#39;CPU&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">embedding_size</span><span class="p">,</span> <span class="s1">&#39;embedding_size&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_table</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="n">param_init</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span><span class="p">]),</span>
                                            <span class="n">name</span><span class="o">=</span><span class="s1">&#39;embedding_table&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">target</span> <span class="o">==</span> <span class="s2">&quot;CPU&quot;</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddinglookup</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_table</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gatherv2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_table</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
<p>EmbeddingLookup, FTRL, LazyAdam and other operators in the current nn directory are encapsulated the heterogeneous interface, and the user only needs to set the target attribute to CPU or DEVICE to switch the execution backend.</p>
<p>For the overall calling process, refer to <a class="reference external" href="https://gitee.com/mindspore/models/tree/r2.0/official/recommend/Wide_and_Deep">https://gitee.com/mindspore/models/tree/r2.0/official/recommend/Wide_and_Deep</a>.</p>
</div>
<div class="section" id="ps-heterogeneity">
<h3>PS Heterogeneity<a class="headerlink" href="#ps-heterogeneity" title="Permalink to this headline">¶</a></h3>
<p>When the EmbeddingTable reaches T level and the single machine memory cannot be put down, Parameter Server is used to pull and update the weights by heterogeneous Pull/Push operators.</p>
<p><img alt="heterogeneous-heter-ps" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.0/docs/mindspore/source_zh_cn/design/images/heter-ps.png" /></p>
<p>Parameter Server encapsulates heterogeneous processes, and users only need to configure parameters to use PS. For the detailed configuration process, refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0/parallel/parameter_server_training.html">Parameter Server training process</a>.</p>
<p>In addition, the process of using PS is also available in the wide&amp;deep network and can be found at: <a class="reference external" href="https://gitee.com/mindspore/models/tree/r2.0/official/recommend/Wide_and_Deep">https://gitee.com/mindspore/models/tree/r2.0/official/recommend/Wide_and_Deep</a>.</p>
</div>
<div class="section" id="constraints">
<h3>Constraints<a class="headerlink" href="#constraints" title="Permalink to this headline">¶</a></h3>
<p>Currently requires the user to specify the back-end of the operator execution and does not support automatic configuration based on the network.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="graph_fusion_engine.html" class="btn btn-neutral float-right" title="Graph-Kernel Fusion Acceleration Engine" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="pluggable_device.html" class="btn btn-neutral float-left" title="Third-Party Hardware Interconnection" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2022, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>