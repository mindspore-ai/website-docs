

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Feature Advice &mdash; MindSpore master documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/training.css" type="text/css" />
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script src="../_static/js/training.js"></script>
        
        
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Release Notes" href="../RELEASE.html" />
    <link rel="prev" title="Inference" href="inference.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../design/overview.html">MindSpore Design Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/programming_paradigm.html">Programming Paradigm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/auto_gradient.html">Functional Differential Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/mindir.html">MindSpore IR (MindIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/all_scenarios.html">Full-scenarios Unification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/dynamic_graph_and_static_graph.html">Combination of Dynamic and Static Graphs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/pluggable_device.html">Third-Party Hardware Interconnection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/distributed_training_design.html">Distributed Training Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/graph_fusion_engine.html">Graph-Kernel Fusion Acceleration Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/data_engine.html">High Performance Data Processing Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/glossary.html">Glossary</a></li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/official_models.html">Official Models</a></li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0/api_cpp/mindspore.html">C++ API↗</a></li>
</ul>
<p class="caption"><span class="caption-text">API Mapping</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/pytorch_api_mapping.html">PyTorch and MindSpore API Mapping Table</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/tensorflow_api_mapping.html">TensorFlow and MindSpore API Mapping Table</a></li>
</ul>
<p class="caption"><span class="caption-text">Migration Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/overview.html">Overview of Migration Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/enveriment_preparation.html">Environment Preparation and Information Acquisition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/analysis_and_preparation.html">Model Analysis and Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/model_development/model_development.html">Constructing MindSpore Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/debug_and_tune.html">Debugging and Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/sample_code.html">Network Migration Debugging Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/faq.html">FAQs</a></li>
</ul>
<p class="caption"><span class="caption-text">Syntax Support</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax_support.html">Static Graph Syntax Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/index_support.html">Tensor Index Support</a></li>
</ul>
<p class="caption"><span class="caption-text">FAQ</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_processing.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="implement_problem.html">Implement Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="network_compilation.html">Network Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="operators_compile.html">Operators Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="usage_migrate_3rd.html">Migration from a Third-party Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="precision_tuning.html">Precision Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_parallel.html">Distributed Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="inference.html">Inference</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Feature Advice</a></li>
</ul>
<p class="caption"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Feature Advice</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/faq/feature_advice.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="feature-advice">
<h1>Feature Advice<a class="headerlink" href="#feature-advice" title="Permalink to this headline">¶</a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r2.0/docs/mindspore/source_en/faq/feature_advice.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.0/resource/_static/logo_source_en.png"></a></p>
<p><font size=3><strong>Q: Is the <code class="docutils literal notranslate"><span class="pre">input=np.random.uniform(...)</span></code> format fixed when the MindIR format is exported?</strong></font></p>
<p>A: The format is not fixed. This step is to create an input for constructing the network structure. You only need to input the correct <code class="docutils literal notranslate"><span class="pre">shape</span></code> in <code class="docutils literal notranslate"><span class="pre">export</span></code>. You can use <code class="docutils literal notranslate"><span class="pre">np.ones</span></code> and <code class="docutils literal notranslate"><span class="pre">np.zeros</span></code> to create an input.</p>
<br/>
<p><font size=3><strong>Q: What framework models and formats can be directly read by MindSpore? Can the PTH Model obtained through training in PyTorch be loaded to the MindSpore framework for use?</strong></font></p>
<p>A: MindSpore uses Protobuf to store training parameters and cannot directly read framework models. A model file stores parameters and their values. You can use APIs of other frameworks to read parameters, obtain the key-value pairs of parameters, and load the key-value pairs to MindSpore. If you want to use the .ckpt file trained by other framework, read the parameters and then call the <code class="docutils literal notranslate"><span class="pre">save_checkpoint</span></code> API of MindSpore to save the file as a .ckpt file that can be read by MindSpore.</p>
<br/>
<p><font size=3><strong>Q: What is the difference between the PyNative and Graph modes?</strong></font></p>
<p>A: Compare through the following four aspects:</p>
<ul class="simple">
<li><p>In terms of network execution: operators used in the two modes are the same. Therefore, when the same network and operators are executed in the two modes, the accuracy is the same. As Graph mode uses graph optimization, calculation graph sinking and other technologies, it has higher performance and efficiency in executing the network.</p></li>
<li><p>In terms of application scenarios: Graph mode requires the network structure to be built at the beginning, and then the framework performs entire graph optimization and execution. This mode is suitable to scenarios where the network is fixed and high performance is required.</p></li>
<li><p>In term of different hardware (such as <code class="docutils literal notranslate"><span class="pre">Ascend</span></code>, <code class="docutils literal notranslate"><span class="pre">GPU</span></code>, and <code class="docutils literal notranslate"><span class="pre">CPU</span></code>) resources: the two modes are supported.</p></li>
<li><p>In terms of code debugging: since operators are executed line by line in PyNative mode, you can directly debug the Python code and view the <code class="docutils literal notranslate"><span class="pre">/api</span></code> output or execution result of the corresponding operator at any breakpoint in the code. In Graph mode, the network is built but not executed in the constructor function. Therefore, you cannot obtain the output of the corresponding operator at breakpoints in the <code class="docutils literal notranslate"><span class="pre">construct</span></code> function. You can only specify operators and print their output results, and then view the results after the network execution is completed.</p></li>
</ul>
<br/>
<p><font size=3><strong>Q: Does MindSpore run only on Huawei <code class="docutils literal notranslate"><span class="pre">Ascend</span></code>?</strong></font></p>
<p>A: MindSpore supports Huawei <code class="docutils literal notranslate"><span class="pre">Ascend</span></code>, <code class="docutils literal notranslate"><span class="pre">GPUs</span></code>, and <code class="docutils literal notranslate"><span class="pre">CPUs</span></code>, and supports heterogeneous computing.</p>
<br/>
<p><font size=3><strong>Q: If MindSpore and PyTorch are installed in an environment, can the syntax of the two frameworks be used together in a Python file?</strong></font></p>
<p>A: You can use the two frameworks in a python file. Pay attention to the differences between types. For example, the tensor types created by the two frameworks are different, but the basic types of Python are general.</p>
<br/>
<p><font size=3><strong>Q: Can MindSpore read a ckpt file of TensorFlow?</strong></font></p>
<p>A: The formats of  <code class="docutils literal notranslate"><span class="pre">ckpt</span></code> of MindSpore and <code class="docutils literal notranslate"><span class="pre">ckpt</span></code>of TensorFlow are not generic. Although both use the <code class="docutils literal notranslate"><span class="pre">Protocol</span></code> Buffers, the definition of <code class="docutils literal notranslate"><span class="pre">proto</span></code> are different. Currently, MindSpore cannot read the TensorFlow or Pytorch <code class="docutils literal notranslate"><span class="pre">ckpt</span></code> files.</p>
<br/>
<p><font size=3><strong>Q: How do I use models trained by MindSpore on Ascend 310? Can they be converted to models used by HiLens Kit?</strong></font></p>
<p>A: Yes. HiLens Kit uses Ascend 310 as the inference core. Therefore, the two questions are essentially the same, which both need to convert as OM model. Ascend 310 requires a dedicated OM model. Use MindSpore to export the ONNX or AIR model and convert it into an OM model supported by Ascend 310. For details, see <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.0/infer/inference.html">Multi-platform Inference</a>.</p>
<br/>
<p><font size=3><strong>Q: Does MindSpore only be run on Huawei own <code class="docutils literal notranslate"><span class="pre">Ascend</span></code>?</strong></font></p>
<p>A: MindSpore supports Huawei’s own <code class="docutils literal notranslate"><span class="pre">Ascend</span></code>, <code class="docutils literal notranslate"><span class="pre">GPU</span></code> and <code class="docutils literal notranslate"><span class="pre">CPU</span></code> at the same time, and supports heterogeneous computing power.</p>
<br/>
<p><font size=3><strong>Q: Can MindSpore be converted to an AIR model on Ascend 310?</strong></font></p>
<p>A: An AIR cannot be exported from the Ascend 310. You need to load a trained checkpoint on the Ascend 910, export an AIR model, and then convert the AIR model into an OM model for inference on the Ascend 310. For details about the Ascend 910 installation, see the MindSpore Installation Guide at <a class="reference external" href="https://www.mindspore.cn/install/en">here</a>.</p>
<br/>
<p><font size=3><strong>Q: Does MindSpore have any limitation on the input size of a single Tensor for exporting and loading models?</strong></font></p>
<p>A: Due to hardware limitations of Protobuf, when exporting to AIR and ONNX formats, the size of model parameters cannot exceed 2G; when exporting to MINDIR format, there is no limit to the size of model parameters. MindSpore only supports the importing of MINDIR and doesn’t support the importing of AIR and ONNX formats. The importing of MINDIR does not have size limitation.</p>
<br/>
<p><font size=3><strong>Q: Does MindSpore need a GPU computing unit? What hardware support is needed?</strong></font></p>
<p>A: MindSpore currently supports CPU, GPU, and Ascend. Currently, you can try out MindSpore through Docker images on laptops or in environments with GPUs. Some models in MindSpore Model Zoo support GPU-based training and inference, and other models are being improved. For distributed parallel training, MindSpore supports multi-GPU training. You can obtain the latest information from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.0/RELEASE.md">project release notes</a>.</p>
<br/>
<p><font size=3><strong>Q: Does MindSpore have any plan on supporting heterogeneous computing hardwares?</strong></font></p>
<p>A: MindSpore provides pluggable device management interface, so that developer could easily integrate other types of heterogeneous computing hardwares (like FPGA) to MindSpore. We welcome more backend support in MindSpore from the community.</p>
<br/>
<p><font size=3><strong>Q: What is the relationship between MindSpore and ModelArts? Can we use MindSpore in ModelArts?</strong></font></p>
<p>A: ModelArts is Huawei public cloud online training and inference platform, and MindSpore is Huawei deep learning framework. The tutorial shows how users can use ModelArts to train ModelsSpore models in detail.</p>
<br/>
<p><font size=3><strong>Q: The recent announced programming language such as taichi got Python extensions that could be directly used as <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">taichi</span> <span class="pre">as</span> <span class="pre">ti</span></code>. Does MindSpore have similar support?</strong></font></p>
<p>A: MindSpore supports Python native expression and <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">mindspore</span></code> related package can be used.</p>
<br/>
<p><font size=3><strong>Q: Does MindSpore support truncated gradient?</strong></font></p>
<p>A: Yes. For details, see <a class="reference external" href="https://gitee.com/mindspore/models/blob/r2.0/official/nlp/Transformer/src/transformer_for_train.py#L35">Definition and Usage of Truncated Gradient</a>.</p>
<br/>
<p><font size=3><strong>Q: What is the MindSpore IR design concept?</strong></font></p>
<p>A: Function expression: All expressions are functions, and differentiation and automatic parallel analysis are easy to implement without side effect. <code class="docutils literal notranslate"><span class="pre">JIT</span></code> compilation capability: The graph-based IR, control flow dependency, and data flow are combined to balance the universality and usability. Graphically complete IR: More conversion <code class="docutils literal notranslate"><span class="pre">Python</span></code> flexible syntax, including recursion, etc.</p>
<br/>
<p><font size=3><strong>Q: What are the advantages and features of MindSpore parallel model training?</strong></font></p>
<p>A: In addition to data parallelism, MindSpore distributed training also supports operator-level model parallelism. The operator input tensor can be tiled and parallelized. On this basis, automatic parallelism is supported. You only need to write a single-device script to automatically tile the script to multiple nodes for parallel execution.</p>
<br/>
<p><font size=3><strong>Q: How does MindSpore implement semantic collaboration and processing? Is the popular Formal Concept Analysis (FCA) used?</strong></font></p>
<p>A: The MindSpore framework does not support FCA. For semantic models, you can call third-party tools to perform FCA in the data preprocessing phase. MindSpore supports Python therefore <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">FCA</span></code> related package could do the trick.</p>
<br/>
<p><font size=3><strong>Q: Does MindSpore have any plan on the edge and device when the training and inference functions of MindSpore on the cloud are relatively mature?</strong></font></p>
<p>A: MindSpore is a unified cloud-edge-device training and inference framework, which supports exporting cloud-side trained models to Ascend AI processors and terminal devices for inference. The optimizations supported in the current inference stage include quantization, operator fusion, and memory overcommitment.</p>
<br/>
<p><font size=3><strong>Q: How does MindSpore support automatic parallelism?</strong></font></p>
<p>A: Automatic parallelism on CPUs and GPUs are being improved. You are advised to use the automatic parallelism on the Ascend 910 AI processor. Follow our open source community and apply for a MindSpore developer experience environment for trial use.</p>
<br/>
<p><font size=3><strong>Q: Does MindSpore have a similar module that can implement object detection algorithms based on TensorFlow?</strong></font></p>
<p>A: The TensorFlow’s object detection Pipeline API belongs to the TensorFlow’s Model module. After MindSpore’s detection models are complete, similar Pipeline APIs will be provided.</p>
<br/>
<p><font size=3><strong>Q: How do I perform transfer learning in PyNative mode?</strong></font></p>
<p>A: PyNative mode is compatible with transfer learning.</p>
<br/>
<p><font size=3><strong>Q: What is the difference between <a class="reference external" href="https://gitee.com/mindspore/models/blob/r2.0/README.md">MindSpore ModelZoo</a> and <a class="reference external" href="https://www.hiascend.com/software/modelzoo">Ascend ModelZoo</a>?</strong></font></p>
<p>A: <code class="docutils literal notranslate"><span class="pre">MindSpore</span> <span class="pre">ModelZoo</span></code> contains models mainly implemented by MindSpore. But these models support different devices including Ascend, GPU, CPU and Mobile. <code class="docutils literal notranslate"><span class="pre">Ascend</span> <span class="pre">ModelZoo</span></code> contains models only running on Ascend which are implemented by different ML platform including MindSpore, PyTorch, TensorFlow and Caffe. You can refer to the corresponding <a class="reference external" href="https://gitee.com/ascend/modelzoo">gitee repository</a>.</p>
<p>The combination of MindSpore and Ascend is overlapping, and this part of the model will be based on MindSpore’s ModelZoo as the main version, and will be released to Ascend ModelZoo regularly.</p>
<br/>
<p><font size=3><strong>Q: What is the relationship between Ascend and NPU?</strong></font></p>
<p>A: NPU refers to a dedicated processor for neural network algorithms. Different companies have different NPU architectures. Ascend is an NPU processor based on the DaVinci architecture developed by Huawei.</p>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="../RELEASE.html" class="btn btn-neutral float-right" title="Release Notes" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="inference.html" class="btn btn-neutral float-left" title="Inference" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2022, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>