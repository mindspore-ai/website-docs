

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Implement Problem &mdash; MindSpore master documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/training.css" type="text/css" />
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script src="../_static/js/training.js"></script>
        
        
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Network Compilation" href="network_compilation.html" />
    <link rel="prev" title="Data Processing" href="data_processing.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../design/overview.html">MindSpore Design Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/programming_paradigm.html">Programming Paradigm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/auto_gradient.html">Functional Differential Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/distributed_training_design.html">Distributed Training Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/dynamic_graph_and_static_graph.html">Combination of Dynamic and Static Graphs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/pluggable_device.html">Third-Party Hardware Interconnection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/graph_fusion_engine.html">Graph-Kernel Fusion Acceleration Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/mindir.html">MindSpore IR (MindIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/all_scenarios.html">Full-scenarios Unification</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r2.0/training_visual_design.html">Design of Visualization↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/data_engine.html">High Performance Data Processing Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/glossary.html">Glossary</a></li>
</ul>
<p class="caption"><span class="caption-text">Specification</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/benchmark.html">Benchmarks</a></li>
<li class="toctree-l1"><a class="reference external" href="https://gitee.com/mindspore/models/blob/r2.0/README.md#table-of-contents">Network List↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/operator_list.html">API List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/syntax_list.html">Syntax Support</a></li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0/api_cpp/mindspore.html">C++ API↗</a></li>
</ul>
<p class="caption"><span class="caption-text">API Mapping</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/pytorch_api_mapping.html">PyTorch and MindSpore API Mapping Table</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/tensorflow_api_mapping.html">TensorFlow and MindSpore API Mapping Table</a></li>
</ul>
<p class="caption"><span class="caption-text">Migration Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/overview.html">Overview of Migration Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/enveriment_preparation.html">Environment Preparation and Information Acquisition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/analysis_and_preparation.html">Model Analysis and Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/model_development/model_development.html">Constructing MindSpore Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/debug_and_tune.html">Debugging and Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/sample_code.html">Network Migration Debugging Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/faq.html">FAQs</a></li>
</ul>
<p class="caption"><span class="caption-text">FAQ</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_processing.html">Data Processing</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Implement Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="network_compilation.html">Network Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="operators_compile.html">Operators Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="usage_migrate_3rd.html">Migration from a Third-party Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="precision_tuning.html">Precision Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_parallel.html">Distributed Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="feature_advice.html">Feature Advice</a></li>
</ul>
<p class="caption"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Implement Problem</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/faq/implement_problem.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="implement-problem">
<h1>Implement Problem<a class="headerlink" href="#implement-problem" title="Permalink to this headline">¶</a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r2.0/docs/mindspore/source_en/faq/implement_problem.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.0/resource/_static/logo_source_en.png"></a></p>
<p><font size=3><strong>Q: How do I use MindSpore to implement multi-scale training?</strong></font></p>
<p>A: During multi-scale training, when different <code class="docutils literal notranslate"><span class="pre">shape</span></code> are used to call <code class="docutils literal notranslate"><span class="pre">Cell</span></code> objects, different graphs are automatically built and called based on different <code class="docutils literal notranslate"><span class="pre">shape</span></code>, to implement the multi-scale training. Note that multi-scale training supports only the non-data sink mode and does not support the data offloading mode. For details, see the multi-scale training implement of <a class="reference external" href="https://gitee.com/mindspore/models/tree/r2.0/official/cv/YOLOv3">yolov3</a>.</p>
<br/>
<p><font size=3><strong>Q: If a <code class="docutils literal notranslate"><span class="pre">tensor</span></code> of MindSpore whose <code class="docutils literal notranslate"><span class="pre">requirements_grad</span></code> is set to <code class="docutils literal notranslate"><span class="pre">False</span></code> is converted into <code class="docutils literal notranslate"><span class="pre">numpy</span></code> for processing and then converted into <code class="docutils literal notranslate"><span class="pre">tensor</span></code>, will the computational graph and backward propagation be affected?</strong></font></p>
<p>A: In PyNative mode, if <code class="docutils literal notranslate"><span class="pre">numpy</span></code> is used for computation, gradient transfer will be interrupted. In the scenario where <code class="docutils literal notranslate"><span class="pre">requirements_grad</span></code> is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, if the backward propagation of <code class="docutils literal notranslate"><span class="pre">tensor</span></code> is not transferred to other parameters, there is no impact. If <code class="docutils literal notranslate"><span class="pre">requirements_grad</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, there is an impact.</p>
<br/>
<p><font size=3><strong>Q: How do I modify the <code class="docutils literal notranslate"><span class="pre">weight</span></code> and <code class="docutils literal notranslate"><span class="pre">bias</span></code> of the fully-connected layer like <code class="docutils literal notranslate"><span class="pre">torch.nn.functional.linear()</span></code>?</strong></font></p>
<p>A: The <code class="docutils literal notranslate"><span class="pre">nn.Dense</span></code> interface is similar to <code class="docutils literal notranslate"><span class="pre">torch.nn.functional.linear()</span></code>. <code class="docutils literal notranslate"><span class="pre">nn.Dense</span></code> can specify the initial values of <code class="docutils literal notranslate"><span class="pre">weight</span></code> and <code class="docutils literal notranslate"><span class="pre">bias</span></code>. Subsequent changes are automatically updated by the optimizer. During the training, you do not need to change the values of the two parameters.</p>
<br/>
<p><font size=3><strong>Q: What is the function of the <code class="docutils literal notranslate"><span class="pre">.meta</span></code> file generated after the model is saved using MindSpore? Can the <code class="docutils literal notranslate"><span class="pre">.meta</span></code> file be used to import the graph structure?</strong></font></p>
<p>A: The <code class="docutils literal notranslate"><span class="pre">.meta</span></code> file is a compiled graph structure. However, this structure cannot be directly imported currently. If you do not know the graph structure, you still need to use the MindIR file to import the network.</p>
<br/>
<p><font size=3><strong>Q: Can the <code class="docutils literal notranslate"><span class="pre">yolov4-tiny-3l.weights</span></code> model file be directly converted into a MindSpore model?</strong></font></p>
<p>A: No. You need to convert the parameters trained by other frameworks into the MindSpore format, and then convert the model into a MindSpore model.</p>
<br/>
<p><font size=3><strong>Q: Why an error message is displayed when MindSpore is used to set <code class="docutils literal notranslate"><span class="pre">model.train</span></code>?</strong></font></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">LossMonitor</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">LossMonitor</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>A: If the offloading mode has been set, it cannot be set to non-offloading mode, which is a restriction on the running mechanism.</p>
<br/>
<p><font size=3><strong>Q: What should I pay attention to when using MindSpore to train a model in the <code class="docutils literal notranslate"><span class="pre">eval</span></code> phase? Can the network and parameters be loaded directly? Does the optimizer need to be used in the Model?</strong></font></p>
<p>A: It mainly depends on what is required in the <code class="docutils literal notranslate"><span class="pre">eval</span></code> phase. For example, the output of the <code class="docutils literal notranslate"><span class="pre">eval</span></code> network of the image classification task is the probability of each class, and the <code class="docutils literal notranslate"><span class="pre">acc</span></code> is circulated with the corresponding label.
In most cases, the training network and parameters can be directly reused. Note that the inference mode needs to be set.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">net</span><span class="o">.</span><span class="n">set_train</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>The optimizer is not required in the <code class="docutils literal notranslate"><span class="pre">eval</span></code> phase. However, if the <code class="docutils literal notranslate"><span class="pre">model.eval</span></code> API of MindSpore needs to be used, the <code class="docutils literal notranslate"><span class="pre">loss</span> <span class="pre">function</span></code> needs to be configured. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a model.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;top_1_accuracy&#39;</span><span class="p">,</span> <span class="s1">&#39;top_5_accuracy&#39;</span><span class="p">})</span>
<span class="c1"># Evaluate the model.</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<br/>
<p><font size=3><strong>Q: How do I use <code class="docutils literal notranslate"><span class="pre">param_group</span></code> in SGD to reduce the learning rate?</strong></font></p>
<p>A: To change the value according to <code class="docutils literal notranslate"><span class="pre">epoch</span></code>, use <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.0/api_python/mindspore.nn.html#dynamic-lr-function">Dynamic LR Function</a> and set <code class="docutils literal notranslate"><span class="pre">step_per_epoch</span></code> to <code class="docutils literal notranslate"><span class="pre">step_size</span></code>. To change the value according to <code class="docutils literal notranslate"><span class="pre">step</span></code>, set <code class="docutils literal notranslate"><span class="pre">step_per_epoch</span></code> to 1. You can also use <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.0/api_python/mindspore.nn.html#learningrateschedule-class">LearningRateSchedule</a>.</p>
<br/>
<p><font size=3><strong>Q: How do I modify parameters (such as the dropout value) on MindSpore?</strong></font></p>
<p>A: When building a network, use <code class="docutils literal notranslate"><span class="pre">if</span> <span class="pre">self.training:</span> <span class="pre">x</span> <span class="pre">=</span> <span class="pre">dropput(x)</span></code>. When inferring, set <code class="docutils literal notranslate"><span class="pre">network.set_train(False)</span></code> before execution to disable the dropout function. During training, set <code class="docutils literal notranslate"><span class="pre">network.set_train(mode_false)</span></code> to True to enable the dropout function.</p>
<br/>
<p><font size=3><strong>Q: How do I view the number of model parameters?</strong></font></p>
<p>A: You can load the checkpoint count directly. Variables in the momentum and optimizer may be counted, so you need to filter them out.
You can refer to the following APIs to collect the number of network parameters:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">count_params</span><span class="p">(</span><span class="n">net</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Count number of parameters in the network</span>
<span class="sd">    Args:</span>
<span class="sd">        net (mindspore.nn.Cell): Mindspore network instance</span>
<span class="sd">    Returns:</span>
<span class="sd">        total_params (int): Total number of trainable params</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">total_params</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">():</span>
        <span class="n">total_params</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">total_params</span>
</pre></div>
</div>
<p><a class="reference external" href="https://gitee.com/mindspore/models/blob/r2.0/research/cv/tinynet/src/utils.py">Script Link</a>.</p>
<br/>
<p><font size=3><strong>Q: How do I monitor the <code class="docutils literal notranslate"><span class="pre">loss</span></code> during training and save the training parameters when the <code class="docutils literal notranslate"><span class="pre">loss</span></code> is the lowest?</strong></font></p>
<p>A: You can customize the <code class="docutils literal notranslate"><span class="pre">callback</span></code> method to implement the early stopping function.
Example: When the loss value decreases to a certain value, the training stops.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">EarlyStop</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">control_loss</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EarlyStop</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_control_loss</span> <span class="o">=</span> <span class="n">control_loss</span>

    <span class="k">def</span> <span class="nf">step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">run_context</span><span class="p">):</span>
        <span class="n">cb_params</span> <span class="o">=</span> <span class="n">run_context</span><span class="o">.</span><span class="n">original_args</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">cb_params</span><span class="o">.</span><span class="n">net_outputs</span>
        <span class="k">if</span> <span class="n">loss</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_control_loss</span><span class="p">:</span>
            <span class="c1"># Stop training</span>
            <span class="n">run_context</span><span class="o">.</span><span class="n">_stop_requested</span> <span class="o">=</span> <span class="kc">True</span>

<span class="n">stop_cb</span> <span class="o">=</span> <span class="n">EarlyStop</span><span class="p">(</span><span class="n">control_loss</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">epoch_size</span><span class="p">,</span> <span class="n">ds_train</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">stop_cb</span><span class="p">])</span>
</pre></div>
</div>
<br/>
<p><font size=3><strong>Q: How do I obtain  <code class="docutils literal notranslate"><span class="pre">feature</span> <span class="pre">map</span></code> with the expected size when <code class="docutils literal notranslate"><span class="pre">nn.Conv2d</span></code> is used?</strong></font></p>
<p>A: For details about how to derive the <code class="docutils literal notranslate"><span class="pre">Conv2d</span> <span class="pre">shape</span></code>, click <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.0/api_python/nn/mindspore.nn.Conv2d.html#mindspore.nn.Conv2d">here</a> Change <code class="docutils literal notranslate"><span class="pre">pad_mode</span></code> of <code class="docutils literal notranslate"><span class="pre">Conv2d</span></code> to <code class="docutils literal notranslate"><span class="pre">same</span></code>. Alternatively, you can calculate the <code class="docutils literal notranslate"><span class="pre">pad</span></code> based on the <code class="docutils literal notranslate"><span class="pre">Conv2d</span> <span class="pre">shape</span></code> derivation formula to keep the <code class="docutils literal notranslate"><span class="pre">shape</span></code> unchanged. Generally, the pad is <code class="docutils literal notranslate"><span class="pre">(kernel_size-1)//2</span></code>.</p>
<br/>
<p><font size=3><strong>Q: Can MindSpore be used to customize a loss function that can return multiple values?</strong></font></p>
<p>A: After customizing the <code class="docutils literal notranslate"><span class="pre">loss</span> <span class="pre">function</span></code>, you need to customize <code class="docutils literal notranslate"><span class="pre">TrainOneStepCell</span></code>. The number of <code class="docutils literal notranslate"><span class="pre">sens</span></code> for implementing gradient calculation is the same as the number of <code class="docutils literal notranslate"><span class="pre">network</span></code> outputs. For details, see the following:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">MyLoss</span><span class="p">()</span>
<span class="n">loss_with_net</span> <span class="o">=</span> <span class="n">MyWithLossCell</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">)</span>
<span class="n">train_net</span> <span class="o">=</span> <span class="n">MyTrainOneStepCell</span><span class="p">(</span><span class="n">loss_with_net</span><span class="p">,</span> <span class="n">optim</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="o">=</span><span class="n">train_net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
<br/>
<p><font size=3><strong>Q: How does MindSpore implement the early stopping function?</strong></font></p>
<p>A: You can refer to <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.0/api_python/train/mindspore.train.EarlyStopping.html">EarlyStopping</a>.</p>
<br/>
<p><font size=3><strong>Q: After a model is trained, how do I save the model output in text or <code class="docutils literal notranslate"><span class="pre">npy</span></code> format?</strong></font></p>
<p>A: The network output is <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>. You need to use the <code class="docutils literal notranslate"><span class="pre">asnumpy()</span></code> method to convert the <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> to <code class="docutils literal notranslate"><span class="pre">NumPy</span></code> and then save the data. For details, see the following:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">out</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;output.npy&quot;</span><span class="p">,</span> <span class="n">out</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
</pre></div>
</div>
<br/>
<p><font size=3><strong>Q: How to handle cache server exception shutdown?</strong></font></p>
<p>A: During the use of the cache server, system resources such as IPC share memory and socket files are allocated. If overflow is allowed, there will be overflowing data files on disk space. In general, if the server is shut down normally via the <code class="docutils literal notranslate"><span class="pre">cache_admin</span> <span class="pre">--stop</span></code> command, these resources will be automatically cleaned up.</p>
<p>However, if the cache server is shut down abnormally, such as the cache service process is killed, the user needs to try to restart the server first. If the startup fails, you should follow the following steps to manually clean up the system resources:</p>
<ul>
<li><p>Delete the IPC resource.</p>
<ol>
<li><p>Check for IPC shared memory residue.</p>
<p>In general, the system allocates 4GB of share memory for the caching service. The following command allows you to view the usage of share memory blocks in the system.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ ipcs -m
------ Shared Memory Segments --------
key        shmid      owner      perms      bytes      nattch     status
0x61020024 15532037   root       666        4294967296 1
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">shmid</span></code> is the share memory block id, <code class="docutils literal notranslate"><span class="pre">bytes</span></code> is the size of the share memory block, and <code class="docutils literal notranslate"><span class="pre">nattch</span></code> is the number of processes linking to the shared memory block.  <code class="docutils literal notranslate"><span class="pre">nattch</span></code> is not 0, which indicates that there are still processes that use the share memory block. Before you delete share memory, you need to stop all processes that use that memory block.</p>
</li>
<li><p>Delete the IPC share memory.</p>
<p>Find the corresponding share memory id, and delete via the following command.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>ipcrm -m {shmid}
</pre></div>
</div>
</li>
</ol>
</li>
<li><p>Delete socket files.</p>
<p>In general, socket files is located <code class="docutils literal notranslate"><span class="pre">/tmp/mindspore/cache</span></code>. Enter the folder, and execute the following command to delete socket files.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>rm cache_server_p{port_number}
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">port_number</span></code> is the port number specified when the user creates the cache server, which defaults to 50052.</p>
</li>
<li><p>Delete data files that overflow to disk space.</p>
<p>Enter the specified overflow data path when you enabled the cache server. In general, the default overflow path is <code class="docutils literal notranslate"><span class="pre">/tmp/mindspore/cache</span></code>. Find the corresponding data folders under the path and delete them one by one.</p>
</li>
</ul>
<br/>
<p><font size=3><strong>Q: Can the <code class="docutils literal notranslate"><span class="pre">vgg16</span></code> model be loaded by using the GPU via Hub and whether can the migration model be done?</strong></font></p>
<p>A: Please manually modify the following two parameters:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Increase **kwargs parameter: as the following</span>
<span class="k">def</span> <span class="nf">vgg16</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">phase</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Increase **kwargs parameter: as the following</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Vgg</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;16&#39;</span><span class="p">],</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span> <span class="n">batch_norm</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_norm</span><span class="p">,</span> <span class="n">phase</span><span class="o">=</span><span class="n">phase</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>
</div>
<br/>
<p><font size=3><strong>Q: How to obtain middle-layer features of a VGG model?</strong></font></p>
<p>A: Obtaining the middle-layer features of a network is not closely related to the specific framework. For the <code class="docutils literal notranslate"><span class="pre">vgg</span></code> model defined in <code class="docutils literal notranslate"><span class="pre">torchvison</span></code>, the <code class="docutils literal notranslate"><span class="pre">features</span></code> field can be used to obtain the “middle-layer features”. The <code class="docutils literal notranslate"><span class="pre">vgg</span></code> source code of <code class="docutils literal notranslate"><span class="pre">torchvison</span></code> is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">VGG</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">init_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VGG</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">features</span> <span class="o">=</span> <span class="n">features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">((</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">vgg16</span></code> defined in ModelZoo of MindSpore can be obtained through the <code class="docutils literal notranslate"><span class="pre">layers</span></code> field as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">network</span> <span class="o">=</span> <span class="n">vgg16</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span>
</pre></div>
</div>
<br/>
<p><font size=3><strong>Q: When MindSpore is used for model training, there are four input parameters for <code class="docutils literal notranslate"><span class="pre">CTCLoss</span></code>: <code class="docutils literal notranslate"><span class="pre">inputs</span></code>, <code class="docutils literal notranslate"><span class="pre">labels_indices</span></code>, <code class="docutils literal notranslate"><span class="pre">labels_values</span></code>, and <code class="docutils literal notranslate"><span class="pre">sequence_length</span></code>. How do I use <code class="docutils literal notranslate"><span class="pre">CTCLoss</span></code> for model training?</strong></font></p>
<p>A: The <code class="docutils literal notranslate"><span class="pre">dataset</span></code> received by the defined <code class="docutils literal notranslate"><span class="pre">model.train</span></code> API can consist of multiple pieces of data, for example, (<code class="docutils literal notranslate"><span class="pre">data1</span></code>, <code class="docutils literal notranslate"><span class="pre">data2</span></code>, <code class="docutils literal notranslate"><span class="pre">data3</span></code>, …). Therefore, the <code class="docutils literal notranslate"><span class="pre">dataset</span></code> can contain <code class="docutils literal notranslate"><span class="pre">inputs</span></code>, <code class="docutils literal notranslate"><span class="pre">labels_indices</span></code>, <code class="docutils literal notranslate"><span class="pre">labels_values</span></code>, and <code class="docutils literal notranslate"><span class="pre">sequence_length</span></code> information. You only need to define the dataset in the corresponding format and transfer it to <code class="docutils literal notranslate"><span class="pre">model.train</span></code>. For details, see <a class="reference external" href="https://www.mindspore.cn/tutorials/en/r2.0/advanced/dataset.html">Data Processing API</a>.</p>
<br/>
<p><font size=3><strong>Q: What are the available recommendation or text generation networks or models provided by MindSpore?</strong></font></p>
<p>A: Currently, recommendation models such as Wide &amp; Deep, DeepFM, and NCF are under development. In the natural language processing (NLP) field, Bert_NEZHA is available and models such as MASS are under development. You can rebuild the network into a text generation network based on the scenario requirements. Please stay tuned for updates on the <a class="reference external" href="https://gitee.com/mindspore/models/blob/r2.0/README.md">MindSpore ModelZoo</a>.</p>
<br/>
<p><font size=3><strong>Q: How do I use MindSpore to fit functions such as <span class="math notranslate nohighlight">\(f(x)=a \times sin(x)+b\)</span>?</strong></font></p>
<p>A: The following is based on the official MindSpore linear fitting case.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># The fitting function is: f(x)=2*sin(x)+3.</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dataset</span> <span class="k">as</span> <span class="n">ds</span>
<span class="kn">from</span> <span class="nn">mindspore.common.initializer</span> <span class="kn">import</span> <span class="n">Normal</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.train</span> <span class="kn">import</span> <span class="n">Model</span><span class="p">,</span> <span class="n">LossMonitor</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_data</span><span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="n">w</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">3.0</span><span class="p">):</span>
    <span class="c1"># f(x)=w * sin(x) + b</span>
    <span class="c1"># f(x)=2 * sin(x) +3</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="n">noise</span>
        <span class="k">yield</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">y</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">create_dataset</span><span class="p">(</span><span class="n">num_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">repeat_size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">GeneratorDataset</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">get_data</span><span class="p">(</span><span class="n">num_data</span><span class="p">)),</span> <span class="n">column_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">,</span><span class="s1">&#39;label&#39;</span><span class="p">])</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">repeat_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">input_data</span>

<span class="k">class</span> <span class="nc">LinearNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LinearNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Normal</span><span class="p">(</span><span class="mf">0.02</span><span class="p">),</span> <span class="n">Normal</span><span class="p">(</span><span class="mf">0.02</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>

    <span class="n">num_data</span> <span class="o">=</span> <span class="mi">1600</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span>
    <span class="n">repeat_size</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.005</span>
    <span class="n">momentum</span> <span class="o">=</span> <span class="mf">0.9</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">LinearNet</span><span class="p">()</span>
    <span class="n">net_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">net_loss</span><span class="p">,</span> <span class="n">opt</span><span class="p">)</span>

    <span class="n">ds_train</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">num_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">repeat_size</span><span class="o">=</span><span class="n">repeat_size</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ds_train</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">LossMonitor</span><span class="p">(),</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
<br/>
<p><font size=3><strong>Q: How do I use MindSpore to fit quadratic functions such as <span class="math notranslate nohighlight">\(f(x)=ax^2+bx+c\)</span>?</strong></font></p>
<p>A: The following code is referenced from the official <a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.0/docs/sample_code/linear_regression.py">MindSpore tutorial code</a>.</p>
<p>Modify the following items to fit <span class="math notranslate nohighlight">\(f(x) = ax^2 + bx + c\)</span>:</p>
<ol class="simple">
<li><p>Dataset generation.</p></li>
<li><p>Network fitting.</p></li>
<li><p>Optimizer.</p></li>
</ol>
<p>The following explains detailed information about the modification:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Since the selected optimizer does not support CPU, so the training computing platform is changed to GPU, which requires readers to install the corresponding GPU version of MindSpore.</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">)</span>

<span class="c1"># Assume that the function to be fitted this time is f(x)=2x^2+3x+4, the data generation function is modified as follows:</span>
<span class="k">def</span> <span class="nf">get_data</span><span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">3.0</span> <span class="p">,</span><span class="n">c</span> <span class="o">=</span> <span class="mi">4</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mf">10.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">)</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="c1"># The y value is generated by the fitting target function ax^2+bx+c.</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="n">a</span> <span class="o">+</span> <span class="n">x</span> <span class="o">*</span> <span class="n">b</span> <span class="o">+</span> <span class="n">c</span> <span class="o">+</span> <span class="n">noise</span>
        <span class="c1"># When a*x^2+b*x+c is fitted, a and b are weight parameters and c is offset parameter bias. The training data corresponding to the two weights are x^2 and x respectively, so the dataset generation mode is changed as follows:</span>
        <span class="k">yield</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x</span><span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">y</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">create_dataset</span><span class="p">(</span><span class="n">num_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">repeat_size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">GeneratorDataset</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">get_data</span><span class="p">(</span><span class="n">num_data</span><span class="p">)),</span> <span class="n">column_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">,</span><span class="s1">&#39;label&#39;</span><span class="p">])</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">repeat_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">input_data</span>

<span class="k">class</span> <span class="nc">LinearNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LinearNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Because the full join function inputs two training parameters, the input value is changed to 2, the first Nomral(0.02) will automatically assign random weights to the input two parameters, and the second Normal is the random bias.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Normal</span><span class="p">(</span><span class="mf">0.02</span><span class="p">),</span> <span class="n">Normal</span><span class="p">(</span><span class="mf">0.02</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">num_data</span> <span class="o">=</span> <span class="mi">1600</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span>
    <span class="n">repeat_size</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.005</span>
    <span class="n">momentum</span> <span class="o">=</span> <span class="mf">0.9</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">LinearNet</span><span class="p">()</span>
    <span class="n">net_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
    <span class="c1">#  RMSProp optimalizer with better effect is selected for quadratic function fitting, Currently, Ascend and GPU computing platforms are supported.</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RMSProp</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">net_loss</span><span class="p">,</span> <span class="n">opt</span><span class="p">)</span>

    <span class="n">ds_train</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">num_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">repeat_size</span><span class="o">=</span><span class="n">repeat_size</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ds_train</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">LossMonitor</span><span class="p">(),</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
<br/>
<p><font size=3><strong>Q: How do I execute a single <code class="docutils literal notranslate"><span class="pre">ut</span></code> case in <code class="docutils literal notranslate"><span class="pre">mindspore/tests</span></code>?</strong></font></p>
<p>A: <code class="docutils literal notranslate"><span class="pre">ut</span></code> cases are usually based on the MindSpore package of the debug version, which is not provided on the official website. You can run <code class="docutils literal notranslate"><span class="pre">sh</span> <span class="pre">build.sh</span></code> to compile based on the source code and then run the <code class="docutils literal notranslate"><span class="pre">pytest</span></code> command. The compilation in debug mode does not depend on the backend. Compile the <code class="docutils literal notranslate"><span class="pre">sh</span> <span class="pre">build.sh</span> <span class="pre">-t</span> <span class="pre">on</span></code> option. For details about how to execute cases, see the <code class="docutils literal notranslate"><span class="pre">tests/runtest.sh</span></code> script.</p>
<br/>
<p><font size=3><strong>Q: For Ascend users, how to get more detailed logs to help position the problems when the <code class="docutils literal notranslate"><span class="pre">run</span> <span class="pre">task</span> <span class="pre">error</span></code> is reported during executing the cases?</strong></font></p>
<p>A: Use the msnpureport tool to set the on-device log level. The tool is stored in <code class="docutils literal notranslate"><span class="pre">/usr/local/Ascend/latest/driver/tools/msnpureport</span></code>.</p>
<ul class="simple">
<li><p>Global-level:</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>/usr/local/Ascend/latest/driver/tools/msnpureport<span class="w"> </span>-g<span class="w"> </span>info
</pre></div>
</div>
<ul class="simple">
<li><p>Module-level</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>/usr/local/Ascend/latest/driver/tools/msnpureport<span class="w"> </span>-m<span class="w"> </span>SLOG:error
</pre></div>
</div>
<ul class="simple">
<li><p>Event-level</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>/usr/local/Ascend/latest/driver/tools/msnpureport<span class="w"> </span>-e<span class="w"> </span>disable/enable
</pre></div>
</div>
<ul class="simple">
<li><p>Multi-device ID-level</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>/usr/local/Ascend/latest/driver/tools/msnpureport<span class="w"> </span>-d<span class="w"> </span><span class="m">1</span><span class="w"> </span>-g<span class="w"> </span>warning
</pre></div>
</div>
<p>Assume that the value range of deviceID is [0, 7], and <code class="docutils literal notranslate"><span class="pre">devices</span> <span class="pre">0–3</span></code> and <code class="docutils literal notranslate"><span class="pre">devices</span> <span class="pre">4–7</span></code> are on the same OS. <code class="docutils literal notranslate"><span class="pre">devices</span> <span class="pre">0</span></code> to <code class="docutils literal notranslate"><span class="pre">device3</span></code> share the same log configuration file and <code class="docutils literal notranslate"><span class="pre">device4</span></code>-<code class="docutils literal notranslate"><span class="pre">device7</span></code> shares the same configuration file. In this way, changing any log level in <code class="docutils literal notranslate"><span class="pre">devices</span> <span class="pre">0</span></code> to <code class="docutils literal notranslate"><span class="pre">device3</span></code> will change that of other <code class="docutils literal notranslate"><span class="pre">device</span></code>. This rule also applies to <code class="docutils literal notranslate"><span class="pre">device4</span></code>-<code class="docutils literal notranslate"><span class="pre">device7</span></code> .</p>
<p>After the <code class="docutils literal notranslate"><span class="pre">Driver</span></code> package is installed (assuming that the installation path is /usr/local/HiAI and the execution file <code class="docutils literal notranslate"><span class="pre">msnpureport.exe</span></code> is in the C:\ProgramFiles\Huawei\Ascend\Driver\tools\ directory on Windows), suppose the user executes the command line directly in the /home/shihangbo/directory, the Device side logs are exported to the current directory and stored in a timestamp-named folder.</p>
<br/>
<p><font size=3><strong>Q: How can I do when the error message <code class="docutils literal notranslate"><span class="pre">Out</span> <span class="pre">of</span> <span class="pre">Memory!!!</span> <span class="pre">total[3212254720]</span> <span class="pre">(dynamic[0]</span> <span class="pre">memory</span> <span class="pre">poll[524288000])</span> <span class="pre">malloc[32611480064]</span> <span class="pre">failed!</span></code> is displayed by performing the training process using the Ascend platform?</strong></font></p>
<p>A: This issue is a memory shortage problem caused by too much memory usage, which can be caused by two possible causes:</p>
<ul class="simple">
<li><p>Set the value of <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> too large. Solution: Reduce the value of <code class="docutils literal notranslate"><span class="pre">batch_size</span></code>.</p></li>
<li><p>Introduce the abnormally large <code class="docutils literal notranslate"><span class="pre">parameter</span></code>, for example, a single data shape is [640,1024,80,81]. The data type is  float32, and the single data size is over 15G. In this way, the two data with the similar size are added together, and the memory occupied is over 3*15G, which easily causes <code class="docutils literal notranslate"><span class="pre">Out</span> <span class="pre">of</span> <span class="pre">Memory</span></code>. Solution: Check the <code class="docutils literal notranslate"><span class="pre">shape</span></code> of the parameter. If it is abnormally large, the shape can be reduced.</p></li>
<li><p>If the following operations cannot solve the problem, you can raise the problem on the <a class="reference external" href="https://www.hiascend.com/forum/forum-0106101385921175002-1.html">official forum</a>, and there are dedicated technical personnels for help.</p></li>
</ul>
<br/>
<p><font size=3><strong>Q: How do I change hyperparameters for calculating loss values during neural network training?</strong></font></p>
<p>A: Sorry, this function is not available yet. You can find the optimal hyperparameters by training, redefining an optimizer, and then training.</p>
<br/>
<p><font size=3><strong>Q: What should I do when error <code class="docutils literal notranslate"><span class="pre">error</span> <span class="pre">while</span> <span class="pre">loading</span> <span class="pre">shared</span> <span class="pre">libraries:</span> <span class="pre">libge_compiler.so:</span> <span class="pre">cannot</span> <span class="pre">open</span> <span class="pre">shared</span> <span class="pre">object</span> <span class="pre">file:</span> <span class="pre">No</span> <span class="pre">such</span> <span class="pre">file</span> <span class="pre">or</span> <span class="pre">directory</span></code> is displayed during application running?</strong></font></p>
<p>A: While installing Ascend 310 AI Processor software packages depended by MindSpore, the <code class="docutils literal notranslate"><span class="pre">CANN</span></code> package should install the full-featured <code class="docutils literal notranslate"><span class="pre">toolkit</span></code> version instead of the <code class="docutils literal notranslate"><span class="pre">nnrt</span></code> version.</p>
<br/>
<p><font size=3><strong>Q: Why does set_ps_context(enable_ps=True) in model_zoo/official/cv/ResNet/train.py in the MindSpore code have to be set before init?</strong></font></p>
<p>A: In MindSpore Ascend mode, if init is called first, all processes will be allocated cards, but in parameter server training mode, the server does not need to allocate cards, and the worker and server will use the same card, resulting in an error: Ascend kernel runtime initialization failed.</p>
<br/>
<p><font size=3><strong>Q: What should I do if the memory continues to increase when resnet50 training is being performed on the CPU ARM platform?</strong></font></p>
<p>A: When resnet50 training is performed on the CPU ARM, some operators are implemented based on the oneDNN library, and the oneDNN library achieves multi-threaded parallelism based on the libgomp library. Currently, there is a problem in libgomp where the number of threads configured for multiple parallel domains is different and the memory consumption continues to grow. The continuous growth of the memory can be controlled by configuring a uniform number of threads globally. For comprehensive performance considerations, it is recommended to configure a unified configuration to 1/4 of the number of physical cores, such as <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">OMP_NUM_THREADS=32</span></code>.</p>
<br/>
<p><font size=3><strong>Q: Why report an error that the stream exceeds the limit when executing the model on the Ascend platform？</strong></font></p>
<p>A: Stream represents an operation queue. Tasks on the same stream are executed in sequence, and different streams can be executed in parallel. Various operations in the network generate tasks and are assigned to streams to control the concurrent mode of task execution. Ascend platform has a limit on the number of tasks on the same stream, and tasks that exceed the limit will be assigned to new streams. The multiple parallel methods of MindSpore will also be assigned to new streams, such as parallel communication operators. Therefore, when the number of assigned streams exceeds the resource limit of the Ascend platform, an error will be reported. Reference solution:</p>
<ul class="simple">
<li><p>Reduce the size of the network model</p></li>
<li><p>Reduce the use of communication operators in the network</p></li>
<li><p>Reduce conditional control statements in the network</p></li>
</ul>
<br/>
<p><font size=3><strong>Q: On the Ascend platform, if an error “Ascend error occurred, error message:” is reported in the log and followed by an error code, such as “E40011”, how to find the cause of the error code?</strong></font></p>
<p>A: When “Ascend error occurred, error message:” appears, it indicates that a module of Ascend CANN is abnormal and the error log is reported.</p>
<p>At this time, there is an error message after the error code. If you need a more detailed possible cause and solution for this exception, please refer to the “error code troubleshooting” section of the corresponding Ascend version document, such as <a class="reference external" href="https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/60RC1alpha02/troublemanage/troubleshooting/troubleshooting_0006.html">CANN Community 6.0.RC1.alpha002 Error Code troubleshooting</a>.</p>
<br/>
<p><font size=3><strong>Q: When the third-party component gensim is used to train the NLP network, the error “ValueError” may be reported. What can I do?</strong></font></p>
<p>A: The following error information is displayed:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt;<span class="w"> </span>import<span class="w"> </span>gensim
Traceback<span class="w"> </span><span class="o">(</span>most<span class="w"> </span>recent<span class="w"> </span>call<span class="w"> </span>last<span class="o">)</span>:
<span class="w">  </span>File<span class="w"> </span><span class="s2">&quot;&lt;stdin&gt;&quot;</span>,<span class="w"> </span>line<span class="w"> </span><span class="m">1</span>,<span class="w"> </span><span class="k">in</span><span class="w"> </span>&lt;module&gt;
<span class="w">  </span>File<span class="w"> </span><span class="s2">&quot;/home/miniconda3/envs/ci39_cj/lib/python3.9/site-packages/gensim/__init__.py&quot;</span>,<span class="w"> </span>line<span class="w"> </span><span class="m">11</span>,<span class="w"> </span><span class="k">in</span><span class="w"> </span>&lt;module&gt;
<span class="w">    </span>from<span class="w"> </span>gensim<span class="w"> </span>import<span class="w"> </span>parsing,<span class="w"> </span>corpora,<span class="w"> </span>matutils,<span class="w"> </span>interfaces,<span class="w"> </span>models,<span class="w"> </span>similarities,<span class="w"> </span>utils<span class="w">  </span><span class="c1"># noqa:F401</span>
<span class="w">  </span>File<span class="w"> </span><span class="s2">&quot;/home/miniconda3/envs/ci39_cj/lib/python3.9/site-packages/gensim/corpora/__init__.py&quot;</span>,<span class="w"> </span>line<span class="w"> </span><span class="m">6</span>,<span class="w"> </span><span class="k">in</span><span class="w"> </span>&lt;module&gt;
<span class="w">    </span>from<span class="w"> </span>.indexedcorpus<span class="w"> </span>import<span class="w"> </span>IndexedCorpus<span class="w">  </span><span class="c1"># noqa:F401 must appear before the other classes</span>
<span class="w">  </span>File<span class="w"> </span><span class="s2">&quot;/home/miniconda3/envs/ci39_cj/lib/python3.9/site-packages/gensim/corpora/indexedcorpus.py&quot;</span>,<span class="w"> </span>line<span class="w"> </span><span class="m">14</span>,<span class="w"> </span><span class="k">in</span><span class="w"> </span>&lt;module&gt;
<span class="w">    </span>from<span class="w"> </span>gensim<span class="w"> </span>import<span class="w"> </span>interfaces,<span class="w"> </span>utils
<span class="w">  </span>File<span class="w"> </span><span class="s2">&quot;/home/miniconda3/envs/ci39_cj/lib/python3.9/site-packages/gensim/interfaces.py&quot;</span>,<span class="w"> </span>line<span class="w"> </span><span class="m">19</span>,<span class="w"> </span><span class="k">in</span><span class="w"> </span>&lt;module&gt;
<span class="w">    </span>from<span class="w"> </span>gensim<span class="w"> </span>import<span class="w"> </span>utils,<span class="w"> </span>matutils
<span class="w">  </span>File<span class="w"> </span><span class="s2">&quot;/home/miniconda3/envs/ci39_cj/lib/python3.9/site-packages/gensim/matutils.py&quot;</span>,<span class="w"> </span>line<span class="w"> </span><span class="m">1024</span>,<span class="w"> </span><span class="k">in</span><span class="w"> </span>&lt;module&gt;
<span class="w">    </span>from<span class="w"> </span>gensim._matutils<span class="w"> </span>import<span class="w"> </span>logsumexp,<span class="w"> </span>mean_absolute_difference,<span class="w"> </span>dirichlet_expectation
<span class="w">  </span>File<span class="w"> </span><span class="s2">&quot;gensim/_matutils.pyx&quot;</span>,<span class="w"> </span>line<span class="w"> </span><span class="m">1</span>,<span class="w"> </span><span class="k">in</span><span class="w"> </span>init<span class="w"> </span>gensim._matutils
ValueError:<span class="w"> </span>numpy.ndarray<span class="w"> </span>size<span class="w"> </span>changed,<span class="w"> </span>may<span class="w"> </span>indicate<span class="w"> </span>binary<span class="w"> </span>incompatibility.<span class="w"> </span>Expected<span class="w"> </span><span class="m">88</span><span class="w"> </span>from<span class="w"> </span>C<span class="w"> </span>header,<span class="w"> </span>got<span class="w"> </span><span class="m">80</span><span class="w"> </span>from<span class="w"> </span>PyObject
</pre></div>
</div>
<p>For details about the error cause, see the <a class="reference external" href="https://github.com/RaRe-Technologies/gensim/issues/3095">gensim</a> or <a class="reference external" href="https://github.com/numpy/numpy/issues/18709">numpy</a> official website.</p>
<p>Solutions:</p>
<p>Method 1: Reinstall the Numpy and Gensim and run the following commands: <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">uninstall</span> <span class="pre">gensim</span> <span class="pre">numpy</span> <span class="pre">-y</span> <span class="pre">&amp;&amp;</span> <span class="pre">pip</span> <span class="pre">install</span> <span class="pre">numpy==1.18.5</span> <span class="pre">gensim</span></code></p>
<p>Method 2: If the problem persists, delete the cache file of the wheel installation package and then perform method 1. (The cache directory of the wheel installation package is <code class="docutils literal notranslate"><span class="pre">~/.cache/pip/wheels</span></code>)</p>
<br/>
<p><font size=3><strong>Q: What should I do if I encounter <code class="docutils literal notranslate"><span class="pre">matplotlib.pyplot.show()</span></code> or <code class="docutils literal notranslate"><span class="pre">plt.show</span></code> not be executed during the documentation sample code is running?</strong></font></p>
<p>A: First confirm whether <code class="docutils literal notranslate"><span class="pre">matplotlib</span></code> is installed. If it is not installed, you can execute <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">matplotlib</span></code> on the command line to install it.</p>
<p>Secondly, because the function of <code class="docutils literal notranslate"><span class="pre">matplotlib.pyplot.show()</span></code> is to display graph data graphically, it is necessary to run the system to support the graph display function. If the system cannot support graph display, the reader needs to comment out the command line of the graph display. Operation will not affect the results of the overall code.</p>
<br/>
<p><font size=3><strong>Q: How to handle running failures when encountering an online runtime provided in the documentation?</strong></font></p>
<p>A: Need to confirm that the following preparations have been done.</p>
<ul class="simple">
<li><p>First, you need to log in to ModelArts through your HUAWEI CLOUD account.</p></li>
<li><p>Secondly, note that the hardware environment supported by the tags in the tutorial document and the hardware environment configured in the example code is Ascend, GPU or CPU. Since the hardware environment used by default after login is CPU, the Ascend environment and GPU environment need to be switched manually by the user.</p></li>
<li><p>Finally, confirm that the current <code class="docutils literal notranslate"><span class="pre">Kernel</span></code> is MindSpore.</p></li>
</ul>
<p>After completing the above steps, you can run the tutorial.</p>
<p>For the specific operation process, please refer to <a class="reference external" href="https://bbs.huaweicloud.com/forum/thread-168982-1-1.html">Based on ModelArts Online Experience MindSpore</a>.</p>
<br/>
<p><font size=3><strong>Q: No error is reported when using result of division in GRAPH mode, but an error is reported when using result of division in PYNATIVE mode？</strong></font></p>
<p>A: In GRAPH mode, since the graph compilation is used, the data type of the output result of the operator is determined at the graph compilation stage.</p>
<p>For example, the following code is executed in GRAPH mode, and the type of input data is int, so the output result is also int type according to graph compilation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">MyTest</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MyTest</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">/</span> <span class="n">y</span>
<span class="n">x</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">MyTest</span><span class="p">()</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">output</span><span class="p">))</span>
</pre></div>
</div>
<p>output：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>4 &lt;class &#39;int&#39;&gt;
</pre></div>
</div>
<p>Change the execution mode and change GRAPH_MODE to PYNATIVE_MODE. Since the Python syntax is used in PyNative mode, the type of any division output to Python syntax is float type, so the execution result is as follows.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>4.0 &lt;class &#39;float&#39;&gt;
</pre></div>
</div>
<p>Therefore, in the scenario where the subsequent operator clearly needs to use int, it is recommended to use Python’s divisibility symbol <code class="docutils literal notranslate"><span class="pre">//</span></code>.</p>
<br/>
<p><font size=3><strong>Q: Why will running the script on GPU stuck for a long time on version 1.8?</strong></font></p>
<p>A: In order to be compatible with more GPU architectures, NVCC compiles CUDA files into PTX files first, and compiles them into binary executable files when using them for the first time. Therefore, compilation time will be consumed.
Compared with the previous version, version 1.8 has added many CUDA operators, resulting in an increase in the compilation time of this part (The time varies according to the equipment. For example, the first compilation time on V100 is about 5 minutes).
This compilation will generate a cache file (taking the Ubuntu system as an example, the cache file is located in <code class="docutils literal notranslate"><span class="pre">~/.nv/computecache</span></code>), and the cache file will be directly loaded during subsequent execution.
Therefore, it will be stuck for several minutes during the first use, and the subsequent use will be a normal time consumption.</p>
<p>Subsequent versions will be pre-compiled and optimized.</p>
<br/>
<p><font size=3><strong>Q: What can I do when the error message <code class="docutils literal notranslate"><span class="pre">MemoryError:</span> <span class="pre">std::bad_alloc</span></code> is reported during the execution of the operator？</strong></font></p>
<p>A: The reason for this error is that the user did not configure the operator parameters correctly, so that the memory space applied by the operator exceeded the system memory limit, and the system failed to allocate memory. The following uses mindspore.ops.UniformCandidateSampler as an example for description.</p>
<ul class="simple">
<li><p>UniformCandidateSampler samples a set of classes by using uniform distribution. According to the parameter <code class="docutils literal notranslate"><span class="pre">num_sampled</span></code> set by the user，the shape of output tensor would be <code class="docutils literal notranslate"><span class="pre">(num_sampled,)</span></code>.</p></li>
<li><p>When the user sets <code class="docutils literal notranslate"><span class="pre">num_sampled=int64.max</span></code>，the memory space requested by the output tensor exceeds the system memory limit, causing <code class="docutils literal notranslate"><span class="pre">bad_alloc</span></code>.</p></li>
</ul>
<p>Therefore, the user needs to set the operator parameters appropriately to avoid such errors.</p>
<br/>
<p><font size=3><strong>Q: How do I understand the “Ascend Error Message” in the error message?</strong></font></p>
<p>A: The “Ascend Error Message” is a fault message thrown after there is an error during CANN execution when CANN (Ascend Heterogeneous Computing Architecture) interface is called by MindSpore, which contains information such as error code and error description. For example:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Traceback (most recent call last):
 File &quot;train.py&quot;, line 292, in &lt;module&gt;
 train_net()
 File  &quot;/home/resnet_csj2/scripts/train_parallel0/src/model_utils/moxing_adapter.py&quot;, line 104, in wrapped_func
 run_func(*args, **kwargs)
 File &quot;train.py&quot;, line 227, in train_net
 set_parameter()
 File &quot;train.py&quot;, line 114, in set_parameter
 init()
 File &quot;/home/miniconda3/envs/ms/lib/python3.7/site-packages/mindspore/communication/management.py&quot;, line 149, in init
 init_hccl()
 RuntimeError: Ascend kernel runtime initialization failed.

 \----------------------------------------------------
 \- Ascend Error Message:
 \----------------------------------------------------
 EJ0001: Failed to initialize the HCCP process. Reason: Maybe the last training process is running. //EJ0001 is the error code, followed by the description and cause of the error. The cause of the error in this example is that the distributed training of the same 8 nodes was started several times, causing process conflicts
 Solution: Wait for 10s after killing the last training process and try again. //The print message here gives the solution to the problem, and this example suggests that the user clean up the process
 TraceBack (most recent call last): //The information printed here is the stack information used by the developer for positioning, and generally the user do not need to pay attention
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>tsd client wait response fail, device response code[1]. unknown device  error.[FUNC:WaitRsp][FILE:process_mode_manager.cpp][LINE:233]
</pre></div>
</div>
<p>In addition, CANN may throw some Inner Errors, for example, the error code is “EI9999: Inner Error”. If you cannot search the case description in MindSpore official website or forum, you can ask for help in the community by raising an issue.</p>
<br/>
<p><font size=3><strong>Q: How to control the Tensor value printed by the ‘print’ method?</strong></font></p>
<p>A: In PyNative dynamic graph mode, you can use numpy native methods such as <code class="docutils literal notranslate"><span class="pre">set_</span> <span class="pre">Printoptions</span></code> Control the output value. In the Graph static graph mode, because the ‘print’ method needs to be converted into an operator, the output value cannot be controlled temporarily. For specific usage of print operator, see [Reference]（ https://www.mindspore.cn/docs/zh-CN/r2.0/api_python/ops/mindspore.ops.Print.html )。
<br/></p>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="network_compilation.html" class="btn btn-neutral float-right" title="Network Compilation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="data_processing.html" class="btn btn-neutral float-left" title="Data Processing" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2022, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>