<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore.ops.operations.math_ops &mdash; MindSpore master documentation</title>
      <link rel="stylesheet" href="../../../../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/css/training.css" type="text/css" /><link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/js/training.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/overview.html">MindSpore Design Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/programming_paradigm.html">Functional and Object-Oriented Fusion Programming Paradigm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/all_scenarios.html">Full-scenarios Unified Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/dynamic_graph_and_static_graph.html">Combination of Dynamic and Static Graphs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/pluggable_device.html">Third-Party Hardware Interconnection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/distributed_training_design.html">Native Distributed Parallel Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/graph_fusion_engine.html">Graph-Kernel Fusion Acceleration Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/data_engine.html">High Performance Data Processing Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/official_models.html">Official Models</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Mapping</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/api_mapping/pytorch_api_mapping.html">PyTorch and MindSpore API Mapping Table</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/api_mapping/tensorflow_api_mapping.html">TensorFlow and MindSpore API Mapping Table</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Migration Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/overview.html">Overview of Migration Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/enveriment_preparation.html">Environment Preparation and Information Acquisition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/analysis_and_preparation.html">Model Analysis and Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/typical_api_comparision.html">Differences Between MindSpore and PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/model_development/model_development.html">Constructing MindSpore Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/debug_and_tune.html">Debugging and Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/sample_code.html">Network Migration Debugging Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/migrator_with_tools.html">Application Practice Guide for Network Migration Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/faq.html">FAQs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Syntax Support</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/static_graph_syntax_support.html">Static Graph Syntax Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/static_graph_syntax/operators.html">Static Graph Syntax —— Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/static_graph_syntax/statements.html">Static Graph Syntax —— Python Statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/static_graph_syntax/python_builtin_functions.html">Static Graph Syntax —— Python Built-in Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/index_support.html">Tensor Index Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Environment Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/env_var_list.html">Environment Variables</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/data_processing.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/implement_problem.html">Implement Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/network_compilation.html">Network Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/operators_compile.html">Operators Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/usage_migrate_3rd.html">Migration from a Third-party Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/precision_tuning.html">Precision Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/distributed_parallel.html">Distributed Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/feature_advice.html">Feature Advice</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
      <li>mindspore.ops.operations.math_ops</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for mindspore.ops.operations.math_ops</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2020-2022 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>

<span class="sd">&quot;&quot;&quot;Operators for math.&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">log</span> <span class="k">as</span> <span class="n">logger</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">signature</span> <span class="k">as</span> <span class="n">sig</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">_checkparam</span> <span class="k">as</span> <span class="n">validator</span>
<span class="kn">from</span> <span class="nn">mindspore.common</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="kn">from</span> <span class="nn">mindspore.common.tensor</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">mindspore.common._decorator</span> <span class="kn">import</span> <span class="n">deprecated</span>
<span class="kn">from</span> <span class="nn">mindspore.ops._utils</span> <span class="kn">import</span> <span class="n">get_broadcast_shape</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.primitive</span> <span class="kn">import</span> <span class="n">Primitive</span><span class="p">,</span> <span class="n">PrimitiveWithInfer</span><span class="p">,</span> <span class="n">PrimitiveWithCheck</span><span class="p">,</span> <span class="n">prim_attr_register</span><span class="p">,</span> <span class="n">_run_op</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">Tensor</span> <span class="k">as</span> <span class="n">Tensor_</span>


<span class="k">def</span> <span class="nf">_infer_shape_reduce</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Common infer for reduce operator&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">reduce_one_axis</span><span class="p">(</span><span class="n">one_axis</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="n">one_axis</span><span class="p">,</span> <span class="o">-</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span> <span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">one_axis</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">one_axis</span> <span class="o">+=</span> <span class="n">dim</span>
        <span class="n">axis_reduce</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">one_axis</span><span class="p">)</span>

    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">],</span> <span class="n">prim_name</span><span class="p">)</span>
    <span class="n">dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">axis_reduce</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">reduce_one_axis</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">axis</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">keep_dims</span><span class="p">:</span>
                <span class="k">return</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">dim</span>
            <span class="k">return</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">one_axis</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axis</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;axis[</span><span class="si">%d</span><span class="s1">]&#39;</span> <span class="o">%</span> <span class="n">index</span><span class="p">,</span> <span class="n">one_axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">prim_name</span><span class="p">)</span>
            <span class="n">reduce_one_axis</span><span class="p">(</span><span class="n">one_axis</span><span class="p">)</span>

    <span class="n">out_shape</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dim</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">axis_reduce</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">keep_dims</span><span class="p">:</span>
                <span class="n">out_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">out_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">out_shape</span>


<span class="k">class</span> <span class="nc">_BinaryOp</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Define binary operators.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize _BinaryOp&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">get_broadcast_shape</span><span class="p">(</span><span class="n">x_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_MathBinaryOp</span><span class="p">(</span><span class="n">_BinaryOp</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Define math binary operators.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">do_infer_dtype</span><span class="p">(</span><span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">,</span> <span class="n">valid_dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="n">prim_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Staticmethod of infer dtype for _MathBinaryOp.&quot;&quot;&quot;</span>
        <span class="n">args_type</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">:</span> <span class="n">y_dtype</span><span class="p">}</span>
        <span class="n">complex_types</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">TensorType</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">complex64</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">TensorType</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">complex128</span><span class="p">)]</span>
        <span class="k">if</span> <span class="n">x_dtype</span> <span class="ow">in</span> <span class="n">complex_types</span> <span class="ow">or</span> <span class="n">y_dtype</span> <span class="ow">in</span> <span class="n">complex_types</span><span class="p">:</span>
            <span class="n">type_infer_dict</span> <span class="o">=</span> <span class="p">{</span>
                <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">complex64</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">complex64</span><span class="p">):</span> <span class="n">mstype</span><span class="o">.</span><span class="n">TensorType</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">complex64</span><span class="p">),</span>
                <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">complex64</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span> <span class="n">mstype</span><span class="o">.</span><span class="n">TensorType</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">complex64</span><span class="p">),</span>
                <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">complex64</span><span class="p">):</span> <span class="n">mstype</span><span class="o">.</span><span class="n">TensorType</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">complex64</span><span class="p">),</span>
                <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">complex128</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">complex128</span><span class="p">):</span> <span class="n">mstype</span><span class="o">.</span><span class="n">TensorType</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">complex128</span><span class="p">),</span>
                <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">complex128</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">):</span> <span class="n">mstype</span><span class="o">.</span><span class="n">TensorType</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">complex128</span><span class="p">),</span>
                <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">complex128</span><span class="p">):</span> <span class="n">mstype</span><span class="o">.</span><span class="n">TensorType</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">complex128</span><span class="p">),</span>
            <span class="p">}</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">x_dtype</span><span class="o">.</span><span class="n">element_type</span><span class="p">(),</span> <span class="n">y_dtype</span><span class="o">.</span><span class="n">element_type</span><span class="p">())</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">type_infer_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Complex math binary op expecting Tensor [Complex64, Complex64],&#39;</span>
                                <span class="o">+</span> <span class="s1">&#39;[Complex64, Float32], [Float32, Complex64], [Complex128, Complex128],&#39;</span>
                                <span class="o">+</span> <span class="s1">&#39;[Complex128, Float64], [Float64, Complex128],&#39;</span>
                                <span class="o">+</span> <span class="sa">f</span><span class="s1">&#39;but got : [</span><span class="si">{</span><span class="nb">format</span><span class="p">(</span><span class="n">x_dtype</span><span class="p">)</span><span class="si">}</span><span class="s1">,</span><span class="si">{</span><span class="nb">format</span><span class="p">(</span><span class="n">y_dtype</span><span class="p">)</span><span class="si">}</span><span class="s1">].&#39;</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">type_infer_dict</span><span class="o">.</span><span class="n">get</span><span class="p">((</span><span class="n">x_dtype</span><span class="o">.</span><span class="n">element_type</span><span class="p">(),</span> <span class="n">y_dtype</span><span class="o">.</span><span class="n">element_type</span><span class="p">()))</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args_type</span><span class="p">,</span> <span class="n">valid_dtype</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_MathBinaryOp</span><span class="o">.</span><span class="n">do_infer_dtype</span><span class="p">(</span><span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_convert_back_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape_value</span><span class="p">,</span> <span class="n">cmp_shape</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cmp_shape</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
            <span class="n">cmp_shape</span> <span class="o">=</span> <span class="n">cmp_shape</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cmp_shape</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">shape_value</span>
        <span class="n">real_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">dim</span> <span class="k">if</span> <span class="n">cmp_dim</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">cmp_dim</span> <span class="k">for</span> <span class="n">dim</span><span class="p">,</span> <span class="n">cmp_dim</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">shape_value</span><span class="p">,</span> <span class="n">cmp_shape</span><span class="p">)]</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">real_shape</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_BitwiseBinaryOp</span><span class="p">(</span><span class="n">_MathBinaryOp</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Define bitwise binary operators.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize _BitwiseBinaryOp&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="s1">&#39;x2&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_check_bitwise_op_input_type</span><span class="p">(</span><span class="n">x1_type</span><span class="p">,</span> <span class="n">x2_type</span><span class="p">,</span> <span class="n">prim</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;x1&#39;</span><span class="p">:</span> <span class="n">x1_type</span><span class="p">,</span> <span class="s1">&#39;x2&#39;</span><span class="p">:</span> <span class="n">x2_type</span><span class="p">}</span>
        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int_type</span> <span class="o">+</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint_type</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="n">prim</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x1_type</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1_type</span><span class="p">,</span> <span class="n">x2_type</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_BitwiseBinaryOp</span><span class="o">.</span><span class="n">_check_bitwise_op_input_type</span><span class="p">(</span><span class="n">x1_type</span><span class="p">,</span> <span class="n">x2_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<div class="viewcode-block" id="Ger"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Ger.html#mindspore.ops.Ger">[docs]</a><span class="k">class</span> <span class="nc">Ger</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Ger product of `x1` and `x2`. Calculate the outer product of two arrays. If `x1` is a 1D Tensor of</span>
<span class="sd">    shape :math:`(m,)` and `x2` is a 1D Tensor of shape :math:`(n,)`, then `output` must be a 2D Tensor of shape</span>
<span class="sd">    :math:`(m, n)`.</span>

<span class="sd">    Refer to :func:`mindspore.ops.ger` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x1** - (Tensor) - 1-D input Tensor.</span>
<span class="sd">        - **x2** - (Tensor) - 1-D input Tensor, has the same dtype as `x1`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, output matrix with the same dtype as inputs.With `x1` shape :math:`(m,)` and</span>
<span class="sd">        `x2` shape of :math:`(n,)`,the `output` has shape :math:`(m, n)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor([1., 2., 3., 4.], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor([1., 2., 3.], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; ger = ops.Ger()</span>
<span class="sd">        &gt;&gt;&gt; output = ger(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 1.  2.  3.]</span>
<span class="sd">         [ 2.  4.  6.]</span>
<span class="sd">         [ 3.  6.  9.]</span>
<span class="sd">         [ 4.  8. 12.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Ger&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="s1">&#39;x2&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Add"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Add.html#mindspore.ops.Add">[docs]</a><span class="k">class</span> <span class="nc">Add</span><span class="p">(</span><span class="n">_MathBinaryOp</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adds two input tensors element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.add` for more details.</span>

<span class="sd">    Note:</span>
<span class="sd">        - One of the two inputs must be a Tensor, when the two inputs have different shapes,</span>
<span class="sd">          they must be able to broadcast to a common shape.</span>
<span class="sd">        - The two inputs can not be bool type at the same time,</span>
<span class="sd">          [True, Tensor(True, bool\_), Tensor(np.array([True]), bool\_)] are all considered bool type.</span>
<span class="sd">        - The two inputs comply with the implicit type conversion rules to make the data types</span>
<span class="sd">          consistent.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor, number.Number, bool]) - The first input is a number.Number or</span>
<span class="sd">          a bool or a tensor whose data type is</span>
<span class="sd">          `number &lt;https://www.mindspore.cn/docs/en/r2.1/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">          `bool_ &lt;https://www.mindspore.cn/docs/en/r2.1/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>
<span class="sd">        - **y** (Union[Tensor, number.Number, bool]) - The second input, when the first input is a Tensor,</span>
<span class="sd">          the second input should be a number.Number or bool value, or a Tensor whose data type is number or bool.</span>
<span class="sd">          When the first input is Scalar, the second input must be a Tensor whose data type is number or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one of the input `x` , `y` after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; # case 1: x and y are both Tensor.</span>
<span class="sd">        &gt;&gt;&gt; add = ops.Add()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4, 5, 6]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = add(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [5. 7. 9.]</span>
<span class="sd">        &gt;&gt;&gt; # case 2: x is a scalar and y is a Tensor</span>
<span class="sd">        &gt;&gt;&gt; add = ops.Add()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(1, mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4, 5, 6]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = add(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [5. 6. 7.]</span>
<span class="sd">        &gt;&gt;&gt; # the data type of x is int32, the data type of y is float32,</span>
<span class="sd">        &gt;&gt;&gt; # and the output is the data format of higher precision float32.</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Float32</span>
<span class="sd">        &gt;&gt;&gt; # case 3: one of x and y is a bool scalar</span>
<span class="sd">        &gt;&gt;&gt; add = ops.Add()</span>
<span class="sd">        &gt;&gt;&gt; x = True</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4, 5, 6]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = add(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [5. 6. 7.]</span>
<span class="sd">        &gt;&gt;&gt; # case 4: one of x and y is a bool Tensor</span>
<span class="sd">        &gt;&gt;&gt; add = ops.Add()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([True, False, True]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4, 5, 6]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = add(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [5. 5. 7.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_infer_specified_add_value</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculate min/max value for output for Add op&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">a</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">b</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
                <span class="n">a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
                <span class="n">b</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
            <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
            <span class="n">out</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
            <span class="k">return</span> <span class="n">out</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_infer_min_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculate min value for output for Add op&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_infer_specified_add_value</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_infer_max_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculate max value for output for Add op&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_infer_specified_add_value</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_infer_shape_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">shape_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_infer_specified_add_value</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">shape_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_back_shape</span><span class="p">(</span><span class="n">shape_value</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_back_shape</span><span class="p">(</span><span class="n">shape_value</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="Addcdiv"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Addcdiv.html#mindspore.ops.Addcdiv">[docs]</a><span class="k">class</span> <span class="nc">Addcdiv</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adds the element-wise division of `x1` by `x2`, multiplied by `value` to `input_data`.</span>
<span class="sd">    It computes the following operation:</span>

<span class="sd">    .. math::</span>
<span class="sd">        y[i] = input\_data[i] + value[i] * (x1[i] / x2[i])</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_data** (Tensor) - The tensor to be added.</span>
<span class="sd">        - **x1** (Tensor) - The numerator tensor.</span>
<span class="sd">        - **x2** (Tensor) - The denominator tensor.</span>
<span class="sd">        - **value** (Tensor) - The multiplier for tensor x1/x2.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as x1/x2.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x1`, `x2`, `value`, `input_data` is not tensor.</span>
<span class="sd">        TypeError: If dtype of `x1`, `x2`, `value`, `input_data` are not the same.</span>
<span class="sd">        ValueError: If `x1` could not be broadcast to `x2`.</span>
<span class="sd">        ValueError: If `value` could not be broadcast to `x1/x2`.</span>
<span class="sd">        ValueError: If `input_data` could not be broadcast to `value*(x1/x2)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_data = Tensor(np.array([1, 1, 1, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([1, 2, 3, 4]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([4, 3, 2, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; value = Tensor([1], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; addcdiv = ops.Addcdiv()</span>
<span class="sd">        &gt;&gt;&gt; y = addcdiv(input_data, x1, x2, value)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [1.25      1.6666667 2.5       5.       ]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Addcdiv &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_data&#39;</span><span class="p">,</span> <span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="s1">&#39;x2&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Addcmul"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Addcmul.html#mindspore.ops.Addcmul">[docs]</a><span class="k">class</span> <span class="nc">Addcmul</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adds the element-wise product of `x1` by `x2`, multiplied by `value` to `input_data`.</span>
<span class="sd">    It computes the following operation:</span>

<span class="sd">    .. math::</span>
<span class="sd">        output[i] = input\_data[i] + value[i] * (x1[i] * x2[i])</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_data** (Tensor) - The tensor to be added.</span>
<span class="sd">        - **x1** (Tensor) - The tensor to be multiplied.</span>
<span class="sd">        - **x2** (Tensor) - The tensor to be multiplied.</span>
<span class="sd">        - **value** (Tensor) - The multiplier for tensor x1*x2.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as x1*x2.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x1`, `x2`, `value`, `input_data` is not tensor.</span>
<span class="sd">        TypeError: If dtype of `x1`, `x2`, `value`, `input_data` are not the same.</span>
<span class="sd">        ValueError: If `x1` could not be broadcast to `x2`.</span>
<span class="sd">        ValueError: If `value` could not be broadcast to `x1` * `x2`.</span>
<span class="sd">        ValueError: If `input_data` could not be broadcast to `value*(x1*x2)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_data = Tensor(np.array([1, 1, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([[1], [2], [3]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([[1, 2, 3]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; value = Tensor([1], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; addcmul = ops.Addcmul()</span>
<span class="sd">        &gt;&gt;&gt; y = addcmul(input_data, x1, x2, value)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[ 2.  3.  4.]</span>
<span class="sd">         [ 3.  5.  7.]</span>
<span class="sd">         [ 4.  7. 10.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Addcmul &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_data&#39;</span><span class="p">,</span> <span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="s1">&#39;x2&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<span class="k">class</span> <span class="nc">AddV2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adds two input tensors element-wise.</span>

<span class="sd">    Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors, and the shapes of them can be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar, the scalar could only be a constant.</span>
<span class="sd">    CPU/Ascend does not support broadcast for now.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = x_{i} + y_{i}</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor]) - The first input is a tensor whose data type is one of</span>
<span class="sd">          uint8, int8, int16, int32, int64, float16, float32, float64,</span>
<span class="sd">          complex64, complex128 currently or scalar.</span>
<span class="sd">        - **y** (Union[Tensor]) - The second input is a tensor whose data type is one of</span>
<span class="sd">          uint8, int8, int16, int32, int64, float16, float32, float64,</span>
<span class="sd">          complex64, complex128 currently or scalar.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the input tensor,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `x` nor `y` is a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` or `y` is not in [float16, float32, float64,</span>
<span class="sd">        uint8, int8, int16, int32, int64, complex64, complex128].</span>
<span class="sd">        ValueError: If the shape of &#39;x&#39; and &#39;y&#39; is not the same for CPU and Ascend.</span>


<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops.operations.math_ops import AddV2</span>
<span class="sd">        &gt;&gt;&gt; addv2 = AddV2()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4, 5, 6]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; output = addv2(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [5 7 9]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize AddV2&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">TensorAdd</span><span class="p">(</span><span class="n">_MathBinaryOp</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Same as operator Add. TensorAdd will be deprecated in the future.</span>
<span class="sd">    Please use Add instead.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@deprecated</span><span class="p">(</span><span class="s2">&quot;1.1&quot;</span><span class="p">,</span> <span class="s2">&quot;Add&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize TensorAdd.&quot;&quot;&quot;</span>
        <span class="n">_MathBinaryOp</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span>


<div class="viewcode-block" id="AssignAdd"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.AssignAdd.html#mindspore.ops.AssignAdd">[docs]</a><span class="k">class</span> <span class="nc">AssignAdd</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates a `Parameter` by adding a value to it.</span>

<span class="sd">    Refer to :func:`mindspore.ops.assign_add` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **variable** (Parameter) - The `Parameter`.</span>
<span class="sd">          :math:`(N,*)` where :math:`*` means, any number of additional dimensions, its rank should be less than 8.</span>
<span class="sd">        - **value** (Union[numbers.Number, Tensor]) - The value to be added to the `variable`.</span>
<span class="sd">          It must have the same shape as `variable` if it is a Tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same data type and shape as original `variable`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops, nn</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.AssignAdd = ops.AssignAdd()</span>
<span class="sd">        ...         self.variable = mindspore.Parameter(initializer(1, [1], mindspore.int64), name=&quot;global_step&quot;)</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def construct(self, x):</span>
<span class="sd">        ...         self.AssignAdd(self.variable, x)</span>
<span class="sd">        ...         return self.variable</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; value = Tensor(np.ones([1]).astype(np.int64)*100)</span>
<span class="sd">        &gt;&gt;&gt; output = net(value)</span>
<span class="sd">        &gt;&gt;&gt; print(net.variable.asnumpy())</span>
<span class="sd">        [101]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;ref&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;value&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize AssignAdd&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;ref&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;ref&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="AssignSub"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.AssignSub.html#mindspore.ops.AssignSub">[docs]</a><span class="k">class</span> <span class="nc">AssignSub</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates a `Parameter` by subtracting a value from it.</span>

<span class="sd">    Refer to :func:`mindspore.ops.assign_sub` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **variable** (Parameter) - The `Parameter`.</span>
<span class="sd">          :math:`(N,*)` where :math:`*` means, any number of additional dimensions, its rank be should be less than 8.</span>
<span class="sd">        - **value** (Union[numbers.Number, Tensor]) - The value to be subtracted from the `variable`.</span>
<span class="sd">          It must have the same shape as `variable` if it is a Tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same data type and shape as original `variable`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops, nn</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.AssignSub = ops.AssignSub()</span>
<span class="sd">        ...         self.variable = mindspore.Parameter(initializer(1, [1], mindspore.int32), name=&quot;global_step&quot;)</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def construct(self, x):</span>
<span class="sd">        ...         self.AssignSub(self.variable, x)</span>
<span class="sd">        ...         return self.variable</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; value = Tensor(np.ones([1]).astype(np.int32)*100)</span>
<span class="sd">        &gt;&gt;&gt; output = net(value)</span>
<span class="sd">        &gt;&gt;&gt; print(net.variable.asnumpy())</span>
<span class="sd">        [-99]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;val&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;value&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize AssignSub&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;val&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;val&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">_Reduce</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Definition of base class of reduction class operators.</span>

<span class="sd">    Args:</span>
<span class="sd">         keep_dims (bool): If ``True`` , keep these reduced dimensions and the length is 1.</span>
<span class="sd">                           If ``False`` , don&#39;t keep these dimensions. Default: ``False`` .</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input_x&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="p">())</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Reduce&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;keep_dims&#39;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="s1">&#39;axis&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">()):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">]</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">_run_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; return reduce op value&quot;&quot;&quot;</span>
        <span class="n">value</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">input_x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">axis</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">prim_map</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s1">&#39;ReduceMax&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">,</span>
                <span class="s1">&#39;ReduceMin&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">,</span>
                <span class="s1">&#39;ReduceProd&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">,</span>
                <span class="s1">&#39;ReduceMean&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">,</span>
                <span class="s1">&#39;ReduceAll&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">,</span>
                <span class="s1">&#39;ReduceAny&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">,</span>
            <span class="p">}</span>
            <span class="n">np_reduce_func</span> <span class="o">=</span> <span class="n">prim_map</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">np_reduce_func</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">value</span> <span class="o">=</span> <span class="n">input_x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                    <span class="k">pass</span>
                <span class="k">elif</span> <span class="n">axis</span><span class="p">:</span>
                    <span class="n">axis</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">axis</span><span class="p">))</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">axis</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">)))</span>
                <span class="n">value</span> <span class="o">=</span> <span class="n">np_reduce_func</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">keep_dims</span><span class="p">)</span>
                <span class="n">value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
                <span class="n">value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">value</span>


<span class="k">class</span> <span class="nc">EuclideanNorm</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the Euclidean norm(aka L2 norm) of a Tensor along the specified axes.</span>
<span class="sd">    The specified `axes` are removed by default.</span>

<span class="sd">    Args:</span>
<span class="sd">        keep_dims (bool, optional): whether to retain the reduced dimensions. If ``True`` , retains them with length 1.</span>
<span class="sd">            If ``False`` , these dimensions are removed. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input Tensor to reduce.</span>
<span class="sd">        - **axes** (Tensor) - The axes to perform reduction on. Must be one of the following types: int32, int64.</span>
<span class="sd">          It must be in range :math:`[-rank(x), rank(x))`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same type as the &#39;x&#39;.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        ValueError: If `axes` is out of range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[3, 5], [4, 12]])).astype(np.int32)</span>
<span class="sd">        &gt;&gt;&gt; axes = Tensor([0])</span>
<span class="sd">        &gt;&gt;&gt; op = ops.EuclideanNorm(keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, axes)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[5 13]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;axes&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<div class="viewcode-block" id="ReduceMean"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ReduceMean.html#mindspore.ops.ReduceMean">[docs]</a><span class="k">class</span> <span class="nc">ReduceMean</span><span class="p">(</span><span class="n">_Reduce</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces a dimension of a tensor by averaging all elements in the dimension, by default. And also can reduce</span>
<span class="sd">    a dimension of `x` along the axis. Determine whether the dimensions of the output and input are the same by</span>
<span class="sd">    controlling `keep_dims`.</span>

<span class="sd">    Args:</span>
<span class="sd">        keep_dims (bool): If ``True`` , keep these reduced dimensions and the length is 1.</span>
<span class="sd">                          If ``False`` , don&#39;t keep these dimensions. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor[Number]) - The input tensor.</span>
<span class="sd">        - **axis** (Union[int, tuple(int), list(int)]) - The dimensions to reduce. Default: ``()`` , reduce all</span>
<span class="sd">          dimensions. Only constant value is allowed. Must be in the range [-r, r).</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same dtype as the `x`.</span>

<span class="sd">        - If `axis` is (), and `keep_dims` is ``False`` ,</span>
<span class="sd">          the output is a 0-D tensor representing the mean of all elements in the input tensor.</span>
<span class="sd">        - If `axis` is int, set as 1, and `keep_dims` is ``False`` ,</span>
<span class="sd">          the shape of output is :math:`(x_0, x_2, ..., x_R)`.</span>
<span class="sd">        - If `axis` is tuple(int) or list(int), set as (1, 2), and `keep_dims` is ``False`` ,</span>
<span class="sd">          the shape of output is :math:`(x_0, x_3, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not one of the following: int, tuple or list.</span>
<span class="sd">        ValueError: If `axis` is out of range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.randn(3, 4, 5, 6).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; op = ops.ReduceMean(keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 1)</span>
<span class="sd">        &gt;&gt;&gt; result = output.shape</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        (3, 1, 5, 6)</span>
<span class="sd">        &gt;&gt;&gt; # case 1: Reduces a dimension by averaging all elements in the dimension.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2]],</span>
<span class="sd">        ... [[4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5], [6, 6, 6, 6, 6, 6]],</span>
<span class="sd">        ... [[6, 6, 6, 6, 6, 6], [8, 8, 8, 8, 8, 8], [10, 10, 10, 10, 10, 10]]]),</span>
<span class="sd">        ... mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = op(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[5.]]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 1, 1)</span>
<span class="sd">        &gt;&gt;&gt; # case 2: Reduces a dimension along the axis 0</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[4. 4. 4. 4. 4. 4.]</span>
<span class="sd">          [5. 5. 5. 5. 5. 5.]</span>
<span class="sd">          [6. 6. 6. 6. 6. 6.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: Reduces a dimension along the axis 1</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[2. 2. 2. 2. 2. 2.]]</span>
<span class="sd">         [[5. 5. 5. 5. 5. 5.]]</span>
<span class="sd">         [[8. 8. 8. 8. 8. 8.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 4: Reduces a dimension along the axis 2</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[ 2.]</span>
<span class="sd">          [ 2.]</span>
<span class="sd">          [ 2.]]</span>
<span class="sd">         [[ 4.]</span>
<span class="sd">          [ 5.]</span>
<span class="sd">          [ 6.]]</span>
<span class="sd">         [[ 6.]</span>
<span class="sd">          [ 8.]</span>
<span class="sd">          [10.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ReduceMean&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ReduceMean</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">keep_dims</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">CumulativeLogsumexp</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the cumulative log-sum-exp of the input tensor `x` along `axis` . For example, with all parameters at</span>
<span class="sd">    default values, if the input `x` is a tensor [a, b, c], the output will be [a, log(exp(a) + exp(b)),</span>
<span class="sd">    log(exp(a) + exp(b) + exp(c))].</span>

<span class="sd">    Args:</span>
<span class="sd">        exclusive (bool, optional): If ``True`` , the last element will be skipped during the calculation and thus an</span>
<span class="sd">                                    exclusive cumulative log-sum-exp will be performed. For example, this operation</span>
<span class="sd">                                    will output [-inf, a, log(exp(a) * exp(b))] with tensor [a, b, c] as the input.</span>
<span class="sd">                                    Note that the minimal value -inf, for performance reasons, is representable by the</span>
<span class="sd">                                    floating point type. Default: ``False`` .</span>
<span class="sd">        reverse (bool, optional): If ``True`` , the function accumulation values will be calculated after the elements</span>
<span class="sd">                                  of `x` on `axis` are flipped, and the calculation result will be flipped afterwards.</span>
<span class="sd">                                  For example, this operation will output [log(exp(c) + exp(b) + exp(a)), log(exp(c) +</span>
<span class="sd">                                  exp(b)), c] with tensor [a, b, c] as the input. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor. Must be one of the following types: float16, float32, float64. The</span>
<span class="sd">          dimension of `x` must greater than 0.</span>
<span class="sd">        - **axis** (Tensor) - A 0-D tensor describing the dimension to compute the cumulative product. Must be one of</span>
<span class="sd">          the following types: int64, int32, int16. Must be in the range [-rank(x), rank(x)). Default: ``0`` .</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same dtype and shape as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` or `axis` not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not in [float16, float32, float64].</span>
<span class="sd">        TypeError: If dtype of `axis` is not in [int16, int32, int64].</span>
<span class="sd">        TypeError: If `exclusive` or `reverse` is not a bool.</span>
<span class="sd">        ValueError: If the dimension of `x` is not greater than 0.</span>
<span class="sd">        RuntimeError: If `axis` is out of range [-rank(x), rank(x)).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 3.0]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; op = ops.CumulativeLogsumexp(exclusive=False, reverse=False)</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, Tensor(0))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.        2.3132617 3.407606 ]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 3.0]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; op = ops.CumulativeLogsumexp(exclusive=True, reverse=False)</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, Tensor(0))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-3.4028235e+38  1.0000000e+00  2.3132617e+00]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 3.0]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; op = ops.CumulativeLogsumexp(exclusive=False, reverse=True)</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, Tensor(0))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [3.407606  3.3132617 3.       ]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 3.0]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; op = ops.CumulativeLogsumexp(exclusive=True, reverse=True)</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, Tensor(0))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 3.3132617e+00  3.0000000e+00 -3.4028235e+38]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize  CumulativeLogsumexp&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;axis&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="n">exclusive</span><span class="p">,</span> <span class="s2">&quot;exclusive&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="n">reverse</span><span class="p">,</span> <span class="s2">&quot;reverse&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<div class="viewcode-block" id="ReduceSum"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ReduceSum.html#mindspore.ops.ReduceSum">[docs]</a><span class="k">class</span> <span class="nc">ReduceSum</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces a dimension of a tensor by summing all elements in the dimension, by default. And also can reduce a</span>
<span class="sd">    dimension of `x` along the axis. Determine whether the dimensions of the output and input are the same by</span>
<span class="sd">    controlling `keep_dims`.</span>

<span class="sd">    Args:</span>
<span class="sd">        keep_dims (bool): If ``True`` , keep these reduced dimensions and the length is 1.</span>
<span class="sd">                          If ``False`` , don&#39;t keep these dimensions. Default: ``False`` .</span>
<span class="sd">        skip_mode (bool): If ``True`` and axis is empty tuple or empty list, the ReduceSum operation isn&#39;t performed,</span>
<span class="sd">                          skip it.</span>
<span class="sd">                          If ``True`` and axis is other values, the ReduceSum calculation is performed normally.</span>
<span class="sd">                          If ``False`` , do reduce. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">         - **x** (Tensor[Number]) - The input tensor.</span>
<span class="sd">         - **axis** (Union[int, tuple(int), list(int)]) - The dimensions to reduce. Default: ``()`` , reduce all</span>
<span class="sd">           dimensions when skip_mode is ``False`` . Only constant value is allowed. Must be in the range [-rank(`x`),</span>
<span class="sd">           rank(`x`)).</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same dtype as the `x`.</span>

<span class="sd">        - If axis is (), keep_dims is ``False`` , and skip_mode is ``False`` ,</span>
<span class="sd">          the output is a 0-D tensor representing the sum of all elements in the input tensor.</span>
<span class="sd">        - If axis is (), and skip_mode is ``True`` ,</span>
<span class="sd">          the ReduceSum operation is not performed, output tensor is equal to the input tensor.</span>
<span class="sd">        - If axis is int, set as 2, and keep_dims is ``False`` ,</span>
<span class="sd">          the shape of output is :math:`(x_1, x_3, ..., x_R)`.</span>
<span class="sd">        - If axis is tuple(int) or list(int), set as (2, 3), and keep_dims is ``False`` ,</span>
<span class="sd">          the shape of output is :math:`(x_1, x_4, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        TypeError: If `skip_mode` is not a bool.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        ValueError: If `axis` is None.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.randn(3, 4, 5, 6).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; op = ops.ReduceSum(keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 1)</span>
<span class="sd">        &gt;&gt;&gt; output.shape</span>
<span class="sd">        (3, 1, 5, 6)</span>
<span class="sd">        &gt;&gt;&gt; # case 1: Reduces a dimension by summing all elements in the dimension.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2], [3, 3, 3, 3, 3, 3]],</span>
<span class="sd">        ...                      [[4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5], [6, 6, 6, 6, 6, 6]],</span>
<span class="sd">        ...                      [[7, 7, 7, 7, 7, 7], [8, 8, 8, 8, 8, 8], [9, 9, 9, 9, 9, 9]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = op(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[270.]]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 1, 1)</span>
<span class="sd">        &gt;&gt;&gt; # case 2: Reduces a dimension along axis 0.</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[12. 12. 12. 12. 12. 12.]</span>
<span class="sd">          [15. 15. 15. 15. 15. 15.]</span>
<span class="sd">          [18. 18. 18. 18. 18. 18.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: Reduces a dimension along axis 1.</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[ 6.  6.  6.  6.  6.  6.]]</span>
<span class="sd">         [[15. 15. 15. 15. 15. 15.]]</span>
<span class="sd">         [[24. 24. 24. 24. 24. 24.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 4: Reduces a dimension along axis 2.</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[ 6.]</span>
<span class="sd">          [12.]</span>
<span class="sd">          [18.]]</span>
<span class="sd">         [[24.]</span>
<span class="sd">          [30.]</span>
<span class="sd">          [36.]]</span>
<span class="sd">         [[42.]</span>
<span class="sd">          [48.]</span>
<span class="sd">          [54.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input_x&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="p">())</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">skip_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Reduce&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;keep_dims&#39;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;skip_mode&#39;</span><span class="p">,</span> <span class="n">skip_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="s1">&#39;axis&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_dims</span> <span class="o">=</span> <span class="n">keep_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">skip_mode</span> <span class="o">=</span> <span class="n">skip_mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__setattr_flag__</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">()):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">]</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">_run_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; return reduce op value&quot;&quot;&quot;</span>
        <span class="n">value</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">input_x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">axis</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">input_x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="k">pass</span>
            <span class="k">elif</span> <span class="n">axis</span><span class="p">:</span>
                <span class="n">axis</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">axis</span><span class="p">))</span>
            <span class="k">elif</span> <span class="n">axis</span> <span class="ow">in</span> <span class="p">((),</span> <span class="p">[])</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">skip_mode</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">input_x</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">axis</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">)))</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">keep_dims</span><span class="p">)</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">value</span></div>


<div class="viewcode-block" id="ReduceAll"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ReduceAll.html#mindspore.ops.ReduceAll">[docs]</a><span class="k">class</span> <span class="nc">ReduceAll</span><span class="p">(</span><span class="n">_Reduce</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces a dimension of a tensor by the &quot;logicalAND&quot; of all elements in the dimension, by default. And also can</span>
<span class="sd">    reduce a dimension of `x` along the axis. Determine whether the dimensions of the output and input are the same by</span>
<span class="sd">    controlling `keep_dims`.</span>

<span class="sd">    Args:</span>
<span class="sd">       keep_dims (bool): If ``True`` , keep these reduced dimensions and the length is 1.</span>
<span class="sd">                         If ``False`` , don&#39;t keep these dimensions. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor[bool]) - The input tensor. The dtype of the tensor to be reduced is bool.</span>
<span class="sd">        - **axis** (Union[int, tuple(int), list(int)]) - The dimensions to reduce. Default: ``()`` , reduce all</span>
<span class="sd">          dimensions. Only constant value is allowed. Must be in the range [-rank(x), rank(x)).</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the dtype is bool.</span>

<span class="sd">        - If axis is (), and keep_dims is ``False`` ,</span>
<span class="sd">          the output is a 0-D tensor representing the &quot;logical and&quot; of all elements in the input tensor.</span>
<span class="sd">        - If axis is int, set as 2, and keep_dims is ``False`` ,</span>
<span class="sd">          the shape of output is :math:`(x_1, x_3, ..., x_R)`.</span>
<span class="sd">        - If axis is tuple(int), set as (2, 3), and keep_dims is ``False`` ,</span>
<span class="sd">          the shape of output is :math:`(x_1, x_4, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not one of the following: int, tuple or list.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[True, False], [True, True]]))</span>
<span class="sd">        &gt;&gt;&gt; op = ops.ReduceAll(keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; # case 1: Reduces a dimension by the &quot;logicalAND&quot; of all elements in the dimension.</span>
<span class="sd">        &gt;&gt;&gt; output = op(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[False]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 1)</span>
<span class="sd">        &gt;&gt;&gt; # case 2: Reduces a dimension along axis 0.</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ True False]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: Reduces a dimension along axis 1.</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[False]</span>
<span class="sd">        [ True]]</span>
<span class="sd">        &gt;&gt;&gt; # case 4: input is a scalar.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(True)</span>
<span class="sd">        &gt;&gt;&gt; op = ops.ReduceAll()</span>
<span class="sd">        &gt;&gt;&gt; output = op(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        True</span>
<span class="sd">    &quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="ReduceAny"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ReduceAny.html#mindspore.ops.ReduceAny">[docs]</a><span class="k">class</span> <span class="nc">ReduceAny</span><span class="p">(</span><span class="n">_Reduce</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces a dimension of a tensor by the &quot;logical OR&quot; of all elements in the dimension, by default. And also can</span>
<span class="sd">    reduce a dimension of `x` along the axis. Determine whether the dimensions of the output and input are the same by</span>
<span class="sd">    controlling `keep_dims`.</span>

<span class="sd">    Args:</span>
<span class="sd">       keep_dims (bool): If ``True`` , keep these reduced dimensions and the length is 1.</span>
<span class="sd">                         If ``False`` , don&#39;t keep these dimensions. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor[bool]) - The input tensor. The dtype of the tensor to be reduced is bool.</span>
<span class="sd">        - **axis** (Union[int, tuple(int), list(int)]) - The dimensions to reduce. Default: ``()`` , reduce all</span>
<span class="sd">          dimensions. Only constant value is allowed. Must be in the range [-rank(x), rank(x)).</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the dtype is bool.</span>

<span class="sd">        - If axis is (), and keep_dims is False,</span>
<span class="sd">          the output is a 0-D tensor representing the &quot;logical or&quot; of all elements in the input tensor.</span>
<span class="sd">        - If axis is int, set as 2, and keep_dims is False,</span>
<span class="sd">          the shape of output is :math:`(x_1, x_3, ..., x_R)`.</span>
<span class="sd">        - If axis is tuple(int), set as (2, 3), and keep_dims is False,</span>
<span class="sd">          the shape of output is :math:`(x_1, x_4, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not one of the following: int, tuple or list.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[True, False], [True, True]]))</span>
<span class="sd">        &gt;&gt;&gt; op = ops.ReduceAny(keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; # case 1: Reduces a dimension by the &quot;logical OR&quot; of all elements in the dimension.</span>
<span class="sd">        &gt;&gt;&gt; output = op(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ True]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 1)</span>
<span class="sd">        &gt;&gt;&gt; # case 2: Reduces a dimension along axis 0.</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ True True]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: Reduces a dimension along axis 1.</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[True]</span>
<span class="sd">        [ True]]</span>
<span class="sd">        &gt;&gt;&gt; # case 4: input is a scalar.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(True)</span>
<span class="sd">        &gt;&gt;&gt; op = ops.ReduceAny()</span>
<span class="sd">        &gt;&gt;&gt; output = op(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        True</span>
<span class="sd">    &quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="ReduceMax"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ReduceMax.html#mindspore.ops.ReduceMax">[docs]</a><span class="k">class</span> <span class="nc">ReduceMax</span><span class="p">(</span><span class="n">_Reduce</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces a dimension of a tensor by the maximum value in this dimension, by default. And also can</span>
<span class="sd">    reduce a dimension of `x` along the axis. Determine whether the dimensions of the output and input are the same by</span>
<span class="sd">    controlling `keep_dims`.</span>

<span class="sd">    Args:</span>
<span class="sd">        keep_dims (bool): If ``True`` , keep these reduced dimensions and the length is 1.</span>
<span class="sd">                          If ``False`` , don&#39;t keep these dimensions. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">         - **x** (Tensor[Number]) - The input tensor.</span>
<span class="sd">         - **axis** (Union[int, tuple(int), list(int)]) - The dimensions to reduce. Default: ``()`` , reduce all</span>
<span class="sd">           dimensions. Only constant value is allowed. Must be in the range [-r, r).</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same dtype as the `x`.</span>

<span class="sd">        - If `axis` is (), and `keep_dims` is False,</span>
<span class="sd">          the output is a 0-D tensor representing the maximum of all elements in the input tensor.</span>
<span class="sd">        - If `axis` is int, set as 1, and `keep_dims` is False,</span>
<span class="sd">          the shape of output is :math:`(x_0, x_2, ..., x_R)`.</span>
<span class="sd">        - If `axis` is tuple(int) or list(int), set as (1, 2), and `keep_dims` is False,</span>
<span class="sd">          the shape of output is :math:`(x_0, x_3, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not one of the following: int, tuple or list.</span>
<span class="sd">        ValueError: If `axis` is out of range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.randn(3, 4, 5, 6).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; op = ops.ReduceMax(keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 1)</span>
<span class="sd">        &gt;&gt;&gt; result = output.shape</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        (3, 1, 5, 6)</span>
<span class="sd">        &gt;&gt;&gt; # case 1: Reduces a dimension by the maximum value of all elements in the dimension.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2], [3, 3, 3, 3, 3, 3]],</span>
<span class="sd">        ...                      [[4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5], [6, 6, 6, 6, 6, 6]],</span>
<span class="sd">        ...                      [[7, 7, 7, 7, 7, 7], [8, 8, 8, 8, 8, 8], [9, 9, 9, 9, 9, 9]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = op(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[9.]]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 1, 1)</span>
<span class="sd">        &gt;&gt;&gt; # case 2: Reduces a dimension along axis 0.</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[7. 7. 7. 7. 7. 7.]</span>
<span class="sd">          [8. 8. 8. 8. 8. 8.]</span>
<span class="sd">          [9. 9. 9. 9. 9. 9.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: Reduces a dimension along axis 1.</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[3. 3. 3. 3. 3. 3.]]</span>
<span class="sd">         [[6. 6. 6. 6. 6. 6.]]</span>
<span class="sd">         [[9. 9. 9. 9. 9. 9.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 4: Reduces a dimension along axis 2.</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1.]</span>
<span class="sd">          [2.]</span>
<span class="sd">          [3.]]</span>
<span class="sd">         [[4.]</span>
<span class="sd">          [5.]</span>
<span class="sd">          [6.]]</span>
<span class="sd">         [[7.]</span>
<span class="sd">          [8.]</span>
<span class="sd">          [9.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ReduceMax.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ReduceMax</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">keep_dims</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__setattr_flag__</span> <span class="o">=</span> <span class="kc">True</span></div>


<div class="viewcode-block" id="ReduceMin"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ReduceMin.html#mindspore.ops.ReduceMin">[docs]</a><span class="k">class</span> <span class="nc">ReduceMin</span><span class="p">(</span><span class="n">_Reduce</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces a dimension of a tensor by the minimum value in the dimension, by default. And also can</span>
<span class="sd">    reduce a dimension of `x` along the axis. Determine whether the dimensions of the output and input are the same by</span>
<span class="sd">    controlling `keep_dims`.</span>

<span class="sd">    Args:</span>
<span class="sd">        keep_dims (bool): If ``True`` , keep these reduced dimensions and the length is 1.</span>
<span class="sd">                          If ``False`` , don&#39;t keep these dimensions. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor[Number]) - The input tensor.</span>
<span class="sd">        - **axis** (Union[int, tuple(int), list(int)]) - The dimensions to reduce. Default: ``()`` , reduce all</span>
<span class="sd">          dimensions. Only constant value is allowed. Must be in the range [-r, r).</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same dtype as the `x`.</span>

<span class="sd">        - If `axis` is (), and `keep_dims` is ``False`` ,</span>
<span class="sd">          the output is a 0-D tensor representing the minimum of all elements in the input tensor.</span>
<span class="sd">        - If `axis` is int, set as 1, and `keep_dims` is ``False`` ,</span>
<span class="sd">          the shape of output is :math:`(x_0, x_2, ..., x_R)`.</span>
<span class="sd">        - If `axis` is tuple(int), set as (1, 2), and `keep_dims` is ``False`` ,</span>
<span class="sd">          the shape of output is :math:`(x_0, x_3, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not one of the following: int, tuple or list.</span>
<span class="sd">        ValueError: If `axis` is out of range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.randn(3, 4, 5, 6).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; op = ops.ReduceMin(keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 1)</span>
<span class="sd">        &gt;&gt;&gt; result = output.shape</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        (3, 1, 5, 6)</span>
<span class="sd">        &gt;&gt;&gt; # case 1: Reduces a dimension by the minimum value of all elements in the dimension.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2], [3, 3, 3, 3, 3, 3]],</span>
<span class="sd">        ...                      [[4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5], [6, 6, 6, 6, 6, 6]],</span>
<span class="sd">        ...                      [[7, 7, 7, 7, 7, 7], [8, 8, 8, 8, 8, 8], [9, 9, 9, 9, 9, 9]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = op(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1.]]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 1, 1)</span>
<span class="sd">        &gt;&gt;&gt; # case 2: Reduces a dimension along axis 0.</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1. 1. 1. 1. 1. 1.]</span>
<span class="sd">          [2. 2. 2. 2. 2. 2.]</span>
<span class="sd">          [3. 3. 3. 3. 3. 3.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: Reduces a dimension along axis 1.</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1. 1. 1. 1. 1. 1.]]</span>
<span class="sd">         [[4. 4. 4. 4. 4. 4.]]</span>
<span class="sd">         [[7. 7. 7. 7. 7. 7.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 4: Reduces a dimension along axis 2.</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1.]</span>
<span class="sd">          [2.]</span>
<span class="sd">          [3.]]</span>
<span class="sd">         [[4.]</span>
<span class="sd">          [5.]</span>
<span class="sd">          [6.]]</span>
<span class="sd">         [[7.]</span>
<span class="sd">          [8.]</span>
<span class="sd">          [9.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span></div>


<span class="k">class</span> <span class="nc">Bucketize</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Bucketizes `input` based on `boundaries`.</span>

<span class="sd">    Args:</span>
<span class="sd">        boundaries (list[float]): A sorted list of floats gives the boundary of the buckets, and no default value.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - A tensor containing the search value(s).</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same shape as the input, and data type is int32.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `boundaries` is not a listFloat.</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class Bucketize(nn.Cell):</span>
<span class="sd">        ...     def __init__(self, boundaries):</span>
<span class="sd">        ...         super().__init__()</span>
<span class="sd">        ...         self.bucketize = ops.Bucketize(boundaries=boundaries)</span>
<span class="sd">        ...     def construct(self, input):</span>
<span class="sd">        ...         return self.bucketize(input)</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([[3, 6, 9], [3, 6, 9]]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; boundaries = list(np.array([1., 3., 5., 7., 9.]))</span>
<span class="sd">        &gt;&gt;&gt; net = Bucketize(boundaries)</span>
<span class="sd">        &gt;&gt;&gt; output = net(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[2 3 5]</span>
<span class="sd">         [2 3 5]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">boundaries</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Bucketize&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;boundaries&quot;</span><span class="p">,</span> <span class="n">boundaries</span><span class="p">,</span> <span class="p">[</span><span class="nb">list</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">one_boundaries</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">boundaries</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;boundaries[</span><span class="si">%d</span><span class="s1">]&#39;</span> <span class="o">%</span> <span class="n">index</span><span class="p">,</span> <span class="n">one_boundaries</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>


<div class="viewcode-block" id="ReduceProd"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ReduceProd.html#mindspore.ops.ReduceProd">[docs]</a><span class="k">class</span> <span class="nc">ReduceProd</span><span class="p">(</span><span class="n">_Reduce</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces a dimension of a tensor by multiplying all elements in the dimension, by default. And also can</span>
<span class="sd">    reduce a dimension of `x` along the axis. Determine whether the dimensions of the output and input are the same by</span>
<span class="sd">    controlling `keep_dims`.</span>

<span class="sd">    Args:</span>
<span class="sd">        keep_dims (bool): If ``True`` , keep these reduced dimensions and the length is 1.</span>
<span class="sd">                          If ``False`` , don&#39;t keep these dimensions. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor[Number]) - The input tensor.</span>
<span class="sd">        - **axis** (Union[int, tuple(int), list(int)]) - The dimensions to reduce. Default: ``()`` , reduce all</span>
<span class="sd">          dimensions. Only constant value is allowed. Must be in the range [-r, r).</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same dtype as the `x`.</span>

<span class="sd">        - If `axis` is (), and `keep_dims` is ``False`` ,</span>
<span class="sd">          the output is a 0-D tensor representing the product of all elements in the input tensor.</span>
<span class="sd">        - If `axis` is int, set as 1, and `keep_dims` is ``False`` ,</span>
<span class="sd">          the shape of output is :math:`(x_0, x_2, ..., x_R)`.</span>
<span class="sd">        - If `axis` is tuple(int), set as (1, 2), and `keep_dims` is ``False`` ,</span>
<span class="sd">          the shape of output is :math:`(x_0, x_3, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not one of the following: int, tuple or list.</span>
<span class="sd">        ValueError: If `axis` is out of range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.randn(3, 4, 5, 6).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; op = ops.ReduceProd(keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 1)</span>
<span class="sd">        &gt;&gt;&gt; result = output.shape</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        (3, 1, 5, 6)</span>
<span class="sd">        &gt;&gt;&gt; # case 1: Reduces a dimension by multiplying all elements in the dimension.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2], [3, 3, 3, 3, 3, 3]],</span>
<span class="sd">        ...                      [[4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5], [6, 6, 6, 6, 6, 6]],</span>
<span class="sd">        ...                      [[7, 7, 7, 7, 7, 7], [8, 8, 8, 8, 8, 8], [9, 9, 9, 9, 9, 9]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = op(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[2.2833798e+33]]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 1, 1)</span>
<span class="sd">        &gt;&gt;&gt; # case 2: Reduces a dimension along axis 0.</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[ 28.  28.  28.  28.  28.  28.]</span>
<span class="sd">          [ 80.  80.  80.  80.  80.  80.]</span>
<span class="sd">          [162. 162. 162. 162. 162. 162.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: Reduces a dimension along axis 1.</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[  6.   6.   6.   6.   6.   6.]]</span>
<span class="sd">         [[120. 120. 120. 120. 120. 120.]]</span>
<span class="sd">         [[504. 504. 504. 504. 504. 504.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 4: Reduces a dimension along axis 2.</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1.00000e+00]</span>
<span class="sd">          [6.40000e+01]</span>
<span class="sd">          [7.29000e+02]]</span>
<span class="sd">         [[4.09600e+03]</span>
<span class="sd">          [1.56250e+04]</span>
<span class="sd">          [4.66560e+04]]</span>
<span class="sd">         [[1.17649e+05]</span>
<span class="sd">          [2.62144e+05]</span>
<span class="sd">          [5.31441e+05]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ReduceProd&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ReduceProd</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">keep_dims</span><span class="p">)</span></div>


<div class="viewcode-block" id="CumProd"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.CumProd.html#mindspore.ops.CumProd">[docs]</a><span class="k">class</span> <span class="nc">CumProd</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the cumulative product of the tensor x along axis.</span>
<span class="sd">    For example, if input is a vector of size N, the result will also be a vector of size N, with elements.</span>

<span class="sd">    .. math::</span>
<span class="sd">        y_i = x_1 * x_2 * x_3 * ... * x_i</span>

<span class="sd">    Args:</span>
<span class="sd">        exclusive (bool): If ``True`` , perform exclusive cumulative product. Default: ``False`` .</span>
<span class="sd">        reverse (bool): If ``True`` , reverse the result along axis. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor[Number]) - The input tensor.</span>
<span class="sd">        - **axis** (int) - The dimensions to compute the cumulative product.</span>
<span class="sd">          Only constant value is allowed.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `exclusive` or `reverse` is not a bool.</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>
<span class="sd">        ValueError: If `axis` is None.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; a, b, c, = 1, 2, 3</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([a, b, c]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; op0 = ops.CumProd()</span>
<span class="sd">        &gt;&gt;&gt; output0 = op0(x, 0) # output=[a, a * b, a * b * c]</span>
<span class="sd">        &gt;&gt;&gt; op1 = ops.CumProd(exclusive=True)</span>
<span class="sd">        &gt;&gt;&gt; output1 = op1(x, 0) # output=[1, a, a * b]</span>
<span class="sd">        &gt;&gt;&gt; op2 = ops.CumProd(reverse=True)</span>
<span class="sd">        &gt;&gt;&gt; output2 = op2(x, 0) # output=[a * b * c, b * c, c]</span>
<span class="sd">        &gt;&gt;&gt; op3 = ops.CumProd(exclusive=True, reverse=True)</span>
<span class="sd">        &gt;&gt;&gt; output3 = op3(x, 0) # output=[b * c, c, 1]</span>
<span class="sd">        &gt;&gt;&gt; print(output0)</span>
<span class="sd">        [1. 2. 6.]</span>
<span class="sd">        &gt;&gt;&gt; print(output1)</span>
<span class="sd">        [1. 1. 2.]</span>
<span class="sd">        &gt;&gt;&gt; print(output2)</span>
<span class="sd">        [6. 6. 3.]</span>
<span class="sd">        &gt;&gt;&gt; print(output3)</span>
<span class="sd">        [6. 3. 1.]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [5, 3, 5]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output4 = op0(x, 0)</span>
<span class="sd">        &gt;&gt;&gt; output5 = op0(x, 1)</span>
<span class="sd">        &gt;&gt;&gt; print(output4)</span>
<span class="sd">        [[ 1.  2.  3.]</span>
<span class="sd">         [ 4. 10. 18.]</span>
<span class="sd">         [20. 30. 90.]]</span>
<span class="sd">        &gt;&gt;&gt; print(output5)</span>
<span class="sd">        [[  1.   2.   6.]</span>
<span class="sd">         [  4.  20. 120.]</span>
<span class="sd">         [  5.  15.  75.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize CumProd.&quot;&quot;&quot;</span>
        <span class="n">cls_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">exclusive</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;exclusive&quot;</span><span class="p">,</span> <span class="n">exclusive</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reverse</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;reverse&quot;</span><span class="p">,</span> <span class="n">reverse</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;axis&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Lcm"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Lcm.html#mindspore.ops.Lcm">[docs]</a><span class="k">class</span> <span class="nc">Lcm</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes least common multiplier of input tensors element-wise.</span>
<span class="sd">    The shape of two inputs should be broadcastable, and data type of them should be</span>
<span class="sd">    one of: int32, int64.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x1** (Tensor) - The first input tensor.</span>
<span class="sd">        - **x2** (Tensor) - The second input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is one</span>
<span class="sd">        with higher digits in the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If data type `x1` or `x2` is not int32 or int64.</span>
<span class="sd">        ValueError: If shape of two inputs are not broadcastable.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([7, 8, 9]))</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([14, 6, 12]))</span>
<span class="sd">        &gt;&gt;&gt; lcm_ = ops.Lcm()</span>
<span class="sd">        &gt;&gt;&gt; y = lcm_(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [14 24 36]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="s1">&#39;x2&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Cdist"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Cdist.html#mindspore.ops.Cdist">[docs]</a><span class="k">class</span> <span class="nc">Cdist</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes batched the p-norm distance between each pair of the two collections of row vectors.</span>

<span class="sd">    Refer to :func:`mindspore.ops.cdist` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        p (float, optional): P value for the p-norm distance to calculate between each vector pair, P ∈ [0,∞].</span>
<span class="sd">            Default: ``2.0`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Input tensor of shape :math:`(B, P, M)`.</span>
<span class="sd">          When :math:`B` is equal to 0, it means this dimension can be ignored,</span>
<span class="sd">          i.e. shape of the tensor is :math:`(P, M)`.</span>
<span class="sd">        - **input_y** (Tensor) - Input tensor of shape :math:`(B, R, M)` with the same dtype as `input_x`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same dtype as `input_x`, which shape is :math:`(B, P, R)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[1.0, 1.0], [2.0, 2.0]]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.array([[[3.0, 3.0], [3.0, 3.0]]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; op = ops.Cdist(p=2.0)</span>
<span class="sd">        &gt;&gt;&gt; output = op(input_x, input_y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[2.8284273 2.8284273]</span>
<span class="sd">          [1.4142137 1.4142137]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">2.0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Cdist&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;p&quot;</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">p</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">p</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Cdist p must be a non-negative value, but got `</span><span class="si">{p}</span><span class="s1">`.&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="s1">&#39;input_y&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="LpNorm"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.LpNorm.html#mindspore.ops.LpNorm">[docs]</a><span class="k">class</span> <span class="nc">LpNorm</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the matrix norm or vector norm of a given tensor.</span>

<span class="sd">    .. math::</span>
<span class="sd">        output = \sum(abs(input)**p)**(1/p)</span>

<span class="sd">    Args:</span>
<span class="sd">        axis(int,list,tuple): Specifies which dimension or dimensions of input to calculate the norm across.</span>
<span class="sd">        p(int, optional): The order of norm. Default: ``2`` .</span>
<span class="sd">        keep_dims(bool, optional): Whether the output tensors have dim retained or not. Default: ``False`` .</span>
<span class="sd">        epsilon(float, optional): A value added to the denominator for numerical stability. Default: ``1e-12`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - Input tensor of type float16, float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same dtype as `input`, its shape depends on `axis`. For example, if the shape of input</span>
<span class="sd">        is :math:`(2, 3, 4)`, `axis` is :math:`[0, 1]`, output shape will be :math:`(4,)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not one of: float16, float32.</span>
<span class="sd">        TypeError: If `p` is not an int.</span>
<span class="sd">        TypeError: If `axis` is not an int, a tuple or a list.</span>
<span class="sd">        TypeError: If `axis` is a tuple or a list, but the element of `axis` is not an int.</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        ValueError: If the element of `axis` is out of the range :math:`[-r, r)`,</span>
<span class="sd">            where :math:`r` is the rank of `input`.</span>
<span class="sd">        ValueError: If the length of shape of `axis` is bigger than the length of shape of `input`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; op = ops.LpNorm(axis=[0, 1], p=2, keep_dims=False)</span>
<span class="sd">        &gt;&gt;&gt; output = op(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 9.165152 10.954452]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize LpNorm&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="s2">&quot;LpNorm&quot;</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;p&quot;</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;epsilon&quot;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="s2">&quot;p&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">(</span><span class="n">epsilon</span><span class="p">,</span> <span class="s2">&quot;epsilon&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">element_of_axis</span> <span class="ow">in</span> <span class="n">axis</span><span class="p">:</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;element_of_axis&quot;</span><span class="p">,</span> <span class="n">element_of_axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="MatMul"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.MatMul.html#mindspore.ops.MatMul">[docs]</a><span class="k">class</span> <span class="nc">MatMul</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multiplies matrix `a` and matrix `b`.</span>

<span class="sd">    .. math::</span>

<span class="sd">        (Output)_{i j}=\sum_{k=1}^{p} a_{i k} b_{k j}=a_{i 1} b_{1 j}+a_{i 2} b_{2 j}+\cdots+a_{i p} b_{p j}, p\in N</span>

<span class="sd">    where the :math:`i,j` indicates the output of the i-th row and j-th column element.</span>

<span class="sd">    Note:</span>
<span class="sd">        If :math:`N * M` cannot be divided by 16, the performance will be poor in ascend environment.</span>
<span class="sd">        The dtype of inputs must be same.</span>

<span class="sd">    Args:</span>
<span class="sd">        transpose_a (bool): If ``True`` , `a` is transposed before multiplication. Default: ``False`` .</span>
<span class="sd">        transpose_b (bool): If ``True`` , `b` is transposed before multiplication. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **a** (Tensor) - The first tensor to be multiplied. The shape of the tensor is :math:`(N, C)`. If</span>
<span class="sd">          `transpose_a` is ``True`` , its shape must be :math:`(C, N)` after transpose.</span>
<span class="sd">        - **b** (Tensor) - The second tensor to be multiplied. The shape of the tensor is :math:`(C, M)`. If</span>
<span class="sd">          `transpose_b` is ``True`` , its shape must be :math:`(M, C)` after transpose.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape of the output tensor is :math:`(N, M)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `transpose_a` or `transpose_b` is not a bool.</span>
<span class="sd">        TypeError: If the dtype of `a` and the dtype of `b` are not the same.</span>
<span class="sd">        ValueError: If the column of matrix dimensions of `a` is not equal to</span>
<span class="sd">                    the row of matrix dimensions of `b`.</span>
<span class="sd">        ValueError: If length of shape of `a` or `b` is not equal to 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; a = Tensor(np.ones(shape=[1, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; b = Tensor(np.ones(shape=[3, 4]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; matmul = ops.MatMul()</span>
<span class="sd">        &gt;&gt;&gt; output = matmul(a, b)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[3. 3. 3. 3.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">transpose_a</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transpose_b</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize MatMul.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="s1">&#39;x2&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="n">cls_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;transpose_a&quot;</span><span class="p">,</span> <span class="n">transpose_a</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;transpose_b&quot;</span><span class="p">,</span> <span class="n">transpose_b</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;transpose_x1&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose_a</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;transpose_x2&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose_b</span><span class="p">)</span></div>


<div class="viewcode-block" id="BatchMatMul"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.BatchMatMul.html#mindspore.ops.BatchMatMul">[docs]</a><span class="k">class</span> <span class="nc">BatchMatMul</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes matrix multiplication between two tensors by batch.</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{output}[..., :, :] = \text{matrix}(x[..., :, :]) * \text{matrix}(y[..., :, :])</span>

<span class="sd">    The rank of both two input tensors must be same and not less than `2`.</span>

<span class="sd">    Args:</span>
<span class="sd">        transpose_a (bool): If ``True`` , the last two dimensions of `x` is transposed before multiplication.</span>
<span class="sd">            Default: ``False`` .</span>
<span class="sd">        transpose_b (bool): If ``True`` , the last two dimensions of `y` is transposed before multiplication.</span>
<span class="sd">            Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The first tensor to be multiplied. The shape of the tensor is :math:`(*B, N, C)`,</span>
<span class="sd">          where :math:`*B` represents the batch size which can be multidimensional, :math:`N` and :math:`C` are the</span>
<span class="sd">          size of the last two dimensions. If `transpose_a` is ``True`` , its shape must be :math:`(*B, C, N)`.</span>
<span class="sd">        - **y** (Tensor) - The second tensor to be multiplied. The shape of the tensor is :math:`(*B, C, M)`. If</span>
<span class="sd">          `transpose_b` is ``True`` , its shape must be :math:`(*B, M, C)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape of the output tensor is :math:`(*B, N, M)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `transpose_a` or `transpose_b` is not a bool.</span>
<span class="sd">        ValueError: If length of shape of `x` is not equal to length of shape of `y` or</span>
<span class="sd">                    length of shape of inputs is less than 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones(shape=[2, 4, 1, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.ones(shape=[2, 4, 3, 4]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; batmatmul = ops.BatchMatMul()</span>
<span class="sd">        &gt;&gt;&gt; output = batmatmul(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 4, 1, 4)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones(shape=[2, 4, 3, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.ones(shape=[2, 4, 3, 4]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; batmatmul = ops.BatchMatMul(transpose_a=True)</span>
<span class="sd">        &gt;&gt;&gt; output = batmatmul(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 4, 1, 4)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">transpose_a</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transpose_b</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize BatchMatMul.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="s1">&#39;x2&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="n">cls_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;transpose_a&quot;</span><span class="p">,</span> <span class="n">transpose_a</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;transpose_b&quot;</span><span class="p">,</span> <span class="n">transpose_b</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;adj_x1&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose_a</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;adj_x2&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose_b</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">Betainc</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the regularized incomplete beta function</span>
<span class="sd">    :math:`I_{x}(a, b)`. It is defined as the ratio of the incomplete beta function</span>
<span class="sd">    to the complete beta function:</span>

<span class="sd">    .. math::</span>

<span class="sd">        I_{x}(a, b)=\frac{B(x ; a, b)}{B(a, b)}</span>

<span class="sd">    where</span>

<span class="sd">    .. math::</span>

<span class="sd">        B(x ; a, b)=\int_{0}^{x} t^{a-1}(1-t)^{b-1} dt</span>

<span class="sd">    is the incomplete beta function and</span>

<span class="sd">    .. math::</span>

<span class="sd">        B(a, b) = \int_0^1 t^{a-1} (1-t)^{b-1} dt</span>

<span class="sd">    is the complete beta function.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **a** (Tensor) - Peak location of beta distribution.</span>
<span class="sd">          A Tensor of types: float32, float64.</span>
<span class="sd">        - **b** (Tensor) - Spread of the beta distribution.</span>
<span class="sd">          A Tensor, must have the same dtype and shape as `a` .</span>
<span class="sd">        - **x** (Tensor) - Upper limit of integration of the incomplete beta function.</span>
<span class="sd">          A Tensor, must have the same dtype and shape as `a` .</span>

<span class="sd">    Outputs:</span>
<span class="sd">        A Tensor, has the same dtype and shape as `a` .</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `a` is not float32 nor float64.</span>
<span class="sd">        TypeError: If either dtype of `b` and `x` is not the same as the `a`.</span>
<span class="sd">        ValueError: If either shape of `b` and `x` is not the same as the `a`.</span>


<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; a = Tensor(np.array([0.3, 0.1, 0.4]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; b = Tensor(np.array([0.4, 0.5, 0.9]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.2, 0.6, 0.5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; betainc = ops.Betainc()</span>
<span class="sd">        &gt;&gt;&gt; print(betainc(a, b, x))</span>
<span class="sd">        [0.41462693 0.8706035  0.7298298 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Betainc&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>


<div class="viewcode-block" id="CumSum"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.CumSum.html#mindspore.ops.CumSum">[docs]</a><span class="k">class</span> <span class="nc">CumSum</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the cumulative sum of input tensor along axis.</span>

<span class="sd">    .. math::</span>

<span class="sd">        y_i = x_1 + x_2 + x_3 + ... + x_i</span>

<span class="sd">    Args:</span>
<span class="sd">        exclusive (bool): By default, this op performs an inclusive cumsum, which means that the first</span>
<span class="sd">            element of the input is identical to the first element of the output. Default: ``False`` .</span>
<span class="sd">        reverse (bool): If ``True`` , perform inverse cumulative sum. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - The input tensor to accumulate.</span>
<span class="sd">        - **axis**  (int) - The axis to accumulate the tensor&#39;s value. Only constant value is allowed.</span>
<span class="sd">          Must be in the range [-rank(input), rank(input)).</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape of the output tensor is consistent with the input tensor&#39;s.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `exclusive` or `reverse` is not a bool.</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[3, 4, 6, 10], [1, 6, 7, 9], [4, 3, 8, 7], [1, 3, 7, 9]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; cumsum = ops.CumSum()</span>
<span class="sd">        &gt;&gt;&gt; # case 1: along the axis 0</span>
<span class="sd">        &gt;&gt;&gt; y = cumsum(x, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[ 3.  4.  6. 10.]</span>
<span class="sd">         [ 4. 10. 13. 19.]</span>
<span class="sd">         [ 8. 13. 21. 26.]</span>
<span class="sd">         [ 9. 16. 28. 35.]]</span>
<span class="sd">        &gt;&gt;&gt; # case 2: along the axis 1</span>
<span class="sd">        &gt;&gt;&gt; y = cumsum(x, 1)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[ 3.  7. 13. 23.]</span>
<span class="sd">         [ 1.  7. 14. 23.]</span>
<span class="sd">         [ 4.  7. 15. 22.]</span>
<span class="sd">         [ 1.  4. 11. 20.]]</span>
<span class="sd">        &gt;&gt;&gt; # Next demonstrate exclusive and reverse, along axis 1</span>
<span class="sd">        &gt;&gt;&gt; # case 3: exclusive = True</span>
<span class="sd">        &gt;&gt;&gt; cumsum = ops.CumSum(exclusive=True)</span>
<span class="sd">        &gt;&gt;&gt; y = cumsum(x, 1)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[ 0.  3.  7. 13.]</span>
<span class="sd">         [ 0.  1.  7. 14.]</span>
<span class="sd">         [ 0.  4.  7. 15.]</span>
<span class="sd">         [ 0.  1.  4. 11.]]</span>
<span class="sd">        &gt;&gt;&gt; # case 4: reverse = True</span>
<span class="sd">        &gt;&gt;&gt; cumsum = ops.CumSum(reverse=True)</span>
<span class="sd">        &gt;&gt;&gt; y = cumsum(x, 1)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[23. 20. 16. 10.]</span>
<span class="sd">         [23. 22. 16.  9.]</span>
<span class="sd">         [22. 18. 15.  7.]</span>
<span class="sd">         [20. 19. 16.  9.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize cumsum&quot;&quot;&quot;</span>
        <span class="n">cls_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;exclusive&#39;</span><span class="p">,</span> <span class="n">exclusive</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;reverse&#39;</span><span class="p">,</span> <span class="n">reverse</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;axis&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="AddN"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.AddN.html#mindspore.ops.AddN">[docs]</a><span class="k">class</span> <span class="nc">AddN</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes addition of all input tensors element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.addn` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union(tuple[Tensor], list[Tensor])) - A tuple or list composed of Tensor, the data type is</span>
<span class="sd">          boolean or numeric.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as each Tensor of `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn, ops</span>
<span class="sd">        &gt;&gt;&gt; class NetAddN(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(NetAddN, self).__init__()</span>
<span class="sd">        ...         self.addN = ops.AddN()</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def construct(self, *z):</span>
<span class="sd">        ...         return self.addN(z)</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = NetAddN()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4, 5, 6]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = net(x, y, x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [10. 14. 18.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize AddN.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;inputs&quot;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;sum&quot;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">check_elim</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">return</span> <span class="kc">True</span><span class="p">,</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the type of &#39;inputs[0]&#39; must be a tensor, but &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">, &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;or the length of &#39;inputs&#39; should not be equal to 1, but got (</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="si">}</span><span class="s2">).&quot;</span><span class="p">)</span></div>


<div class="viewcode-block" id="AccumulateNV2"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.AccumulateNV2.html#mindspore.ops.AccumulateNV2">[docs]</a><span class="k">class</span> <span class="nc">AccumulateNV2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes accumulation of all input tensors element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.accumulate_n` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union(tuple[Tensor], list[Tensor])) - The input tuple or list</span>
<span class="sd">          is made up of multiple tensors whose dtype is number to be added together.</span>
<span class="sd">          Each element of tuple or list should have the same shape.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as each entry of the `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops, nn</span>
<span class="sd">        &gt;&gt;&gt; class NetAccumulateNV2(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(NetAccumulateNV2, self).__init__()</span>
<span class="sd">        ...         self.accumulateNV2 = ops.AccumulateNV2()</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def construct(self, *z):</span>
<span class="sd">        ...         return self.accumulateNV2(z)</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = NetAccumulateNV2()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4, 5, 6]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = net(x, y, x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [10. 14. 18.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize AccumulateNV2.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__setattr_flag__</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;inputs&quot;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;sum&quot;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">check_elim</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">return</span> <span class="kc">True</span><span class="p">,</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the type of &#39;inputs[0]&#39; must be a tensor, &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">, &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;or the length of &#39;inputs&#39; should not be equal to 1, but got (</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="si">}</span><span class="s2">).&quot;</span><span class="p">)</span></div>


<div class="viewcode-block" id="Neg"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Neg.html#mindspore.ops.Neg">[docs]</a><span class="k">class</span> <span class="nc">Neg</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a tensor with negative values of the input tensor element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.neg` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor whose dtype is Number.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as input.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; neg = ops.Neg()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, -1, 2, 0, -3.5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = neg(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-1.  -2.   1.  -2.   0.   3.5]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Neg&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="InplaceUpdateV2"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.InplaceUpdateV2.html#mindspore.ops.InplaceUpdateV2">[docs]</a><span class="k">class</span> <span class="nc">InplaceUpdateV2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates specified values in `x` to `v` according to `indices`.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.inplace_update` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - A tensor which to be inplace updated. It can be one of the following data types:</span>
<span class="sd">          float32, float16 and int32.</span>
<span class="sd">        - **indices** (Union[int, tuple]): Indices into the left-most dimension of `x`, and determines which rows of x</span>
<span class="sd">          to update with v. It is an int or tuple, whose value is in [0, the first dimension size of x).</span>
<span class="sd">        - **v** (Tensor) - A tensor with the same type as `x` and the same dimension size as `x` except</span>
<span class="sd">          the first dimension, which must be the same as the size of `indices`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the input `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; indices = (0, 1)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2], [3, 4], [5, 6]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; v = Tensor(np.array([[0.5, 1.0], [1.0, 1.5]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; inplace_update_v2 = ops.InplaceUpdateV2()</span>
<span class="sd">        &gt;&gt;&gt; output = inplace_update_v2(x, indices, v)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0.5 1. ]</span>
<span class="sd">         [1.  1.5]</span>
<span class="sd">         [5.  6. ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize InplaceUpdateV2&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="s1">&#39;v&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">v</span><span class="p">]</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">_run_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span></div>


<div class="viewcode-block" id="InplaceUpdate"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.InplaceUpdate.html#mindspore.ops.InplaceUpdate">[docs]</a><span class="k">class</span> <span class="nc">InplaceUpdate</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The InplaceUpdate interface is deprecated. Please use the :class:`mindspore.ops.InplaceUpdateV2` instead.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        Deprecated</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@deprecated</span><span class="p">(</span><span class="s2">&quot;2.0&quot;</span><span class="p">,</span> <span class="s2">&quot;ops.InplaceUpdateV2&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize InplaceUpdate&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;v&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">indices</span> <span class="o">=</span> <span class="n">indices</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;indices&quot;</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">indices</span> <span class="o">=</span> <span class="p">(</span><span class="n">indices</span><span class="p">,)</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;item of indices&quot;</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="InplaceAdd"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.InplaceAdd.html#mindspore.ops.InplaceAdd">[docs]</a><span class="k">class</span> <span class="nc">InplaceAdd</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adds `v` into specified rows of `x`. Computes `y` = `x`; y[i,] += `v`.</span>

<span class="sd">    Refer to :func:`mindspore.ops.inplace_add` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        indices (Union[int, tuple]): Indices into the left-most dimension of `x`, and determines which rows of `x`</span>
<span class="sd">            to add with `v`. It is an integer or a tuple, whose value is in [0, the first dimension size of `x`).</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The tensor to be added. It has shape :math:`(N,*)` where :math:`*` means</span>
<span class="sd">          any number of additional dimensions.</span>
<span class="sd">        - **input_v** (Tensor) - The value tensor add to `x`. It has the same dimension sizes as `x` except</span>
<span class="sd">          the first dimension, whose size must be the same as `indices`. It has the same data type with `x`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; indices = (0, 1)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2], [3, 4], [5, 6]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_v = Tensor(np.array([[0.5, 1.0], [1.0, 1.5]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; inplaceAdd = ops.InplaceAdd(indices)</span>
<span class="sd">        &gt;&gt;&gt; output = inplaceAdd(x, input_v)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1.5 3. ]</span>
<span class="sd">         [4.  5.5]</span>
<span class="sd">         [5.  6. ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize InplaceAdd&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;v&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">indices</span> <span class="o">=</span> <span class="n">indices</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">indices</span> <span class="o">=</span> <span class="p">(</span><span class="n">indices</span><span class="p">,)</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;item of indices&quot;</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="InplaceIndexAdd"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.InplaceIndexAdd.html#mindspore.ops.InplaceIndexAdd">[docs]</a><span class="k">class</span> <span class="nc">InplaceIndexAdd</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adds Tensor `updates` to specified axis and indices of Tensor `var` element-wise.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.inplace_index_add` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (int): The dimension along which to index. It should be in range :math:`[0, len(var.dim))`.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - The input Parameter to add to, with data type uint8, int8, int16, int32,</span>
<span class="sd">          float16, float32, float64.</span>
<span class="sd">        - **indices** (Tensor) - The indies along `axis` to perform the addition. A 1D Tensor</span>
<span class="sd">          of shape :math:`(updates.shape[axis],)`, every value of it</span>
<span class="sd">          should be in range :math:`[0, var.shape[axis])` with data type int32.</span>
<span class="sd">        - **updates** (Tensor) - The input Tensor with the value to add. Must have same data type as `var`.</span>
<span class="sd">          The shape must be the same as `var` except the `axis` th dimension.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, updated result, has the same shape and dtype as `var`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops, Parameter</span>
<span class="sd">        &gt;&gt;&gt; var = Parameter(Tensor(np.array([[1, 2], [3, 4], [5, 6]]), mindspore.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[0.5, 1.0], [1.0, 1.5]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; inplaceIndexAdd = ops.InplaceIndexAdd(axis=0)</span>
<span class="sd">        &gt;&gt;&gt; var = inplaceIndexAdd(var, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(var)</span>
<span class="sd">        [[1.5 3. ]</span>
<span class="sd">         [4.  5.5]</span>
<span class="sd">         [5.  6. ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;updates&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize InplaceIndexAdd&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="s1">&#39;updates&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="InplaceSub"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.InplaceSub.html#mindspore.ops.InplaceSub">[docs]</a><span class="k">class</span> <span class="nc">InplaceSub</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Subtracts `v` into specified rows of `x`. Computes :math:`y = x`; :math:`y[i,] -= input\_v`.</span>

<span class="sd">    Refer to :func:`mindspore.ops.inplace_sub` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        indices (Union[int, tuple]): Indices into the left-most dimension of `x`, and determines which rows of `x`</span>
<span class="sd">            to subtract by `v`. It is an integer or a tuple, whose value is in [0, the first dimension size of `x`).</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The tensor to be subtracted. It has shape :math:`(N,*)` where :math:`*` means</span>
<span class="sd">          any number of additional dimensions.</span>
<span class="sd">        - **input_v** (Tensor) - The value tensor subtract from `x`. It has the same dimension sizes as `x` except</span>
<span class="sd">          the first dimension, whose size must be the same as `indices`. It has the same data type with `x`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; indices = (0, 1)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2], [3, 4], [5, 6]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_v = Tensor(np.array([[0.5, 1.0], [1.0, 1.5]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; inplaceSub = ops.InplaceSub(indices)</span>
<span class="sd">        &gt;&gt;&gt; output = inplaceSub(x, input_v)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0.5 1. ]</span>
<span class="sd">         [2.  2.5]</span>
<span class="sd">         [5.  6. ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize InplaceSub&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;v&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">indices</span> <span class="o">=</span> <span class="n">indices</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">indices</span> <span class="o">=</span> <span class="p">(</span><span class="n">indices</span><span class="p">,)</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;item of indices&quot;</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;indices&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">)</span></div>


<div class="viewcode-block" id="Sub"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Sub.html#mindspore.ops.Sub">[docs]</a><span class="k">class</span> <span class="nc">Sub</span><span class="p">(</span><span class="n">_MathBinaryOp</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Subtracts the second input tensor from the first input tensor element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.sub` for more details.</span>

<span class="sd">    Note:</span>
<span class="sd">        - One of the two inputs must be a Tensor, when the two inputs have different shapes,</span>
<span class="sd">          they must be able to broadcast to a common shape.</span>
<span class="sd">        - The two inputs can not be bool type at the same time,</span>
<span class="sd">          [True, Tensor(True, bool\_), Tensor(np.array([True]), bool\_)] are all considered bool type.</span>
<span class="sd">        - The two inputs comply with the implicit type conversion rules to make the data types</span>
<span class="sd">          consistent.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor, number.Number, bool]) - The first input is a number.Number or</span>
<span class="sd">          a bool or a tensor whose data type is</span>
<span class="sd">          `number &lt;https://www.mindspore.cn/docs/en/r2.1/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">          `bool_ &lt;https://www.mindspore.cn/docs/en/r2.1/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>
<span class="sd">        - **y** (Union[Tensor, number.Number, bool]) - The second input, when the first input is a Tensor,</span>
<span class="sd">          the second input should be a number.Number or bool value, or a Tensor whose data type is number or bool.</span>
<span class="sd">          When the first input is Scalar, the second input must be a Tensor whose data type is number or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the two inputs after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4, 5, 6]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; sub = ops.Sub()</span>
<span class="sd">        &gt;&gt;&gt; output = sub(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-3 -3 -3]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">y</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="Mul"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Mul.html#mindspore.ops.Mul">[docs]</a><span class="k">class</span> <span class="nc">Mul</span><span class="p">(</span><span class="n">_MathBinaryOp</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multiplies two tensors element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.mul` for more details.</span>

<span class="sd">    Note:</span>
<span class="sd">        - One of the two inputs must be a Tensor, when the two inputs have different shapes,</span>
<span class="sd">          they must be able to broadcast to a common shape.</span>
<span class="sd">        - The two inputs can not be bool type at the same time,</span>
<span class="sd">          [True, Tensor(True, bool\_), Tensor(np.array([True]), bool\_)] are all considered bool type.</span>
<span class="sd">        - The two inputs comply with the implicit type conversion rules to make the data types</span>
<span class="sd">          consistent.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor, number.Number, bool]) - The first input is a number.Number or</span>
<span class="sd">          a bool or a tensor whose data type is</span>
<span class="sd">          `number &lt;https://www.mindspore.cn/docs/en/r2.1/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">          `bool_ &lt;https://www.mindspore.cn/docs/en/r2.1/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>
<span class="sd">        - **y** (Union[Tensor, number.Number, bool]) - The second input, when the first input is a Tensor,</span>
<span class="sd">          the second input should be a number.Number or bool value, or a Tensor whose data type is number or bool.</span>
<span class="sd">          When the first input is Scalar, the second input must be a Tensor whose data type is number or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4.0, 5.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; mul = ops.Mul()</span>
<span class="sd">        &gt;&gt;&gt; output = mul(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 4. 10. 18.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Let x/y using same sig_dtype to enable implicit conversion for compatibility</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">rw</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_READ</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">rw</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_READ</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Xdivy.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_infer_specified_mul_value</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculate min/max value for output of Mul op&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
                <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>
            <span class="n">out</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
            <span class="k">return</span> <span class="n">out</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_infer_min_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculate min value for output for Mul op&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_infer_specified_mul_value</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_infer_max_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculate max value for output for Mul op&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_infer_specified_mul_value</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_infer_shape_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">shape_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_infer_specified_mul_value</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">shape_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_back_shape</span><span class="p">(</span><span class="n">shape_value</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_back_shape</span><span class="p">(</span><span class="n">shape_value</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="SquaredDifference"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.SquaredDifference.html#mindspore.ops.SquaredDifference">[docs]</a><span class="k">class</span> <span class="nc">SquaredDifference</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Subtracts the second input tensor from the first input tensor element-wise and returns square of it.</span>

<span class="sd">    Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be bool at the same time, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = (x_{i} - y_{i}) * (x_{i} - y_{i}) = (x_{i} - y_{i})^2</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor, Number, bool]) - The first input is a number, or a bool, or a tensor.</span>
<span class="sd">        - **y** (Union[Tensor, Number, bool]) - The second input is a number, or a bool when the first input</span>
<span class="sd">          is a tensor, or a tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: if `x` and `y` is not a Number or a bool or a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([2.0, 4.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; squared_difference = ops.SquaredDifference()</span>
<span class="sd">        &gt;&gt;&gt; output = squared_difference(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 4. 9.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize _BinaryOp&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Square"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Square.html#mindspore.ops.Square">[docs]</a><span class="k">class</span> <span class="nc">Square</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns square of a tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = (x_{i})^2</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; square = ops.Square()</span>
<span class="sd">        &gt;&gt;&gt; output = square(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 4. 9.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Square&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Rsqrt"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Rsqrt.html#mindspore.ops.Rsqrt">[docs]</a><span class="k">class</span> <span class="nc">Rsqrt</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes reciprocal of square root of input tensor element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.rsqrt` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input Tensor, each element must be a non-negative,</span>
<span class="sd">          if an element is negative, the calculation result is nan.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same type and shape as `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor([[4, 4], [9, 9]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; rsqrt = ops.Rsqrt()</span>
<span class="sd">        &gt;&gt;&gt; output = rsqrt(input_tensor)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0.5        0.5       ]</span>
<span class="sd">         [0.33333334 0.33333334]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Rsqrt&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Sqrt"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Sqrt.html#mindspore.ops.Sqrt">[docs]</a><span class="k">class</span> <span class="nc">Sqrt</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns square root of a tensor element-wise.</span>

<span class="sd">    Note:</span>
<span class="sd">        When there are some negative number, it will return a Tensor whose specific position is nan.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} =  \sqrt{x_{i}}</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and data type as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 4.0, 9.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; sqrt = ops.Sqrt()</span>
<span class="sd">        &gt;&gt;&gt; output = sqrt(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 2. 3.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Sqrt&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Reciprocal"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Reciprocal.html#mindspore.ops.Reciprocal">[docs]</a><span class="k">class</span> <span class="nc">Reciprocal</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns reciprocal of a tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} =  \frac{1}{x_{i}}</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; reciprocal = ops.Reciprocal()</span>
<span class="sd">        &gt;&gt;&gt; output = reciprocal(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.   0.5  0.25]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Reciprocal&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;GPU&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="s2">&quot;GPU&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="s2">&quot;OTHER&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">x</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="Pow"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Pow.html#mindspore.ops.Pow">[docs]</a><span class="k">class</span> <span class="nc">Pow</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the `y` power of each element in `x`.</span>

<span class="sd">    Refer to :func:`mindspore.ops.pow` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor, number.Number, bool]) - The first input is a number.Number or</span>
<span class="sd">          a bool or a tensor whose data type is</span>
<span class="sd">          `number &lt;https://www.mindspore.cn/docs/en/r2.1/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">          `bool_ &lt;https://www.mindspore.cn/docs/en/r2.1/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>
<span class="sd">        - **y** (Union[Tensor, number.Number, bool]) - The second input, when the first input is a Tensor,</span>
<span class="sd">          the second input should be a number.Number or bool value, or a Tensor whose data type is number or bool\_.</span>
<span class="sd">          When the first input is Scalar, the second input must be a Tensor whose data type is number or bool\_.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = 3.0</span>
<span class="sd">        &gt;&gt;&gt; pow = ops.Pow()</span>
<span class="sd">        &gt;&gt;&gt; output = pow(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1.  8. 64.]</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([2.0, 4.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; pow = ops.Pow()</span>
<span class="sd">        &gt;&gt;&gt; output = pow(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1. 16. 64.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize _BinaryOp&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="s1">&#39;x2&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">power</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">power</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">power</span> <span class="o">=</span> <span class="n">power</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">power</span><span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="Exp"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Exp.html#mindspore.ops.Exp">[docs]</a><span class="k">class</span> <span class="nc">Exp</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns exponential of a tensor element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.exp` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.0, 1.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; exp = ops.Exp()</span>
<span class="sd">        &gt;&gt;&gt; output = exp(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1.        2.718282 20.085537]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Exp&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;base&quot;</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;scale&quot;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;shift&quot;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span></div>


<div class="viewcode-block" id="Logit"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Logit.html#mindspore.ops.Logit">[docs]</a><span class="k">class</span> <span class="nc">Logit</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the logit of a tensor element-wise. Element in `x` is clamped to [eps, 1-eps].</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.logit` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        eps (float, optional): The epsilon. The input clamp bound is defined as [eps, 1-eps]. Default: ``-1.0`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor of type float16, float32 or float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same shape and dtype as the `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.1, 0.2, 0.3]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; op = ops.Logit(eps=1e-5)</span>
<span class="sd">        &gt;&gt;&gt; output = op(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-2.1972246 -1.3862944 -0.8472978]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eps</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Exp&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;eps&quot;</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;eps&quot;</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<span class="k">class</span> <span class="nc">ReduceStd</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the standard-deviation and mean of the input Tensor along</span>
<span class="sd">    dimension(s) specified by `axis`.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (Union[int, tuple(int), list(int)], optional): The dimensions to reduce.</span>
<span class="sd">            Default: ``()`` , reduce all dimensions. Only constant value is allowed.</span>
<span class="sd">            Let `r` be rank of `input_x`, it should be in the range :math:`[-r,r)`.</span>
<span class="sd">        unbiased (bool, optional):  Whether to use Bessel&#39;s correction.</span>
<span class="sd">            If ``True`` , will use the Bessel correction unbiased estimation.</span>
<span class="sd">            If ``False`` , will through the biased estimation to calculate the standard deviation.</span>
<span class="sd">            Default: ``True`` .</span>
<span class="sd">        keep_dims (bool, optional): Whether the output Tensor has dim retained or not.</span>
<span class="sd">            If ``True`` , keep these reduced dimensions specified by `axis` and the length is 1.</span>
<span class="sd">            If ``False`` , don&#39;t keep these dimensions.</span>
<span class="sd">            Default: ``Fasle`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor[Number]) - The input Tensor with shape</span>
<span class="sd">          :math:`(N, *)` where :math:`*` means any number of additional dimensions.</span>
<span class="sd">          Supported dtypes: float16, float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple(output_std, output_mean) containing the standard deviation and mean.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        ValueError: If `axis` is not one of the following: int, tuple or list.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1, 2, 3], [-1, 1, 4]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; op = ops.ReduceStd(axis=1, unbiased=True, keep_dims=False)</span>
<span class="sd">        &gt;&gt;&gt; output = op(input_x)</span>
<span class="sd">        &gt;&gt;&gt; output_std, output_mean = output[0], output[1]</span>
<span class="sd">        &gt;&gt;&gt; print(output_std)</span>
<span class="sd">        [1.        2.5166113]</span>
<span class="sd">        &gt;&gt;&gt; print(output_mean)</span>
<span class="sd">        [2.        1.3333334]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(),</span> <span class="n">unbiased</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ReduceStd &quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;unbiased&quot;</span><span class="p">,</span> <span class="n">unbiased</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">element_of_axis</span> <span class="ow">in</span> <span class="n">axis</span><span class="p">:</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;element_of_axis&quot;</span><span class="p">,</span> <span class="n">element_of_axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output_std&#39;</span><span class="p">,</span> <span class="s1">&#39;output_mean&#39;</span><span class="p">])</span>


<div class="viewcode-block" id="Einsum"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Einsum.html#mindspore.ops.Einsum">[docs]</a><span class="k">class</span> <span class="nc">Einsum</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sums the product of the elements of the input Tensor along</span>
<span class="sd">    dimensions specified notation based on the Einstein summation convention(Einsum).</span>
<span class="sd">    You can use this operator to perform diagonal/reducesum/transpose/matmul/mul/inner product operations, etc.</span>

<span class="sd">    Args:</span>
<span class="sd">        equation (str): An attribute, represent the operation you want to do.</span>
<span class="sd">            the value can contain only letters([a-z][A-Z]), commas(,), ellipsis(...),</span>
<span class="sd">            and arrow(-&gt;). the letters represent inputs&#39;s tensor dimension,</span>
<span class="sd">            commas(,)represent separate tensors, ellipsis(...) indicates</span>
<span class="sd">            the tensor dimension that you do not care about, the left of the</span>
<span class="sd">            arrow(-&gt;) indicates the input tensors,</span>
<span class="sd">            and the right of it indicates the desired output dimension.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** () - Input tensor used for calculation.</span>
<span class="sd">          The inputs must be a tuple/list of Tensors.</span>
<span class="sd">          When the inputs are only one tensor, you can input (tensor, ).</span>
<span class="sd">          Dtypes of them should be float16/float32/float64 and dtype of the tensor(s) must be the same.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape of it can be obtained from the equation,</span>
<span class="sd">        and the data type is the same as input tensors.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If equation itself is invalid, or the equation does not match the input tensor.</span>
<span class="sd">        TypeError: If dtype of the input Tensors are not the same or dtype is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; equation = &quot;i-&gt;&quot;</span>
<span class="sd">        &gt;&gt;&gt; einsum = ops.Einsum(equation)</span>
<span class="sd">        &gt;&gt;&gt; output = einsum([x])</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [7.]</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([2.0, 4.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; equation = &quot;i,i-&gt;i&quot;</span>
<span class="sd">        &gt;&gt;&gt; einsum = ops.Einsum(equation)</span>
<span class="sd">        &gt;&gt;&gt; output = einsum((x, y))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 2. 8. 12.]</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([[2.0, 3.0], [1.0, 2.0], [4.0, 5.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; equation = &quot;ij,jk-&gt;ik&quot;</span>
<span class="sd">        &gt;&gt;&gt; einsum = ops.Einsum(equation)</span>
<span class="sd">        &gt;&gt;&gt; output = einsum((x, y))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[16. 22.]</span>
<span class="sd">        [37. 52.]]</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; equation = &quot;ij-&gt;ji&quot;</span>
<span class="sd">        &gt;&gt;&gt; einsum = ops.Einsum(equation)</span>
<span class="sd">        &gt;&gt;&gt; output = einsum((x,))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 4.]</span>
<span class="sd">        [2. 5.]</span>
<span class="sd">        [3. 6.]]</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; equation = &quot;ij-&gt;j&quot;</span>
<span class="sd">        &gt;&gt;&gt; einsum = ops.Einsum(equation)</span>
<span class="sd">        &gt;&gt;&gt; output = einsum((x,))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [5. 7. 9.]</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; equation = &quot;...-&gt;&quot;</span>
<span class="sd">        &gt;&gt;&gt; einsum = ops.Einsum(equation)</span>
<span class="sd">        &gt;&gt;&gt; output = einsum((x,))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [21.]</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([2.0, 4.0, 1.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; equation = &quot;j,i-&gt;ji&quot;</span>
<span class="sd">        &gt;&gt;&gt; einsum = ops.Einsum(equation)</span>
<span class="sd">        &gt;&gt;&gt; output = einsum((x, y))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 2. 4. 1.]</span>
<span class="sd">        [ 4. 8. 2.]</span>
<span class="sd">        [ 6. 12. 3.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">equation</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">equation</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;the equation must be str!&quot;</span><span class="p">)</span>
        <span class="n">seg_equation</span> <span class="o">=</span> <span class="n">equation</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;-&gt;&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">seg_equation</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;the equation can contain only one arrow !&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;equation&#39;</span><span class="p">,</span> <span class="n">equation</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;inputs&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<span class="k">class</span> <span class="nc">Diagonal</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create a tensor with specific diagonal elements of input. This operator extracts the diagonal elements with</span>
<span class="sd">    offset from the 2-D sub-tensors which specified by dim1 and dim2.</span>
<span class="sd">    The shape of output tensor can be dertermined by removing dim1 and dim2 form the shape of input and appending</span>
<span class="sd">    a dimension at the end. The size of the last dimension is the length of diagonal.</span>

<span class="sd">    Args:</span>
<span class="sd">        offset (int): The offset of main diagonal, which controls which diagonal to consider. If :math:`offset=0`,</span>
<span class="sd">            return the main diagonal elements with respect to dim1 and dim2. If :math:`offset&gt;0`, return the</span>
<span class="sd">            diagonal elements that are `offset` units upward from the main diagonal. If :math:`offset&lt;0`, return the</span>
<span class="sd">            diagonal elements that are `offset` units downward from the main diagonal. Default: ``0`` .</span>
<span class="sd">        dim1 (int): The first dimension with respect to which to take diagonal. Default: ``0`` .</span>
<span class="sd">        dim2 (int): The second dimension with respect to which to take diagonal. Default: ``1`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input to take diagonal, with float32 or double data type.</span>
<span class="sd">          The input must be at least 2-dimensional.</span>
<span class="sd">          The shape is :math:`(N_{0}, N_{1}, *)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - A tensor whose values are diagonal of input, with the same data type as input.</span>
<span class="sd">          The shape of the output is one dimension lower than the input.</span>
<span class="sd">          If the shape of `x` is :math:`(d_{0}, d_{1}, ..., d_{n-1})`, the size of the `dim1` dimension</span>
<span class="sd">          is :math:`d_{i}` and the size of the `dim2` dimension is :math:`d_{j}`, the shape of `y` is the same</span>
<span class="sd">          as the input shape with `dim1` and `dim2` dimension removed and the diagonal dimension appended.</span>
<span class="sd">          If the `offset` is nonnegative, the size of output&#39;s last dimension is</span>
<span class="sd">          :math:`max(min(d_{i}, d_{j}-offset), 0)`. But if the `offset` is negative, the size of output&#39;s</span>
<span class="sd">          last dimension is :math:`max(min(d_{i} + offset, d_{j}), 0)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x` is neither float32 nor double.</span>
<span class="sd">        TypeError: If `offset` is not an int.</span>
<span class="sd">        TypeError: If `dim1` is not an int.</span>
<span class="sd">        TypeError: If `dim2` is not an int.</span>
<span class="sd">        ValueError: If the dimension of input is less than 2 dimensions.</span>
<span class="sd">        ValueError: If `dim1` is not in range of [-len(x.shape), len(x.shape)).</span>
<span class="sd">        ValueError: If `dim2` is not in range of [-len(x.shape), len(x.shape)).</span>
<span class="sd">        ValueError: If `dim1` and `dim2` are identical.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``  ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[ 0.,  1.,  2.,  3.], [ 4.,  5.,  6.,  7.], [ 8.,  9., 10., 11.]],</span>
<span class="sd">        ... [[12., 13., 14., 15.], [16., 17., 18., 19.], [20., 21., 22., 23.]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; diagonal_ops = ops.Diagonal(offset=1, dim1=-1, dim2=1)</span>
<span class="sd">        &gt;&gt;&gt; y = diagonal_ops(x)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[ 4.  9.]</span>
<span class="sd">         [16. 21.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim2</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Diagonal&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_is_int</span><span class="p">(</span><span class="n">offset</span><span class="p">,</span> <span class="s2">&quot;offset&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_is_int</span><span class="p">(</span><span class="n">dim1</span><span class="p">,</span> <span class="s2">&quot;dim1&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_is_int</span><span class="p">(</span><span class="n">dim2</span><span class="p">,</span> <span class="s2">&quot;dim2&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<div class="viewcode-block" id="Expm1"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Expm1.html#mindspore.ops.Expm1">[docs]</a><span class="k">class</span> <span class="nc">Expm1</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns exponential then minus 1 of a tensor element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.expm1` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.0, 2.0, 3.0, 5.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; expm1 = ops.Expm1()</span>
<span class="sd">        &gt;&gt;&gt; output = expm1(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [  0.         6.389056  19.085537 147.41316 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Expm1.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<span class="k">class</span> <span class="nc">Histogram</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the histogram of Tensor element distribution.</span>

<span class="sd">    The elements are sorted into equal width bins between `min` and `max`.</span>
<span class="sd">    If `min` and `max` are both zero, the minimum and maximum values of the data are used.</span>

<span class="sd">    Elements lower than min and higher than max are ignored.</span>

<span class="sd">    Args:</span>
<span class="sd">        bins (int, optional): Number of histogram bins, optional. Default: ``100`` . If specified, must be positive.</span>
<span class="sd">        min (float, optional): An optional float of the lower end of the range (inclusive). Default value is ``0.0`` .</span>
<span class="sd">        max (float, optional): An optional float of the upper end of the range (inclusive). Default value is ``0.0`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - the input tensor, type support list: [float16, float32, int32].</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, 1-D Tensor with type int32.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `x` datetype not in support list.</span>
<span class="sd">        TypeError: If attr `min` or `max` is not float.</span>
<span class="sd">        TypeError: If attr `bins` is not int.</span>
<span class="sd">        ValueError: If attr value `min` &gt; `max`.</span>
<span class="sd">        ValueError: If attr `bins` &lt;= 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([1., 2, 1])</span>
<span class="sd">        &gt;&gt;&gt; op = ops.Histogram(bins=4, min=0.0, max=3.0)</span>
<span class="sd">        &gt;&gt;&gt; y = op(x)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [0 2 1 0]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>  <span class="c1"># pylint: disable=W0622</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Histogram.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;bins&quot;</span><span class="p">,</span> <span class="n">bins</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;min&quot;</span><span class="p">,</span> <span class="nb">min</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;max&quot;</span><span class="p">,</span> <span class="nb">max</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">bins</span><span class="p">,</span> <span class="s1">&#39;bins&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;min&#39;</span><span class="p">,</span> <span class="nb">min</span><span class="p">,</span> <span class="s1">&#39;max&#39;</span><span class="p">,</span> <span class="nb">max</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<div class="viewcode-block" id="HistogramFixedWidth"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.HistogramFixedWidth.html#mindspore.ops.HistogramFixedWidth">[docs]</a><span class="k">class</span> <span class="nc">HistogramFixedWidth</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a rank 1 histogram counting the number of entries in values that fall into every bin. The bins are equal</span>
<span class="sd">    width and determined by the inputs `range` and the arguments `nbins`.</span>

<span class="sd">    Args:</span>
<span class="sd">        nbins (int): The number of histogram bins, the type is a positive integer.</span>
<span class="sd">        dtype (str, optional): An optional attribute. The dtype must be str. Default: ``&#39;int32&#39;`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Numeric Tensor. Must be one of the following types: int32, float32, float16.</span>
<span class="sd">        - **range** (Tensor) - Must have the same data type as `x`, and the shape is :math:`(2,)`.</span>
<span class="sd">          x &lt;= range[0] will be mapped to histogram[0], x &gt;= range[1] will be mapped to histogram[-1].</span>

<span class="sd">    Outputs:</span>
<span class="sd">        1-D Tensor, whose length is the type is `nbins` with dtype of int32.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `dtype` is not a str or `nbins` is not an int.</span>
<span class="sd">        ValueError: If `nbins` is less than 1.</span>
<span class="sd">        ValueError: If `dtype` is not &#39;int32&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([-1.0, 0.0, 1.5, 2.0, 5.0, 15], mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; range_op = Tensor([0.0, 5.0], mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; hist = ops.HistogramFixedWidth(5)</span>
<span class="sd">        &gt;&gt;&gt; output = hist(x, range_op)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2 1 1 0 2]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nbins</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;int32&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize HistogramFixedWidth.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nbins</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;nbins&quot;</span><span class="p">,</span> <span class="n">nbins</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">nbins</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s2">&quot;nbins&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">valid_values</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;int32&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">valid_values</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;range&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;dtype&#39;</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span></div>


<div class="viewcode-block" id="Log"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Log.html#mindspore.ops.Log">[docs]</a><span class="k">class</span> <span class="nc">Log</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the natural logarithm of a tensor element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.log` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Input Tensor of any dimension. The value must be greater than 0.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; log = ops.Log()</span>
<span class="sd">        &gt;&gt;&gt; output = log(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.        0.6931472 1.3862944]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Log.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;cust_aicpu&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;base&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;shift&#39;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span></div>


<div class="viewcode-block" id="Log1p"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Log1p.html#mindspore.ops.Log1p">[docs]</a><span class="k">class</span> <span class="nc">Log1p</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the natural logarithm of one plus the input tensor element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.log1p` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor. The value must be greater than -1.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as the `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; log1p = ops.Log1p()</span>
<span class="sd">        &gt;&gt;&gt; output = log1p(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.6931472 1.0986123 1.609438 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Log1p.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Hypot"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Hypot.html#mindspore.ops.Hypot">[docs]</a><span class="k">class</span> <span class="nc">Hypot</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes hypotenuse of input tensors element-wise as legs of a right triangle.</span>
<span class="sd">    The shape of two inputs should be broadcastable, and data type of them should be</span>
<span class="sd">    one of: float32, float64.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x1** (Tensor) - The first input tensor.</span>
<span class="sd">        - **x2** (Tensor) - The second input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is one</span>
<span class="sd">        with higher precision in the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If data type `x1` or `x2` is not float32 or float64.</span>
<span class="sd">        ValueError: If shape of two inputs are not broadcastable.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([3., 5., 7.]))</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([4., 12., 24.]))</span>
<span class="sd">        &gt;&gt;&gt; hypot_ = ops.Hypot()</span>
<span class="sd">        &gt;&gt;&gt; y = hypot_(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [ 5. 13. 25.]</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(2.1, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(2.1, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; hypot_ = ops.Hypot()</span>
<span class="sd">        &gt;&gt;&gt; y = hypot_(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        2.9698484</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="s1">&#39;x2&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Heaviside"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Heaviside.html#mindspore.ops.Heaviside">[docs]</a><span class="k">class</span> <span class="nc">Heaviside</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies the Heaviside step function for input `x` element-wise.</span>

<span class="sd">    .. math::</span>
<span class="sd">            \text { heaviside }(\text { x, values })=\left\{\begin{array}{ll}</span>
<span class="sd">            0, &amp; \text { if x }&lt;0 \\</span>
<span class="sd">            \text { values, } &amp; \text { if x }==0 \\</span>
<span class="sd">            1, &amp; \text { if x }&gt;0</span>
<span class="sd">            \end{array}\right.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor. With real number data type.</span>
<span class="sd">        - **values** (Tensor) - The values to use where `x` is zero.</span>
<span class="sd">          It should be able to broadcast with `x` have the same dtype as `x`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same type as `x` and `values`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` or `values` is not Tensor.</span>
<span class="sd">        TypeError: If data type `x` and `values` is different.</span>
<span class="sd">        ValueError: If shape of two inputs are not broadcastable.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-1.5, 0., 2.]))</span>
<span class="sd">        &gt;&gt;&gt; values = Tensor(np.array([0.5]))</span>
<span class="sd">        &gt;&gt;&gt; heaviside = ops.Heaviside()</span>
<span class="sd">        &gt;&gt;&gt; y = heaviside(x, values)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [0.  0.5 1. ]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;values&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Erf"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Erf.html#mindspore.ops.Erf">[docs]</a><span class="k">class</span> <span class="nc">Erf</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Gauss error function of `x` element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.erf` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Input Tensor of Gaussian error function. Its data type</span>
<span class="sd">          must be float16 float32 or float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-1, 0, 1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; erf = ops.Erf()</span>
<span class="sd">        &gt;&gt;&gt; output = erf(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-0.8427168   0.          0.8427168   0.99530876  0.99997765]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Erf&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Erfc"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Erfc.html#mindspore.ops.Erfc">[docs]</a><span class="k">class</span> <span class="nc">Erfc</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the complementary error function of `x` element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.erfc` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor with a dtype of float16, float32 or float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``  ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-1, 0, 1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; erfc = ops.Erfc()</span>
<span class="sd">        &gt;&gt;&gt; output = erfc(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.8427168e+00 1.0000000e+00 1.5728319e-01 4.6912432e-03 2.2351742e-05]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Erfc&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Minimum"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Minimum.html#mindspore.ops.Minimum">[docs]</a><span class="k">class</span> <span class="nc">Minimum</span><span class="p">(</span><span class="n">_MathBinaryOp</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the minimum of input tensors element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.minimum` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor, Number, bool]) - The first input is a number or</span>
<span class="sd">          a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        - **y** (Union[Tensor, Number, bool]) - The second input is a number or</span>
<span class="sd">          a bool when the first input is a tensor or a tensor whose data type is number or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; # case 1 : same data type</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 5.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4.0, 2.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; minimum = ops.Minimum()</span>
<span class="sd">        &gt;&gt;&gt; output = minimum(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 2. 3.]</span>
<span class="sd">        &gt;&gt;&gt; # case 2 : different data type</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 5.0, 3.0]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4.0, 2.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = minimum(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Float32</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="Maximum"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Maximum.html#mindspore.ops.Maximum">[docs]</a><span class="k">class</span> <span class="nc">Maximum</span><span class="p">(</span><span class="n">_MathBinaryOp</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the maximum of input tensors element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.maximum` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor, Number, bool]) - The first input is a number or</span>
<span class="sd">          a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        - **y** (Union[Tensor, Number, bool]) - The second input is a number or</span>
<span class="sd">          a bool when the first input is a tensor or a tensor whose data type is number or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; # case 1 : same data type</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 5.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4.0, 2.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; maximum = ops.Maximum()</span>
<span class="sd">        &gt;&gt;&gt; output = maximum(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [4. 5. 6.]</span>
<span class="sd">        &gt;&gt;&gt; # case 2 : different data type</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 5.0, 3.0]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4.0, 2.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = maximum(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Float32</span>
<span class="sd">    &quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="RealDiv"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.RealDiv.html#mindspore.ops.RealDiv">[docs]</a><span class="k">class</span> <span class="nc">RealDiv</span><span class="p">(</span><span class="n">_MathBinaryOp</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Divides the first input tensor by the second input tensor in floating-point type element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.div` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor, Number, bool]) - The first input is a number or</span>
<span class="sd">          a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        - **y** (Union[Tensor, Number, bool]) - The second input is a number or</span>
<span class="sd">          a bool when the first input is a tensor or a tensor whose data type is number or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4.0, 5.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; realdiv = ops.RealDiv()</span>
<span class="sd">        &gt;&gt;&gt; output = realdiv(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.25 0.4  0.5 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="n">y</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="Div"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Div.html#mindspore.ops.Div">[docs]</a><span class="k">class</span> <span class="nc">Div</span><span class="p">(</span><span class="n">_MathBinaryOp</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the quotient of dividing the first input tensor by the second input tensor element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.div` for more details.</span>

<span class="sd">    Note:</span>
<span class="sd">        - One of the two inputs must be a Tensor, when the two inputs have different shapes,</span>
<span class="sd">          they must be able to broadcast to a common shape.</span>
<span class="sd">        - The two inputs can not be bool type at the same time,</span>
<span class="sd">          [True, Tensor(True, bool\_), Tensor(np.array([True]), bool\_)] are all considered bool type.</span>
<span class="sd">        - The two inputs comply with the implicit type conversion rules to make the data types</span>
<span class="sd">          consistent.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor, number.Number, bool]) - The first input is a number.Number or</span>
<span class="sd">          a bool or a tensor whose data type is</span>
<span class="sd">          `number &lt;https://www.mindspore.cn/docs/en/r2.1/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">          `bool_ &lt;https://www.mindspore.cn/docs/en/r2.1/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>
<span class="sd">        - **y** (Union[Tensor, number.Number, bool]) - The second input, when the first input is a Tensor,</span>
<span class="sd">          the second input should be a number.Number or bool value, or a Tensor whose data type is number or bool.</span>
<span class="sd">          When the first input is Scalar, the second input must be a Tensor whose data type is number or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one of the input `x` , `y` after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; # case 1 :has same data type and shape of the two inputs</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-4.0, 5.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([3.0, 2.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; div = ops.Div()</span>
<span class="sd">        &gt;&gt;&gt; output = div(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-1.3333334  2.5        2.        ]</span>
<span class="sd">        &gt;&gt;&gt; # case 2 : different data type and shape of the two inputs</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-4.0, 5.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(2, mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = div(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-2.  2.5  3.]</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Float32</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_infer_specified_div_value</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculate min/max value for output of Div op&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
                <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="n">y</span>
            <span class="n">out</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
            <span class="k">return</span> <span class="n">out</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_infer_min_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculate min value for output for Div op&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_infer_specified_div_value</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_infer_max_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculate max value for output for Div op&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_infer_specified_div_value</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_infer_shape_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">shape_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_infer_specified_div_value</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">shape_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_back_shape</span><span class="p">(</span><span class="n">shape_value</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_back_shape</span><span class="p">(</span><span class="n">shape_value</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="DivNoNan"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.DivNoNan.html#mindspore.ops.DivNoNan">[docs]</a><span class="k">class</span> <span class="nc">DivNoNan</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Operates a safe division between `x1` and `x2` element-wise. Returns 0 if element of `x2` is zero.</span>

<span class="sd">    Inputs of `x1` and `x2` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be bool at the same time, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    .. math::</span>
<span class="sd">        output_{i} = \begin{cases}</span>
<span class="sd">        0, &amp; \text{ if } x2_{i} = 0\\</span>
<span class="sd">        x1_{i} / x2_{i}, &amp; \text{ if } x2_{i} \ne 0</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x1** (Union[Tensor, number.Number, bool]) - The first input is a number.Number or</span>
<span class="sd">          a bool or a tensor whose data type is</span>
<span class="sd">          `number &lt;https://www.mindspore.cn/docs/en/r2.1/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">          `bool_ &lt;https://www.mindspore.cn/docs/en/r2.1/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>
<span class="sd">        - **x2** (Union[Tensor, number.Number, bool]) - The second input is a number.Number or</span>
<span class="sd">          a bool when the first input is a bool or a tensor whose data type is number or bool\_.</span>
<span class="sd">          When the first input is Scalar, the second input must be a Tensor whose data type is number or bool\_.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x1` and `x2` is not a number.Number or a bool or a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([-1.0, 0., 1.0, 5.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([0., 0., 0., 2.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; div_no_nan = ops.DivNoNan()</span>
<span class="sd">        &gt;&gt;&gt; output = div_no_nan(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.  0.  0.  2.5 2. ]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize DivNoNan&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="s1">&#39;x2&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="MulNoNan"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.MulNoNan.html#mindspore.ops.MulNoNan">[docs]</a><span class="k">class</span> <span class="nc">MulNoNan</span><span class="p">(</span><span class="n">_MathBinaryOp</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes `x` * `y` element-wise. If `y` is zero, no matter what `x` is, it will return 0.</span>

<span class="sd">    Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors, the shapes of them could be broadcasted.</span>
<span class="sd">    When the inputs are one tensor and one scalar, the scalar could only be a constant.</span>

<span class="sd">    .. math::</span>
<span class="sd">        output_{ij} = \begin{cases}</span>
<span class="sd">        0, &amp; y_{ij} = 0;\\</span>
<span class="sd">        x_{ij} * y_{ij}, &amp; otherwise.</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Note:</span>
<span class="sd">        The shapes of `x` and `y` should be the same or can be broadcasted.</span>
<span class="sd">        This is noncommutative: if `y` is NaN or infinite and `x` is 0, the result will be NaN.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor]) - The first input is a tensor whose data type is one of</span>
<span class="sd">          int32, int64, float16, float32, float64, complex64, complex128 currently or scalar.</span>
<span class="sd">        - **y** (Union[Tensor]) - The second input is a tensor whose data type is one of</span>
<span class="sd">          int32, int64, float16, float32, float64, complex64, complex128 currently or scalar.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the shape after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `x` nor `y` is a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; # case 1 : same data type and shape of two inputs, there are some 0 in y.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[-1.0, 6.0, np.inf], [np.nan, -7.0, 4.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([[-1.0, 4.0, 0], [0, -3.0, 1.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; mul_no_nan = ops.MulNoNan()</span>
<span class="sd">        &gt;&gt;&gt; output = mul_no_nan(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 1. 24. 0.]</span>
<span class="sd">        [ 0. 21. 4.]]</span>
<span class="sd">        &gt;&gt;&gt; # case 2 : the shape of two inputs is same, there are some 0 in x, y.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[-1.0, 6.0, 0], [0, np.nan, 4.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([[-1.0, 4.0, np.inf], [np.nan, 0, 1.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = mul_no_nan(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 1. 24. nan]</span>
<span class="sd">         [nan  0. 4.]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Float32</span>
<span class="sd">        &gt;&gt;&gt; # case 3 : the y is a scalar.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[-1.0, 6.0, 0], [0, np.nan, 4.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(0, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = mul_no_nan(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 0. 0.]</span>
<span class="sd">         [0. 0. 0.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize _BinaryOp&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="FloorDiv"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.FloorDiv.html#mindspore.ops.FloorDiv">[docs]</a><span class="k">class</span> <span class="nc">FloorDiv</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Divides the first input tensor by the second input tensor element-wise and round down to the closest integer.</span>

<span class="sd">    Refer to :func:`mindspore.ops.floor_div` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor, Number, bool]) - The first input is a Number or</span>
<span class="sd">          a bool or a tensor whose data type is Number or bool.</span>
<span class="sd">        - **y** (Union[Tensor, Number, bool]) - The second input is a Number or</span>
<span class="sd">          a bool when the first input is a tensor or a tensor whose data type is Number or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([2, 4, -1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([3, 3, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; floor_div = ops.FloorDiv()</span>
<span class="sd">        &gt;&gt;&gt; output = floor_div(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0  1 -1]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(2.0, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(2.0, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; floor_div = ops.FloorDiv()</span>
<span class="sd">        &gt;&gt;&gt; output = floor_div(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        1.0</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize FloorDiv.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="TruncateDiv"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.TruncateDiv.html#mindspore.ops.TruncateDiv">[docs]</a><span class="k">class</span> <span class="nc">TruncateDiv</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Divides the first input tensor by the second input tensor element-wise and rounds the results</span>
<span class="sd">    of division towards zero. Equivalent to C-style integer division.</span>

<span class="sd">    Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be bool at the same time, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    Note:</span>
<span class="sd">        Broadcasting is supported.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor, Number, bool]) - The first input is a number, or a bool,</span>
<span class="sd">          or a tensor whose data type is number or bool.</span>
<span class="sd">        - **y** (Union[Tensor, Number, bool]) - The second input is a number, or a bool when the first input</span>
<span class="sd">          is a tensor, or a tensor whose data type is number or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` and `y` is not one of the following: Tensor, Number, bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([2, 4, -1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([3, 3, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; truncate_div = ops.TruncateDiv()</span>
<span class="sd">        &gt;&gt;&gt; output = truncate_div(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0 1 0]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize TruncateDiv.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="TruncateMod"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.TruncateMod.html#mindspore.ops.TruncateMod">[docs]</a><span class="k">class</span> <span class="nc">TruncateMod</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the remainder of division element-wise.</span>

<span class="sd">    Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be bool at the same time, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - The input data does not support 0.</span>
<span class="sd">        - When the elements of input exceed 2048, the accuracy of operator cannot guarantee the requirement of</span>
<span class="sd">          double thousandths in the mini form.</span>
<span class="sd">        - Due to different architectures, the calculation results of this operator on NPU and CPU may be inconsistent.</span>
<span class="sd">        - If shape is expressed as (D1,D2... ,Dn), then D1\*D2... \*DN&lt;=1000000,n&lt;=8.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor, numbers.Number, bool]) - The first input is a number, or a bool,</span>
<span class="sd">          or a tensor whose data type is number or bool.</span>
<span class="sd">        - **y** (Union[Tensor, numbers.Number, bool]) - The second input is a number, or a bool when the first input</span>
<span class="sd">          is a tensor, or a tensor whose data type is number or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `x` nor `y` is one of the following: Tensor, number, bool.</span>
<span class="sd">        TypeError: If neither `x` nor `y` is a Tensor.</span>
<span class="sd">        ValueError: If the shape `x` and `y` cannot be broadcasted to each other.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([2, 4, -1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([3, 3, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; truncate_mod = ops.TruncateMod()</span>
<span class="sd">        &gt;&gt;&gt; output = truncate_mod(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 2  1 -1]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize TruncateMod.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Mod"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Mod.html#mindspore.ops.Mod">[docs]</a><span class="k">class</span> <span class="nc">Mod</span><span class="p">(</span><span class="n">_MathBinaryOp</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the remainder of dividing the first input tensor by the second input tensor element-wise.</span>

<span class="sd">    Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar. When the inputs are two tensors,</span>
<span class="sd">    both dtypes cannot be bool, and the shapes of them could be broadcast. When the inputs are one tensor</span>
<span class="sd">    and one scalar, the scalar could only be a constant.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = x_{i} \text{ % } y_{i}</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - The input data does not support 0.</span>
<span class="sd">        - When the elements of input exceed 2048, the accuracy of operator cannot guarantee the requirement of</span>
<span class="sd">          double thousandths in the mini form.</span>
<span class="sd">        - Due to different architectures, the calculation results of this operator on NPU and CPU may be inconsistent.</span>
<span class="sd">        - If shape is expressed as :math:`(D1, D2, ..., Dn)`, then :math:`D1*D2... *DN&lt;=1000000,n&lt;=8`.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor, numbers.Number, bool]) - The first input is a number, a bool</span>
<span class="sd">          or a tensor whose data type is number.</span>
<span class="sd">        - **y** (Union[Tensor, numbers.Number, bool]) - When the first input is a tensor, The second input</span>
<span class="sd">          could be a number, a bool or a tensor whose data type is number. When the first input is a number or a bool</span>
<span class="sd">          the second input must be a tensor whose data type is number.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `x` nor `y` is one of the following: Tensor, number, bool.</span>
<span class="sd">        TypeError: If neither `x` nor `y` is a Tensor.</span>
<span class="sd">        ValueError: If the shape `x` and `y` cannot be broadcasted to each other.</span>


<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-4.0, 5.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([3.0, 2.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; mod = ops.Mod()</span>
<span class="sd">        &gt;&gt;&gt; output = mod(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-1.  1.  0.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">fmod</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
        <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="Floor"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Floor.html#mindspore.ops.Floor">[docs]</a><span class="k">class</span> <span class="nc">Floor</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Rounds a tensor down to the closest integer element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.floor` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor, its data type must be float16,</span>
<span class="sd">          float32 or float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.1, 2.5, -1.5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; floor = ops.Floor()</span>
<span class="sd">        &gt;&gt;&gt; output = floor(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1.  2. -2.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Floor.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="FloorMod"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.FloorMod.html#mindspore.ops.FloorMod">[docs]</a><span class="k">class</span> <span class="nc">FloorMod</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the remainder of division element-wise, and it&#39;s a flooring divide.</span>

<span class="sd">    Refer to :func:`mindspore.ops.floor_mod` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor, Number, bool]) - The first input is a number or</span>
<span class="sd">          a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        - **y** (Union[Tensor, Number, bool]) -  The second input is a number or</span>
<span class="sd">          a bool when the first input is a tensor, or it can be a tensor whose data type is number or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision of the two inputs.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([2, 4, -1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([3, 3, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; floor_mod = ops.FloorMod()</span>
<span class="sd">        &gt;&gt;&gt; output = floor_mod(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2 1 2]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize FloorMod.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Ceil"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Ceil.html#mindspore.ops.Ceil">[docs]</a><span class="k">class</span> <span class="nc">Ceil</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Rounds a tensor up to the closest integer element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.ceil` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor with a dtype of float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.1, 2.5, -1.5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; ceil_op = ops.Ceil()</span>
<span class="sd">        &gt;&gt;&gt; output = ceil_op(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 2.  3. -1.]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(2.1, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ceil_op(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        3.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Ceil.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Xdivy"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Xdivy.html#mindspore.ops.Xdivy">[docs]</a><span class="k">class</span> <span class="nc">Xdivy</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Divides the first input tensor by the second input tensor element-wise. Returns zero when `x` is zero.</span>

<span class="sd">    Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be bool at the same time, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor, Number, bool]) - The first input is a number, or a bool,</span>
<span class="sd">          or a tensor whose data type is float16, float32, float64, complex64, complex128 or bool.</span>
<span class="sd">        - **y** (Union[Tensor, Number, bool]) - The second input is a number,</span>
<span class="sd">          or a bool when the first input is a tensor, or a tensor whose data type is float16,</span>
<span class="sd">          float32, float64, complex64, complex128 or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` and `y` is not one of the following: Tensor, Number, bool.</span>
<span class="sd">        TypeError: If dtype of `x` and &#39;y&#39; is not in [float16, float32, float64, complex64, complex128, bool].</span>
<span class="sd">        ValueError: If `x` could not be broadcast to a tensor with shape of `y`.</span>
<span class="sd">        RuntimeError: If the data type of `x`, `y` conversion of Parameter is given</span>
<span class="sd">                      but data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([2, 4, -1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([2, 2, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; xdivy = ops.Xdivy()</span>
<span class="sd">        &gt;&gt;&gt; output = xdivy(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1.   2.  -0.5]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Let x/y using same sig_dtype to enable implicit conversion for compatibility</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">rw</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_READ</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">rw</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_READ</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Xdivy.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Infer shape for output of Xdivy</span>
<span class="sd">        :param x_shape: input shape of x</span>
<span class="sd">        :param y_shape: input shape of y</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">output_shape</span> <span class="o">=</span> <span class="n">get_broadcast_shape</span><span class="p">(</span><span class="n">x_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Infer type for output of Xdivy</span>
<span class="sd">        :param x_dtype: input type of x</span>
<span class="sd">        :param y_dtype: input type of y</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="n">y_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span>
                                                    <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">complex64</span><span class="p">,</span>
                                                     <span class="n">mstype</span><span class="o">.</span><span class="n">complex128</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Infer value for constant folding</span>
<span class="sd">        :param x:</span>
<span class="sd">        :param y:</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="n">y</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="Xlogy"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Xlogy.html#mindspore.ops.Xlogy">[docs]</a><span class="k">class</span> <span class="nc">Xlogy</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the first input tensor multiplied by the logarithm of second input tensor element-wise.</span>
<span class="sd">    Returns zero when `x` is zero.</span>

<span class="sd">    Refer to :func:`mindspore.ops.xlogy` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor, number.Number, bool]) - The first input is a number.Number or</span>
<span class="sd">          a bool or a tensor whose data type is</span>
<span class="sd">          `number &lt;https://www.mindspore.cn/docs/en/r2.1/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">          `bool_ &lt;https://www.mindspore.cn/docs/en/r2.1/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>
<span class="sd">        - **y** (Union[Tensor, number.Number, bool]) - The second input is a number.Number or</span>
<span class="sd">          a bool when the first input is a tensor or a tensor whose data type is number or bool\_.</span>
<span class="sd">          When the first input is Scalar, the second input must be a Tensor whose data type is number or bool\_.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-5, 0, 4]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([2, 2, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; xlogy = ops.Xlogy()</span>
<span class="sd">        &gt;&gt;&gt; output = xlogy(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-3.465736   0.        2.7725887]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Xlogy.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Acosh"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Acosh.html#mindspore.ops.Acosh">[docs]</a><span class="k">class</span> <span class="nc">Acosh</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes inverse hyperbolic cosine of the inputs element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.acosh` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input Tensor. Input value must be in range [1, inf].</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and type as `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, dtype</span>
<span class="sd">        &gt;&gt;&gt; acosh = ops.Acosh()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 1.5, 3.0, 100.0]), dtype.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = acosh(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.        0.9624237 1.7627472 5.298292 ]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(2.6)</span>
<span class="sd">        &gt;&gt;&gt; output = acosh(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        1.609438</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Acosh&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Cosh"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Cosh.html#mindspore.ops.Cosh">[docs]</a><span class="k">class</span> <span class="nc">Cosh</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes hyperbolic cosine of input element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.cosh` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input Tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; cosh = ops.Cosh()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.24, 0.83, 0.31, 0.09]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = cosh(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.0289385 1.364684 1.048436 1.0040528]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(2.1, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = cosh(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        4.144313</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Cosh&quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="Asinh"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Asinh.html#mindspore.ops.Asinh">[docs]</a><span class="k">class</span> <span class="nc">Asinh</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes inverse hyperbolic sine of the input element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.asinh` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input Tensor, its rank should be less than 8.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and type as `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; asinh = ops.Asinh()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-5.0, 1.5, 3.0, 100.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = asinh(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-2.3124382  1.1947632  1.8184465  5.298342 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Asinh&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Sinc"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Sinc.html#mindspore.ops.Sinc">[docs]</a><span class="k">class</span> <span class="nc">Sinc</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the normalized sinc of input.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.sinc` for more details.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops.operations.math_ops as ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, dtype</span>
<span class="sd">        &gt;&gt;&gt; sinc = ops.Sinc()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.62, 0.28, 0.43, 0.62]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = sinc(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.47735003 0.8759357  0.7224278  0.47735003]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Sinc&quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="Sinh"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Sinh.html#mindspore.ops.Sinh">[docs]</a><span class="k">class</span> <span class="nc">Sinh</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes hyperbolic sine of the input element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.sinh` for more details.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; sinh = ops.Sinh()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.62, 0.28, 0.43, 0.62]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = sinh(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.6604918  0.28367308 0.44337422 0.6604918 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Sinh&quot;&quot;&quot;</span></div>


<span class="k">class</span> <span class="nc">_LogicBinaryOp</span><span class="p">(</span><span class="n">_BinaryOp</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Define logic binary operators.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">do_infer_dtype</span><span class="p">(</span><span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">,</span> <span class="n">valid_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="n">prim_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Staticmethod of infer dtype for _LogicBinaryOp.&quot;&quot;&quot;</span>
        <span class="n">args_dtype</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">:</span> <span class="n">y_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args_dtype</span><span class="p">,</span> <span class="n">valid_type</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mstype</span><span class="o">.</span><span class="n">TensorType</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_LogicBinaryOp</span><span class="o">.</span><span class="n">do_infer_dtype</span><span class="p">(</span><span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">,</span> <span class="n">prim_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Quantile</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the q-th quantiles of all elements in the input tensor, doing a linear interpolation when the</span>
<span class="sd">    q-th quantile lies between two data points.</span>

<span class="sd">    Refer to :func:`mindspore.ops.quantile` and :func:`mindspore.ops.nanquantile` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        dim (int, optional): The dimension to reduce. By default, `axis` is ``None`` resulting in the</span>
<span class="sd">            input tensor being flattened before computation. Default: ``None`` .</span>
<span class="sd">        keep_dims (bool, optional): Whether the output tensor has dim retained or not. Default: ``False`` .</span>
<span class="sd">        ignore_nan (bool, optional): Whether to ignore NaN values in the input. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">          Supported dtypes: float32, float64.</span>
<span class="sd">        - **q** (Union[float, Tensor]) - A scalar or 1D tensor of quantile values in the range [0, 1].</span>
<span class="sd">          Supported dtypes: float32, float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same dtype as the `input`.</span>

<span class="sd">    Supported Platforms:</span>


<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; quantile = ops.Quantile()</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([0.0700, -0.5446,  0.9214]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; q = Tensor(np.array([0, 0.5, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = quantile(input, q)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-0.5446  0.07  0.9214]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ignore_nan</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Quantile&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;dim&quot;</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;dim&quot;</span><span class="p">,</span> <span class="mi">10000</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">keep_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">ignore_nan</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;ignore_nan&quot;</span><span class="p">,</span> <span class="n">ignore_nan</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;ignore_nan&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>


<div class="viewcode-block" id="Equal"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Equal.html#mindspore.ops.Equal">[docs]</a><span class="k">class</span> <span class="nc">Equal</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the equivalence between two tensors element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.equal` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor, Number]) - The first input is a Number or</span>
<span class="sd">          a tensor whose data type is Number.</span>
<span class="sd">        - **y** (Union[Tensor, Number]) - The second input is a Number</span>
<span class="sd">          when the first input is a tensor or a tensor whose data type is Number.</span>
<span class="sd">          The data type is the same as the first input.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, it has the same shape as the `x` and `y` after broadcasting, and the data type is bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; # case 1: The shape of two inputs are different</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; equal = ops.Equal()</span>
<span class="sd">        &gt;&gt;&gt; output = equal(x, 2.0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False True False]</span>
<span class="sd">        &gt;&gt;&gt; # case 2: The shape of two inputs are the same</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([1, 2, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; equal = ops.Equal()</span>
<span class="sd">        &gt;&gt;&gt; output = equal(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True  True False]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Equal&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="ApproximateEqual"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ApproximateEqual.html#mindspore.ops.ApproximateEqual">[docs]</a><span class="k">class</span> <span class="nc">ApproximateEqual</span><span class="p">(</span><span class="n">_LogicBinaryOp</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns ``True`` if abs(x-y) is smaller than tolerance element-wise, otherwise False.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \begin{cases}</span>
<span class="sd">        &amp; \text{ if } \left | x_{i} - y_{i} \right | &lt; \text{tolerance},\ \ True  \\</span>
<span class="sd">        &amp; \text{ if } \left | x_{i} - y_{i} \right | \ge \text{tolerance},\ \  False</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    where `tolerance` indicates Acceptable maximum tolerance.</span>

<span class="sd">    Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower precision data type will be converted to</span>
<span class="sd">    the relatively highest precision data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        tolerance (float): The maximum deviation that two elements can be considered equal. Default: ``1e-05`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - A tensor. Must be one of the following types: float32, float16.</span>
<span class="sd">          :math:`(N,*)` where :math:`*` means, any number of additional dimensions, its rank should be less than 8.</span>
<span class="sd">        - **y** (Tensor) - A tensor of the same type and shape as `x`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the shape of `x`, and the data type is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `tolerance` is not a float.</span>
<span class="sd">        TypeError: If the data type of `x`, `y` conversion of Parameter is given</span>
<span class="sd">                      but data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([2, 3, 6]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; approximate_equal = ops.ApproximateEqual(2.)</span>
<span class="sd">        &gt;&gt;&gt; output = approximate_equal(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True  True  False]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tolerance</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ApproximateEqual&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;tolerance&quot;</span><span class="p">,</span> <span class="n">tolerance</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="EqualCount"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.EqualCount.html#mindspore.ops.EqualCount">[docs]</a><span class="k">class</span> <span class="nc">EqualCount</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the number of the same elements of two tensors.</span>

<span class="sd">    The two input tensors must have the same data type and shape.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The first input tensor. If the data type and shape of `y` are determined, then `x`</span>
<span class="sd">          must be the same as `y`, and vice versa.</span>
<span class="sd">          :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **y** (Tensor) - The second input tensor. If the data type and shape of `x` are determined, then `y`</span>
<span class="sd">          must be the same as `x`, and vice versa.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the type same as input tensor and shape as :math:`(1,)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` or `y` is not a Tensor.</span>
<span class="sd">        ValueError: If shape of `x` is not equal to shape of `y`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([1, 2, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; equal_count = ops.EqualCount()</span>
<span class="sd">        &gt;&gt;&gt; output = equal_count(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize EqualCount&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="NotEqual"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.NotEqual.html#mindspore.ops.NotEqual">[docs]</a><span class="k">class</span> <span class="nc">NotEqual</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the non-equivalence of two tensors element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.ne` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor, Number, bool]) - The first input is a number or</span>
<span class="sd">          a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        - **y** (Union[Tensor, Number, bool]) - The second input is a number or</span>
<span class="sd">          a bool when the first input is a tensor or a tensor whose data type is number or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, it has the same shape as the `x` and `y` after broadcasting, and the data type is bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; not_equal = ops.NotEqual()</span>
<span class="sd">        &gt;&gt;&gt; output = not_equal(x, 2.0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True False  True]</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([1, 2, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; not_equal = ops.NotEqual()</span>
<span class="sd">        &gt;&gt;&gt; output = not_equal(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False False  True]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize NotEqual&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="s1">&#39;x2&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Greater"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Greater.html#mindspore.ops.Greater">[docs]</a><span class="k">class</span> <span class="nc">Greater</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compare the value of the input parameters :math:`x,y` element-wise, and the output result is a bool value.</span>

<span class="sd">    Refer to :func:`mindspore.ops.gt` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor, number.Number, bool]) - The first input is a number.Number or</span>
<span class="sd">          a bool or a tensor whose data type is</span>
<span class="sd">          `number &lt;https://www.mindspore.cn/docs/en/r2.1/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">          `bool_ &lt;https://www.mindspore.cn/docs/en/r2.1/api_python/mindspore.html#mindspore.dtype&gt;`_ .</span>
<span class="sd">        - **y** (Union[Tensor, number.Number, bool]) - The second input, when the first input is a Tensor,</span>
<span class="sd">          the second input should be a number.Number or bool value, or a Tensor whose data type is number or bool\_.</span>
<span class="sd">          When the first input is Scalar, the second input must be a Tensor whose data type is number or bool\_.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([1, 1, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; greater = ops.Greater()</span>
<span class="sd">        &gt;&gt;&gt; output = greater(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False True False]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Infer value for Greater.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">greater</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="GreaterEqual"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.GreaterEqual.html#mindspore.ops.GreaterEqual">[docs]</a><span class="k">class</span> <span class="nc">GreaterEqual</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Given two Tensors, compares them element-wise to check if each element in the first</span>
<span class="sd">    Tensor is greater than or equal to the corresponding element in the second Tensor.</span>

<span class="sd">    Refer to :func:`mindspore.ops.ge` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor, Number, bool]) - The first input is a number or</span>
<span class="sd">          a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        - **y** (Union[Tensor, Number, bool]) - The second input is a number or</span>
<span class="sd">          a bool when the first input is a tensor or a tensor whose data type is number or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([1, 1, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; greater_equal = ops.GreaterEqual()</span>
<span class="sd">        &gt;&gt;&gt; output = greater_equal(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [True True False]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">greater_equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="Lerp"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Lerp.html#mindspore.ops.Lerp">[docs]</a><span class="k">class</span> <span class="nc">Lerp</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Does a linear interpolation of two tensors start and end based on a float or tensor weight.</span>

<span class="sd">    Refer to :func:`mindspore.ops.lerp` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **start** (Tensor) - The tensor with the starting points. Data type must be float16, float32 or float64.</span>
<span class="sd">        - **end** (Tensor) - The tensor with the ending points. Data type must be the same as `start`.</span>
<span class="sd">        - **weight** (Union[float, Tensor]) - The weight for the interpolation formula. Must be a float</span>
<span class="sd">          or a scalar tensor with float16 or float32 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same type and shape as input `start`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; start = Tensor(np.array([1., 2., 3., 4.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; end = Tensor(np.array([10., 10., 10., 10.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; lerp = ops.Lerp()</span>
<span class="sd">        &gt;&gt;&gt; output = lerp(start, end, 0.5)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [5.5 6. 6.5 7. ]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;start&#39;</span><span class="p">,</span> <span class="s1">&#39;end&#39;</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Gcd"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Gcd.html#mindspore.ops.Gcd">[docs]</a><span class="k">class</span> <span class="nc">Gcd</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes greatest common divisor of input tensors element-wise.</span>
<span class="sd">    The shape of two inputs should be broadcastable, and data type of them should be</span>
<span class="sd">    one of: int32, int64.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x1** (Tensor) - The first input tensor.</span>
<span class="sd">        - **x2** (Tensor) - The second input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is one</span>
<span class="sd">        with higher precision in the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If data type `x1` or `x2` is not int32 or int64.</span>
<span class="sd">        ValueError: If shape of two inputs are not broadcastable.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([7, 8, 9]))</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([14, 6, 12]))</span>
<span class="sd">        &gt;&gt;&gt; gcd_ = ops.Gcd()</span>
<span class="sd">        &gt;&gt;&gt; y = gcd_(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [7 2 3]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="s1">&#39;x2&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Less"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Less.html#mindspore.ops.Less">[docs]</a><span class="k">class</span> <span class="nc">Less</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the boolean value of :math:`x &lt; y` element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.less` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor, Number, bool]) -  The first input is a number or</span>
<span class="sd">          a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        - **y** (Union[Tensor, Number, bool]) - The second input is a number or</span>
<span class="sd">          a bool when the first input is a tensor, or it can be a tensor whose data type is number or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,and the data type is bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([1, 1, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; less = ops.Less()</span>
<span class="sd">        &gt;&gt;&gt; output = less(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False False True]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">less</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="LessEqual"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.LessEqual.html#mindspore.ops.LessEqual">[docs]</a><span class="k">class</span> <span class="nc">LessEqual</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the boolean value of :math:`x &lt;= y` element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.le` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor, number.Number, bool]) - The first input is a number.Number or</span>
<span class="sd">          a bool or a tensor whose data type is</span>
<span class="sd">          `number &lt;https://www.mindspore.cn/docs/en/r2.1/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">          `bool_ &lt;https://www.mindspore.cn/docs/en/r2.1/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>
<span class="sd">        - **y** (Union[Tensor, number.Number, bool]) - The second input, when the first input is a Tensor,</span>
<span class="sd">          the second input should be a number.Number or bool value, or a Tensor whose data type is number or bool\_.</span>
<span class="sd">          When the first input is Scalar, the second input must be a Tensor whose data type is number or bool\_.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([1, 1, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; less_equal = ops.LessEqual()</span>
<span class="sd">        &gt;&gt;&gt; output = less_equal(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True False  True]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">less_equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="LogicalNot"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.LogicalNot.html#mindspore.ops.LogicalNot">[docs]</a><span class="k">class</span> <span class="nc">LogicalNot</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the &quot;logical NOT&quot; of a tensor element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.logical_not` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor, the dtype must be bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the `x`, and the dtype is bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([True, False, True]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; logical_not = ops.LogicalNot()</span>
<span class="sd">        &gt;&gt;&gt; output = logical_not(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False  True False]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize LogicalNot&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="LogicalAnd"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.LogicalAnd.html#mindspore.ops.LogicalAnd">[docs]</a><span class="k">class</span> <span class="nc">LogicalAnd</span><span class="p">(</span><span class="n">_LogicBinaryOp</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the &quot;logical AND&quot; of two tensors element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.logical_and` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor, bool]) - The first input is a bool or a tensor whose data type can be implicitly</span>
<span class="sd">          converted to bool.</span>
<span class="sd">        - **y** (Union[Tensor, bool]) - The second input is a bool when the first input is a tensor or</span>
<span class="sd">          a tensor whose data type can be implicitly converted to bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the `x` and `y` after broadcasting, and the data type is bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([True, False, True]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([True, True, False]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; logical_and = ops.LogicalAnd()</span>
<span class="sd">        &gt;&gt;&gt; output = logical_and(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True False False]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(1, mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(0, mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.LogicalAnd()(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        False</span>
<span class="sd">        &gt;&gt;&gt; x = True</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(0, mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.LogicalAnd()(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        False</span>
<span class="sd">        &gt;&gt;&gt; x = True</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([True, False]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.LogicalAnd()(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [True False]</span>
<span class="sd">    &quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="LogicalOr"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.LogicalOr.html#mindspore.ops.LogicalOr">[docs]</a><span class="k">class</span> <span class="nc">LogicalOr</span><span class="p">(</span><span class="n">_LogicBinaryOp</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the &quot;logical OR&quot; of two tensors element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.logical_or` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor, bool]) - The first input is a bool or a tensor whose data type can be implicitly</span>
<span class="sd">          converted to bool.</span>
<span class="sd">        - **y** (Union[Tensor, bool]) - The second input is a bool when the first input is a tensor or</span>
<span class="sd">          a tensor whose data type can be implicitly converted to bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the `x` and `y` after broadcasting, and the data type is bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([True, False, True]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([True, True, False]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; logical_or = ops.LogicalOr()</span>
<span class="sd">        &gt;&gt;&gt; output = logical_or(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True  True  True]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(1, mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(0, mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.LogicalOr()(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        True</span>
<span class="sd">        &gt;&gt;&gt; x = True</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(0, mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.LogicalOr()(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        True</span>
<span class="sd">        &gt;&gt;&gt; x = True</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([True, False]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.LogicalOr()(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [True True]</span>
<span class="sd">    &quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="LogicalXor"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.LogicalXor.html#mindspore.ops.LogicalXor">[docs]</a><span class="k">class</span> <span class="nc">LogicalXor</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the &quot;logical XOR&quot; of two tensors element-wise.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.logical_xor` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor, bool]) - The first input is a bool or a tensor whose data type can be implicitly</span>
<span class="sd">          converted to bool.</span>
<span class="sd">        - **y** (Union[Tensor, bool]) - The second input is a bool when the first input is a tensor or</span>
<span class="sd">          a tensor whose data type can be implicitly converted to bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the `x` and `y` after broadcasting, and the data type is bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([True, False, True]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([True, True, False]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; logical_xor = ops.LogicalXor()</span>
<span class="sd">        &gt;&gt;&gt; output = logical_xor(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ False True True]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(1, mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(0, mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.LogicalXor()(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        True</span>
<span class="sd">        &gt;&gt;&gt; x = True</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(0, mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.LogicalXor()(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        True</span>
<span class="sd">        &gt;&gt;&gt; x = True</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([True, False]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.LogicalXor()(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False True]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize LogicalXor&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="IsNan"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.IsNan.html#mindspore.ops.IsNan">[docs]</a><span class="k">class</span> <span class="nc">IsNan</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Determines which elements are NaN for each position.</span>

<span class="sd">    Refer to :func:`mindspore.ops.isnan` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape of input, and the dtype is bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; is_nan = ops.IsNan()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([np.log(-1), 1, np.log(0)]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = is_nan(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True False False]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(2.1, mindspore.float64)</span>
<span class="sd">        &gt;&gt;&gt; output = is_nan(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        False</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize IsNan&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="IsInf"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.IsInf.html#mindspore.ops.IsInf">[docs]</a><span class="k">class</span> <span class="nc">IsInf</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Determines which elements are inf or -inf for each position.</span>

<span class="sd">    Refer to :func:`mindspore.ops.isinf` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape of input, and the dtype is bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; is_inf = ops.IsInf()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([np.log(-1), 1, np.log(0)]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = is_inf(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False False True]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(2.1, mindspore.float64)</span>
<span class="sd">        &gt;&gt;&gt; output = is_inf(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        False</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize IsInf&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="IsFinite"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.IsFinite.html#mindspore.ops.IsFinite">[docs]</a><span class="k">class</span> <span class="nc">IsFinite</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Determines which elements are finite for each position.</span>

<span class="sd">    Refer to :func:`mindspore.ops.isfinite` for more details.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; is_finite = ops.IsFinite()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([np.log(-1), 1, np.log(0)]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = is_finite(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False  True False]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize IsFinite&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="FloatStatus"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.FloatStatus.html#mindspore.ops.FloatStatus">[docs]</a><span class="k">class</span> <span class="nc">FloatStatus</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Determines if the elements contain Not a Number(NaN), infinite or negative infinite. 0 for normal, 1 for overflow.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor. The data type must be float16, float32 or float64.</span>
<span class="sd">          :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the shape of :math:`(1,)`, and the dtype is `mindspore.dtype.float32`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x` is not in [float16, float32, float64].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; float_status = ops.FloatStatus()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([np.log(-1), 1, np.log(0)]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; result = float_status(x)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [1.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize FloatStatus&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="NPUAllocFloatStatus"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.NPUAllocFloatStatus.html#mindspore.ops.NPUAllocFloatStatus">[docs]</a><span class="k">class</span> <span class="nc">NPUAllocFloatStatus</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Allocates a flag to store the overflow status.</span>

<span class="sd">    The flag is a tensor whose shape is :math:`(8,)` and data type is `mindspore.dtype.float32`.</span>

<span class="sd">    Note:</span>
<span class="sd">        Please refer to the Examples of :class:`mindspore.ops.NPUGetFloatStatus`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the shape of :math:`(8,)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; alloc_status = ops.NPUAllocFloatStatus()</span>
<span class="sd">        &gt;&gt;&gt; output = alloc_status()</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0. 0. 0. 0. 0. 0. 0. 0.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize NPUAllocFloatStatus&quot;&quot;&quot;</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;The &#39;NPUAllocFloatStatus&#39; operator will be deprecated in the future, &quot;</span>
                       <span class="s2">&quot;please use &#39;nn.TrainOneStepWithLossScaleCell&#39; or &#39;amp.all_finite&#39;.&quot;</span><span class="p">)</span></div>


<div class="viewcode-block" id="NPUGetFloatStatus"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.NPUGetFloatStatus.html#mindspore.ops.NPUGetFloatStatus">[docs]</a><span class="k">class</span> <span class="nc">NPUGetFloatStatus</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    `mindspore.ops.NPUGetFloatStatus` updates the flag which is</span>
<span class="sd">    the output tensor of :class:`mindspore.ops.NPUAllocFloatStatus` with the latest overflow status.</span>


<span class="sd">    Note:</span>
<span class="sd">        The flag is a tensor whose shape is :math:`(8,)` and data type is `mindspore.dtype.float32`.</span>
<span class="sd">        If the sum of the flag equals to 0, there is no overflow happened. If the sum of the</span>
<span class="sd">        flag is bigger than 0, there is overflow happened.</span>
<span class="sd">        In addition, there are strict sequencing requirements for use, i.e., before</span>
<span class="sd">        using the NPUGetFloatStatus operator, need to ensure that the NPUClearFlotStatus</span>
<span class="sd">        and your compute has been executed. We use :class:`mindspore.ops.Depend` to ensure the execution order.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The output tensor of `NPUAllocFloatStatus`.</span>
<span class="sd">          The data type must be float16 or float32.</span>
<span class="sd">          :math:`(N,*)` where :math:`*` means, any number of additional dimensions, its rank should be less than 8.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `x`. All the elements in the tensor will be zero.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.common.tensor import Tensor</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super().__init__()</span>
<span class="sd">        ...         self.alloc_status = ops.NPUAllocFloatStatus()</span>
<span class="sd">        ...         self.get_status = ops.NPUGetFloatStatus()</span>
<span class="sd">        ...         self.clear_status = ops.NPUClearFloatStatus()</span>
<span class="sd">        ...         self.sub = ops.Sub()</span>
<span class="sd">        ...         self.neg = ops.Neg()</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def construct(self, x):</span>
<span class="sd">        ...         init = self.alloc_status()</span>
<span class="sd">        ...         clear_status = self.clear_status(init)</span>
<span class="sd">        ...         x = ops.depend(x, clear_status)</span>
<span class="sd">        ...         res = self.sub(x, self.neg(x))</span>
<span class="sd">        ...         init = ops.depend(init, res)</span>
<span class="sd">        ...         get_status = self.get_status(init)</span>
<span class="sd">        ...         res = ops.depend(res, get_status)</span>
<span class="sd">        ...         return res</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; value = 5</span>
<span class="sd">        &gt;&gt;&gt; data = np.full((2, 3), value, dtype=np.float16)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(data, dtype=mstype.float16)</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; res = net(x)</span>
<span class="sd">        &gt;&gt;&gt; print(res)</span>
<span class="sd">        [[10. 10. 10.]</span>
<span class="sd">         [10. 10. 10.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize NPUGetFloatStatus&quot;&quot;&quot;</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;The &#39;NPUGetFloatStatus&#39; operator will be deprecated in the future, &quot;</span>
                       <span class="s2">&quot;please use &#39;nn.TrainOneStepWithLossScaleCell&#39; or &#39;amp.all_finite&#39;.&quot;</span><span class="p">)</span></div>


<div class="viewcode-block" id="NPUClearFloatStatus"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.NPUClearFloatStatus.html#mindspore.ops.NPUClearFloatStatus">[docs]</a><span class="k">class</span> <span class="nc">NPUClearFloatStatus</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Clears the flag which stores the overflow status.</span>

<span class="sd">    Note:</span>
<span class="sd">        The flag is in the register on the `Ascend` device. It will be reset and can not be reused again after the</span>
<span class="sd">        `NPUClearFloatStatus` is called.</span>
<span class="sd">        In addition, there are strict sequencing requirements for use, i.e., before using the NPUGetFloatStatus</span>
<span class="sd">        operator, need to ensure that the NPUClearFlotStatus and your compute has been executed.</span>
<span class="sd">        We use :class:`mindspore.ops.Depend` on ensure the execution order.</span>

<span class="sd">        Please refer to the Examples of :class:`mindspore.ops.NPUGetFloatStatus`.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The output tensor of `NPUAllocFloatStatus`.</span>
<span class="sd">          The data type must be float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `x`. All the elements in the tensor will be zero.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.common.tensor import Tensor</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super().__init__()</span>
<span class="sd">        ...         self.alloc_status = ops.NPUAllocFloatStatus()</span>
<span class="sd">        ...         self.get_status = ops.NPUGetFloatStatus()</span>
<span class="sd">        ...         self.clear_status = ops.NPUClearFloatStatus()</span>
<span class="sd">        ...         self.sub = ops.Sub()</span>
<span class="sd">        ...         self.neg = ops.Neg()</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def construct(self, x):</span>
<span class="sd">        ...         init = self.alloc_status()</span>
<span class="sd">        ...         clear_status = self.clear_status(init)</span>
<span class="sd">        ...         x = ops.depend(x, clear_status)</span>
<span class="sd">        ...         res = self.sub(x, self.neg(x))</span>
<span class="sd">        ...         init = ops.depend(init, res)</span>
<span class="sd">        ...         get_status = self.get_status(init)</span>
<span class="sd">        ...         res = ops.depend(res, get_status)</span>
<span class="sd">        ...         return res</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; value = 5</span>
<span class="sd">        &gt;&gt;&gt; data = np.full((2, 3), value, dtype=np.float16)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(data, dtype=mstype.float16)</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; res = net(x)</span>
<span class="sd">        &gt;&gt;&gt; print(res)</span>
<span class="sd">        [[10. 10. 10.]</span>
<span class="sd">         [10. 10. 10.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize NPUClearFloatStatus&quot;&quot;&quot;</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;The &#39;NPUClearFloatStatus&#39; operator will be deprecated in the future,&quot;</span>
                       <span class="s2">&quot;please use &#39;nn.TrainOneStepWithLossScaleCell&#39; or &#39;amp.all_finite&#39;.&quot;</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">NPUGetFloatStatusV2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get the flag for storage overflow status. This flag is located in a register at a</span>
<span class="sd">    fixed address on the `Ascend` device, and overflow information is automatically</span>
<span class="sd">    written to this register.</span>
<span class="sd">    The flag is a one-dimensional Tensor with shape :math:`(8,)` and data type `mindspore.dtype.int32`.</span>
<span class="sd">    If the value of flag is zero, no overflow has occurred, otherwise, overflow.</span>
<span class="sd">    When performing overflow detection on the network, you should first call `NPUClearFloatStatusV2` to</span>
<span class="sd">    reset the register before the detection, and then call `NPUGetFloatStatusV2` to get the register</span>
<span class="sd">    status after the network execution is completed.</span>

<span class="sd">    Note:</span>
<span class="sd">        - In order to avoid mis-optimization by the compiler, additional input is added to</span>
<span class="sd">          this operator. The input is defined as a shape of: math:`(8,)` and data type of</span>
<span class="sd">          `mindspore.dtype.int32` Tensor, meaningless.</span>
<span class="sd">        - Since this op lacks contextual dependencies with parameters in the network,</span>
<span class="sd">          :class:`mindspore.ops.Depend` needs to be used to ensure order of execution.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        Tensor, an additional input created to avoid compiler optimization, is specified as shape :math:`(8,)`,</span>
<span class="sd">        data type is `mindspore.dtype.int32`, and has no actual meaning.</span>
<span class="sd">        Usually use the output of `NPUClearFloatStatusV2`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, shape and data type are the same as input. If all are zero, it means no overflow, otherwise, overflow.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not int32.</span>
<span class="sd">        ValueError: If shape of `x` is not equal to :math:`(8,)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops, nn, Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops.operations.math_ops import NPUGetFloatStatusV2, NPUClearFloatStatusV2</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super().__init__()</span>
<span class="sd">        ...         self.clear_status = NPUClearFloatStatusV2()</span>
<span class="sd">        ...         self.get_status = NPUGetFloatStatusV2()</span>
<span class="sd">        ...         self.sub = ops.Sub()</span>
<span class="sd">        ...         self.neg = ops.Neg()</span>
<span class="sd">        ...         self.equal = ops.Equal()</span>
<span class="sd">        ...         self.reduce_all = ops.ReduceAll(keep_dims=False)</span>
<span class="sd">        ...         self.base = Tensor([0], dtype=ms.int32)</span>
<span class="sd">        ...         self.logic_not = ops.LogicalNot()</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def construct(self, x):</span>
<span class="sd">        ...         init = Tensor([0]*8, dtype=ms.int32)</span>
<span class="sd">        ...         clear_status = self.clear_status(init)</span>
<span class="sd">        ...         x = ops.depend(x, clear_status)</span>
<span class="sd">        ...         res = self.sub(x, self.neg(x))</span>
<span class="sd">        ...         init = ops.depend(init, res)</span>
<span class="sd">        ...         get_status = self.get_status(init)</span>
<span class="sd">        ...         flag = self.equal(self.base, get_status)</span>
<span class="sd">        ...         overall_finite = self.reduce_all(flag)</span>
<span class="sd">        ...         overflow = self.logic_not(overall_finite)</span>
<span class="sd">        ...         return overflow</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; value = 65504</span>
<span class="sd">        &gt;&gt;&gt; data = np.full((2, 3), value, dtype=np.float16)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(data, dtype=ms.float16)</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; res = net(x)</span>
<span class="sd">        &gt;&gt;&gt; print(res)</span>
<span class="sd">        True</span>
<span class="sd">        &gt;&gt;&gt; value = 10</span>
<span class="sd">        &gt;&gt;&gt; data = np.full((2, 3), value, dtype=np.float16)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(data, dtype=ms.float16)</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; res = net(x)</span>
<span class="sd">        &gt;&gt;&gt; print(res)</span>
<span class="sd">        False</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize NPUGetFloatStatusV2&quot;&quot;&quot;</span>



<span class="k">class</span> <span class="nc">NPUClearFloatStatusV2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Clear the flag for storage overflow status. This flag is located in a register at a</span>
<span class="sd">    fixed address on the `Ascend` device, and overflow information is automatically</span>
<span class="sd">    written to this register.</span>
<span class="sd">    The flag is a one-dimensional Tensor with shape :math:`(8,)` and data type `mindspore.dtype.int32`.</span>
<span class="sd">    If the value of flag is zero, no overflow has occurred, otherwise, overflow.</span>
<span class="sd">    When performing overflow detection on the network, you should first call `NPUClearFloatStatusV2` to</span>
<span class="sd">    reset the register before the detection, and then call `NPUGetFloatStatusV2` to get the register</span>
<span class="sd">    status after the network execution is completed.</span>

<span class="sd">    Note:</span>
<span class="sd">        - In order to avoid mis-optimization by the compiler, additional input and output are added to</span>
<span class="sd">          this operator. The input and output are defined as a shape of: math:`(8,)` and data type of</span>
<span class="sd">          `mindspore.dtype.int32` Tensor, meaningless.</span>
<span class="sd">        - Since this op lacks contextual dependencies with parameters in the network,</span>
<span class="sd">          :class:`mindspore.ops.Depend` needs to be used to ensure order of execution.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        Tensor, an additional input created to avoid compiler optimization, is specified as shape :math:`(8,)`,</span>
<span class="sd">        data type is `mindspore.dtype.int32`, and has no actual meaning.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, shape and data type are the same as input, meaningless.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not int32.</span>
<span class="sd">        ValueError: If shape of `x` is not equal to :math:`(8,)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops, nn, Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops.operations.math_ops import NPUGetFloatStatusV2, NPUClearFloatStatusV2</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super().__init__()</span>
<span class="sd">        ...         self.clear_status = NPUClearFloatStatusV2()</span>
<span class="sd">        ...         self.get_status = NPUGetFloatStatusV2()</span>
<span class="sd">        ...         self.sub = ops.Sub()</span>
<span class="sd">        ...         self.neg = ops.Neg()</span>
<span class="sd">        ...         self.equal = ops.Equal()</span>
<span class="sd">        ...         self.reduce_all = ops.ReduceAll(keep_dims=False)</span>
<span class="sd">        ...         self.base = Tensor([0], dtype=ms.int32)</span>
<span class="sd">        ...         self.logic_not = ops.LogicalNot()</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def construct(self, x):</span>
<span class="sd">        ...         init = Tensor([0]*8, dtype=ms.int32)</span>
<span class="sd">        ...         clear_status = self.clear_status(init)</span>
<span class="sd">        ...         x = ops.depend(x, clear_status)</span>
<span class="sd">        ...         res = self.sub(x, self.neg(x))</span>
<span class="sd">        ...         init = ops.depend(init, res)</span>
<span class="sd">        ...         get_status = self.get_status(init)</span>
<span class="sd">        ...         flag = self.equal(self.base, get_status)</span>
<span class="sd">        ...         overall_finite = self.reduce_all(flag)</span>
<span class="sd">        ...         overflow = self.logic_not(overall_finite)</span>
<span class="sd">        ...         return overflow</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; value = 65504</span>
<span class="sd">        &gt;&gt;&gt; data = np.full((2, 3), value, dtype=np.float16)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(data, dtype=ms.float16)</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; res = net(x)</span>
<span class="sd">        &gt;&gt;&gt; print(res)</span>
<span class="sd">        True</span>
<span class="sd">        &gt;&gt;&gt; value = 10</span>
<span class="sd">        &gt;&gt;&gt; data = np.full((2, 3), value, dtype=np.float16)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(data, dtype=ms.float16)</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; res = net(x)</span>
<span class="sd">        &gt;&gt;&gt; print(res)</span>
<span class="sd">        False</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize NPUClearFloatStatusV2&quot;&quot;&quot;</span>


<div class="viewcode-block" id="Cos"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Cos.html#mindspore.ops.Cos">[docs]</a><span class="k">class</span> <span class="nc">Cos</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes cosine of input element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.cos` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input Tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; cos = ops.Cos()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.24, 0.83, 0.31, 0.09]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = cos(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.971338 0.6748758 0.95233357 0.9959527]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Cos&quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="ACos"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ACos.html#mindspore.ops.ACos">[docs]</a><span class="k">class</span> <span class="nc">ACos</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes arccosine of input tensors element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.acos` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input Tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; acos = ops.ACos()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.74, 0.04, 0.30, 0.56]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = acos(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.737726  1.5307857 1.2661036 0.9764105]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(0.5)</span>
<span class="sd">        &gt;&gt;&gt; output = acos(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        1.0471978</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ACos&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Sin"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Sin.html#mindspore.ops.Sin">[docs]</a><span class="k">class</span> <span class="nc">Sin</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes sine of the input element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.sin` for more details.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; sin = ops.Sin()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.62, 0.28, 0.43, 0.62]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = sin(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.5810352 0.27635565 0.41687083 0.5810352]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Sin.&quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="Asin"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Asin.html#mindspore.ops.Asin">[docs]</a><span class="k">class</span> <span class="nc">Asin</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes arcsine of input tensor element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.asin` for more details.</span>

<span class="sd">    Note:</span>
<span class="sd">        Complex64 and complex128 are not supported on Ascend currently.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input Tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; asin = ops.Asin()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.74, 0.04, 0.30, 0.56]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = asin(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.8330704  0.04001067 0.30469266 0.5943858 ]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(0.12)</span>
<span class="sd">        &gt;&gt;&gt; output = asin(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.12028988</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Asin&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="NMSWithMask"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.NMSWithMask.html#mindspore.ops.NMSWithMask">[docs]</a><span class="k">class</span> <span class="nc">NMSWithMask</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Non-maximum Suppression. When object detection problem is performed in the computer vision field,</span>
<span class="sd">    object detection algorithm generates</span>
<span class="sd">    a plurality of bounding boxes. Use the box with the highest score, calculate the overlap between other boxes and</span>
<span class="sd">    the current box, and delete the box based on a certain threshold(IOU). On Ascend platform, the input box score is</span>
<span class="sd">    ignored, which only selects boexs based on the IOU between boxes, which means if you want to remove boxes that has</span>
<span class="sd">    lower scores, you need to sort the input boxes by score in descending order in advance. The IOU is as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{IOU} = \frac{\text{Area of Overlap}}{\text{Area of Union}}</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Only supports up to 2864 input boxes at one time.</span>

<span class="sd">    Args:</span>
<span class="sd">        iou_threshold (float): Specifies the threshold of overlap boxes with respect to</span>
<span class="sd">            IOU. Default: ``0.5`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **bboxes** (Tensor) - The shape of tensor is :math:`(N, 5)`. Input bounding boxes.</span>
<span class="sd">          `N` is the number of input bounding boxes. Every bounding box</span>
<span class="sd">          contains 5 values, the first 4 values are the coordinates(x0, y0, x1, y1) of bounding box which</span>
<span class="sd">          represents the point of top-left and bottom-right, and the last value is the score of this bounding box.</span>
<span class="sd">          The data type must be float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        tuple[Tensor], tuple of three tensors, they are output_boxes, output_idx and selected_mask.</span>

<span class="sd">        - **output_boxes** (Tensor) - The shape of tensor is :math:`(N, 5)`. On GPU and CPU platform, it is a sorted</span>
<span class="sd">          list of bounding boxes by sorting the input `bboxes` in descending order of score. On Ascend platform,</span>
<span class="sd">          it is same as input `bboxes`.</span>
<span class="sd">        - **output_idx** (Tensor) - The shape of tensor is :math:`(N,)`. The indexes list of `output_boxes`.</span>
<span class="sd">        - **selected_mask** (Tensor) - The shape of tensor is :math:`(N,)`. A mask list of</span>
<span class="sd">          valid output bounding boxes. Apply this mask on `output_boxes` to get the list of bounding boxes after</span>
<span class="sd">          non-max suppression calculation, or apply this mask on `output_idx` to get the indexes list of bounding boxes</span>
<span class="sd">          after non-max suppression calculation.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the `iou_threshold` is not a float number.</span>
<span class="sd">        ValueError:  if the first dimension of input Tensor is less than or equal to 0.</span>
<span class="sd">        TypeError: if the dtype of the `bboxes` is not float16 or float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; bbox = np.array([[100.0, 100.0, 50.0, 68.0, 0.63], [150.0, 75.0, 165.0, 115.0, 0.55],</span>
<span class="sd">        ...                  [12.0, 190.0, 288.0, 200.0, 0.9], [28.0, 130.0, 106.0, 172.0, 0.3]])</span>
<span class="sd">        &gt;&gt;&gt; bbox[:, 2] += bbox[:, 0]</span>
<span class="sd">        &gt;&gt;&gt; bbox[:, 3] += bbox[:, 1]</span>
<span class="sd">        &gt;&gt;&gt; inputs = Tensor(bbox, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; nms = ops.NMSWithMask(0.1)</span>
<span class="sd">        &gt;&gt;&gt; output_boxes, indices, mask = nms(inputs)</span>
<span class="sd">        &gt;&gt;&gt; indices_np = indices.asnumpy()</span>
<span class="sd">        &gt;&gt;&gt; print(indices_np[mask.asnumpy()])</span>
<span class="sd">        [0 1 2]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">iou_threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize NMSWithMask&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;iou_threshold&quot;</span><span class="p">,</span> <span class="n">iou_threshold</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;bboxes&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;selected_boxes&#39;</span><span class="p">,</span> <span class="s1">&#39;selected_idx&#39;</span><span class="p">,</span> <span class="s1">&#39;selected_mask&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_ge</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;enable_ge&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bboxes_shape</span><span class="p">):</span>
        <span class="n">cls_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">bboxes_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;bboxes rank&quot;</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">bboxes_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">bboxes_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;bboxes.shape[0]&quot;</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="n">bboxes_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">5</span><span class="p">,</span> <span class="s2">&quot;bboxes.shape[1]&quot;</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="n">num</span> <span class="o">=</span> <span class="n">bboxes_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">bboxes_shape</span><span class="p">,</span> <span class="p">(</span><span class="n">num</span><span class="p">,),</span> <span class="p">(</span><span class="n">num</span><span class="p">,)</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bboxes_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;bboxes&quot;</span><span class="p">,</span> <span class="n">bboxes_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">bboxes_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span></div>


<div class="viewcode-block" id="Abs"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Abs.html#mindspore.ops.Abs">[docs]</a><span class="k">class</span> <span class="nc">Abs</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns absolute value of a tensor element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.abs` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as the `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-1.0, 1.0, 0.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; abs = ops.Abs()</span>
<span class="sd">        &gt;&gt;&gt; output = abs(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 1. 0.]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(3.6)</span>
<span class="sd">        &gt;&gt;&gt; output = abs(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        3.6</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Abs&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Sign"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Sign.html#mindspore.ops.Sign">[docs]</a><span class="k">class</span> <span class="nc">Sign</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs sign on the tensor element-wise.</span>

<span class="sd">    .. math::</span>
<span class="sd">        sign(x) = \begin{cases} -1, &amp;if\ x &lt; 0 \cr</span>
<span class="sd">        0, &amp;if\ x = 0 \cr</span>
<span class="sd">        1, &amp;if\ x &gt; 0\end{cases}</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor of any dimension.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">         &gt;&gt;&gt; import mindspore</span>
<span class="sd">         &gt;&gt;&gt; import numpy as np</span>
<span class="sd">         &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">         &gt;&gt;&gt; x = Tensor(np.array([[2.0, 0.0, -1.0]]), mindspore.float32)</span>
<span class="sd">         &gt;&gt;&gt; sign = ops.Sign()</span>
<span class="sd">         &gt;&gt;&gt; output = sign(x)</span>
<span class="sd">         &gt;&gt;&gt; print(output)</span>
<span class="sd">         [[ 1.  0. -1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span></div>


<div class="viewcode-block" id="Round"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Round.html#mindspore.ops.Round">[docs]</a><span class="k">class</span> <span class="nc">Round</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns half to even of a tensor element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.round` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and type as the `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.8, 1.5, 2.3, 2.5, -4.5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; round = ops.Round()</span>
<span class="sd">        &gt;&gt;&gt; output = round(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1.  2.  2.  2. -4.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Round&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Tan"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Tan.html#mindspore.ops.Tan">[docs]</a><span class="k">class</span> <span class="nc">Tan</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes tangent of `x` element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.tan` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Input tensor of any dimension.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; tan = ops.Tan()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-1.0, 0.0, 1.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = tan(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-1.5574081 0. 1.5574081]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Tan&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Atan"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Atan.html#mindspore.ops.Atan">[docs]</a><span class="k">class</span> <span class="nc">Atan</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the trigonometric inverse tangent of the input element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.atan` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor): The input Tensor. Supported dtypes:</span>

<span class="sd">          - Ascend: float16, float32.</span>
<span class="sd">          - GPU/CPU: float16, float32, float64, complex64, complex128.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        A Tensor, has the same type as the input.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 0.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; atan = ops.Atan()</span>
<span class="sd">        &gt;&gt;&gt; output = atan(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.7853982 0.       ]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Atan&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Atanh"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Atanh.html#mindspore.ops.Atanh">[docs]</a><span class="k">class</span> <span class="nc">Atanh</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes inverse hyperbolic tangent of the input element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.atanh` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor): The input Tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        A Tensor, has the same type as the input.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0, -0.5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; atanh = ops.Atanh()</span>
<span class="sd">        &gt;&gt;&gt; output = atanh(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0.         -0.54930615]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Atanh&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Atan2"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Atan2.html#mindspore.ops.Atan2">[docs]</a><span class="k">class</span> <span class="nc">Atan2</span><span class="p">(</span><span class="n">_MathBinaryOp</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns arctangent of x/y element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.atan2` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor, Number.number) - The input tensor or scalar.</span>
<span class="sd">        - **y** (Tensor, Number.number) - The input tensor or scalar. It has the same shape with `x` or</span>
<span class="sd">          its shape is able to broadcast with `x`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is same as `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([1, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; atan2 = ops.Atan2()</span>
<span class="sd">        &gt;&gt;&gt; output = atan2(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.        0.7853982]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Atan2&quot;&quot;&quot;</span>
        <span class="n">_MathBinaryOp</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></div>


<div class="viewcode-block" id="SquareSumAll"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.SquareSumAll.html#mindspore.ops.SquareSumAll">[docs]</a><span class="k">class</span> <span class="nc">SquareSumAll</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the square sum of a tensor element-wise.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \left\{\begin{matrix}out_{x} = {\textstyle \sum_{0}^{N}} (x_{i})^2</span>
<span class="sd">        \\out_{y} = {\textstyle \sum_{0}^{N}} (y_{i})^2</span>
<span class="sd">        \end{matrix}\right.</span>

<span class="sd">    Note:</span>
<span class="sd">        SquareSumAll only supports float16 and float32 data type.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor. The data type must be float16 or float32.</span>
<span class="sd">          :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **y** (Tensor) - The input tensor has the same type and shape as the `x`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **output_x** (Tensor) - The same type as the `x`.</span>
<span class="sd">        - **output_y** (Tensor) - The same type as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `x` nor `y` is a Tensor.</span>
<span class="sd">        ValueError: If `x` and `y` are not the same shape.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0, 0, 2, 0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([0, 0, 2, 4]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; square_sum_all = ops.SquareSumAll()</span>
<span class="sd">        &gt;&gt;&gt; output = square_sum_all(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[], dtype=Float32, value= 4),</span>
<span class="sd">         Tensor(shape=[], dtype=Float32, value= 20))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SquareSumAll&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output_x&#39;</span><span class="p">,</span> <span class="s1">&#39;output_y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="BitwiseAnd"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.BitwiseAnd.html#mindspore.ops.BitwiseAnd">[docs]</a><span class="k">class</span> <span class="nc">BitwiseAnd</span><span class="p">(</span><span class="n">_BitwiseBinaryOp</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns bitwise `and` of two tensors element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.bitwise_and` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The first input tensor with shape</span>
<span class="sd">          :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **y** (Tensor) - The second input tensor with same type as the `x`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same type as the `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0, 0, 1, -1, 1, 1, 1]), mindspore.int16)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([0, 1, 1, -1, -1, 2, 3]), mindspore.int16)</span>
<span class="sd">        &gt;&gt;&gt; bitwise_and = ops.BitwiseAnd()</span>
<span class="sd">        &gt;&gt;&gt; output = bitwise_and(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0  0  1 -1  1  0  1]</span>
<span class="sd">    &quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="BitwiseOr"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.BitwiseOr.html#mindspore.ops.BitwiseOr">[docs]</a><span class="k">class</span> <span class="nc">BitwiseOr</span><span class="p">(</span><span class="n">_BitwiseBinaryOp</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns bitwise `or` of two tensors element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.bitwise_or` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The first input tensor with shape</span>
<span class="sd">          :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **y** (Tensor) - The second input tensor with same type as the `x`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same type as the `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0, 0, 1, -1, 1, 1, 1]), mindspore.int16)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([0, 1, 1, -1, -1, 2, 3]), mindspore.int16)</span>
<span class="sd">        &gt;&gt;&gt; bitwise_or = ops.BitwiseOr()</span>
<span class="sd">        &gt;&gt;&gt; output = bitwise_or(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0  1  1 -1 -1  3  3]</span>
<span class="sd">    &quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="BitwiseXor"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.BitwiseXor.html#mindspore.ops.BitwiseXor">[docs]</a><span class="k">class</span> <span class="nc">BitwiseXor</span><span class="p">(</span><span class="n">_BitwiseBinaryOp</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns bitwise `xor` of two tensors element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.bitwise_xor` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The first input tensor with shape</span>
<span class="sd">          :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **y** (Tensor) - The second input tensor with same type as the `x`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same type as the `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0, 0, 1, -1, 1, 1, 1]), mindspore.int16)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([0, 1, 1, -1, -1, 2, 3]), mindspore.int16)</span>
<span class="sd">        &gt;&gt;&gt; bitwise_xor = ops.BitwiseXor()</span>
<span class="sd">        &gt;&gt;&gt; output = bitwise_xor(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0  1  0  0 -2  3  2]</span>
<span class="sd">    &quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="BesselI0"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.BesselI0.html#mindspore.ops.BesselI0">[docs]</a><span class="k">class</span> <span class="nc">BesselI0</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes BesselI0 of input element-wise.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.bessel_i0` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor.</span>
<span class="sd">          Data type must be float16, float32 or float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; bessel_i0 = ops.BesselI0()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.24, 0.83, 0.31, 0.09]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = bessel_i0(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.0144521 1.1797839 1.0241698 1.0020262]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">)</span></div>


<div class="viewcode-block" id="BesselI1"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.BesselI1.html#mindspore.ops.BesselI1">[docs]</a><span class="k">class</span> <span class="nc">BesselI1</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes BesselI1 of input element-wise.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.bessel_i1` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor.</span>
<span class="sd">          Data type must be float16, float32 or float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; bessel_i1 = ops.BesselI1()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.24, 0.83, 0.31, 0.09]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = bessel_i1(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.1208661  0.45177728 0.1568694  0.04504559]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize BesselI1&quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="BesselI0e"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.BesselI0e.html#mindspore.ops.BesselI0e">[docs]</a><span class="k">class</span> <span class="nc">BesselI0e</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes BesselI0e of input element-wise.</span>

<span class="sd">    The formula is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        BesselI0e(x) = \exp(|x|) * bessel\_i0(x)</span>

<span class="sd">    where bessel_i0 is Bessel function of the first kind with 0 order.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor.</span>
<span class="sd">          Data type must be float16, float32 or float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; bessel_i0e = ops.BesselI0e()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.24, 0.83, 0.31, 0.09]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = bessel_i0e(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.7979961  0.5144438  0.75117415  0.9157829 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize BesselI0e&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="s1">&#39;output&#39;</span><span class="p">)</span></div>


<div class="viewcode-block" id="BesselI1e"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.BesselI1e.html#mindspore.ops.BesselI1e">[docs]</a><span class="k">class</span> <span class="nc">BesselI1e</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes BesselI1e of input element-wise.</span>

<span class="sd">    The formula is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        BesselI1e(x) = \exp(|x|) * bessel\_i1(x)</span>

<span class="sd">    where bessel_i1 is Bessel function of the first kind with 1 order.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor.</span>
<span class="sd">          Data type must be float16 or float32, float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; bessel_i1e = ops.BesselI1e()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.24, 0.83, 0.31, 0.09]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = bessel_i1e(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.09507662 0.19699717 0.11505538 0.04116856]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize BesselI1e&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="s1">&#39;output&#39;</span><span class="p">)</span></div>


<div class="viewcode-block" id="BesselK0"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.BesselK0.html#mindspore.ops.BesselK0">[docs]</a><span class="k">class</span> <span class="nc">BesselK0</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes BesselK0 of input element-wise.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor.</span>
<span class="sd">          Data type must be float16, float32, float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor of float16, float32, float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; bessel_k0 = ops.BesselK0()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.24, 0.83, 0.31, 0.09]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = bessel_k0(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.579826  0.5402144 1.3424659 2.5310173]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize BesselK0&quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="BesselK1"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.BesselK1.html#mindspore.ops.BesselK1">[docs]</a><span class="k">class</span> <span class="nc">BesselK1</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes BesselK1 of input element-wise.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor.</span>
<span class="sd">          Data type must be float16, float32, float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor of float16, float32, float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; bessel_k1 = ops.BesselK1()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.24, 0.83, 0.31, 0.09]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = bessel_k1(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [3.9190812  0.8143549  2.9440577 10.974864]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize BesselK1&quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="BesselK0e"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.BesselK0e.html#mindspore.ops.BesselK0e">[docs]</a><span class="k">class</span> <span class="nc">BesselK0e</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes BesselK0e of input element-wise.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor.</span>
<span class="sd">          Data type must be float16, float32, float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor of float16, float32, float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; bessel_k0e = ops.BesselK0e()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.24, 0.83, 0.31, 0.09]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = bessel_k0e(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2.0083523 1.2388839 1.8303517 2.769374 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize BesselK0e&quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="BesselK1e"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.BesselK1e.html#mindspore.ops.BesselK1e">[docs]</a><span class="k">class</span> <span class="nc">BesselK1e</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes BesselK1e of input element-wise.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor.</span>
<span class="sd">          Data type must be float16, float32, float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor of float16, float32, float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; bessel_k1e = ops.BesselK1e()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.24, 0.83, 0.31, 0.09]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = bessel_k1e(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 4.9821286  1.8675754  4.0140023 12.008413 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize BesselK1e&quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="BesselJ0"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.BesselJ0.html#mindspore.ops.BesselJ0">[docs]</a><span class="k">class</span> <span class="nc">BesselJ0</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes BesselJ0 of input element-wise.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor.</span>
<span class="sd">          Data type must be float16, float32 or float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor of float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; bessel_j0 = ops.BesselJ0()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.5, 1., 2., 4.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = bessel_j0(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.93846981  0.76519769  0.22389078  -0.39714981]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize BesselJ0&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="BesselJ1"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.BesselJ1.html#mindspore.ops.BesselJ1">[docs]</a><span class="k">class</span> <span class="nc">BesselJ1</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes BesselJ1 of input element-wise.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor.</span>
<span class="sd">          Data type must be float16, float32 or float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor of float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; bessel_j1 = ops.BesselJ1()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.5, 1., 2., 4.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = bessel_j1(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.24226846  0.44005059  0.57672481  -0.06604333]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize BesselJ1&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="BesselY0"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.BesselY0.html#mindspore.ops.BesselY0">[docs]</a><span class="k">class</span> <span class="nc">BesselY0</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes BesselY0 of input element-wise.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor.</span>
<span class="sd">          Data type must be float16, float32 or float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor of float16, float32, float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; bessel_y0 = ops.BesselY0()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.5, 1., 2., 4.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = bessel_y0(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-0.44451873  0.08825696  0.51037567  -0.01694074]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize BesselY0&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="BesselY1"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.BesselY1.html#mindspore.ops.BesselY1">[docs]</a><span class="k">class</span> <span class="nc">BesselY1</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes BesselY1 of input element-wise.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor.</span>
<span class="sd">          Data type must be float16, float32 or float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor of float16, float32, float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; bessel_y1 = ops.BesselY1()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.5, 1., 2., 4.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = bessel_y1(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-1.47147239  -0.78121282  -0.10703243  0.39792571]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize BesselY1&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Inv"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Inv.html#mindspore.ops.Inv">[docs]</a><span class="k">class</span> <span class="nc">Inv</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes Reciprocal of input tensor element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.inv` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Input tensor, it must be one of the following types: float16, float32 or int32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and data type as `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; inv = ops.Inv()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.25, 0.4, 0.31, 0.52]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = inv(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [4.        2.5       3.2258065 1.923077 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span></div>


<div class="viewcode-block" id="Invert"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Invert.html#mindspore.ops.Invert">[docs]</a><span class="k">class</span> <span class="nc">Invert</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Flips all bits of input tensor element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.invert` for more details.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; invert = ops.Invert()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([25, 4, 13, 9]), mindspore.int16)</span>
<span class="sd">        &gt;&gt;&gt; output = invert(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-26 -5 -14 -10]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Invert&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Eps"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Eps.html#mindspore.ops.Eps">[docs]</a><span class="k">class</span> <span class="nc">Eps</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create a Tensor with the same data type and shape as input, and the element value is the minimum value that the</span>
<span class="sd">    corresponding data type can be expressed.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Tensor of any dimension used to obtain the minimum value that its data type can be expressed.</span>
<span class="sd">          The data type must be float16, float32 or float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same type and shape as `x`, but filled with `x` dtype minimum val.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If data type of `x` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([4, 1, 2, 3], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.Eps()(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.5258789e-05 1.5258789e-05 1.5258789e-05 1.5258789e-05]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Eps&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">x_nptype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">dtype_to_nptype</span><span class="p">(</span><span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">element_type</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">x_nptype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
            <span class="n">min_val</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="mi">14</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">x_nptype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
            <span class="n">min_val</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="mi">16</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">min_val</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="mi">52</span><span class="p">)</span>

        <span class="n">res</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">],</span> <span class="n">min_val</span><span class="p">,</span> <span class="n">x_nptype</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">res</span><span class="p">),</span>
            <span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">],</span>
            <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="LinSpace"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.LinSpace.html#mindspore.ops.LinSpace">[docs]</a><span class="k">class</span> <span class="nc">LinSpace</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a Tensor whose value is `num` evenly spaced in the interval `start` and `stop` (including `start` and</span>
<span class="sd">    `stop`), and the length of the output Tensor is `num`.</span>

<span class="sd">    Refer to :func:`mindspore.ops.linspace` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **start** (Tensor) -  Start value of interval, 0-D Tensor with dtype float32 or float64.</span>
<span class="sd">        - **stop** (Tensor) - Last value of interval, 0-D Tensor with dtype  float32 or float64.</span>
<span class="sd">        - **num** (int) - Number of ticks in the interval, inclusive of `start` and `stop`.</span>
<span class="sd">          Supported dtypes: int32, int64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as `start`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; linspace = ops.LinSpace()</span>
<span class="sd">        &gt;&gt;&gt; start = Tensor(1, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; stop = Tensor(10, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; num = 5</span>
<span class="sd">        &gt;&gt;&gt; output = linspace(start, stop, num)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1.    3.25  5.5   7.75 10.  ]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize LinSpace&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;start&#39;</span><span class="p">,</span> <span class="s1">&#39;stop&#39;</span><span class="p">,</span> <span class="s1">&#39;num&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="MatrixInverse"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.MatrixInverse.html#mindspore.ops.MatrixInverse">[docs]</a><span class="k">class</span> <span class="nc">MatrixInverse</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the inverse of the input matrix. If the matrix is irreversible, an error may be reported or an unknown</span>
<span class="sd">    result may be returned.</span>

<span class="sd">    Note:</span>
<span class="sd">        The parameter &#39;adjoint&#39; is only supporting ``False``  right now, because complex number is not supported at</span>
<span class="sd">        present.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        adjoint (bool) : An optional bool. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - A matrix to be calculated. The matrix must be at least two dimensions, and the last two</span>
<span class="sd">          dimensions must be the same size.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same type and shape as input `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `adjoint` is not a bool.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        ValueError: If the last two dimensions of `x` is not same size.</span>
<span class="sd">        ValueError: If the dimension of `x` is less than 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[-0.710504  , -1.1207525],</span>
<span class="sd">        ...                       [-1.7651395 , -1.7576632]],</span>
<span class="sd">        ...                      [[ 0.52412605,  1.9070215],</span>
<span class="sd">        ...                       [ 1.3384849 ,  1.4274558]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; matrix_inverse = ops.MatrixInverse(adjoint=False)</span>
<span class="sd">        &gt;&gt;&gt; output = matrix_inverse(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[ 2.4095478  -1.5364188 ]</span>
<span class="sd">          [-2.419797    0.9740167 ]]</span>
<span class="sd">         [[-0.79111797  1.0569006 ]</span>
<span class="sd">          [ 0.74180895 -0.2904787 ]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">adjoint</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize MatrixInverse&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;adjoint&#39;</span><span class="p">,</span> <span class="n">adjoint</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">MatrixPower</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the n-th power of a batch of square matrices.</span>
<span class="sd">    When n equals 0, it returns a group of identity matrices. If n is negative,</span>
<span class="sd">    it computes the inverse of each matrix (if possible) raised to the power of abs(n).</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        n (int) : The exponent, a required int.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - A 3-D Tensor. The shape is :math:`(b, m, m)`, represents b m-D square matrices.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - A 3-D Tensor. Data type and shape are the same as `x`&#39;s.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the data type of `n` is not int.</span>
<span class="sd">        TypeError: If x is not a Tensor.</span>
<span class="sd">        ValueError: If `x` is not a 3-D tensor.</span>
<span class="sd">        ValueError: If shape[1] and shape[2] of `x` are not the same.</span>
<span class="sd">        ValueError: If n is negative but got input x has singular matrices.</span>
<span class="sd">        ValueError: If `n` &lt; 0 and input is int type.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[[0, 1], [-1, 0]], [[1, 0], [0, -1]]], dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; matrix_power = ops.MatrixPower(n=2)</span>
<span class="sd">        &gt;&gt;&gt; y = matrix_power(x)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[[-1.  0.]</span>
<span class="sd">          [-0. -1.]]</span>
<span class="sd">         [[ 1.  0.]</span>
<span class="sd">          [ 0.  1.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;MatrixPower&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;n&quot;</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">MatrixDeterminant</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the value of the determinant for one or more square matrices.</span>

<span class="sd">    Refer to :func:`mindspore.ops.det` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - A matrix to be calculated. The matrix must be at least two dimensions, and the last two</span>
<span class="sd">          dimensions must be the same size.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is `x_shape[:-2]`, the dtype is same as `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[-4.5, -1.5], [7.0, 6.0]], [[2.5, 0.5], [3.0, 9.0]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; op = ops.MatrixDeterminant()</span>
<span class="sd">        &gt;&gt;&gt; output = op(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-16.5 21. ]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize MatrixDeterminant.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="s2">&quot;MatrixDeterminant&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">LogMatrixDeterminant</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the sign and logarithm of the determinant of one or more square matrices.</span>

<span class="sd">    Refer to :func:`mindspore.ops.slogdet` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - A matrix to be calculated. The matrix must be at least two dimensions, and the last two</span>
<span class="sd">          dimensions must be the same size. Supported dtypes: float32, float64, complex64 and complex128.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **sign** (Tensor) - The signs of the log determinants. The shape is `x_shape[:-2]`, the dtype is same as `x`.</span>
<span class="sd">        - **y** (Tensor) - The absolute values of the log determinants. The shape is `x_shape[:-2]`, the dtype is same</span>
<span class="sd">          as `x`.</span>

<span class="sd">    Supported Platforms:</span>


<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[-4.5, -1.5], [7.0, 6.0]], [[2.5, 0.5], [3.0, 9.0]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; op = ops.LogMatrixDeterminant()</span>
<span class="sd">        &gt;&gt;&gt; sign, output = op(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(sign)</span>
<span class="sd">        [-1.   1.]</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2.80336046e+00    3.04452229e+00]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize LogMatrixDeterminant.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="s2">&quot;LogMatrixDeterminant&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;sign&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">MatrixLogarithm</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return the matrix logarithm of one or more square matrices.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - x is a tensor. The shape of tensor is :math:`[..., M, M]`.</span>
<span class="sd">          Must be one of the following types:complex64, complex128. And shape must be 2D-7D.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - has the same shape and type as input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not one of: complex64, complex128.</span>
<span class="sd">        ValueError: If the dimension of `x` is less to 2.</span>
<span class="sd">        ValueError: If the size of last two dimensions are not equal.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[1 + 2j, 2 + 1j], [4 + 1j, 5 + 2j]])</span>
<span class="sd">        &gt;&gt;&gt; matrix_logarithm = ops.MatrixLogarithm()</span>
<span class="sd">        &gt;&gt;&gt; y = matrix_logarithm(x)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[0.69155775+1.71618359j 0.64665196-0.34928196j]</span>
<span class="sd">         [1.02426074-0.88736831j 1.44677531+0.6400109j ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize MatrixLogarithm&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>


<div class="viewcode-block" id="IndexAdd"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.IndexAdd.html#mindspore.ops.IndexAdd">[docs]</a><span class="k">class</span> <span class="nc">IndexAdd</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adds tensor `y` to specified axis and indices of tensor `x`. The axis should be in [-len(x.dim),  len(x.dim) - 1],</span>
<span class="sd">    and indices should be in [0, the size of `x` - 1] at the axis dimension.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (int): The dimension along which to index.</span>
<span class="sd">        use_lock (bool): Whether to enable a lock to protect the updating process of variable tensors.</span>
<span class="sd">            If ``True`` , when updating the value of `x`, this process will be protected by a lock by using atomic</span>
<span class="sd">            operation.</span>
<span class="sd">            If ``False`` , the result may be unpredictable. Default: ``True`` .</span>
<span class="sd">        check_index_bound (bool): If ``True`` , check index boundary. If ``False`` , don&#39;t check index boundary.</span>
<span class="sd">            Default: ``True`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Parameter) - The input Parameter to add to.</span>
<span class="sd">        - **indices** (Tensor) - Add the value of `x` and `y` along the dimension of the `axis` according to the</span>
<span class="sd">          specified index value, with data type int32.</span>
<span class="sd">          The `indices` must be 1D with the same size as the size of `y` in the `axis` dimension. The values</span>
<span class="sd">          of `indices` should be in [0, b), where the b is the size of `x` in the `axis` dimension.</span>
<span class="sd">        - **y** (Tensor) - The input tensor with the value to add. Must have same data type as `x`.</span>
<span class="sd">          The shape must be the same as `x` except the `axis` th dimension.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Parameter.</span>
<span class="sd">        TypeError: If neither `indices` nor `y` is a Tensor.</span>
<span class="sd">        ValueError: If axis is out of `x` rank&#39;s range.</span>
<span class="sd">        ValueError: If `x` rank is not the same as `y` rank.</span>
<span class="sd">        ValueError: If shape of `indices` is not 1D or size of `indices` is not equal to dimension of y[axis].</span>
<span class="sd">        ValueError: If `y`&#39;s shape is not the same as `x` except the `axis` th dimension.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn, ops, Parameter</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.index_add = ops.IndexAdd(axis=1)</span>
<span class="sd">        ...         self.x = Parameter(Tensor(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), mindspore.float32),</span>
<span class="sd">        ...                 name=&quot;name_x&quot;)</span>
<span class="sd">        ...         self.indices = Tensor(np.array([0, 2]), mindspore.int32)</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def construct(self, y):</span>
<span class="sd">        ...         return self.index_add(self.x, self.indices, y)</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([[0.5, 1.0], [1.0, 1.5], [2.0, 2.5]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; output = net(y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 1.5  2.   4. ]</span>
<span class="sd">         [ 5.   5.   7.5]</span>
<span class="sd">         [ 9.   8.  11.5]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input_y&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">use_lock</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">check_index_bound</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize InplaceAdd&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="s1">&#39;input_y&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="Erfinv"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Erfinv.html#mindspore.ops.Erfinv">[docs]</a><span class="k">class</span> <span class="nc">Erfinv</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the inverse error function of input. The inverse error function is defined in the range (-1, 1).</span>

<span class="sd">    Refer to :func:`mindspore.ops.erfinv` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor to compute with, with data type float16, float32 or float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0, 0.5, -0.9]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; erfinv = ops.Erfinv()</span>
<span class="sd">        &gt;&gt;&gt; output = erfinv(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0.          0.47695306 -1.1630805 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Erfinv&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Conj"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Conj.html#mindspore.ops.Conj">[docs]</a><span class="k">class</span> <span class="nc">Conj</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a tensor of complex numbers that are the complex conjugate of each element in input.</span>
<span class="sd">    The complex numbers in input must be of the form a + bj, where a is the real part and b is the imaginary part.</span>

<span class="sd">    The complex conjugate returned by this operation is of the form a - bj.</span>

<span class="sd">    If input is real, it is returned unchanged.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - The input tensor to compute to. Must have numeric type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same dtype as the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">       TypeError: If the dtype of input is not a numeric type.</span>
<span class="sd">       TypeError: If the input is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.asarray(np.complex(1.3+0.4j)), mindspore.complex64)</span>
<span class="sd">        &gt;&gt;&gt; conj = ops.Conj()</span>
<span class="sd">        &gt;&gt;&gt; output = conj(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (1.3-0.4j)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Conj&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="ComplexAbs"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ComplexAbs.html#mindspore.ops.ComplexAbs">[docs]</a><span class="k">class</span> <span class="nc">ComplexAbs</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a Tensor that contains the magnitudes of the input.</span>

<span class="sd">    The complex numbers in input must be of the form :math:`a + bj`,</span>
<span class="sd">    where :math:`a` is the real part and :math:`b` is the imaginary part.</span>

<span class="sd">    .. math::</span>

<span class="sd">        y = \sqrt{a^2+b^2}</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - A Tensor, types: complex64, complex128.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as x. If the type of x is complex64, the type of output is float32.</span>
<span class="sd">        If the type of x is complex128, the type of output is float64.</span>

<span class="sd">    Raises:</span>
<span class="sd">       TypeError: If the input is not a Tensor.</span>
<span class="sd">       TypeError: If the input type is not complex64 or complex128.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.asarray(np.complex(3+4j)), mindspore.complex64)</span>
<span class="sd">        &gt;&gt;&gt; complex_abs = ops.ComplexAbs()</span>
<span class="sd">        &gt;&gt;&gt; output = complex_abs(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        5.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ComplexAbs&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Real"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Real.html#mindspore.ops.Real">[docs]</a><span class="k">class</span> <span class="nc">Real</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a Tensor that is the real part of the input.</span>
<span class="sd">    If input is real, it is returned unchanged.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - The input tensor to compute with.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">       TypeError: If the input is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.asarray(np.complex(1.3+0.4j)), mindspore.complex64)</span>
<span class="sd">        &gt;&gt;&gt; real = ops.Real()</span>
<span class="sd">        &gt;&gt;&gt; output = real(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        1.3</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Real&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Complex"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Complex.html#mindspore.ops.Complex">[docs]</a><span class="k">class</span> <span class="nc">Complex</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a complex Tensor from the real part and the imag part.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **real** (Tensor) - The real input tensor. types: float32, float64.</span>
<span class="sd">        - **imag** (Tensor) - The imag input tensor. types: float32, float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the complex type.</span>

<span class="sd">    Raises:</span>
<span class="sd">       TypeError: If the dtype of input is not one of: float32, float64.</span>
<span class="sd">       TypeError: If the dtypes of two inputs are not same.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; real = Tensor(np.array([1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; imag = Tensor(np.array([2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; complex = ops.Complex()</span>
<span class="sd">        &gt;&gt;&gt; output = complex(real, imag)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.+2.j]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Complex&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;real&#39;</span><span class="p">,</span> <span class="s1">&#39;imag&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Imag"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Imag.html#mindspore.ops.Imag">[docs]</a><span class="k">class</span> <span class="nc">Imag</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a new tensor containing imaginary value of the input.</span>
<span class="sd">    If input is real, it is returned zeros.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - The input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">       TypeError: If the input is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.asarray(np.complex(1.3+0.4j)), mindspore.complex64)</span>
<span class="sd">        &gt;&gt;&gt; imag = ops.Imag()</span>
<span class="sd">        &gt;&gt;&gt; output = imag(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.4</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Imag&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Angle"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Angle.html#mindspore.ops.Angle">[docs]</a><span class="k">class</span> <span class="nc">Angle</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the element-wise argument of a complex tensor.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.angle` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - The input tensor. Supported types: complex64, complex128.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the float32 or float64 type and the same shape as input.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor([-1.5 + 7.8j, 3 + 5.75j], mindspore.complex64)</span>
<span class="sd">        &gt;&gt;&gt; angle = ops.Angle()</span>
<span class="sd">        &gt;&gt;&gt; output = angle(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.7607845 1.0899091]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Angle&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Trunc"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Trunc.html#mindspore.ops.Trunc">[docs]</a><span class="k">class</span> <span class="nc">Trunc</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a new tensor with the truncated integer values of the elements of input.</span>

<span class="sd">    Refer to :func:`mindspore.ops.trunc` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Input tensor of any dimension.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the same shape and data type as `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([3.4742, 0.5466, -0.8008, -3.9079]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.Trunc()(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 3.  0. -0. -3.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Trunc&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<span class="k">class</span> <span class="nc">TridiagonalMatMul</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return the result of a multiplication of two matrices, where the left one is a Tridiagonal Matrix.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **superdiag** (Tensor) - Superdiagonals of Tridiagonal Matrices to the left of multiplication.</span>
<span class="sd">          Data types must be: float16, float32, double, complex64, complex128.</span>
<span class="sd">          The shape is :math:`(..., 1, M)`.</span>
<span class="sd">          Last element is ignored.</span>
<span class="sd">        - **maindiag** (Tensor) - Maindiagonals of Tridiagonal Matrices to the left of multiplication.</span>
<span class="sd">          Data types must be: float16, float32, double, complex64, complex128.</span>
<span class="sd">          The shape is :math:`(..., 1, M)`.</span>
<span class="sd">        - **subdiag** (Tensor) - Subdiagonals of Tridiagonal Matrices to the left of multiplication.</span>
<span class="sd">          Data types must be: float16, float32, double, complex64, complex128.</span>
<span class="sd">          The shape is :math:`(..., 1, M)`.</span>
<span class="sd">          First element is ignored.</span>
<span class="sd">        - **rhs** (Tensor) - MxN Matrices to the right of multiplication.</span>
<span class="sd">          Data types must be: float16, float32, double, complex64, complex128.</span>
<span class="sd">          The shape is :math:`(..., 1, M)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same shape and data type as the `rhs`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtypes of `superdiag`, `maindiag`, `subdiag` and `rhs`</span>
<span class="sd">                   are not float16, float32, double, complex64, complex128.</span>
<span class="sd">        ValueError: If the col of input `superdiag`, the col of input `maindiag`,</span>
<span class="sd">                    the col of input `subdiag` and the row of input `rhs` are not equal.</span>
<span class="sd">        ValueError: If the row of input `superdiag`, the row of input `maindiag` and</span>
<span class="sd">                    the row of input `subdiag` are not 1.</span>
<span class="sd">        ValueError: If the rank of input `superdiag`, the rank of input `maindiag`,</span>
<span class="sd">                    the rank of input `subdiag` and rank row of input `rhs`</span>
<span class="sd">                    are not equal to or greater than 2.</span>
<span class="sd">        ValueError: If the shape of input `superdiag`, the shape of input `maindiag` and</span>
<span class="sd">                    the shape of input `subdiag` are not same.</span>
<span class="sd">        ValueError: If the shape of input `superdiag` ignoring the last two elements,</span>
<span class="sd">                    the shape of input `maindiag` ignoring the last two elements,</span>
<span class="sd">                    the shape of input `subdiag` ignoring the last two elements and</span>
<span class="sd">                    the shape of input `rhs` ignoring the last two elements</span>
<span class="sd">                    are not same.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; tridiagonalmatmul = ops.TridiagonalMatMul()</span>
<span class="sd">        &gt;&gt;&gt; superdiag = Tensor(np.array([[1, 2, 3]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; maindiag = Tensor(np.array([[1, 2, 3]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; subdiag = Tensor(np.array([[1, 2, 3]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; rhs = Tensor(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = tridiagonalmatmul(superdiag,maindiag,subdiag,rhs)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 2.  2.  2. ]</span>
<span class="sd">         [ 6.  6.  6.]</span>
<span class="sd">         [ 6.  6.  6.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize TridiagonalMatMul&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span>
            <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;superdiag&#39;</span><span class="p">,</span> <span class="s1">&#39;maindiag&#39;</span><span class="p">,</span> <span class="s1">&#39;subdiag&#39;</span><span class="p">,</span> <span class="s1">&#39;rhs&#39;</span><span class="p">],</span>
            <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>


<div class="viewcode-block" id="Igamma"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Igamma.html#mindspore.ops.Igamma">[docs]</a><span class="k">class</span> <span class="nc">Igamma</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates lower regularized incomplete Gamma function.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.igamma` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **a** (Tensor) - The input tensor.</span>
<span class="sd">        - **x** (Tensor) - The input tensor. It should have the same dtype with `a`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same dtype as `a` and `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; a = Tensor(np.array([2.0, 4.0, 6.0, 8.0]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([2.0, 3.0, 4.0, 5.0]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; igamma = ops.Igamma()</span>
<span class="sd">        &gt;&gt;&gt; output = igamma(a, x)</span>
<span class="sd">        &gt;&gt;&gt; print (output)</span>
<span class="sd">        [0.593994  0.35276785  0.21486944  0.13337152]</span>
<span class="sd">        &gt;&gt;&gt; a = Tensor(2.1, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(2.1, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; igamma = ops.Igamma()</span>
<span class="sd">        &gt;&gt;&gt; output = igamma(a, x)</span>
<span class="sd">        &gt;&gt;&gt; print (output)</span>
<span class="sd">        0.5917439</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Igamma&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;z&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Igammac"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Igammac.html#mindspore.ops.Igammac">[docs]</a><span class="k">class</span> <span class="nc">Igammac</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the upper regularized incomplete Gamma function Q(a, x).</span>

<span class="sd">    Refer to :func:`mindspore.ops.igammac` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **a** (Tensor) - The input tensor.</span>
<span class="sd">        - **x** (Tensor) - The input tensor. It should have the same dtype with `a`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same dtype as `a` and `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; a = Tensor(np.array([2.0, 4.0, 6.0, 8.0]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([2.0, 3.0, 4.0, 5.0]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; igammac = ops.Igammac()</span>
<span class="sd">        &gt;&gt;&gt; output = igammac(a, x)</span>
<span class="sd">        &gt;&gt;&gt; print (output)</span>
<span class="sd">        [0.40600586 0.6472318  0.7851304  0.8666283 ]</span>
<span class="sd">        &gt;&gt;&gt; a = Tensor(2.1, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(2.1, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; igammac = ops.Igammac()</span>
<span class="sd">        &gt;&gt;&gt; output = igammac(a, x)</span>
<span class="sd">        &gt;&gt;&gt; print (output)</span>
<span class="sd">        0.40825662</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Igammac&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;z&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="IsClose"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.IsClose.html#mindspore.ops.IsClose">[docs]</a><span class="k">class</span> <span class="nc">IsClose</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a tensor of Boolean values indicating whether two input tensors</span>
<span class="sd">    are element-wise equal within a given tolerance.</span>

<span class="sd">    Refer to :func:`mindspore.ops.isclose` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        rtol(float, optional): Relative tolerance. Default: ``1e-05`` .</span>
<span class="sd">        atol(float, optional): Absolute tolerance. Default: ``1e-08`` .</span>
<span class="sd">        equal_nan(bool, optional): If ``True`` , then two NaNs will be considered equal. Default: ``True`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - First tensor to compare, with data type belongs to float32, float16, int32.</span>
<span class="sd">        - **other** (Tensor) - Second tensor to compare, with data type belongs to float32, float16, int32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same shape as `input` and `other` after broadcasting, its dtype is bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import IsClose</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([1.3, 2.1, 3.2, 4.1, 5.1]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor(np.array([1.3, 3.3, 2.3, 3.1, 5.1]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; isclose = IsClose()</span>
<span class="sd">        &gt;&gt;&gt; output = isclose(input, other)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True False False False  True]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">,</span> <span class="n">equal_nan</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize IsClose&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;rtol&#39;</span><span class="p">,</span> <span class="n">rtol</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;atol&#39;</span><span class="p">,</span> <span class="n">atol</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;equal_nan&#39;</span><span class="p">,</span> <span class="n">equal_nan</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;Ascend&quot;</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">equal_nan</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For IsClose, the `equal_nan` must be True on Ascend, but got False.&quot;</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">(</span><span class="n">rtol</span><span class="p">,</span> <span class="s1">&#39;rtol&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">(</span><span class="n">atol</span><span class="p">,</span> <span class="s1">&#39;atol&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">MatrixExp</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the matrix exponential of a square matrix. Supports batched inputs.</span>

<span class="sd">    Refer to :func:`mindspore.ops.matrix_exp` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The shape of tensor is :math:`(*, n, n)` where * is zero or more batch dimensions.</span>
<span class="sd">          Supported dtypes: float64, float32, float16, complex64, complex128.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Supported Platforms:</span>


<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; matrix_exp = ops.MatrixExp()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2], [0, 1]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = matrix_exp(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[2.7182817 5.436563 ]</span>
<span class="sd">        [0.        2.7182817]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize MatrixExp&quot;&quot;&quot;</span>


<div class="viewcode-block" id="MatrixSolve"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.MatrixSolve.html#mindspore.ops.MatrixSolve">[docs]</a><span class="k">class</span> <span class="nc">MatrixSolve</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Solves systems of linear equations.</span>

<span class="sd">    Args:</span>
<span class="sd">        adjoint (bool, optional): Indicates whether the adjoint of the</span>
<span class="sd">            matrix is used during the computation. Default: ``False`` ,  use its transpose instead.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **matrix** (Tensor) - A tensor of shape :math:`(..., M, M)`,</span>
<span class="sd">          is a matrix of coefficients for a system of linear equations.</span>
<span class="sd">        - **rhs** (Tensor) - A tensor of shape :math:`(..., M, K)`,</span>
<span class="sd">          is a matrix of the resulting values of a system of linear equations.</span>
<span class="sd">          `rhs` must have the same type as `matrix`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, a matrix composed of solutions to a system of linear equations,</span>
<span class="sd">        which has the same type and shape as `rhs`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `adjoint` is not the type of bool.</span>
<span class="sd">        TypeError: If the type of `matrix` is not one of the following dtype:</span>
<span class="sd">                   mstype.float16, mstype.float32, mstype.float64, mstype.complex64,</span>
<span class="sd">                   mstype.complex128.</span>
<span class="sd">        TypeError: If the type of `matrix` is not the same as that of `rhs`.</span>
<span class="sd">        ValueError: If the rank of `matrix` less than 2.</span>
<span class="sd">        ValueError: If the dimension of `matrix` is not the same as `rhs` .</span>
<span class="sd">        ValueError: If the inner-most 2 dimension of `matrix` is not the same.</span>
<span class="sd">        ValueError: If the inner-most 2 dimension of `rhs` does not match `matrix` .</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; matrix = Tensor(np.array([[1.0  , 4.0],</span>
<span class="sd">        ...                       [2.0 , 7.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; rhs = Tensor(np.array([[1.0]  , [3.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; matrix_solve = ops.MatrixSolve(adjoint = False)</span>
<span class="sd">        &gt;&gt;&gt; output = matrix_solve(matrix, rhs)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[5.0]</span>
<span class="sd">         [-1.0]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">adjoint</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;MatrixSolve&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">adjoint</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;adjoint&quot;</span><span class="p">,</span> <span class="n">adjoint</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">MatrixSolveLs</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Solves one or more linear least-squares problems.</span>

<span class="sd">    If `fast` is `True`,then the solution is computed by solving the normal equations using Cholesky decomposition.</span>
<span class="sd">    If `fast` is `False` an algorithm based on the numerically robust complete orthogonal decomposition is used. This</span>
<span class="sd">    path is typically 6-7 times slower than the fast path. If `fast` is `False` then `l2_regularizer` is ignored.</span>

<span class="sd">    Args:</span>
<span class="sd">        fast (bool): An optional bool. Default: ``True`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **matrix** (Tensor) -  A Tensor. Must be one of the following data types: float64, float32, complex64,</span>
<span class="sd">          complex128. Shape is :math:`(*, M, N)`.</span>
<span class="sd">        - **rhs** (Tensor) -  A Tensor. Must have the same data type as matrix. Shape is :math:`(*, M, K)`.</span>
<span class="sd">          `matrix` and `rhs` should have the same dimensions except the last one.</span>
<span class="sd">        - **l2_regularizer** (Tensor) - A Tensor of type float64. Scalar tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor of shape :math:`(*, N, K)` with the same data type as `matrix`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `matrix`, `rhs` or `l2_regularizer` is not tensor.</span>
<span class="sd">        TypeError: If either of `matrix` and `rhs` is not float32, float64, complex64 or complex128.</span>
<span class="sd">        TypeError: If `l2_regularizer` is not float64.</span>
<span class="sd">        TypeError: If `fast` is not bool.</span>
<span class="sd">        ValueError: If dimensions of `matrix` or `rhs` is less than 2.</span>
<span class="sd">        ValueError: If shape of `matrix` dose not match the shape of `rhs`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; matrix_solve_ls = ops.MatrixSolveLs(fast=True)</span>
<span class="sd">        &gt;&gt;&gt; matrix = Tensor([[3, 0, 0, 0], [2, 1, 0, 0], [1, 0, 1, 0], [1, 1, 1, 1]], mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; rhs = Tensor(np.array([[4], [2], [4], [2]]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; l2 = Tensor(0.0, mstype.float64)</span>
<span class="sd">        &gt;&gt;&gt; output = matrix_solve_ls(matrix, rhs, l2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 1.3333334]</span>
<span class="sd">        [-0.6666667]</span>
<span class="sd">        [ 2.6666665]</span>
<span class="sd">        [-1.3333333]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fast</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize MatrixSolveLs&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;fast&#39;</span><span class="p">,</span> <span class="n">fast</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Lu</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the LU decomposition of one or more square matrices.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_idx_type (:class:`mindspore.dtype`): An optional data type of `mindspore.dtype.int32`.</span>
<span class="sd">            Default: ``mindspore.dtype.int32`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - A tensor of shape `[..., M, M]` whose inner-most 2 dimensions form</span>
<span class="sd">          matrices of size `[M, M]`, with data type float32, float64, complex64, complex128.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **lu** (Tensor) - A tensor of shape `[..., M, M]` whose strictly lower triangular part denotes the lower</span>
<span class="sd">          triangular factor `L` with unit diagonal. Upper triangular part denotes the upper triangular factor `U`.</span>
<span class="sd">        - **p** (Tensor) - Permutation of the rows encoded as a list of indices in `0..M-1`, shape is `[..., M]`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the dtype of `input` is not one of the following dtype:</span>
<span class="sd">            float32, float64, complex64, complex128.</span>
<span class="sd">        TypeError: If `output_idx_type` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If `input` rank is less than 2.</span>
<span class="sd">        ValueError: If input[-1] is not equal to input[-2].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([[2.5,3.1,3.5], [4.7,1.9,0.2], [1.1,3.6,2.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; lu, p = ops.Lu(output_idx_type=mindspore.int32)(input)</span>
<span class="sd">        &gt;&gt;&gt; print(lu)</span>
<span class="sd">        [[4.7        1.9        0.2       ]</span>
<span class="sd">         [0.23404257 3.155319   1.9531915 ]</span>
<span class="sd">         [0.5319149  0.6621713  2.1002696 ]]</span>
<span class="sd">        &gt;&gt;&gt; print(p)</span>
<span class="sd">        [1 2 0]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_idx_type</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;Lu&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;lu&#39;</span><span class="p">,</span> <span class="s1">&#39;p&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_name</span><span class="p">(</span><span class="s2">&quot;output_idx_type&quot;</span><span class="p">,</span> <span class="n">output_idx_type</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;output_idx_type&#39;</span><span class="p">,</span> <span class="n">output_idx_type</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">LuSolve</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the solution y to the system of linear equations :math:`Ay = b` ,</span>
<span class="sd">    given LU decomposition A and column vector b.</span>

<span class="sd">    LU decomposition of a matrix can be generated from :func:`mindspore.scipy.linalg.lu` .</span>

<span class="sd">    Note:</span>
<span class="sd">        The batch dimensions of lu_pivots must match the batch dimensions of lu_data, the size of the dimension and the</span>
<span class="sd">        number of each dimension must be the same. For example, lu_data is :math:`(3, 3, 2, 2)` lu_pivots is</span>
<span class="sd">        :math:`(3, 3, 2)`,</span>
<span class="sd">        lu_data&#39;s batch dimensions is :math:`(3, 3)`, lu_pivots&#39;s batch dimensions is :math:`(3, 3)`.</span>

<span class="sd">        The batch dimensions of lu_data must match the batch dimensions of x, the batch dimensions may have</span>
<span class="sd">        different sizes, from right to left, the corresponding dimensions must be equal. For example, lu_data</span>
<span class="sd">        is :math:`(3, 3, 2, 2)` x is :math:`(2, 3, 3, 2, 1)`, lu_data&#39;s batch dimensions is</span>
<span class="sd">        :math:`(3, 3)`, x&#39;s batch dimensions is :math:`(2, 3, 3)`.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Column vector `b` in the above equation. It has shape :math:`(*, m, k)`,</span>
<span class="sd">          where :math:`*` is batch dimensions, with data type float32, float16.</span>
<span class="sd">        - **lu_data** (Tensor) - LU decomposition. It has shape :math:`(*, m, m)`, where * is batch</span>
<span class="sd">          dimensions, that can be decomposed into an upper triangular matrix U and a lower triangular</span>
<span class="sd">          matrix L, with data type float32, float16.</span>
<span class="sd">        - **lu_pivots** (Tensor) - Permutation matrix P of LU decomposition. It has</span>
<span class="sd">          shape :math:`(*, m)`, where :math:`*` is batch dimensions, that can be converted</span>
<span class="sd">          to a permutation matrix P, with data type int32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the same data type as the x and lu_data.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x` or `lu_data` is not one of: float32, float16.</span>
<span class="sd">        TypeError: If dtype of `lu_pivots` is not: int32.</span>
<span class="sd">        TypeError: If `x`, `lu_data` or `lu_pivots` is not Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not same as dtype of `lu_data`.</span>
<span class="sd">        ValueError: If the batch dimensions of lu_pivots does not match the batch dimensions of lu_data.</span>
<span class="sd">        ValueError: If `x` dimension less than 2, `lu_data` dimension less than 2 or `lu_pivots` dimension less than 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1], [3], [3]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; lu_data = Tensor(np.array([[2, 1, 1], [0.5, 1, 1.5], [0.5, 0, 2.5]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; lu_pivots = Tensor(np.array([2, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; net = ops.LuSolve()</span>
<span class="sd">        &gt;&gt;&gt; y = net(x, lu_data, lu_pivots)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[ 1.9000002]</span>
<span class="sd">         [-1.4000001]</span>
<span class="sd">         [ 0.6      ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>


<div class="viewcode-block" id="LuUnpack"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.LuUnpack.html#mindspore.ops.LuUnpack">[docs]</a><span class="k">class</span> <span class="nc">LuUnpack</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts `LU_data` and `LU_pivots` back into P, L and U matrices, where</span>
<span class="sd">    P is a permutation matrix, L is a lower triangular matrix, and U is an</span>
<span class="sd">    upper triangular matrix. Typically, `LU_data` and `LU_pivots` are generated</span>
<span class="sd">    from the LU decomposition of a matrix.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.lu_unpack` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        unpack_data (bool, optional): A flag indicating if the LU_data should be unpacked.</span>
<span class="sd">            If ``False`` , then the returned L and U are None. Default: ``True`` .</span>
<span class="sd">        unpack_pivots (bool, optional): A flag indicating if the LU_pivots should be unpacked</span>
<span class="sd">            into a permutation matrix P. If ``False`` , then the returned P is None. Default: ``True`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **LU_data** (Tensor) - The packed LU factorization data. The shape of a tensor is :math:`(*, M, N)`,</span>
<span class="sd">          where :math:`*` is batch dimensions, with data type int8, uint8, int16, int32, int64, float16,</span>
<span class="sd">          float32, float64. The dims of LU_data must be equal to or greater than 2.</span>
<span class="sd">        - **LU_pivots** (Tensor) - The packed LU factorization pivots. The shape of a tensor is :math:`(*, min(M, N))`,</span>
<span class="sd">          where :math:`*` is batch dimensions, with data type int8, uint8, int16, int32, int64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **pivots** (Tensor) - The permutation matrix of LU factorization. The shape is :math:`(*, M, M)`,</span>
<span class="sd">          the dtype is same as `LU_data`.</span>
<span class="sd">        - **L** (Tensor) - The L matrix of LU factorization. The dtype is the same as `LU_data`.</span>
<span class="sd">        - **U** (Tensor) - The U matrix of LU factorization. The dtype is the same as `LU_data`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; LU_data = Tensor(np.array([[[-0.3806, -0.4872,  0.5536],</span>
<span class="sd">        ...                             [-0.1287,  0.6508, -0.2396],</span>
<span class="sd">        ...                             [ 0.2583,  0.5239,  0.6902]],</span>
<span class="sd">        ...                             [[ 0.6706, -1.1782,  0.4574],</span>
<span class="sd">        ...                             [-0.6401, -0.4779,  0.6701],</span>
<span class="sd">        ...                             [ 0.1015, -0.5363,  0.6165]]]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; LU_pivots = Tensor(np.array([[1, 3, 3],</span>
<span class="sd">        ...                              [2, 3, 3]]), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; lu_unpack = ops.LuUnpack()</span>
<span class="sd">        &gt;&gt;&gt; pivots, L, U = lu_unpack(LU_data, LU_pivots)</span>
<span class="sd">        &gt;&gt;&gt; print(pivots)</span>
<span class="sd">        [[[1. 0. 0.]</span>
<span class="sd">          [0. 0. 1.]</span>
<span class="sd">          [0. 1. 0.]]</span>
<span class="sd">        &lt;BLANKLINE&gt;</span>
<span class="sd">         [[0. 0. 1.]</span>
<span class="sd">          [1. 0. 0.]</span>
<span class="sd">          [0. 1. 0.]]]</span>
<span class="sd">        &gt;&gt;&gt; print(L)</span>
<span class="sd">        [[[ 1.      0.      0.    ]</span>
<span class="sd">          [-0.1287  1.      0.    ]</span>
<span class="sd">          [ 0.2583  0.5239  1.    ]]</span>
<span class="sd">        &lt;BLANKLINE&gt;</span>
<span class="sd">         [[ 1.      0.      0.    ]</span>
<span class="sd">          [-0.6401  1.      0.    ]</span>
<span class="sd">          [ 0.1015 -0.5363  1.    ]]]</span>
<span class="sd">        &gt;&gt;&gt; print(U)</span>
<span class="sd">        [[[-0.3806 -0.4872  0.5536]</span>
<span class="sd">          [ 0.      0.6508 -0.2396]</span>
<span class="sd">          [ 0.      0.      0.6902]]</span>
<span class="sd">        &lt;BLANKLINE&gt;</span>
<span class="sd">         [[ 0.6706 -1.1782  0.4574]</span>
<span class="sd">          [ 0.     -0.4779  0.6701]</span>
<span class="sd">          [ 0.      0.      0.6165]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">unpack_data</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">unpack_pivots</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize LuUnpack&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;unpack_data&quot;</span><span class="p">,</span> <span class="n">unpack_data</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;unpack_pivots&quot;</span><span class="p">,</span> <span class="n">unpack_pivots</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">Lgamma</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the natural logarithm of the absolute value of the gamma function on input.</span>

<span class="sd">    Refer to :func:`mindspore.ops.lgamma` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor. The dtype can be float16, float32 or float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same dtype as `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.5, 3.2, 8.5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; lgamma = ops.Lgamma()</span>
<span class="sd">        &gt;&gt;&gt; output = lgamma(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.5723649 0.8854049 9.549267 ]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(2.1, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = lgamma(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.045437694</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Lgamma&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>


<div class="viewcode-block" id="Digamma"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Digamma.html#mindspore.ops.Digamma">[docs]</a><span class="k">class</span> <span class="nc">Digamma</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the grad of the lgamma function on input.</span>

<span class="sd">    .. math::</span>
<span class="sd">        P(x) = grad(ln(gamma(x)))</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor. With type of float16 or float32 or float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same dtype as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If x is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of input x is not float16 or float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.5, 0.5, 9]).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; digamma = ops.Digamma()</span>
<span class="sd">        &gt;&gt;&gt; output = digamma(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0.0365 -1.964   2.14  ]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Digamma&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Polygamma"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Polygamma.html#mindspore.ops.Polygamma">[docs]</a><span class="k">class</span> <span class="nc">Polygamma</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the :math:`a`th derivative of the polygamma function on `x`.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.polygamma` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **a** (Tensor) - The order of the polygamma function, it has shape :math:`()`,</span>
<span class="sd">          supported types: int32, int64.</span>
<span class="sd">        - **x** (Tensor) - The tensor to compute the :math:`a`-th derivative of the polygamma function with,</span>
<span class="sd">          supported types: float16, float32, float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same dtype as `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, -0.5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; a = Tensor(np.array(1), mindspore.int64)</span>
<span class="sd">        &gt;&gt;&gt; polygamma = ops.Polygamma()</span>
<span class="sd">        &gt;&gt;&gt; output = polygamma(a, x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.644934 8.934802]</span>
<span class="sd">        &gt;&gt;&gt; a = Tensor(np.array(2), mindspore.int64)</span>
<span class="sd">        &gt;&gt;&gt; output = polygamma(a, x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-2.404114  -0.8287967]</span>
<span class="sd">        &gt;&gt;&gt; a = Tensor(np.array(3), mindspore.int64)</span>
<span class="sd">        &gt;&gt;&gt; output = polygamma(a, x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [  6.4939404 193.40909  ]</span>
<span class="sd">        &gt;&gt;&gt; a = Tensor(np.array(4), mindspore.int64)</span>
<span class="sd">        &gt;&gt;&gt; output = polygamma(a, x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-24.886265   -3.4742498]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Polygamma&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<span class="k">class</span> <span class="nc">CholeskyInverse</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the inverse of the positive definite matrix using cholesky matrix factorization given its Cholesky factor.</span>

<span class="sd">    Refer to :func:`mindspore.ops.cholesky_inverse` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        upper(bool, optional): Whether to return a lower or upper triangular matrix. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor whose rank is 2. Supported dtypes: float32, float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[2,0,0], [4,1,0], [-1,1,2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; net = ops.CholeskyInverse()</span>
<span class="sd">        &gt;&gt;&gt; y = net(x)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[ 5.8125 -2.625   0.625 ]</span>
<span class="sd">         [-2.625   1.25   -0.25  ]</span>
<span class="sd">         [ 0.625  -0.25    0.25  ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize CholeskyInverse&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;upper&quot;</span><span class="p">,</span> <span class="n">upper</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">upper</span> <span class="o">=</span> <span class="n">upper</span>


<div class="viewcode-block" id="Cross"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Cross.html#mindspore.ops.Cross">[docs]</a><span class="k">class</span> <span class="nc">Cross</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the cross product of vectors in dimension `dim` of x1 and x2.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.cross` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        dim (int): Spefcified dim along which to cumpute cross product with. Default: ``-65530`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x1** (Tensor) - Input Tensor.</span>
<span class="sd">        - **x2** (Tensor) - Another input Tensor, must have the same shape and</span>
<span class="sd">          the same type as `x1`, and the size of their `dim` dimension should be 3.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and type as inputs.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; cross = ops.Cross(dim = 0)</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor([1, 2, 3], mstype.int8)</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor([1, 2, 3], mstype.int8)</span>
<span class="sd">        &gt;&gt;&gt; output = cross(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0 0 0]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">65530</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;dim&#39;</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="s1">&#39;x2&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<span class="k">class</span> <span class="nc">RaggedRange</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a `RaggedTensor` containing the specified sequences of numbers.</span>

<span class="sd">    Args:</span>
<span class="sd">        Tsplits (mindspore.dtype): An mindspore.dtype from: mindspore.int32, mindspore.int64.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **starts** (Tensor) - The starts of each range, whose type is int32, int64, float32 or float64,</span>
<span class="sd">          and shape is 0D or 1D.</span>
<span class="sd">        - **limits** (Tensor) - The limits of each range, whose type and shape should be same as input `starts`.</span>
<span class="sd">        - **deltas** (Tensor) - The deltas of each range, whose type and shape should be same as input `starts`,</span>
<span class="sd">          and each element in the tensor should not be equal to 0.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **rt_nested_splits** (Tensor) - The nested splits of the return `RaggedTensor`,</span>
<span class="sd">          and type of the tensor is `Tsplits`,</span>
<span class="sd">          shape of the tensor is equal to shape of input `starts` plus 1.</span>
<span class="sd">        - **rt_dense_values**  (Tensor) - The dense values of the return `RaggedTensor`,</span>
<span class="sd">          and type of the tensor should be same as input `starts`.</span>
<span class="sd">          Let size of input `starts`, input `limits` and input `deltas` are i,</span>

<span class="sd">          - if type of the input `starts`, input `limits` and input `deltas`</span>
<span class="sd">            are int32 or int64, shape of the output `rt_dense_values` is equal to</span>
<span class="sd">            :math:`sum(abs(limits[i] - starts[i]) + abs(deltas[i] - 1) / abs(deltas[i]))`.</span>
<span class="sd">          - if type of the input `starts`, input `limits` and input `deltas`</span>
<span class="sd">            are float32 or float64, shape of the output `rt_dense_values` is equal to</span>
<span class="sd">            :math:`sum(ceil(abs((limits[i] - starts[i]) / deltas[i])))`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If any input is not Tensor.</span>
<span class="sd">        TypeError: If the type of `starts` is not one of the following dtype: int32, int64, float32, float64.</span>
<span class="sd">        TypeError: If the type of `starts`, `limits` and `deltas` are not same.</span>
<span class="sd">        TypeError: If the type of `Tsplits` is not one of the following dtype: mstype.int32, mstype.int64.</span>
<span class="sd">        ValueError: If the inputs `starts`, `limits`, and `deltas` are not 0D or 1D.</span>
<span class="sd">        ValueError: If the input `deltas` is equal to 0.</span>
<span class="sd">        ValueError: If the shape of `starts`, `limits` and `deltas` are not same.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; raggedrange = ops.RaggedRange(Tsplits=mstype.int64)</span>
<span class="sd">        &gt;&gt;&gt; starts = Tensor(np.array([2, 5, 8]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; limits = Tensor(np.array([3, 5, 12]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; deltas = Tensor(np.array([1, 1, 1]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; (rt_nested_splits, rt_dense_values) = raggedrange(starts, limits, deltas)</span>
<span class="sd">        &gt;&gt;&gt; print(rt_nested_splits)</span>
<span class="sd">        [0 1 1 5]</span>
<span class="sd">        &gt;&gt;&gt; print(rt_dense_values)</span>
<span class="sd">        [ 2  8  9 10 11]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Tsplits</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize RaggedRange.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span> <span class="mi">1000000</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;starts&#39;</span><span class="p">,</span> <span class="s1">&#39;limits&#39;</span><span class="p">,</span> <span class="s1">&#39;deltas&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;rt_nested_splits&#39;</span><span class="p">,</span> <span class="s1">&#39;rt_dense_values&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;Tsplits&quot;</span><span class="p">,</span> <span class="n">Tsplits</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">Type</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">valid_values</span> <span class="o">=</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_name</span><span class="p">(</span><span class="s2">&quot;Tsplits&quot;</span><span class="p">,</span> <span class="n">Tsplits</span><span class="p">,</span> <span class="n">valid_values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<div class="viewcode-block" id="Trace"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Trace.html#mindspore.ops.Trace">[docs]</a><span class="k">class</span> <span class="nc">Trace</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the sum of the diagonal elements in a 2-D matrix.</span>

<span class="sd">    Note:</span>
<span class="sd">        Input must be matrix, and complex number is not supported at present.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - A matrix to be calculated. The matrix must be two dimensional.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, 0D Tensor with 1 element, it has the same data type as input `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        ValueError: If the dimension of `x` is not equal to 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; trace = ops.Trace()</span>
<span class="sd">        &gt;&gt;&gt; output = trace(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        15.0</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(1, 13).reshape(3, 4), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; trace = ops.Trace()</span>
<span class="sd">        &gt;&gt;&gt; output = trace(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        18.0</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(12, 0, -1).reshape(4, 3), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; trace = ops.Trace()</span>
<span class="sd">        &gt;&gt;&gt; output = trace(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        24.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span></div>


<div class="viewcode-block" id="Median"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Median.html#mindspore.ops.Median">[docs]</a><span class="k">class</span> <span class="nc">Median</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the median and its corresponding indices of input tensor in the `axis` dimension.</span>
<span class="sd">    If `global_median` is True, computes the  median of all elements of tensor.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - `indices` does not necessarily contain the first occurrence of each median value found in the `input`,</span>
<span class="sd">          unless it is unique. The specific implementation of this API is device-specific.</span>
<span class="sd">          The results may be different on CPU and GPU.</span>
<span class="sd">        - When attr `global_median` is ``True`` , the value of the second output tensor `indices` is meaningless.</span>

<span class="sd">    Args:</span>
<span class="sd">        global_median (bool, optional): Whether the output tensor is the median of all</span>
<span class="sd">            input tensor elements or not. Default: ``False`` .</span>
<span class="sd">        axis (int, optional): The specified dimension to compute median. Default: ``0`` .</span>
<span class="sd">        keep_dims (bool, optional): Whether the output tensor need to retain `axis` dimension or not.</span>
<span class="sd">            Default: ``False`` .</span>
<span class="sd">        ignore_nan (bool, optional): Whether to ignore the NaN values in input Tensor. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - A Tensor to calculate median with.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - Median, has the same dtype as the `x`.</span>

<span class="sd">          - If `global_median` is ``True`` , the `y` has only one element.</span>
<span class="sd">          - If `keep_dims` is ``True`` , the `y` has the same shape as the `x` except the size</span>
<span class="sd">            of `y` in dimension `axis` is 1.</span>
<span class="sd">          - Otherwise, the `y` lacks `axis` dimension than input.</span>

<span class="sd">        - **indices** (Tensor) - Indices, Has the same shape as the `y`, with dtype int64.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If input `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `global_median` , `keep_dims` or `ignore_nan` is assigned a nonboolean value.</span>
<span class="sd">        TypeError: If `axis` is not int.</span>
<span class="sd">        ValueError: If `axis` is not in range of [-x.dim, x.dim-1].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # case 1 : common median compute</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[5, 1, 2],[3, 5, 7], [1, 6, 4]]).astype(np.int64))</span>
<span class="sd">        &gt;&gt;&gt; median = ops.Median(global_median=False, axis=0, keep_dims=False)</span>
<span class="sd">        &gt;&gt;&gt; y = median(x)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        (Tensor(shape=[3], dtype=Int64, value= [3, 5, 4]), Tensor(shape=[3], dtype=Int64, value= [1, 1, 2]))</span>
<span class="sd">        &gt;&gt;&gt; # case 2 : global median compute</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 7, 6],[5, 1, 3],[9, 17, 1]]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; median = ops.Median(global_median=True)</span>
<span class="sd">        &gt;&gt;&gt; y = median(x)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        (Tensor(shape=[], dtype=Int32, value= 5), Tensor(shape=[], dtype=Int64, value= 0))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">global_median</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ignore_nan</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;cust_aicpu&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;global_median&quot;</span><span class="p">,</span> <span class="n">global_median</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">global_median</span> <span class="o">=</span> <span class="n">global_median</span>
        <span class="k">if</span> <span class="n">global_median</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;ignore_nan&quot;</span><span class="p">,</span> <span class="n">ignore_nan</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">SparseSegmentMean</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the mean along sparse segments of a Tensor.</span>

<span class="sd">    Refer to :func:`mindspore.ops.sparse_segment_mean` for more details.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops.operations.math_ops import SparseSegmentMean</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[0, 1, 2], [1, 2, 3], [3, 6, 7]], dtype=mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor([0, 1, 2], dtype=mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; segment_ids = Tensor([1,2,2], dtype=mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; sparse_segment_mean = SparseSegmentMean()</span>
<span class="sd">        &gt;&gt;&gt; out = sparse_segment_mean(x, indices, segment_ids)</span>
<span class="sd">        &gt;&gt;&gt; print(out)</span>
<span class="sd">        [[0. 0. 0.]</span>
<span class="sd">         [0. 1. 2.]</span>
<span class="sd">         [2. 4. 5.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SparseSegmentMean&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="s1">&#39;segment_ids&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>


<div class="viewcode-block" id="Zeta"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Zeta.html#mindspore.ops.Zeta">[docs]</a><span class="k">class</span> <span class="nc">Zeta</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the Hurwitz zeta function ζ(x,q) of input Tensor.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \zeta \left ( x,q \right )=  \textstyle \sum_{n=0} ^ {\infty} \left ( q+n\right )^{-x}</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - A Tensor, types: float32, float64.</span>
<span class="sd">        - **q** (Tensor) - A Tensor, must have the same shape and type as `x`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same dtype and shape as the x.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If either of `x` and `q` is not tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is neither float32 nor float64.</span>
<span class="sd">        TypeError: If dtype of `q` is neither float32 nor float64.</span>
<span class="sd">        ValueError: If shape of `x` is not same as the `q`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([10.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; q = Tensor(np.array([1.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; zeta = ops.Zeta()</span>
<span class="sd">        &gt;&gt;&gt; z = zeta(x, q)</span>
<span class="sd">        &gt;&gt;&gt; print(z)</span>
<span class="sd">        [1.0009946]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Zeta&quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="Bernoulli"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Bernoulli.html#mindspore.ops.Bernoulli">[docs]</a><span class="k">class</span> <span class="nc">Bernoulli</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Randomly set the elements of output to 0 or 1 with the probability of P which follows the Bernoulli distribution.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.bernoulli` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        seed (int, optional): The seed value for random generating. The value of `seed` must be -1 or a</span>
<span class="sd">            positive integer, and -1 means using the current timestamp. Default: ``-1`` .</span>
<span class="sd">        offset (int, optional): Used to change the starting position during the generation of</span>
<span class="sd">            random number sequence. Default: ``0`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Input Tensor.</span>
<span class="sd">        - **p** (Union[Tensor, float], optional) - Success probability, representing the probability of</span>
<span class="sd">          setting 1 for the corresponding position of the current Tensor. It has the same shape as `x`,</span>
<span class="sd">          the value of `p` must be in the range `[0, 1]`. Default: ``0.5`` .</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - with the same shape and type as `x` .</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor([0.1, 0.2, 0.3], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; bernoulli = ops.Bernoulli()</span>
<span class="sd">        &gt;&gt;&gt; output = bernoulli(input_x, Tensor([1.0]))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 1. 1.]</span>
<span class="sd">        &gt;&gt;&gt; input_p = Tensor([0.0, 1.0, 1.0], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = bernoulli(input_x, input_p)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0. 1. 1.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seed</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Bernoulli&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;p&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;seed&quot;</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">seed</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="n">seed</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Seed must be -1 or a non-negative integer, but got </span><span class="si">{</span><span class="n">seed</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">TridiagonalSolve</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return the results of tridiagonal systems of equations.</span>

<span class="sd">    Solve the tridiagonal systems of equations like:AX = B.</span>
<span class="sd">    and only the main diagonal, superdiagonal and subdiagonal has values.</span>
<span class="sd">    The type of diagonals and rhs should be the same.</span>
<span class="sd">    The penultimate dimension of diagonals must be 3.</span>

<span class="sd">    Args:</span>
<span class="sd">        partial_pivoting (bool): decide if use the method of partial_pivoting. Default: ``True`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **diagonals** [Tensor] - The input tensor A of the equation AX = B, with data type of float32,</span>
<span class="sd">          float64, complex64, complex128.</span>
<span class="sd">          The penultimate dimension of diagonals must be 3.</span>
<span class="sd">          Diagonals and rhs must have the same rank and the same type.</span>
<span class="sd">        - **rhs** [Tensor] - The input tensor B of the equation AX = B, with data type of float32,</span>
<span class="sd">          float64, complex64, complex128.</span>
<span class="sd">          The penultimate dimension of rhs should be the same to the last dimension of diagonals.</span>
<span class="sd">          Diagonals and rhs must have the same rank and the same type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same type and shape as the input &quot;rhs&quot;.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `diagonals` and &quot;rhs&quot; are not a float32, float64, complex64 or complex128.</span>
<span class="sd">        TypeError: If the args `partial_pivoting` is not bool.</span>
<span class="sd">        ValueError: If the last second value of the &quot;diagonals&quot; is not &quot;3&quot;.</span>
<span class="sd">        ValueError: If the last value of the &quot;diagonals&quot; is not equal to the last second value of the &quot;rhs&quot;.</span>
<span class="sd">        ValueError: If diagonals and rhs have different rank of shape.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``CPU``</span>
<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; diagonals = Tensor(np.array([[1.0,2.0,3.0],[2.0,3.0,4.0],[3.0,4.0,5.0]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; rhs = Tensor(np.array([[1.0],[2.0],[3.0]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; y = P.TridiagonalSolve()(diagonals,rhs)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 0. ]</span>
<span class="sd">         [ 1. ]</span>
<span class="sd">         [-0.5]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">partial_pivoting</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;diagonals&#39;</span><span class="p">,</span> <span class="s1">&#39;rhs&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">partial_pivoting</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span>
            <span class="s2">&quot;partial_pivoting&quot;</span><span class="p">,</span> <span class="n">partial_pivoting</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<div class="viewcode-block" id="Renorm"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Renorm.html#mindspore.ops.Renorm">[docs]</a><span class="k">class</span> <span class="nc">Renorm</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Renormalizes the sub-tensors along dimension `dim`, and each sub-tensor&#39;s p-norm should not exceed the</span>
<span class="sd">    &#39;maxnorm&#39;. The values of current sub-tensor don&#39;t need change if the p-norm of the sub-tensor is less than</span>
<span class="sd">    `maxnorm`. Otherwise the sub-tensor needs to be modified to the original value of the corresponding position</span>
<span class="sd">    divided by the p-norm of the substensor and then multiplied by `maxnorm`.</span>

<span class="sd">    Refer to :func:`mindspore.ops.renorm` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        p (int): Power of norm calculation.</span>
<span class="sd">        dim (int): The dimension that expected to get the slice-tensor.</span>
<span class="sd">        maxnorm (float32): Max norm.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - A Tensor, types: float32 or float16.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same dtype and shape as input.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = ops.Renorm(p=1, dim=0, maxnorm=5.)(x)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[1.       1.        1.        ]</span>
<span class="sd">        [1.6666666 1.6666666 1.6666666 ]</span>
<span class="sd">        [1.6666667 1.6666667 1.6666667 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">maxnorm</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Renorm.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">int</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Renorm op don&#39;t support non-positive-norm, but got</span><span class="si">{</span><span class="n">p</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;p&quot;</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;dim&quot;</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;maxnorm&quot;</span><span class="p">,</span> <span class="n">maxnorm</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;p&quot;</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="n">p</span><span class="p">))</span></div>


<div class="viewcode-block" id="Cholesky"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Cholesky.html#mindspore.ops.Cholesky">[docs]</a><span class="k">class</span> <span class="nc">Cholesky</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs the Cholesky decomposition on a single or a batch of</span>
<span class="sd">    symmetric positive-definite matrices.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.cholesky` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        upper (bool, optional): Flag that indicates whether to return a upper or lower triangular matrix.</span>
<span class="sd">            Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor of shape :math:`(*, N, N)`, where :math:`*` is zero or more batch dimensions</span>
<span class="sd">          consisting of symmetric positive-definite matrices, with float32 or float64 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and data type as `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1.0, 1.0], [1.0, 2.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; cholesky = ops.Cholesky(upper=False)</span>
<span class="sd">        &gt;&gt;&gt; output = cholesky(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 0.]</span>
<span class="sd">         [1. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Cholesky&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;upper&#39;</span><span class="p">,</span> <span class="n">upper</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">STFT</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies Short-time Fourier transform (STFT) on input signal.</span>

<span class="sd">    STFT segments the signal into narrow time intervals and takes the Fourier transform</span>
<span class="sd">    of each segment to quantify the change of a nonstationary signal’s frequency</span>
<span class="sd">    and phase content over time.</span>

<span class="sd">    Refer to :func:`mindspore.ops.stft` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        n_fft (int): The size of Fourier transform.</span>
<span class="sd">        hop_length (int): The distance between neighboring sliding window frames.</span>
<span class="sd">        win_length (int): the size of window frame and STFT filter.</span>
<span class="sd">        normalized (bool): controls whether to return the normalized STFT results.</span>
<span class="sd">        onesided (bool): controls whether to return half of results to</span>
<span class="sd">            avoid redundancy for real inputs.</span>
<span class="sd">        return_complex (bool): If ``True`` , return a complex tensor. If False, return</span>
<span class="sd">            a real tensor with an extra last dimension for the real and imaginary components.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Time sequence of stft, must be either a 1-D time tensor or a 2-D tensor.</span>
<span class="sd">        - **window** (Tensor) - the optional window function.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, containing the result after STFT.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import STFT</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = ms.Tensor(np.random.rand(2,7192), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; window = ms.Tensor(np.random.rand(64), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; stft = STFT(64, 16, 64, False, True, True)</span>
<span class="sd">        &gt;&gt;&gt; output = stft(x, window)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 33, 446)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_fft</span><span class="p">,</span> <span class="n">hop_length</span><span class="p">,</span> <span class="n">win_length</span><span class="p">,</span> <span class="n">normalized</span><span class="p">,</span> <span class="n">onesided</span><span class="p">,</span> <span class="n">return_complex</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize STFT.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;window&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;n_fft&#39;</span><span class="p">,</span> <span class="n">n_fft</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;hop_length&#39;</span><span class="p">,</span> <span class="n">hop_length</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;win_length&#39;</span><span class="p">,</span> <span class="n">win_length</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;normalized&#39;</span><span class="p">,</span> <span class="n">normalized</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;onesided&#39;</span><span class="p">,</span> <span class="n">onesided</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;return_complex&#39;</span><span class="p">,</span> <span class="n">return_complex</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">CholeskySolve</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the solution of a set of linear equations with a positive definite matrix,</span>
<span class="sd">    according to its Cholesky decomposition factor `u` , and outputs the result as `c`.</span>

<span class="sd">    If `upper` is set to ``True`` , `u` is upper triangular and `c` is returned such that:</span>

<span class="sd">    .. math::</span>
<span class="sd">        c = (u^{T}u)^{{-1}}b</span>

<span class="sd">    If `upper` is set to `False`, `u` is lower triangular and `c` is returned such that:</span>

<span class="sd">    .. math::</span>
<span class="sd">        c = (uu^{T})^{{-1}}b</span>

<span class="sd">    Args:</span>
<span class="sd">        upper (bool, optional): A flag indicates whether to treat the Cholesky factor</span>
<span class="sd">            as an upper or a lower triangular matrix. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x1** (Tensor) - Tensor of shape :math:`(*, N, M)`, indicating 2D or 3D matrices,</span>
<span class="sd">          with float32 or float64 data type.</span>
<span class="sd">        - **x2** (Tensor) - Tensor of shape :math:`(*, N, N)`, indicating 2D or 3D square matrices composed of</span>
<span class="sd">          upper or lower triangular Cholesky factor, with float32 or float64 data type.</span>
<span class="sd">          x1 and x2 must have the same type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and data type as `x1`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `upper` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `x1` and `x2` is not one of: float64, float32.</span>
<span class="sd">        TypeError: If `x1` is not a Tensor.</span>
<span class="sd">        TypeError: If `x2` is not a Tensor.</span>
<span class="sd">        ValueError: If `x1` and `x2` have different batch size.</span>
<span class="sd">        ValueError: If `x1` and `x2` have different row numbers.</span>
<span class="sd">        ValueError: If `x1` is not 2D or 3D matrices.</span>
<span class="sd">        ValueError: If `x2` is not 2D or 3D square matrices.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([[2, 0, 0], [4, 1, 0], [-1, 1, 2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; net = ops.CholeskySolve()</span>
<span class="sd">        &gt;&gt;&gt; y = net(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[ 5.8125 -2.625   0.625 ]</span>
<span class="sd">         [-2.625   1.25   -0.25  ]</span>
<span class="sd">         [ 0.625  -0.25    0.25  ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize CholeskySolve&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="s1">&#39;x2&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;upper&#39;</span><span class="p">,</span> <span class="n">upper</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<div class="viewcode-block" id="FFTWithSize"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.FFTWithSize.html#mindspore.ops.FFTWithSize">[docs]</a><span class="k">class</span> <span class="nc">FFTWithSize</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fourier transform, can be adjusted by parameters to achieve FFT/IFFT/RFFT/IRFFT.</span>

<span class="sd">    For fft, it computes the following expression:</span>

<span class="sd">    .. math::</span>
<span class="sd">        X[\omega_1, \dots, \omega_d] =</span>
<span class="sd">            \sum_{n_1=0}^{N_1-1} \dots \sum_{n_d=0}^{N_d-1} x[n_1, \dots, n_d]</span>
<span class="sd">             e^{-j\ 2 \pi \sum_{i=0}^d \frac{\omega_i n_i}{N_i}},</span>

<span class="sd">    where :math:`d` = `signal_ndim` is number of dimensions for the</span>
<span class="sd">    signal, and :math:`N_i` is the size of signal dimension :math:`i`.</span>

<span class="sd">    For ifft, it computes the following expression:</span>

<span class="sd">    .. math::</span>
<span class="sd">        X[\omega_1, \dots, \omega_d] =</span>
<span class="sd">            \frac{1}{\prod_{i=1}^d N_i} \sum_{n_1=0}^{N_1-1} \dots \sum_{n_d=0}^{N_d-1} x[n_1, \dots, n_d]</span>
<span class="sd">             e^{\ j\ 2 \pi \sum_{i=0}^d \frac{\omega_i n_i}{N_i}},</span>

<span class="sd">    where :math:`d` = `signal_ndim` is number of dimensions for the</span>
<span class="sd">    signal, and :math:`N_i` is the size of signal dimension :math:`i`.</span>

<span class="sd">    Note:</span>
<span class="sd">        - FFT/IFFT requires complex64 or complex128 inputs, return complex64 or complex128 outputs.</span>
<span class="sd">        - RFFT requires bool, uint8, int8, int16, int32, int64, float32 and float64 inputs,</span>
<span class="sd">          return complex64 or complex128 outputs.</span>
<span class="sd">        - IRFFT requires complex64 or complex128 inputs, return float32 or float64 outputs.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        signal_ndim (int): The number of dimensions in each signal, this controls how many dimensions</span>
<span class="sd">            of the fourier transform are realized, can only be 1, 2 or 3.</span>
<span class="sd">        inverse (bool): Whether it is the inverse transformation, used to select from FFT and RFFT or IFFT and IRFFT.</span>

<span class="sd">            - when set to ``True``: IFFT and IRFFT.</span>
<span class="sd">            - when set to ``False``: FFT and RFFT.</span>

<span class="sd">        real (bool): Whether it is the real transformation, combines with `inverse` to select a specific</span>
<span class="sd">            transformation mode:</span>

<span class="sd">            - `inverse` is ``False`` ,  `real` is ``False`` : corresponds to FFT.</span>
<span class="sd">            - `inverse` is ``True`` , `real` is ``False`` : corresponds to IFFT.</span>
<span class="sd">            - `inverse` is ``False`` , `real` is ``True`` : corresponds to RFFT.</span>
<span class="sd">            - `inverse` is ``True`` , `real` is ``True``  : corresponds to IRFFT.</span>

<span class="sd">        norm (str, optional): The normalization, optional values: [ ``&quot;backward&quot;`` , ``&quot;forward&quot;`` , ``&quot;ortho&quot;`` ].</span>
<span class="sd">            Default value: ``&quot;backward&quot;`` .</span>

<span class="sd">            - ``&quot;backward&quot;`` has the direct transforms unscaled and the inverse transforms scaled by :math:`1/n`,</span>
<span class="sd">              where n is the input x&#39;s element numbers.</span>
<span class="sd">            - ``&quot;ortho&quot;`` has both direct and inverse transforms are scaled by :math:`1/\sqrt n`.</span>
<span class="sd">            - ``&quot;forward&quot;`` has the direct transforms scaled by :math:`1/n` and the inverse transforms unscaled.</span>

<span class="sd">        onesided (bool, optional): Controls whether the input is halved to avoid redundancy. Default: ``True`` .</span>
<span class="sd">        signal_sizes (tuple, optional): Size of the original signal (the signal before rfft, no batch dimension),</span>
<span class="sd">            only in IRFFT mode and set `onesided` to ``True`` requires the parameter, the following conditions must be</span>
<span class="sd">            satisfied. Default: ``()`` .</span>

<span class="sd">            - The length of `signal_sizes` is equal to the signal_ndim of the IRFFT:</span>
<span class="sd">              :math:`len(signal_sizes)=signal_ndim`.</span>
<span class="sd">            - The last dimension of `signal_sizes` divided by 2 is equal to</span>
<span class="sd">              the last dimension of the IRFFT input: :math:`signal_size[-1]/2+1=x.shape[-1]`.</span>
<span class="sd">            - `signal_sizes` has exactly the same dimensions as the input shape</span>
<span class="sd">              except for the last dimension: :math:`signal_sizes[:-1]=x.shape[:-1]`.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The dimension of the input tensor must be greater than or equal to signal_ndim.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        A tensor containing the complex-to-complex, real-to-complex or complex-to-real Fourier transform result.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input type of FFT/IFFT/IRFFT is not one of: complex64, complex128.</span>
<span class="sd">        TypeError: If the input type is not Tensor.</span>
<span class="sd">        ValueError: If `x` dimension is less than signal_ndim.</span>
<span class="sd">        ValueError: If signal_ndim is greater than 3 or less than 1.</span>
<span class="sd">        ValueError: If norm is none of &quot;backward&quot;, &quot;forward&quot; or &quot;ortho&quot;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; # case FFT: signal_ndim: 1, inverse: False, real: False.</span>
<span class="sd">        &gt;&gt;&gt; fft_in = Tensor(np.array([2, 1, 2]), mindspore.complex64)</span>
<span class="sd">        &gt;&gt;&gt; fft_net = ops.FFTWithSize(signal_ndim=1, inverse=False, real=False)</span>
<span class="sd">        &gt;&gt;&gt; fft_output = fft_net(fft_in)</span>
<span class="sd">        &gt;&gt;&gt; print(fft_output)</span>
<span class="sd">        [5.        +0.j         0.5       +0.86602545j 0.50000006-0.8660255j ]</span>
<span class="sd">        &gt;&gt;&gt; # case IFFT: signal_ndim: 1, inverse: True, real: False.</span>
<span class="sd">        &gt;&gt;&gt; ifft_in = fft_output</span>
<span class="sd">        &gt;&gt;&gt; ifft_net = ops.FFTWithSize(signal_ndim=1, inverse=True, real=False)</span>
<span class="sd">        &gt;&gt;&gt; ifft_output = ifft_net(ifft_in)</span>
<span class="sd">        &gt;&gt;&gt; print(ifft_output)</span>
<span class="sd">        [2.        -1.9868216e-08j 0.99999994+0.0000000e+00j</span>
<span class="sd">         1.9999999 +7.9472862e-08j]</span>
<span class="sd">        &gt;&gt;&gt; # case RFFT2D: signal_ndim: 2, inverse: False, real: True.</span>
<span class="sd">        &gt;&gt;&gt; rfft_in = Tensor(np.array([[2, 1, 2], [3, 1, 6]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; rfft_net = ops.FFTWithSize(signal_ndim=2, inverse=False, real=True)</span>
<span class="sd">        &gt;&gt;&gt; rfft_output = rfft_net(rfft_in)</span>
<span class="sd">        &gt;&gt;&gt; print(rfft_output)</span>
<span class="sd">        [[ 1.5000000e+01+1.1920929e-07j -2.3841858e-07+5.1961522e+00j]</span>
<span class="sd">         [-5.0000000e+00-2.9802322e-08j  9.9999988e-01-3.4641016e+00j]]</span>
<span class="sd">        &gt;&gt;&gt; # case IRFFT2D: signal_ndim: 2, inverse: True, real: True.</span>
<span class="sd">        &gt;&gt;&gt; irfft_in = rfft_output</span>
<span class="sd">        &gt;&gt;&gt; irfft_net = ops.FFTWithSize(signal_ndim=2, inverse=True, real=True, signal_sizes=rfft_in.shape)</span>
<span class="sd">        &gt;&gt;&gt; irfft_output = irfft_net(irfft_in)</span>
<span class="sd">        &gt;&gt;&gt; print(irfft_output)</span>
<span class="sd">        [[2.         1.         2.        ]</span>
<span class="sd">         [3.         0.99999994 5.9999995 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">signal_ndim</span><span class="p">,</span> <span class="n">inverse</span><span class="p">,</span> <span class="n">real</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="s2">&quot;backward&quot;</span><span class="p">,</span> <span class="n">onesided</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">signal_sizes</span><span class="o">=</span><span class="p">()):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize FFTWithSize.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;signal_ndim&#39;</span><span class="p">,</span> <span class="n">signal_ndim</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;inverse&#39;</span><span class="p">,</span> <span class="n">inverse</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;real&#39;</span><span class="p">,</span> <span class="n">real</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;norm&#39;</span><span class="p">,</span> <span class="n">norm</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;onesided&#39;</span><span class="p">,</span> <span class="n">onesided</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;signal_sizes&#39;</span><span class="p">,</span> <span class="n">signal_sizes</span><span class="p">,</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="Polar"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Polar.html#mindspore.ops.Polar">[docs]</a><span class="k">class</span> <span class="nc">Polar</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts polar coordinates to Cartesian coordinates.</span>

<span class="sd">    Refer to :func:`mindspore.ops.polar` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **abs** (Tensor) - Radial distance. Tensor of any dimension,</span>
<span class="sd">          must be one of the following types: float32, float64.</span>

<span class="sd">        - **angle** (Tensor) - Polar angle. It has the same shape and dtype as `abs`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and data type as `abs`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; polar = ops.Polar()</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([1, 2]), mindspore.float64)</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([3, 4]), mindspore.float64)</span>
<span class="sd">        &gt;&gt;&gt; output = polar(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-0.9899925 +0.14112001j -1.30728724-1.51360499j]</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(2.1, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(2.1, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = polar(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (-1.0601766+1.8127397j)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Polar&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;abs&#39;</span><span class="p">,</span> <span class="s1">&#39;angle&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="NextAfter"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.NextAfter.html#mindspore.ops.NextAfter">[docs]</a><span class="k">class</span> <span class="nc">NextAfter</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the next representable floating-point value after `x1` towards `x2` element-wise.</span>

<span class="sd">    Say there are two float32 numbers :math:`a, b`, and let the</span>
<span class="sd">    representable delta of float32 data type is :math:`eps`.</span>
<span class="sd">    If :math:`a &lt; b`,</span>
<span class="sd">    then the next representable of :math:`a` towards :math:`b` is :math:`a+eps`,</span>
<span class="sd">    If :math:`a &gt; b`,</span>
<span class="sd">    the next representable of :math:`a` towards :math:`b` is :math:`a-eps`.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} =  nextafter({x1_{i}, x2_{i}})</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x1** (Tensor) - The input Tensor of any dimension.</span>
<span class="sd">          Must be one of the following types: float32, float64.</span>

<span class="sd">        - **x2** (Tensor) - The input Tensor of any dimension.</span>
<span class="sd">          Must be one of the following types: float32, float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and data type as `x1`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `x1` nor `x2` is a Tensor.</span>
<span class="sd">        TypeError: If the dtype of `x1` and `x2` is not one of: float32, float64.</span>
<span class="sd">        TypeError: If the dtypes of `x1` and `x2` are not same.</span>
<span class="sd">        ValueError: If `x1`&#39;s shape is not the same as `x2`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; nextafter = ops.NextAfter()</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.asarray([0.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.asarray([0.1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = nextafter(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.e-45]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize NextAfter&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="s1">&#39;x2&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="TrilIndices"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.TrilIndices.html#mindspore.ops.TrilIndices">[docs]</a><span class="k">class</span> <span class="nc">TrilIndices</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the indices of the lower triangular elements in a `row` * `col` matrix</span>
<span class="sd">    and returns them as a 2-by-N Tensor.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.tril_indices` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        row (int): number of rows in the 2-D matrix.</span>
<span class="sd">        col (int): number of columns in the 2-D matrix.</span>
<span class="sd">        offset (int, optional): diagonal offset from the main diagonal. Default: ``0`` .</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): The specified type of output tensor.</span>
<span class="sd">            An optional data type of ``mstype.int32`` and ``mstype.int64`` . Default: ``mstype.int32`` .</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - indices of the elements in lower triangular part of matrix. The type specified by `dtype`.</span>
<span class="sd">          The shape of output is :math:`(2, tril\_size)`, where :math:`tril\_size` is the number of elements in the</span>
<span class="sd">          lower triangular matrix.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; net = ops.TrilIndices(4, 3, -1, mstype.int64)</span>
<span class="sd">        &gt;&gt;&gt; output = net()</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1 2 2 3 3 3]</span>
<span class="sd">         [0 0 1 0 1 2]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Int64</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize TrilIndices&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s2">&quot;row&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s2">&quot;col&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;offset&quot;</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">valid_values</span> <span class="o">=</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_name</span><span class="p">(</span><span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">valid_values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">MatrixTriangularSolve</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a new tensor with the solution of a linear equation system with an</span>
<span class="sd">    upper or lower triangular matrix.</span>

<span class="sd">    Note:</span>
<span class="sd">        Only GPU platforms now support the broadcast mechanism.</span>

<span class="sd">    Args:</span>
<span class="sd">        lower (bool, optional): If ``True`` , the innermost matrices in `matrix` is</span>
<span class="sd">            are lower triangular. Default: ``True`` .</span>
<span class="sd">        adjoint (bool, optional): Indicates whether the adjoint of the</span>
<span class="sd">            matrix is used during the computation. Default: ``False`` ,  use its transpose instead.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **matrix** (Tensor) - Tensor of shape :math:`(*, M, M)`,</span>
<span class="sd">          with float32, float64, complex64 and complex128 data type.</span>
<span class="sd">        - **rhs** (Tensor) - Tensor of shape :math:`(*, M, N)`,</span>
<span class="sd">          with float32, float64, complex64 and complex128 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the shape of :math:`(*, M, N)` and the same data type as `matrix`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `matrix` or `rhs` is not a Tensor.</span>
<span class="sd">        TypeError: If `lower` or `adjoint` is not bool.</span>
<span class="sd">        ValueError: For GPU platform, if the batch sizes of `matrix` and `rhs` do not satisfy broadcasting rules.</span>
<span class="sd">            For other platforms, if the batch sizes of `matrix` and `rhs` are not equal.</span>
<span class="sd">        ValueError: If the inner-most 2 dimensions of `matrix` are not equal.</span>
<span class="sd">        ValueError: If the second-last dimensions of `matrix` and `rhs` are not equal.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; matrix_triangular_solve = ops.MatrixTriangularSolve(lower=True, adjoint=False)</span>
<span class="sd">        &gt;&gt;&gt; matrix = np.array([[3, 0, 0, 0], [2, 1, 0, 0], [1, 0, 1, 0], [1, 1, 1, 1]])</span>
<span class="sd">        &gt;&gt;&gt; rhs = np.array([[1, 0],[2, 2],[1, 5],[0, 3]])</span>
<span class="sd">        &gt;&gt;&gt; output = matrix_triangular_solve(Tensor(matrix, mindspore.float32), Tensor(rhs, mindspore.float32))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 0.33333334  0.        ]</span>
<span class="sd">         [ 1.3333333   2.        ]</span>
<span class="sd">         [ 0.6666666   5.        ]</span>
<span class="sd">         [-2.3333333  -4.        ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">adjoint</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize MatrixTriangularSolve&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;adjoint&#39;</span><span class="p">,</span> <span class="n">adjoint</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;lower&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">CompareAndBitpack</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compare values of `x` to `threshold` and pack resulting bits into a `uint8`.</span>

<span class="sd">    Each comparison returns a boolean ``True`` (if x_value &gt; threshold) or and ``False`` otherwise.</span>

<span class="sd">    Given an `x` shaped :math:`(s_0, s_1, ..., s_n)`, the output is a `uint8`</span>
<span class="sd">    Tensor shaped :math:`(s_0, s_1, ..., s_n / 8)`.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Input tensor. Values to compare against `threshold` and bitpack. The data type must be</span>
<span class="sd">          bool, float16, float32, float64, int8, int16, int32, int64.</span>
<span class="sd">          Note: Currently, the innermost dimension of the tensor must be divisible by 8.</span>
<span class="sd">        - **threshold** (Tensor) - A 0D Tensor, whose data type is same as x.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the uint8 type.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` or `threshold` is not a Tensor.</span>
<span class="sd">        TypeError: If the dtype of &#39;x&#39; is not one of: bool, float16, float32, float64, int8, int16, int32, int64.</span>
<span class="sd">        TypeError: If `threshold`&#39;s type is not as same &#39;x&#39;.</span>
<span class="sd">        ValueError: If `threshold` is not a 0D Tensor.</span>
<span class="sd">        ValueError: If `x` is a 0D Tensor.</span>
<span class="sd">        ValueError: If the innermost dimension of `x`&#39;s shape is not disvisible by 8.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3, 4, 5, 6, 7, 8]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; threshold = Tensor(6, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; net = ops.CompareAndBitpack()</span>
<span class="sd">        &gt;&gt;&gt; output = net(x, threshold)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [3]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize CompareAndBitPack&quot;&quot;&quot;</span>


<div class="viewcode-block" id="NanToNum"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.NanToNum.html#mindspore.ops.NanToNum">[docs]</a><span class="k">class</span> <span class="nc">NanToNum</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replaces `NaN`, positive infinity and negative infinity values in the input Tensor with the values</span>
<span class="sd">    specified by `nan`, `posinf` and `neginf` respectively.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.nan_to_num` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        nan (float, optional): The value to replace `NaN`. Default value is ``0.0`` .</span>
<span class="sd">        posinf (float, optional): If a Number, the value to replace positive infinity values with. If None, positive</span>
<span class="sd">          infinity values are replaced with the greatest finite value representable by `x`&#39;s dtype.</span>
<span class="sd">          Default value is ``None`` .</span>
<span class="sd">        neginf (float, optional): if a Number, the value to replace negative infinity values with. If None, negative</span>
<span class="sd">          infinity values are replaced with the lowest finite value representable by `x`&#39;s dtype.</span>
<span class="sd">          Default value is ``None`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Input Tensor of any dimensions. Supported data types: float32 or float16.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; nan_to_num = ops.NanToNum()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([float(&#39;nan&#39;), float(&#39;inf&#39;), -float(&#39;inf&#39;), 3.14]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = nan_to_num(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0.0000000e+00  3.4028235e+38 -3.4028235e+38  3.1400001e+00]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">posinf</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">neginf</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize NanToNum&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">nan</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;nan&quot;</span><span class="p">,</span> <span class="n">nan</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;nan_none&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">posinf</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;posinf&quot;</span><span class="p">,</span> <span class="n">posinf</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;posinf_none&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">neginf</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;neginf&quot;</span><span class="p">,</span> <span class="n">neginf</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;neginf_none&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="Orgqr"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Orgqr.html#mindspore.ops.Orgqr">[docs]</a><span class="k">class</span> <span class="nc">Orgqr</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the explicit representation of the orthogonal matrix :math:`Q`</span>
<span class="sd">    returned by :class:`mindspore.ops.Geqrf`.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.orgqr` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Tensor of shape :math:`(*, M, N)`, indicating 2D or 3D matrices,</span>
<span class="sd">          with float32, float64, complex64 and complex128 data type.</span>
<span class="sd">        - **tau** (Tensor) - Indicates the reflecting coefficient in Householder transformation, it has</span>
<span class="sd">          shape :math:`(*, K)`, where `K` is less than or equal to `N`, and it has the same type as `x`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and data type as `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[-114.6, 10.9, 1.1], [-0.304, 38.07, 69.38], [-0.45, -0.17, 62.]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; tau = Tensor(np.array([1.55, 1.94, 0.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; net = ops.Orgqr()</span>
<span class="sd">        &gt;&gt;&gt; y = net(x, tau)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[-0.54999995 -0.2128925   0.8137956 ]</span>
<span class="sd">         [ 0.47119996 -0.8752807   0.08240613]</span>
<span class="sd">         [ 0.69749993  0.42560163  0.57772595]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Orgqr&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;tau&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="TriuIndices"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.TriuIndices.html#mindspore.ops.TriuIndices">[docs]</a><span class="k">class</span> <span class="nc">TriuIndices</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the indices of the upper triangular elements in a `row` * `col` matrix</span>
<span class="sd">    and returns them as a 2-by-N Tensor.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.triu_indices` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        row (int): number of rows in the 2-D matrix.</span>
<span class="sd">        col (int): number of columns in the 2-D matrix.</span>
<span class="sd">        offset (int, optional): diagonal offset from the main diagonal. Default: ``0`` .</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): The specified type of output tensor.</span>
<span class="sd">            An optional data type of ``mstype.int32`` and ``mstype.int64`` . Default: ``mstype.int32`` .</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - indices of the elements in lower triangular part of matrix. The type specified by `dtype`.</span>
<span class="sd">          The shape of output is :math:`(2, tril\_size)`, where :math:`tril\_size` is the number of elements in the</span>
<span class="sd">          lower triangular matrix.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; net = ops.TriuIndices(5, 4, 2, mstype.int64)</span>
<span class="sd">        &gt;&gt;&gt; output = net()</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0 0 1]</span>
<span class="sd">         [2 3 3]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Int64</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize TriuIndices&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s2">&quot;row&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s2">&quot;col&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;offset&quot;</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">valid_values</span> <span class="o">=</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_name</span><span class="p">(</span><span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">valid_values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">Fmin</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the minimum of input tensors element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.fmin` for more detail.</span>

<span class="sd">    Supported Platforms:</span>


<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([1.0, 5.0, 3.0]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([4.0, 2.0, 6.0]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; fmin = ops.Fmin()</span>
<span class="sd">        &gt;&gt;&gt; output = fmin(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 2. 3.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Fmin&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;ignore_nan&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x1, x2&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>


<div class="viewcode-block" id="Fmax"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Fmax.html#mindspore.ops.Fmax">[docs]</a><span class="k">class</span> <span class="nc">Fmax</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the maximum of input tensors element-wise.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.fmax` for more detail.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([1.0, 5.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([4.0, 2.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; fmax = ops.Fmax()</span>
<span class="sd">        &gt;&gt;&gt; output = fmax(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [4. 5. 6.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Fmax&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;ignore_nan&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x1, x2&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<span class="k">class</span> <span class="nc">Eig</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the eigenvalues and eigenvectors of a square matrix(batch square matrices).</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        compute_v (bool, optional): If ``True`` , compute both eigenvalues and eigenvectors;</span>
<span class="sd">            If `False`, just eigenvalues will be computed. Default: ``False`` .</span>
<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Square matrices of shape :math:`(*, N, N)`,</span>
<span class="sd">          with float32, float64, complex64 or complex128 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **eigen_values** (Tensor) - Shape :math:`(*, N)`. Each inner most vector represents eigenvalues of</span>
<span class="sd">          the corresponding matrix. The eigenvalues may not have an order.</span>
<span class="sd">        - **eigen_vectors** (Tensor) - If `compute_v` is `False`, it’s an empty tensor. Otherwise, this tensor</span>
<span class="sd">          has shape :math:`(*, N, N)`, whose columns represent normalized (unit length) eigenvectors of corresponding</span>
<span class="sd">          eigenvalues.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `compute_v` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `x` is not one of: float64, float32, complex64 or complex128.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        ValueError: If `x` is not a square(batch squares).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1.0, 0.0], [0.0, 2.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; eig = ops.Eig(compute_v=True)</span>
<span class="sd">        &gt;&gt;&gt; u, v = eig(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(u)</span>
<span class="sd">        [1.+0.j 2.+0.j]</span>
<span class="sd">        &gt;&gt;&gt; print(v)</span>
<span class="sd">        [[1.+0.j 0.+0.j]</span>
<span class="sd">         [0.+0.j 1.+0.j]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">compute_v</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Eig&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;eigen_values&#39;</span><span class="p">,</span> <span class="s1">&#39;eigen_vectors&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;compute_v&#39;</span><span class="p">,</span> <span class="n">compute_v</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">SelfAdjointEig</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the eigenvalues and (optionally) eigenvectors of each inner matrix in input</span>
<span class="sd">    such that input[..., :, :] = v[..., :, :] * diag(e[..., :]).</span>
<span class="sd">    The eigenvalues are sorted in non-decreasing order.</span>

<span class="sd">    Args:</span>
<span class="sd">         compute_v(bool): If ``True``  then eigenvectors will be computed and returned in v;</span>
<span class="sd">              If ``False`` , only the eigenvalues will be computed. Default: ``True`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">         - **x** (Tensor) - Must be one of the following types:</span>
<span class="sd">           float64, float32, complex64, complex128. Tensor input of shape :math:`[...,N, N]`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">         - **eigen_value** (Tensor) - Has the same type as input, the shape is :math:`[...,N]`.</span>
<span class="sd">         - **eigen_vector** (Tensor) - If `compute_v` is `False`, it’s an empty tensor.</span>
<span class="sd">           Otherwise, it has the same type and shape as input, the shape is the same as the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">         TypeError: If `compute_v` is not a bool.</span>
<span class="sd">         TypeError: If dtype of `x` is not one of: float64, float32, complex64 or complex128.</span>
<span class="sd">         TypeError: If `x` is not a Tensor.</span>
<span class="sd">         ValueError: If `x` is not a square(batch squares).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">         ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">           &gt;&gt;&gt; from mindspore.ops.operations.math_ops import SelfAdjointEig</span>
<span class="sd">           &gt;&gt;&gt; input_x = Tensor(np.array([[1.0, 0.0], [0.0, 2.0]]).astype(np.float32))</span>
<span class="sd">           &gt;&gt;&gt; SelfAdjointEig = SelfAdjointEig()</span>
<span class="sd">           &gt;&gt;&gt; eigen_value, eigen_vector = SelfAdjointEig(input_x)</span>
<span class="sd">           &gt;&gt;&gt; print(eigen_value)</span>
<span class="sd">           [1.  2.]</span>
<span class="sd">           &gt;&gt;&gt; print(eigen_vector)</span>
<span class="sd">           [[1.  0.]</span>
<span class="sd">            [0.  1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">compute_v</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SelfAdjointEig.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;eigen_value&#39;</span><span class="p">,</span> <span class="s1">&#39;eigen_vector&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;compute_v&quot;</span><span class="p">,</span> <span class="n">compute_v</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Qr</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the QR decomposition of one or more matrices. If `full_matrices` is ``True`` , compute full-sized q and r,</span>
<span class="sd">    If ``False`` (the default), compute the P columns of q where P is minimum of the 2 innermost dimensions of x.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        full_matrices (bool, optional): Whether compute full-sized QR decomposition. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - A matrix to be calculated. The matrix must be at least two dimensions.</span>
<span class="sd">          types: float16, float32, float64, complex64, complex128.</span>
<span class="sd">          Define the shape of x as :math:`(..., m, n)` p as the minimum values of m and n.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **q** (Tensor) - The orthonormal matrices of x.</span>
<span class="sd">          If `full_matrices` is ``True`` , the shape is :math:`(m, m)`, else the shape is :math:`(m, p)`.</span>
<span class="sd">          The dtype of `q` is same as `x`.</span>
<span class="sd">        - **r** (Tensor) - The upper triangular matrices of x.</span>
<span class="sd">          If `full_matrices` is ``True`` , the shape is :math:`(m, n)`, else the shape is :math:`(p, n)`.</span>
<span class="sd">          The dtype of `r` is same as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `full_matrices` is not a bool.</span>
<span class="sd">        ValueError: If the dimension of `x` is less than 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; qr_op = ops.Qr(full_matrices=False)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[20., -31, 7], [4, 270, -90], [-8, 17, -32]], mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; q, r = qr_op(x)</span>
<span class="sd">        &gt;&gt;&gt; print(q)</span>
<span class="sd">        [[-0.912871    0.16366126  0.37400758]</span>
<span class="sd">         [-0.18257418 -0.9830709  -0.01544376]</span>
<span class="sd">         [ 0.36514837 -0.08238228  0.92729706]]</span>
<span class="sd">        &gt;&gt;&gt; print(r)</span>
<span class="sd">        [[ -21.908903  -14.788506  -1.6431675]</span>
<span class="sd">        [    0.       -271.9031    92.25824  ]</span>
<span class="sd">        [    0.          0.       -25.665514 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Qr&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;full_matrices&#39;</span><span class="p">,</span> <span class="n">full_matrices</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Cauchy</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create a tensor of shape `size` with random numbers drawn from Cauchy distribution.</span>
<span class="sd">    It is defined as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        f(x)= \frac{1}{\pi} \frac{\sigma}{(x-median)^2 +\sigma^2}</span>

<span class="sd">    Args:</span>
<span class="sd">        size (list[int]): The size of tensor.</span>
<span class="sd">        median (float, optional): the location parameter, specifying the location</span>
<span class="sd">            of the peak of the distribution. Default: 0.0.</span>
<span class="sd">        sigma (float, optional): the scale parameter which specifies the half-width</span>
<span class="sd">            at half-maximum. Default: 1.0.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor with cauchy distribution data. Tensor shape is size, and data type is float32.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `sigma` is not a float.</span>
<span class="sd">        TypeError: If `median` is not a float.</span>
<span class="sd">        TypeError: If `size` is not a list.</span>
<span class="sd">        ValueError: If `size` list is empty.</span>
<span class="sd">        ValueError: If data of `size` is not a positive integer.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; size = [1]</span>
<span class="sd">        &gt;&gt;&gt; net = ops.Cauchy(size)</span>
<span class="sd">        &gt;&gt;&gt; y = net()</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [0.03128606]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">median</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;median&#39;</span><span class="p">,</span> <span class="n">median</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;sigma&#39;</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;size&#39;</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">size_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">size_</span><span class="p">,</span> <span class="s1">&#39;size[</span><span class="si">%d</span><span class="s1">]&#39;</span> <span class="o">%</span> <span class="n">index</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<div class="viewcode-block" id="Ormqr"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Ormqr.html#mindspore.ops.Ormqr">[docs]</a><span class="k">class</span> <span class="nc">Ormqr</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.</span>
<span class="sd">    Multiplies a(m, n) matrix C (given by other) with a matrix Q, where Q is represented using Householder</span>
<span class="sd">    reflectors (x, tau), which is the output of geqrf().</span>

<span class="sd">    Refer to :func:`mindspore.ops.ormqr` for more details.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        left (bool, optional): controls the order of multiplication. If ``True`` , compute op(Q)*C.</span>
<span class="sd">            If ``False`` , compute C*op(Q). Default: ``True`` .</span>
<span class="sd">        transpose(bool, optional): controls whether the matrix Q is conjugate transposed or not.Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Tensor of shape :math:`(*, mn, k)` where the value of mn depending on `left`,</span>
<span class="sd">          When `left` is ``True``, the value of mn is equal to m; otherwise, the value of mn is equal to n.</span>
<span class="sd">          and `*` is zero or more batch dimensions.</span>
<span class="sd">        - **tau** (Tensor) - Tensor of shape :math:`(*, min(mn, k))` where `*` is zero or more batch dimensions,</span>
<span class="sd">          and its type is the same as `x`.</span>
<span class="sd">        - **other** (Tensor) - Tensor of shape :math:`(*, m, n)` where `*` is zero or more batch dimensions,</span>
<span class="sd">          and its type is the same as `x`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - the output Tensor, has the same shape and data type as `other`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` or `tau` or `other` is not Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` or `tau` or `other` is not one of: float64, float32, complex64, complex128.</span>
<span class="sd">        ValueError: If `x` or `other` is less than 2D.</span>
<span class="sd">        ValueError: If rank(x) - rank(tau) != 1.</span>
<span class="sd">        ValueError: If tau.shape[:-2] != x.shape[:-2]</span>
<span class="sd">        ValueError: If other.shape[:-2] != x.shape[:-2]</span>
<span class="sd">        ValueError: If left == True, other.shape[-2] &lt; tau.shape[-1].</span>
<span class="sd">        ValueError: If left == True, other.shape[-2] != x.shape[-2].</span>
<span class="sd">        ValueError: If left == False, other.shape[-1] &lt; tau.shape[-1].</span>
<span class="sd">        ValueError: If left == False, other.shape[-1] != x.shape[-2].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[-114.6, 10.9, 1.1], [-0.304, 38.07, 69.38], [-0.45, -0.17, 62]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; tau = Tensor(np.array([1.55, 1.94, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor(np.array([[-114.6, 10.9, 1.1],</span>
<span class="sd">        ...                          [-0.304, 38.07, 69.38],</span>
<span class="sd">        ...                          [-0.45, -0.17, 62]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; net = ops.Ormqr()</span>
<span class="sd">        &gt;&gt;&gt; y = net(x, tau, other)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[  63.82713   -13.823125 -116.28614 ]</span>
<span class="sd">         [ -53.659264  -28.157839  -70.42702 ]</span>
<span class="sd">         [ -79.54292    24.00183   -41.34253 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transpose</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Ormqr&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;tau&#39;</span><span class="p">,</span> <span class="s1">&#39;other&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">left</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="n">left</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transpose</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;transpose&#39;</span><span class="p">,</span> <span class="n">transpose</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">left</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;transpose&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">Roll</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Rolls the elements of a tensor along an axis.</span>

<span class="sd">    Refer to :func:`mindspore.ops.roll` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        shift (Union[list(int), tuple(int), int]): Specifies the number of places by which elements are shifted</span>
<span class="sd">            positively (towards larger indices) along the specified dimension. Negative shifts will roll the elements</span>
<span class="sd">            in the opposite direction.</span>
<span class="sd">        axis (Union[list(int), tuple(int), int]): Specifies the dimension indexes of shape to be rolled.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and type as `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([0, 1, 2, 3, 4]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; op = ops.Roll(shift=2, axis=0)</span>
<span class="sd">        &gt;&gt;&gt; output = op(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [3. 4. 0. 1. 2.]</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; op = ops.Roll(shift=-1, axis=0)</span>
<span class="sd">        &gt;&gt;&gt; output = op(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[5. 6. 7. 8. 9.]</span>
<span class="sd">         [0. 1. 2. 3. 4.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shift</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Roll&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;GPU&quot;</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;shift&quot;</span><span class="p">,</span> <span class="n">shift</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shift</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;shift&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">shift</span><span class="p">])</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">axis</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shift</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">shift</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;shift size&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">axis</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;shift size&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="n">axis</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shift</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_is_int</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>