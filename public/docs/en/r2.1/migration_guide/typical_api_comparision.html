<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Differences Between MindSpore and PyTorch &mdash; MindSpore master documentation</title>
      <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/training.css" type="text/css" /><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/js/training.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Constructing MindSpore Network" href="model_development/model_development.html" />
    <link rel="prev" title="Model Analysis and Preparation" href="analysis_and_preparation.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../design/overview.html">MindSpore Design Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/programming_paradigm.html">Functional and Object-Oriented Fusion Programming Paradigm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/all_scenarios.html">Full-scenarios Unified Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/dynamic_graph_and_static_graph.html">Combination of Dynamic and Static Graphs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/pluggable_device.html">Third-Party Hardware Interconnection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/distributed_training_design.html">Native Distributed Parallel Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/graph_fusion_engine.html">Graph-Kernel Fusion Acceleration Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/data_engine.html">High Performance Data Processing Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/official_models.html">Official Models</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Mapping</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/pytorch_api_mapping.html">PyTorch and MindSpore API Mapping Table</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/tensorflow_api_mapping.html">TensorFlow and MindSpore API Mapping Table</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Migration Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview of Migration Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="enveriment_preparation.html">Environment Preparation and Information Acquisition</a></li>
<li class="toctree-l1"><a class="reference internal" href="analysis_and_preparation.html">Model Analysis and Preparation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Differences Between MindSpore and PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_development/model_development.html">Constructing MindSpore Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="debug_and_tune.html">Debugging and Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="sample_code.html">Network Migration Debugging Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="migrator_with_tools.html">Application Practice Guide for Network Migration Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">FAQs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Syntax Support</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax_support.html">Static Graph Syntax Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax/operators.html">Static Graph Syntax —— Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax/statements.html">Static Graph Syntax —— Python Statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax/python_builtin_functions.html">Static Graph Syntax —— Python Built-in Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/index_support.html">Tensor Index Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Environment Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/env_var_list.html">Environment Variables</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/data_processing.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/implement_problem.html">Implement Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/network_compilation.html">Network Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/operators_compile.html">Operators Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/usage_migrate_3rd.html">Migration from a Third-party Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/precision_tuning.html">Precision Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/distributed_parallel.html">Distributed Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/feature_advice.html">Feature Advice</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Differences Between MindSpore and PyTorch</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/migration_guide/typical_api_comparision.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="differences-between-mindspore-and-pytorch">
<h1>Differences Between MindSpore and PyTorch<a class="headerlink" href="#differences-between-mindspore-and-pytorch" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.1/docs/mindspore/source_en/migration_guide/typical_api_comparision.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.1/resource/_static/logo_source_en.png" /></a></p>
<section id="differences-between-mindspore-and-pytorch-apis">
<h2>Differences Between MindSpore and PyTorch APIs<a class="headerlink" href="#differences-between-mindspore-and-pytorch-apis" title="Permalink to this headline"></a></h2>
<section id="torch-device">
<h3>torch.device<a class="headerlink" href="#torch-device" title="Permalink to this headline"></a></h3>
<p>When building a model, PyTorch usually uses torch.device to specify the device to which the model and data are bound, that is, whether the device is on the CPU or GPU. If multiple GPUs are supported, you can also specify the GPU sequence number. After binding a device, you need to deploy the model and data to the device. The code is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="c1"># bind to the GPU 0 if GPU is available, otherwise bind to CPU</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span> <span class="c1"># Single GPU or CPU</span>
<span class="c1"># deploy model to specified hardware</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># deploy data to specified hardware</span>
<span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># distribute training on multiple GPUs</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># set available device</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;CUDA_VISIBLE_DEVICE&#39;</span><span class="p">]</span><span class="o">=</span><span class="s1">&#39;1&#39;</span>
    <span class="n">model</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
</pre></div>
</div>
<p>In MindSpore, the <code class="docutils literal notranslate"><span class="pre">device_target</span></code> parameter in context specifies the device bound to the model, and the <code class="docutils literal notranslate"><span class="pre">device_id</span> <span class="pre">parameter</span></code> specifies the device sequence number. Different from PyTorch, once the device is successfully set, the input data and model are copied to the specified device for execution by default. You do not need to and cannot change the type of the device where the data and model run. The sample code is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s1">&#39;Ascend&#39;</span><span class="p">,</span> <span class="n">device_id</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># define net</span>
<span class="n">Model</span> <span class="o">=</span> <span class="o">..</span>
<span class="c1"># define dataset</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="o">..</span>
<span class="c1"># training, automatically deploy to Ascend according to device_target</span>
<span class="n">Model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<p>In addition, the <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> returned after the network runs is copied to the CPU device by default. You can directly access and modify the <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, including converting the <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> to the <code class="docutils literal notranslate"><span class="pre">numpy</span></code> format. Unlike PyTorch, you do not need to run the <code class="docutils literal notranslate"><span class="pre">tensor.cpu</span></code> command and then convert the <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> to the NumPy format.</p>
</section>
<section id="nn-module">
<h3>nn.Module<a class="headerlink" href="#nn-module" title="Permalink to this headline"></a></h3>
<p>When PyTorch is used to build a network structure, the <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> class is used. Generally, network elements are defined and initialized in the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> function, and the graph structure expression of the network is defined in the <code class="docutils literal notranslate"><span class="pre">forward</span></code> function. Objects of these classes are invoked to build and train the entire model. <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> not only provides us with graph building interfaces, but also provides us with some common <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html">APIs</a> to help us execute more complex logic.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">nn.Cell</span></code> class in MindSpore plays the same role as the <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> class in PyTorch. Both classes are used to build graph structures. MindSpore also provides various <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.1/api_python/nn/mindspore.nn.Cell.html">APIs</a> for developers. Although the names are not the same, the mapping of common functions in <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> can be found in <code class="docutils literal notranslate"><span class="pre">nn.Cell</span></code>.</p>
<p>The following uses several common methods as examples:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="text-left head"><p>Common Method</p></th>
<th class="text-left head"><p>nn.Module</p></th>
<th class="text-left head"><p>nn.Cell</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Obtain child elements.</p></td>
<td class="text-left"><p>named_children</p></td>
<td class="text-left"><p>cells_and_names</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Add subelements.</p></td>
<td class="text-left"><p>add_module</p></td>
<td class="text-left"><p>insert_child_to_cell</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Obtain parameters of an element.</p></td>
<td class="text-left"><p>parameters</p></td>
<td class="text-left"><p>get_parameters</p></td>
</tr>
</tbody>
</table>
</section>
<section id="data-size">
<h3>Data Size<a class="headerlink" href="#data-size" title="Permalink to this headline"></a></h3>
<p>In PyTorch, there are four types of objects that can store data: <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="docutils literal notranslate"><span class="pre">Variable</span></code>, <code class="docutils literal notranslate"><span class="pre">Parameter</span></code>, and <code class="docutils literal notranslate"><span class="pre">Buffer</span></code>. The default behaviors of the four types of objects are different. When the gradient is not required, the <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> and <code class="docutils literal notranslate"><span class="pre">Buffer</span></code> data objects are used. When the gradient is required, the <code class="docutils literal notranslate"><span class="pre">Variable</span></code> and <code class="docutils literal notranslate"><span class="pre">Parameter</span></code> data objects are used. When PyTorch designs the four types of data objects, the functions are redundant. (In addition, <code class="docutils literal notranslate"><span class="pre">Variable</span></code> will be discarded.)</p>
<p>MindSpore optimizes the data object design logic and retains only two types of data objects: <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> and <code class="docutils literal notranslate"><span class="pre">Parameter</span></code>. The <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> object only participates in calculation and does not need to perform gradient derivation or parameter update on it. The <code class="docutils literal notranslate"><span class="pre">Parameter</span></code> data object has the same meaning as the <code class="docutils literal notranslate"><span class="pre">Parameter</span></code> data object of PyTorch. The <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> attribute determines whether to perform gradient derivation or parameter update on the <code class="docutils literal notranslate"><span class="pre">Parameter</span></code> data object. During network migration, all data objects that are not updated in PyTorch can be declared as <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> in MindSpore.</p>
</section>
<section id="gradient-derivation">
<h3>Gradient Derivation<a class="headerlink" href="#gradient-derivation" title="Permalink to this headline"></a></h3>
<p>The operator and interface differences involved in gradient derivation are mainly caused by different automatic differentiation principles of MindSpore and PyTorch.</p>
</section>
<section id="torch-no-grad">
<h3>torch.no_grad<a class="headerlink" href="#torch-no-grad" title="Permalink to this headline"></a></h3>
<p>In PyTorch, by default, information required for backward propagation is recorded when forward computation is performed. In the inference phase or in a network where backward propagation is not required, this operation is redundant and time-consuming. Therefore, PyTorch provides <code class="docutils literal notranslate"><span class="pre">torch.no_grad</span></code> to cancel this process.</p>
<p>MindSpore constructs a backward graph based on the forward graph structure only when <code class="docutils literal notranslate"><span class="pre">GradOperation</span></code> is invoked. No information is recorded during forward execution. Therefore, MindSpore does not need this interface. It can be understood that forward calculation of MindSpore is performed in <code class="docutils literal notranslate"><span class="pre">torch.no_grad</span></code> mode.</p>
</section>
<section id="retain-graph">
<h3>retain_graph<a class="headerlink" href="#retain-graph" title="Permalink to this headline"></a></h3>
<p>PyTorch is function-based automatic differentiation. Therefore, by default, the recorded information is automatically cleared after each backward propagation is performed for the next iteration. As a result, when we want to reuse the backward graph and gradient information, the information fails to be obtained because it has been deleted. Therefore, PyTorch provides <code class="docutils literal notranslate"><span class="pre">backward(retain_graph=True)</span></code> to proactively retain the information.</p>
<p>MindSpore does not require this function. MindSpore is an automatic differentiation based on the computational graph. The backward graph information is permanently recorded in the computational graph after <code class="docutils literal notranslate"><span class="pre">GradOperation</span></code> is invoked. You only need to invoke the computational graph again to obtain the gradient information.</p>
</section>
<section id="high-order-derivatives">
<h3>High-order Derivatives<a class="headerlink" href="#high-order-derivatives" title="Permalink to this headline"></a></h3>
<p>Automatic differentiation based on computational graphs also has an advantage that we can easily implement high-order derivation. After the <code class="docutils literal notranslate"><span class="pre">GradOperation</span></code> operation is performed on the forward graph for the first time, a first-order derivative may be obtained. In this case, the computational graph is updated to a backward graph structure of the forward graph + the first-order derivative. However, after the <code class="docutils literal notranslate"><span class="pre">GradOperation</span></code> operation is performed on the updated computational graph again, a second-order derivative may be obtained, and so on. Through automatic differentiation based on computational graph, we can easily obtain the higher order derivative of a network.</p>
</section>
</section>
<section id="differences-between-mindspore-and-pytorch-operators">
<h2>Differences Between MindSpore and PyTorch Operators<a class="headerlink" href="#differences-between-mindspore-and-pytorch-operators" title="Permalink to this headline"></a></h2>
<p>Most operators and APIs of MindSpore are similar to those of TensorFlow, but the default behavior of some operators is different from that of PyTorch or TensorFlow. During network script migration, if developers do not notice these differences and directly use the default behavior, the network may be inconsistent with the original migration network, affecting network training. It is recommended that developers align not only the used operators but also the operator attributes during network migration. Here we summarize several common difference operators.</p>
<section id="nn-dropout">
<h3>nn.Dropout<a class="headerlink" href="#nn-dropout" title="Permalink to this headline"></a></h3>
<p>Dropout is often used to prevent training overfitting. It has an important probability value parameter. The meaning of this parameter in MindSpore is completely opposite to that in PyTorch and TensorFlow.</p>
<p>In MindSpore, the probability value corresponds to the <code class="docutils literal notranslate"><span class="pre">keep_prob</span></code> attribute of the Dropout operator, indicating the probability that the input is retained. <code class="docutils literal notranslate"><span class="pre">1-keep_prob</span></code> indicates the probability that the input is set to 0.</p>
<p>In PyTorch and TensorFlow, the probability values correspond to the attributes <code class="docutils literal notranslate"><span class="pre">p</span></code> and <code class="docutils literal notranslate"><span class="pre">rate</span></code> of the Dropout operator, respectively. They indicate the probability that the input is set to 0, which is opposite to the meaning of <code class="docutils literal notranslate"><span class="pre">keep_prob</span></code> in MindSpore.nn.Dropout.</p>
<p>For more information, visit <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.1/api_python/nn/mindspore.nn.Dropout.html#mindspore.nn.Dropout">MindSpore Dropout</a>, <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html">PyTorch Dropout</a>, and <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/nn/dropout">TensorFlow Dropout</a>.</p>
</section>
<section id="nn-batchnorm2d">
<h3>nn.BatchNorm2d<a class="headerlink" href="#nn-batchnorm2d" title="Permalink to this headline"></a></h3>
<p>BatchNorm is a special regularization method in the CV field. It has different computation processes during training and inference and is usually controlled by operator attributes. BatchNorm of MindSpore and PyTorch uses two different parameter groups at this point.</p>
<ul class="simple">
<li><p>Difference 1</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">torch.nn.BatchNorm2d</span></code> status under different parameters</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="text-left head"><p>training</p></th>
<th class="text-left head"><p>track_running_stats</p></th>
<th class="text-left head"><p>Status</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>True</p></td>
<td class="text-left"><p>True</p></td>
<td class="text-left"><p>Expected training status. <code class="docutils literal notranslate"><span class="pre">running_mean</span></code> and <code class="docutils literal notranslate"><span class="pre">running_var</span></code> trace the statistical features of the batch in the entire training process. Each group of input data is normalized based on the mean and var statistical features of the current batch, and then <code class="docutils literal notranslate"><span class="pre">running_mean</span></code> and <code class="docutils literal notranslate"><span class="pre">running_var</span></code> are updated.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>True</p></td>
<td class="text-left"><p>False</p></td>
<td class="text-left"><p>Each group of input data is normalized based on the statistics feature of the current batch, but the <code class="docutils literal notranslate"><span class="pre">running_mean</span></code> and <code class="docutils literal notranslate"><span class="pre">running_var</span></code> parameters do not exist.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>False</p></td>
<td class="text-left"><p>True</p></td>
<td class="text-left"><p>Expected inference status. The BN uses <code class="docutils literal notranslate"><span class="pre">running_mean</span></code> and <code class="docutils literal notranslate"><span class="pre">running_var</span></code> for normalization and does not update them.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>False</p></td>
<td class="text-left"><p>False</p></td>
<td class="text-left"><p>The effect is the same as that of the second status. The only difference is that this is the inference status and does not learn the weight and bias parameters. Generally, this status is not used.</p></td>
</tr>
</tbody>
</table>
<p><code class="docutils literal notranslate"><span class="pre">mindspore.nn.BatchNorm2d</span></code> status under different parameters</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="text-left head"><p>use_batch_statistics</p></th>
<th class="text-left head"><p>Status</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>True</p></td>
<td class="text-left"><p>Expected training status. <code class="docutils literal notranslate"><span class="pre">moving_mean</span></code> and <code class="docutils literal notranslate"><span class="pre">moving_var</span></code> trace the statistical features of the batch in the entire training process. Each group of input data is normalized based on the mean and var statistical features of the current batch, and then <code class="docutils literal notranslate"><span class="pre">moving_mean</span></code> and <code class="docutils literal notranslate"><span class="pre">moving_var</span></code> are updated.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Fasle</p></td>
<td class="text-left"><p>Expected inference status. The BN uses <code class="docutils literal notranslate"><span class="pre">moving_mean</span></code> and <code class="docutils literal notranslate"><span class="pre">moving_var</span></code> for normalization and does not update them.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>None</p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">use_batch_statistics</span></code> is automatically set. For training, set <code class="docutils literal notranslate"><span class="pre">use_batch_statistics</span></code> to <code class="docutils literal notranslate"><span class="pre">True</span></code>. For inference, <code class="docutils literal notranslate"><span class="pre">set</span> <span class="pre">use_batch_statistics</span></code> to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></td>
</tr>
</tbody>
</table>
<p>Compared with <code class="docutils literal notranslate"><span class="pre">torch.nn.BatchNorm2d</span></code>, <code class="docutils literal notranslate"><span class="pre">mindspore.nn.BatchNorm2d</span></code> does not have two redundant states and retains only the most commonly used training and inference states.</p>
<ul class="simple">
<li><p>Difference 2</p></li>
</ul>
<p>The meaning of the momentum parameter of the BatchNorm series operators in MindSpore is opposite to that in PyTorch. The relationship is as follows:</p>
<div class="math notranslate nohighlight">
\[momentum_{pytorch} = 1 - momentum_{mindspore}\]</div>
<p>References: <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.1/api_python/nn/mindspore.nn.BatchNorm2d.html">mindspore.nn.BatchNorm2d</a>, <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html">torch.nn.BatchNorm2d</a></p>
</section>
<section id="ops-transpose">
<h3>ops.Transpose<a class="headerlink" href="#ops-transpose" title="Permalink to this headline"></a></h3>
<p>During axis transformation, PyTorch usually uses two operators: <code class="docutils literal notranslate"><span class="pre">Tensor.permute</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.transpose</span></code>. MindSpore and TensorFlow provide only the <code class="docutils literal notranslate"><span class="pre">transpose</span></code> operator. Note that the <code class="docutils literal notranslate"><span class="pre">Tensor.permute</span></code> contains the functions of the <code class="docutils literal notranslate"><span class="pre">torch.transpose</span></code>. The <code class="docutils literal notranslate"><span class="pre">torch.transpose</span></code> supports only the exchange of two axes at the same time, whereas the <code class="docutils literal notranslate"><span class="pre">Tensor.permute</span></code> supports the exchange of multiple axes at the same time.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># PyTorch code</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">ret1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">dim0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ret1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">ret2</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ret2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>torch.Size([4, 2, 3, 1])
torch.Size([4, 3, 2, 1])
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">transpose</span></code> operator of MindSpore has the same function as that of TensorFlow. Although the operator is named <code class="docutils literal notranslate"><span class="pre">transpose</span></code>, it can transform multiple axes at the same time, which is equivalent to <code class="docutils literal notranslate"><span class="pre">Tensor.permute</span></code>. Therefore, MindSpore does not provide operators similar to <code class="docutils literal notranslate"><span class="pre">torch.tranpose</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># MindSpore code</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">ret</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Transpose</span><span class="p">()(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ret</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>(4, 3, 2, 1)
</pre></div>
</div>
<p>For more information, visit <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.1/api_python/ops/mindspore.ops.Transpose.html#mindspore.ops.Transpose">MindSpore Transpose</a>, <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.transpose.html">PyTorch Transpose</a>, <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.permute.html">PyTorch Permute</a>, and <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/transpose">TensforFlow Transpose</a>.</p>
</section>
<section id="conv-and-pooling">
<h3>Conv and Pooling<a class="headerlink" href="#conv-and-pooling" title="Permalink to this headline"></a></h3>
<p>For operators similar to convolution and pooling, the size of the output feature map of the operator depends on variables such as the input feature map, step, kernel_size, and padding.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">pad_mode</span></code> is set to <code class="docutils literal notranslate"><span class="pre">valid</span></code>, the height and width of the output feature map are calculated as follows:</p>
<p><img alt="conv-formula" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.1/docs/mindspore/source_zh_cn/migration_guide/model_development/images/conv_formula.png" /></p>
<p>If pad_mode (corresponding to the padding attribute in PyTorch, which has a different meaning from pad_mode) is set to <code class="docutils literal notranslate"><span class="pre">same</span></code>, automatic padding needs to be performed on the input feature map sometimes. When the padding element is an even number, padding elements are evenly distributed on the top, bottom, left, and right of the feature map. In this case, the behavior of this type of operators in MindSpore, PyTorch, and TensorFlow is the same.</p>
<p>However, when the padding element is an odd number, PyTorch is preferentially filled on the left and upper sides of the input feature map.</p>
<p><img alt="padding1" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.1/docs/mindspore/source_zh_cn/migration_guide/model_development/images/padding_pattern1.png" /></p>
<p>MindSpore and TensorFlow are preferentially filled on the right and bottom of the feature map.</p>
<p><img alt="padding2" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.1/docs/mindspore/source_zh_cn/migration_guide/model_development/images/padding_pattern2.png" /></p>
<p>The following is an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># mindspore example</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">9</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MaxPool</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">op</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">data</span><span class="p">)))</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[[[0. 1. 2.]
   [3. 4. 5.]
   [6. 7. 8.]]]]
[[[[4. 5.]
   [7. 8.]]]]
</pre></div>
</div>
<p>During MindSpore model migration, if the PyTorch pre-training model is loaded to the model and fine-tune is performed in MindSpore, the difference may cause precision decrease. Developers need to pay special attention to the convolution whose padding policy is same.</p>
<p>To keep consistent with the PyTorch behavior, you can use the <code class="docutils literal notranslate"><span class="pre">ops.Pad</span></code> operator to manually pad elements, and then use the convolution and pooling operations when <code class="docutils literal notranslate"><span class="pre">pad_mode=&quot;valid&quot;</span></code> is set.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">9</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="c1"># only padding on top left of feature map</span>
<span class="n">pad</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Pad</span><span class="p">(((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)))</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pad</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MaxPool</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;vaild&#39;</span><span class="p">)(</span><span class="n">data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[[[0. 2.]
   [6. 8.]]]]
</pre></div>
</div>
</section>
<section id="different-default-weight-initialization">
<h3>Different Default Weight Initialization<a class="headerlink" href="#different-default-weight-initialization" title="Permalink to this headline"></a></h3>
<p>We know that weight initialization is very important for network training. Generally, each operator has an implicit declaration weight. In different frameworks, the implicit declaration weight may be different. Even if the operator functions are the same, if the implicitly declared weight initialization mode distribution is different, the training process is affected or even cannot be converged.</p>
<p>Common operators that implicitly declare weights include Conv, Dense(Linear), Embedding, and LSTM. The Conv and Dense operators differ greatly. The Conv and Dense operators of MindSpore and PyTorch have the same distribution of weight and bias initialization methods for implicit declarations.</p>
<ul>
<li><p>Conv2d</p>
<ul class="simple">
<li><p>mindspore.nn.Conv2d (weight: <span class="math notranslate nohighlight">\(\mathcal{U} (-\sqrt{k},\sqrt{k} )\)</span>, bias: <span class="math notranslate nohighlight">\(\mathcal{U} (-\sqrt{k},\sqrt{k} )\)</span>)</p></li>
<li><p>torch.nn.Conv2d (weight: <span class="math notranslate nohighlight">\(\mathcal{U} (-\sqrt{k},\sqrt{k} )\)</span>, bias: <span class="math notranslate nohighlight">\(\mathcal{U} (-\sqrt{k},\sqrt{k} )\)</span>)</p></li>
<li><p>tf.keras.Layers.Conv2D (weight: glorot_uniform, bias: zeros)</p></li>
</ul>
<p>In the preceding information, <span class="math notranslate nohighlight">\(k=\frac{groups}{c_{in}*\prod_{i}^{}{kernel\_size[i]}}\)</span></p>
</li>
<li><p>Dense(Linear)</p>
<ul class="simple">
<li><p>mindspore.nn.Dense (weight: <span class="math notranslate nohighlight">\(\mathcal{U} (-\sqrt{k},\sqrt{k} )\)</span>, bias: <span class="math notranslate nohighlight">\(\mathcal{U} (-\sqrt{k},\sqrt{k} )\)</span>)</p></li>
<li><p>torch.nn.Linear (weight: <span class="math notranslate nohighlight">\(\mathcal{U} (-\sqrt{k},\sqrt{k} )\)</span>, bias: <span class="math notranslate nohighlight">\(\mathcal{U} (-\sqrt{k},\sqrt{k} )\)</span>)</p></li>
<li><p>tf.keras.Layers.Dense (weight: glorot_uniform, bias: zeros)</p></li>
</ul>
<p>In the preceding information, <span class="math notranslate nohighlight">\(k=\frac{groups}{in\_features}\)</span></p>
</li>
</ul>
<p>For a network without normalization, for example, a GAN network without the BatchNorm operator, the gradient is easy to explode or disappear. Therefore, weight initialization is very important. Developers should pay attention to the impact of weight initialization.</p>
</section>
</section>
<section id="differences-between-mindspore-and-pytorch-execution-processes">
<h2>Differences Between MindSpore and PyTorch Execution Processes<a class="headerlink" href="#differences-between-mindspore-and-pytorch-execution-processes" title="Permalink to this headline"></a></h2>
<section id="automatic-differentiation">
<h3>Automatic Differentiation<a class="headerlink" href="#automatic-differentiation" title="Permalink to this headline"></a></h3>
<p>Both MindSpore and PyTorch provide the automatic differentiation function. After the forward network is defined, automatic backward propagation and gradient update can be implemented through simple interface invoking. However, it should be noted that MindSpore and PyTorch use different logic to build backward graphs. This difference also brings differences in API design.</p>
<section id="pytorch-automatic-differentiation">
<h4>PyTorch Automatic Differentiation<a class="headerlink" href="#pytorch-automatic-differentiation" title="Permalink to this headline"></a></h4>
<p>As we know, PyTorch is an automatic differentiation based on computation path tracing. After a network structure is defined, no backward graph is created. Instead, during the execution of the forward graph, <code class="docutils literal notranslate"><span class="pre">Variable</span></code> or <code class="docutils literal notranslate"><span class="pre">Parameter</span></code> records the backward function corresponding to each forward computation and generates a dynamic computational graph, it is used for subsequent gradient calculation. When <code class="docutils literal notranslate"><span class="pre">backward</span></code> is called at the final output, the chaining rule is applied to calculate the gradient from the root node to the leaf node. The nodes stored in the dynamic computational graph of PyTorch are actually <code class="docutils literal notranslate"><span class="pre">Function</span></code> objects. Each time an operation is performed on <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, a <code class="docutils literal notranslate"><span class="pre">Function</span></code> object is generated, which records necessary information in backward propagation. During backward propagation, the <code class="docutils literal notranslate"><span class="pre">autograd</span></code> engine calculates gradients in backward order by using the <code class="docutils literal notranslate"><span class="pre">backward</span></code> of the <code class="docutils literal notranslate"><span class="pre">Function</span></code>. You can view this point through the hidden attribute of the <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p>
<p>For example, run the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">1</span>
<span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>The gradient result of x in the process from obtaining the definition of <code class="docutils literal notranslate"><span class="pre">x</span></code> to obtaining the output <code class="docutils literal notranslate"><span class="pre">y</span></code> is automatically obtained.</p>
<p>Note that the backward of PyTorch is accumulated. After the update, you need to clear the optimizer.</p>
</section>
<section id="mindspore-automatic-differentiation">
<h4>MindSpore Automatic Differentiation<a class="headerlink" href="#mindspore-automatic-differentiation" title="Permalink to this headline"></a></h4>
<p>In graph mode, MindSpore’s automatic differentiation is based on the graph structure. Different from PyTorch, MindSpore does not record any information during forward computation and only executes the normal computation process (similar to PyTorch in PyNative mode). Then the question comes. If the entire forward computation is complete and MindSpore does not record any information, how does MindSpore know how backward propagation is performed?</p>
<p>When MindSpore performs automatic differentiation, the forward graph structure needs to be transferred. The automatic differentiation process is to obtain backward propagation information by analyzing the forward graph. The automatic differentiation result is irrelevant to the specific value in the forward computation and is related only to the forward graph structure. Through the automatic differentiation of the forward graph, the backward propagation process is obtained. The backward propagation process is expressed through a graph structure, that is, the backward graph. The backward graph is added after the user-defined forward graph to form a final computational graph. However, the backward graph and backward operators added later are not aware of and cannot be manually added. They can only be automatically added through the interface provided by MindSpore. In this way, errors are avoided during backward graph build.</p>
<p>Finally, not only the forward graph is executed, but also the graph structure contains both the forward operator and the backward operator added by MindSpore. That is, MindSpore adds an invisible <code class="docutils literal notranslate"><span class="pre">Cell</span></code> after the defined forward graph, the <code class="docutils literal notranslate"><span class="pre">Cell</span></code> is a backward operator derived from the forward graph.</p>
<p>The interface that helps us build the backward graph is <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.1/api_python/ops/mindspore.ops.GradOperation.html">GradOperation</a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">ops</span>

<span class="k">class</span> <span class="nc">GradNetWrtX</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GradNetWrtX</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">net</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">gradient_function</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">gradient_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>According to the document introduction, GradOperation is not an operator. Its input and output are not tensors, but cells, that is, the defined forward graph and the backward graph obtained through automatic differentiation. Why is the input a graph structure? To construct a backward graph, you do not need to know the specific input data. You only need to know the structure of the forward graph. With the forward graph, you can calculate the structure of the backward graph. Then, you can treat the forward graph and backward graph as a new computational graph, the new computational graph is like a function. For any group of data you enter, it can calculate not only the positive output, but also the gradient of ownership weight. Because the graph structure is fixed and does not save intermediate variables, the graph structure can be invoked repeatedly.</p>
<p>Similarly, when we add an optimizer structure to the network, the optimizer also adds optimizer-related operators. That is, we add optimizer operators that are not perceived to the computational graph. Finally, the computational graph is built.</p>
<p>In MindSpore, most operations are finally converted into real operator operations and finally added to the computational graph. Therefore, the number of operators actually executed in the computational graph is far greater than the number of operators defined at the beginning.</p>
<p>MindSpore provides the <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.1/api_python/nn/mindspore.nn.TrainOneStepCell.html">TrainOneStepCell</a> and <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.1/api_python/nn/mindspore.nn.TrainOneStepWithLossScaleCell.html">TrainOneStepWithLossScaleCell</a> APIs to package the entire training process. If other operations, such as gradient cropping, specification, and intermediate variable return, are performed in addition to the common training process, you need to customize the training cell. For details, see <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.1/migration_guide/model_development/training_and_evaluation_procession.html">Inference and Training Process</a>.</p>
</section>
</section>
<section id="learning-rate-update">
<h3>Learning Rate Update<a class="headerlink" href="#learning-rate-update" title="Permalink to this headline"></a></h3>
<section id="pytorch-learning-rate-lr-update-policy">
<h4>PyTorch Learning Rate (LR) Update Policy<a class="headerlink" href="#pytorch-learning-rate-lr-update-policy" title="Permalink to this headline"></a></h4>
<p>PyTorch provides the <code class="docutils literal notranslate"><span class="pre">torch.optim.lr_scheduler</span></code> package for dynamically modifying LR. When using the package, you need to explicitly call <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code> and <code class="docutils literal notranslate"><span class="pre">scheduler.step()</span></code> to update LR. For details, see <a class="reference external" href="https://pytorch.org/docs/1.12/optim.html#how-to-adjust-learning-rate">How Do I Adjust the Learning Rate</a>.</p>
</section>
<section id="mindspore-learning-rate-update-policy">
<h4>MindSpore Learning Rate Update Policy<a class="headerlink" href="#mindspore-learning-rate-update-policy" title="Permalink to this headline"></a></h4>
<p>The learning rate of MindSpore is packaged in the optimizer. Each time the optimizer is invoked, the learning rate update step is automatically updated. For details, see <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.1/migration_guide/model_development/learning_rate_and_optimizer.html">Learning Rate and Optimizer</a>.</p>
</section>
</section>
<section id="distributed-scenarios">
<h3>Distributed Scenarios<a class="headerlink" href="#distributed-scenarios" title="Permalink to this headline"></a></h3>
<section id="pytorch-distributed-settings">
<h4>PyTorch Distributed Settings<a class="headerlink" href="#pytorch-distributed-settings" title="Permalink to this headline"></a></h4>
<p>Generally, data parallelism is used in distributed scenarios. For details, see <a class="reference external" href="https://pytorch.org/docs/1.12/notes/ddp.html">DDP</a>. The following is an example of PyTorch:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">import</span> <span class="nn">torch.multiprocessing</span> <span class="k">as</span> <span class="nn">mp</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">torch.nn.parallel</span> <span class="kn">import</span> <span class="n">DistributedDataParallel</span> <span class="k">as</span> <span class="n">DDP</span>


<span class="k">def</span> <span class="nf">example</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">):</span>
    <span class="c1"># create default process group</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="s2">&quot;gloo&quot;</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>
    <span class="c1"># create local model</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
    <span class="c1"># construct DDP model</span>
    <span class="n">ddp_model</span> <span class="o">=</span> <span class="n">DDP</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">rank</span><span class="p">])</span>
    <span class="c1"># define loss function and optimizer</span>
    <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">ddp_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

    <span class="c1"># forward pass</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">ddp_model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">rank</span><span class="p">))</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
    <span class="c1"># backward pass</span>
    <span class="n">loss_fn</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># update parameters</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">mp</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">example</span><span class="p">,</span>
        <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">world_size</span><span class="p">,),</span>
        <span class="n">nprocs</span><span class="o">=</span><span class="n">world_size</span><span class="p">,</span>
        <span class="n">join</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="c1"># Environment variables which need to be</span>
    <span class="c1"># set when using c10d&#39;s default &quot;env&quot;</span>
    <span class="c1"># initialization mode.</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;MASTER_ADDR&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;localhost&quot;</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;MASTER_PORT&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;29500&quot;</span>
    <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="mindspore-distributed-settings">
<h4>MindSpore Distributed Settings<a class="headerlink" href="#mindspore-distributed-settings" title="Permalink to this headline"></a></h4>
<p>The distributed configuration of MindSpore uses runtime configuration items. For details, see <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.1/parallel/overview.html">Distributed Parallel Training Mode</a>. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.communication.management</span> <span class="kn">import</span> <span class="n">init</span><span class="p">,</span> <span class="n">get_rank</span><span class="p">,</span> <span class="n">get_group_size</span>

<span class="c1"># Initialize the multi-device environment.</span>
<span class="n">init</span><span class="p">()</span>

<span class="c1"># Obtain the number of devices in a distributed scenario and the logical ID of the current process.</span>
<span class="n">group_size</span> <span class="o">=</span> <span class="n">get_group_size</span><span class="p">()</span>
<span class="n">rank_id</span> <span class="o">=</span> <span class="n">get_rank</span><span class="p">()</span>

<span class="c1"># Configure data parallel mode in distributed mode.</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">DATA_PARALLEL</span><span class="p">,</span> <span class="n">gradients_mean</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="differences-with-pytorch-optimizer">
<h2>Differences with PyTorch Optimizer<a class="headerlink" href="#differences-with-pytorch-optimizer" title="Permalink to this headline"></a></h2>
<section id="optimizer-support-differences">
<h3>Optimizer Support Differences<a class="headerlink" href="#optimizer-support-differences" title="Permalink to this headline"></a></h3>
<p>A comparison of the similarities and differences between the optimizers supported by both PyTorch and MindSpore is detailed in the <a class="reference external" href="https://mindspore.cn/docs/en/r2.1/note/api_mapping/pytorch_api_mapping.html#torch-optim">API mapping table</a>. Optimizers not supported in MindSpore at the moment: LBFGS, NAdam, RAdam.</p>
</section>
<section id="optimizer-execution-and-usage-differences">
<h3>Optimizer Execution and Usage Differences<a class="headerlink" href="#optimizer-execution-and-usage-differences" title="Permalink to this headline"></a></h3>
<p>When PyTorch executes the optimizer in a single step, it is usually necessary to manually execute the <code class="docutils literal notranslate"><span class="pre">zero_grad()</span></code> method to set the historical gradient to 0 (or None), then use <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code> to calculate the gradient of the current training step, and finally call the <code class="docutils literal notranslate"><span class="pre">step()</span></code> method of the optimizer to update the network weights;</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">ExponentialLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<p>The use of the optimizer in MindSpore requires only a direct calculation of the gradients and then uses <code class="docutils literal notranslate"><span class="pre">optimizer(grads)</span></code> to perform the update of the network weights.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">grad_fn</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="n">forward_fn</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">_</span><span class="p">),</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">grad_fn</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
</section>
<section id="hyperparameter-differences">
<h3>Hyperparameter Differences<a class="headerlink" href="#hyperparameter-differences" title="Permalink to this headline"></a></h3>
<section id="hyperparameter-names">
<h4>Hyperparameter Names<a class="headerlink" href="#hyperparameter-names" title="Permalink to this headline"></a></h4>
<p>Similarities and differences between network weight and learning rate parameter names:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>PyTorch</p></th>
<th class="head"><p>MindSpore</p></th>
<th class="head"><p>Differences</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>network weight</p></td>
<td><p>params</p></td>
<td><p>params</p></td>
<td><p>The parameters are the same</p></td>
</tr>
<tr class="row-odd"><td><p>learning rate</p></td>
<td><p>lr</p></td>
<td><p>learning_rate</p></td>
<td><p>The parameters are different</p></td>
</tr>
</tbody>
</table>
<p>MindSpore:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
</pre></div>
</div>
<p>PyTorch:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">optim</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="hyperparameter-configuration-methods">
<h4>Hyperparameter Configuration Methods<a class="headerlink" href="#hyperparameter-configuration-methods" title="Permalink to this headline"></a></h4>
<ul>
<li><p>The parameters are not grouped:</p>
<p>The data types of the <code class="docutils literal notranslate"><span class="pre">params</span></code> different: input types in PyTorch are <code class="docutils literal notranslate"><span class="pre">iterable(Tensor)</span></code> and <code class="docutils literal notranslate"><span class="pre">iterable(dict)</span></code>, which support iterator types, while input types in MindSpore are <code class="docutils literal notranslate"><span class="pre">list(Parameter)</span></code>, <code class="docutils literal notranslate"><span class="pre">list(dict)</span></code>, which do not support iterators.</p>
<p>Other hyperparameter configurations and support differences are detailed in the <a class="reference external" href="https://mindspore.cn/docs/en/r2.1/note/api_mapping/pytorch_api_mapping.html#torch-optim">API mapping table</a>.</p>
</li>
<li><p>The parameters are grouped:</p>
<p>PyTorch supports all parameter groupings:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">([</span>
            <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">parameters</span><span class="p">()},</span>
            <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">classifier</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">}</span>
        <span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</pre></div>
</div>
<p>MindSpore supports certain key groupings: “params”, “lr”, “weight_decay”, “grad_centralization”, “order_params”.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">conv_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()))</span>
<span class="n">no_conv_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()))</span>
<span class="n">group_params</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">conv_params</span><span class="p">,</span> <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">0.02</span><span class="p">},</span>
        <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">no_conv_params</span><span class="p">}]</span>

<span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">group_params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="runtime-hyperparameter-modification">
<h4>Runtime Hyperparameter Modification<a class="headerlink" href="#runtime-hyperparameter-modification" title="Permalink to this headline"></a></h4>
<p>PyTorch supports modifying arbitrary optimizer parameters during training, and provides <code class="docutils literal notranslate"><span class="pre">LRScheduler</span></code> for dynamically modifying the learning rate;</p>
<p>MindSpore currently does not support modifying optimizer parameters during training, but provides a way to modify the learning rate and weight decay. See the <a class="reference internal" href="#learning-rate"><span class="std std-doc">Learning Rate</span></a> and <a class="reference internal" href="#weight-decay"><span class="std std-doc">Weight Decay</span></a> sections for details.</p>
</section>
</section>
<section id="learning-rate">
<h3>Learning Rate<a class="headerlink" href="#learning-rate" title="Permalink to this headline"></a></h3>
<section id="dynamic-learning-rate-differences">
<h4>Dynamic Learning Rate Differences<a class="headerlink" href="#dynamic-learning-rate-differences" title="Permalink to this headline"></a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">LRScheduler</span></code> class is defined in PyTorch to manage the learning rate. To use dynamic learning rates, pass an <code class="docutils literal notranslate"><span class="pre">optimizer</span></code> instance into the <code class="docutils literal notranslate"><span class="pre">LRScheduler</span></code> subclass, call <code class="docutils literal notranslate"><span class="pre">scheduler.step()</span></code> in a loop to perform learning rate modifications, and synchronize the changes to the optimizer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">ExponentialLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<p>There are two implementations of dynamic learning rates in MindSpore, <code class="docutils literal notranslate"><span class="pre">Cell</span></code> and <code class="docutils literal notranslate"><span class="pre">list</span></code>. Both types of dynamic learning rates are used in the same way and are passed into the optimizer after instantiation is complete. The former computes the learning rate at each step in the internal <code class="docutils literal notranslate"><span class="pre">construct</span></code>, while the latter pre-generates the learning rate list directly according to the computational logic, and updates the learning rate internally during the training process. Please refer to <a class="reference external" href="https://mindspore.cn/docs/en/r2.1/api_python/mindspore.nn.html#dynamic-learning-rate">Dynamic Learning Rate</a> for details.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">polynomial_decay_lr</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PolynomialDecayLR</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">end_learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">decay_steps</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">power</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">polynomial_decay_lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>

<span class="n">grad_fn</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="n">forward_fn</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">_</span><span class="p">),</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">grad_fn</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
</section>
<section id="custom-learning-rate-differences">
<h4>Custom Learning Rate Differences<a class="headerlink" href="#custom-learning-rate-differences" title="Permalink to this headline"></a></h4>
<p>PyTorch dynamic learning rate module, <code class="docutils literal notranslate"><span class="pre">LRScheduler</span></code>, provides a <code class="docutils literal notranslate"><span class="pre">LambdaLR</span></code> interface for custom learning rate adjustment rules, which can be specified by passing lambda expressions or custom functions.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">lbd</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">epoch</span><span class="p">:</span> <span class="n">epoch</span> <span class="o">//</span> <span class="mi">5</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">LambdaLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">lr_lambda</span><span class="o">=</span><span class="n">lbd</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">train</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">validate</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<p>MindSpore does not provide a similar lambda interface. Custom learning rate adjustment rules can be implemented through custom functions or custom <code class="docutils literal notranslate"><span class="pre">LearningRateSchedule</span></code>.</p>
<p>Way 1: Define the calculation logic specified by the python function, and return a list of learning rates:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">dynamic_lr</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">total_step</span><span class="p">,</span> <span class="n">step_per_epoch</span><span class="p">):</span>
    <span class="n">lrs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">total_step</span><span class="p">):</span>
        <span class="n">current_epoch</span> <span class="o">=</span> <span class="n">i</span> <span class="o">//</span> <span class="n">step_per_epoch</span>
        <span class="n">factor</span> <span class="o">=</span> <span class="n">current_epoch</span> <span class="o">//</span> <span class="mi">5</span>
        <span class="n">lrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lr</span> <span class="o">*</span> <span class="n">factor</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">lrs</span>

<span class="n">decay_lr</span> <span class="o">=</span> <span class="n">dynamic_lr</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">total_step</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">step_per_epoch</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">decay_lr</span><span class="p">)</span>
</pre></div>
</div>
<p>Way 2: Inherit <code class="docutils literal notranslate"><span class="pre">LearningRateSchedule</span></code> and define the change policy in the <code class="docutils literal notranslate"><span class="pre">construct</span></code> method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DynamicDecayLR</span><span class="p">(</span><span class="n">LearningRateSchedule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">step_per_epoch</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DynamicDecayLR</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step_per_epoch</span> <span class="o">=</span> <span class="n">step_per_epoch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cast</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">global_step</span><span class="p">):</span>
        <span class="n">current_epoch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">global_step</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">//</span> <span class="n">step_per_epoch</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="p">(</span><span class="n">current_epoch</span> <span class="o">//</span> <span class="mi">5</span><span class="p">)</span>

<span class="n">decay_lr</span> <span class="o">=</span> <span class="n">DynamicDecayLR</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">step_per_epoch</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">decay_lr</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="obatining-the-learning-rate">
<h4>Obatining the Learning Rate<a class="headerlink" href="#obatining-the-learning-rate" title="Permalink to this headline"></a></h4>
<p>PyTorch:</p>
<ul class="simple">
<li><p>In the fixed learning rate scenario, the learning rate is usually viewed and printed by <code class="docutils literal notranslate"><span class="pre">optimizer.state_dict()</span></code>. For example, when parameters are grouped, use <code class="docutils literal notranslate"><span class="pre">optimizer.state_dict()['param_groups'][n]['lr']</span></code> for the nth parameter group, and use <code class="docutils literal notranslate"><span class="pre">optimizer.state_dict()['param_groups'][0]['lr']</span></code> when the parameters are not grouped;</p></li>
<li><p>In the dynamic learning rate scenario, you can use the <code class="docutils literal notranslate"><span class="pre">get_lr</span></code> method of the <code class="docutils literal notranslate"><span class="pre">LRScheduler</span></code> to get the current learning rate or the <code class="docutils literal notranslate"><span class="pre">print_lr</span></code> method to print the learning rate.</p></li>
</ul>
<p>MindSpore:</p>
<ul class="simple">
<li><p>The interface to view the learning rate directly is not provided at present, and the problem will be fixed in the subsequent version.</p></li>
</ul>
</section>
</section>
<section id="weight-decay">
<h3>Weight Decay<a class="headerlink" href="#weight-decay" title="Permalink to this headline"></a></h3>
<p>Modify weight decay in PyTorch:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">optim</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">decay_factor</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
        <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;weight_decay&quot;</span><span class="p">]</span> <span class="o">*=</span> <span class="n">decay_factor</span>
</pre></div>
</div>
<p>Implement dynamic weight decay in MindSpore: Users can inherit the class of ‘Cell’ custom dynamic weight decay and pass it into the optimizer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ExponentialWeightDecay</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_decay</span><span class="p">,</span> <span class="n">decay_rate</span><span class="p">,</span> <span class="n">decay_steps</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ExponentialWeightDecay</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span> <span class="o">=</span> <span class="n">weight_decay</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decay_rate</span> <span class="o">=</span> <span class="n">decay_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decay_steps</span> <span class="o">=</span> <span class="n">decay_steps</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">global_step</span><span class="p">):</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">global_step</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">decay_steps</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span> <span class="o">*</span> <span class="n">ops</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">decay_rate</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>

<span class="n">weight_decay</span> <span class="o">=</span> <span class="n">ExponentialWeightDecay</span><span class="p">(</span><span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">decay_steps</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="saving-and-loading-optimizer-state">
<h3>Saving and Loading Optimizer State<a class="headerlink" href="#saving-and-loading-optimizer-state" title="Permalink to this headline"></a></h3>
<p>PyTorch optimizer module provides <code class="docutils literal notranslate"><span class="pre">state_dict()</span></code> for viewing and saving the optimizer state, and <code class="docutils literal notranslate"><span class="pre">load_state_dict</span></code> for loading the optimizer state.</p>
<ul>
<li><p>Optimizer saving. You can use <code class="docutils literal notranslate"><span class="pre">torch.save()</span></code> to save the obtained <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> to a pkl file:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">save_path</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Optimizer loading. You can use <code class="docutils literal notranslate"><span class="pre">torch.load()</span></code> to load the saved <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> and then use <code class="docutils literal notranslate"><span class="pre">load_state_dict</span></code> to load the obtained <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> into the optimizer:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">state_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">save_path</span><span class="p">)</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
<p>MindSpore optimizer module is inherited from <code class="docutils literal notranslate"><span class="pre">Cell</span></code>. The optimizer is saved and loaded in the same way as the network is saved and loaded, usually in conjunction with <code class="docutils literal notranslate"><span class="pre">save_checkpoint</span></code> and <code class="docutils literal notranslate"><span class="pre">load_checkpoint</span></code>.</p>
<ul>
<li><p>Optimizer saving. You can use <code class="docutils literal notranslate"><span class="pre">mindspore.save_checkpoint()</span></code> to save the optimizer instance to a ckpt file:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">state_dict</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span> <span class="n">save_path</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Optimizer loading. You can use <code class="docutils literal notranslate"><span class="pre">mindspore.load_checkpoint()</span></code> to load the saved ckpt file, and then use <code class="docutils literal notranslate"><span class="pre">load_param_into_net</span></code> to load the obtained <code class="docutils literal notranslate"><span class="pre">param_dict</span></code> into the optimizer:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">param_dict</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">(</span><span class="n">save_path</span><span class="p">)</span>
<span class="n">mindspore</span><span class="o">.</span><span class="n">load_param_into_net</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</section>
</section>
<section id="differences-between-random-generators">
<h2>Differences between Random Generators<a class="headerlink" href="#differences-between-random-generators" title="Permalink to this headline"></a></h2>
<section id="api-names">
<h3>API names<a class="headerlink" href="#api-names" title="Permalink to this headline"></a></h3>
<p>There is no difference between the APIs, except that MindSpore is missing <code class="docutils literal notranslate"><span class="pre">Tensor.random_</span></code>, because MindSpore does not support in-place manipulations.</p>
</section>
<section id="seed-generator">
<h3>seed &amp; generator<a class="headerlink" href="#seed-generator" title="Permalink to this headline"></a></h3>
<p>MindSpore uses <code class="docutils literal notranslate"><span class="pre">seed</span></code> to control the generation of a random number while PyTorch uses <code class="docutils literal notranslate"><span class="pre">torch.generator</span></code>.</p>
<ol class="arabic">
<li><p>There are 2 levels of random seed, graph-level and op-level. Graph-level seed is used as a global variable, and in most cases, users do not have to set the graph-level seed, they only care about the op-level seed (the parameter <code class="docutils literal notranslate"><span class="pre">seed</span></code> in the APIs, are all op-level seeds). If a program uses a random generator algorithm twice, the results are different even thought they are using the same seed. Nevertheless, if the user runs the script again, the same results should be obtained. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># If a random op is called twice within one program, the two results will be different:</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">ops</span>

<span class="n">minval</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">maxval</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">minval</span><span class="p">,</span> <span class="n">maxval</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># generates &#39;A1&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">minval</span><span class="p">,</span> <span class="n">maxval</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># generates &#39;A2&#39;</span>
<span class="c1"># If the same program runs again, it repeat the results:</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">minval</span><span class="p">,</span> <span class="n">maxval</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># generates &#39;A1&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">minval</span><span class="p">,</span> <span class="n">maxval</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># generates &#39;A2&#39;</span>
</pre></div>
</div>
</li>
<li><p>torch.Generator is often used as a key argument. A default generator will be used (torch.default_generator), when the user does not assign one to the function. torch.Generator.seed could be set with the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">G</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span>
<span class="n">G</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>It is the same as using the default generator with seed=1. e.g.: torch.manual_seed(1).</p>
<p>The state of a generator in PyTorch is a Tensor of 5056 elements with dtype=uint8. When using the same generator in the script, the state of the generator will be changed. With 2 or more generators, i.e. g1 and g2, user can set g2.set_state(g1.get_state()) to make g2 have the exact same state as g1. In other words, using g2 is the same as using the g1 of that state. If g1 and g2 have the same seed and state, the random number generated by those generator are the same.</p>
</li>
</ol>
</section>
</section>
<section id="differences-between-initializations">
<h2>Differences between Initializations<a class="headerlink" href="#differences-between-initializations" title="Permalink to this headline"></a></h2>
<section id="api-names-1">
<h3>API Names<a class="headerlink" href="#api-names-1" title="Permalink to this headline"></a></h3>
<p>Every API from <code class="docutils literal notranslate"><span class="pre">torch.nn.init</span></code> could correspond to MindSpore, except <code class="docutils literal notranslate"><span class="pre">torch.nn.init.calculate_gain()</span></code>. For more information, please refer to <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.1/note/api_mapping/pytorch_api_mapping.html">PyTorch and MindSpore API Mapping Table</a>.</p>
<p>Note: <code class="docutils literal notranslate"><span class="pre">gain</span></code> is used to describe the influence of the non-linearity to the standard deviation of the data. Because non-linearity will affect the standard deviation, the gradient may explode or vanish.</p>
</section>
<section id="torch-nn-init">
<h3>torch.nn.init<a class="headerlink" href="#torch-nn-init" title="Permalink to this headline"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">torch.nn.init</span></code> takes a Tensor as input, and the input Tensor will be changed to the target in-place.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

</pre></div>
</div>
<p>After running the code above, x is no longer an uninitialized Tensor, and its elements will follow the uniform distribution.</p>
</section>
<section id="mindspore-copmmon-initializer">
<h3>mindspore.copmmon.initializer<a class="headerlink" href="#mindspore-copmmon-initializer" title="Permalink to this headline"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">mindspore.common.initializer</span></code> is used for delayed initialization in parallel mode. Only after calling <code class="docutils literal notranslate"><span class="pre">init_data()</span></code>, the elements will be assigned based on its <code class="docutils literal notranslate"><span class="pre">init</span></code>.
Every Tensor could only use <code class="docutils literal notranslate"><span class="pre">init_data</span></code> once.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
<span class="kn">import</span> <span class="nn">mindspore</span>
<span class="kn">from</span> <span class="nn">mindspore.common</span> <span class="n">initialzier</span> <span class="kn">import</span> <span class="nn">initializer</span><span class="o">,</span> <span class="nn">Uniform</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">initializer</span><span class="p">(</span><span class="n">Uniform</span><span class="p">(),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

</pre></div>
</div>
<p>After running the code above, <code class="docutils literal notranslate"><span class="pre">x</span></code> is still not fully initialized. If it is used for further calculation, 0 will be used. However, when printing the Tensor, <code class="docutils literal notranslate"><span class="pre">init_data()</span></code>
will be called automatically.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="analysis_and_preparation.html" class="btn btn-neutral float-left" title="Model Analysis and Preparation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="model_development/model_development.html" class="btn btn-neutral float-right" title="Constructing MindSpore Network" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
        <script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>