<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Using Third-party Operator Libraries Based on Customized Interfaces &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Method for Converting TensorFlow Models to MindSpore Models" href="tensorflow2mindspore.html" />
    <link rel="prev" title="FAQs" href="faq.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../design/overview.html">MindSpore Design Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/programming_paradigm.html">Functional and Object-Oriented Fusion Programming Paradigm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/dynamic_graph_and_static_graph.html">Combination of Dynamic and Static Graphs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/distributed_training_design.html">Distributed Parallel Native</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/data_engine.html">High Performance Data Processing Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/all_scenarios.html">Full-scenarios Unified Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/graph_fusion_engine.html">Graph-Kernel Fusion Acceleration Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/pluggable_device.html">Third-Party Hardware Interconnection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/official_models.html">Official Models</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.experimental.html">mindspore.experimental</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Mapping</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/pytorch_api_mapping.html">PyTorch and MindSpore API Mapping Table</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Migration Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview of Migration Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="enveriment_preparation.html">Environment Preparation and Information Acquisition</a></li>
<li class="toctree-l1"><a class="reference internal" href="analysis_and_preparation.html">Model Analysis and Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="typical_api_comparision.html">Differences between PyTorch and MindSpore</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_development/model_development.html">Constructing MindSpore Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="debug_and_tune.html">Debugging and Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="sample_code.html">Network Migration Debugging Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="migrator_with_tools.html">Application Practice Guide for Network Migration Tool</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="faq.html">FAQs</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Using Third-party Operator Libraries Based on Customized Interfaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="tensorflow2mindspore.html">Method for Converting TensorFlow Models to MindSpore Models</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Syntax Support</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax_support.html">Static Graph Syntax Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax/operators.html">Static Graph Syntax - Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax/statements.html">Static Graph Syntax - Python Statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax/python_builtin_functions.html">Static Graph Syntax - Python Built-in Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/index_support.html">Tensor Index Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Environment Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/env_var_list.html">Environment Variables</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/data_processing.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/implement_problem.html">Implement Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/network_compilation.html">Network Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/operators_compile.html">Operators Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/usage_migrate_3rd.html">Migration from a Third-party Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/precision_tuning.html">Precision Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/distributed_parallel.html">Distributed Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/feature_advice.html">Feature Advice</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="faq.html">FAQs</a> &raquo;</li>
      <li>Using Third-party Operator Libraries Based on Customized Interfaces</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/migration_guide/use_third_party_op.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section class="tex2jax_ignore mathjax_ignore" id="using-third-party-operator-libraries-based-on-customized-interfaces">
<h1>Using Third-party Operator Libraries Based on Customized Interfaces<a class="headerlink" href="#using-third-party-operator-libraries-based-on-customized-interfaces" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.2/docs/mindspore/source_en/migration_guide/use_third_party_op.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.2/resource/_static/logo_source_en.svg" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>When lacking of the built-in operators during developing a network, you can use the primitive in <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.2/api_python/ops/mindspore.ops.Custom.html#mindspore-ops-custom">Custom</a> to easily and quickly define and use different types of customized operators.</p>
<p>Developers can choose different customized operator development methods according to their needs. For details, please refer to the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.2/operation/op_custom.html">Usage Guide</a> of Custom operator.</p>
<p>One of the development methods for customized operators, the <code class="docutils literal notranslate"><span class="pre">aot</span></code> method, has its own special use. The <code class="docutils literal notranslate"><span class="pre">aot</span></code> can call the corresponding <code class="docutils literal notranslate"><span class="pre">cpp</span></code>/<code class="docutils literal notranslate"><span class="pre">cuda</span></code> functions by loading a pre-compiled <code class="docutils literal notranslate"><span class="pre">so</span></code>. Therefore. When a third-party library provides <code class="docutils literal notranslate"><span class="pre">API</span></code>, a <code class="docutils literal notranslate"><span class="pre">cpp</span></code>/<code class="docutils literal notranslate"><span class="pre">cuda</span></code> function, you can try to call its function interface in <code class="docutils literal notranslate"><span class="pre">so</span></code>, which is described below by taking <code class="docutils literal notranslate"><span class="pre">Aten</span></code> library in PyTorch as an example.</p>
</section>
<section id="pytorch-aten-operator-matching">
<h2>PyTorch Aten Operator Matching<a class="headerlink" href="#pytorch-aten-operator-matching" title="Permalink to this headline"></a></h2>
<p>When lacking of built-in operators during migrating a network that uses the PyTorch Aten operator, we can use the <code class="docutils literal notranslate"><span class="pre">aot</span></code> development of the <code class="docutils literal notranslate"><span class="pre">Custom</span></code> operator to call the PyTorch Aten operator for fast verification.</p>
<p>PyTorch provides a way to support the introduction of PyTorch header files, so that <code class="docutils literal notranslate"><span class="pre">cpp/cuda</span></code> code can be written by using related data structures and compiled into <code class="docutils literal notranslate"><span class="pre">so</span></code>. Reference: <a class="reference external" href="https://pytorch.org/docs/stable/_modules/torch/utils/cpp_extension.html#CppExtension">https://pytorch.org/docs/stable/_modules/torch/utils/cpp_extension.html#CppExtension</a>.</p>
<p>Using a combination of the two approaches, the customized operator can call the PyTorch Aten operator, which is used as follows:</p>
<section id="1-downloading-project-files">
<h3>1. Downloading Project Files<a class="headerlink" href="#1-downloading-project-files" title="Permalink to this headline"></a></h3>
<p>The project files can be downloaded <a class="reference external" href="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/migration_guide/test_custom_pytorch.tar">here</a>.</p>
<p>Use the following command to extract the zip package and get the folder <code class="docutils literal notranslate"><span class="pre">test_custom_pytorch</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tar<span class="w"> </span>xvf<span class="w"> </span>test_custom_pytorch.tar
</pre></div>
</div>
<p>The folder contains the following files:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>test_custom_pytorch
├── env.sh                           # set PyTorch/lib into LD_LIBRARY_PATH
├── leaky_relu.cpp                   # an example of use Aten CPU operator
├── leaky_relu.cu                    # an example of use Aten GPU operator
├── ms_ext.cpp                       # convert Tensors between MindSpore and PyTorch
├── ms_ext.h                         # convert API
├── README.md
├── run_cpu.sh                       # a script to run cpu case
├── run_gpu.sh                       # a script to run gpu case
├── setup.py                         # a script to compile cpp/cu into so
├── test_cpu_op_in_gpu_device.py     # a test file to run Aten CPU operator on GPU device
├── test_cpu_op.py                   # a test file to run Aten CPU operator on CPU device
└── test_gpu_op.py                   # a test file to run Aten GPU operator on GPU device
</pre></div>
</div>
<p>Using PyTorch Aten arithmetic focuses on env.sh, setup.py, leaky_relu.cpp/cu, and test_*.py.</p>
<p>Among them, env.sh is used to set environment variables, setup.py is used to compile so, leaky_relu.cpp/cu is a reference for used to write the source code for calling PyTorch Aten operator in reference, and test_*.py is used to call Custom operator in reference.</p>
</section>
<section id="2-writing-source-code-file-that-calls-pytorch-aten-operator">
<h3>2. Writing Source Code File that Calls PyTorch Aten Operator<a class="headerlink" href="#2-writing-source-code-file-that-calls-pytorch-aten-operator" title="Permalink to this headline"></a></h3>
<p>Refer to leaky_relu.cpp/cu to write the source code file that calls the PyTorch Aten operator.</p>
<p>Since customized operators of type <code class="docutils literal notranslate"><span class="pre">aot</span></code> are compiled by using <code class="docutils literal notranslate"><span class="pre">AOT</span></code>, the developer is required to write the source code files corresponding to the operator implementation functions based on a specific interface and compile the source code files into a dynamic link library in advance, and the framework will automatically call the functions in the dynamic link library for execution during the web runtime. As for the development language of the operator implementation, <code class="docutils literal notranslate"><span class="pre">CUDA</span></code> is supported for <code class="docutils literal notranslate"><span class="pre">GPU</span></code> platform, and <code class="docutils literal notranslate"><span class="pre">C</span></code> and <code class="docutils literal notranslate"><span class="pre">C++</span></code> are supported for <code class="docutils literal notranslate"><span class="pre">CPU</span></code> platform. The interface specification of the operator implementation functions in the source code file is as follows.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">extern</span><span class="w"> </span><span class="s">&quot;C&quot;</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">func_name</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">nparam</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">**</span><span class="n">params</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="o">*</span><span class="n">ndims</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="o">**</span><span class="n">shapes</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">**</span><span class="n">dtypes</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">stream</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">extra</span><span class="p">);</span>
</pre></div>
</div>
<p>If <code class="docutils literal notranslate"><span class="pre">cpu</span></code> operator is called, taking <code class="docutils literal notranslate"><span class="pre">leaky_relu.cpp</span></code> as an example, the file provides <code class="docutils literal notranslate"><span class="pre">LeakyRelu</span></code> required by <code class="docutils literal notranslate"><span class="pre">AOT</span></code>, which calls <code class="docutils literal notranslate"><span class="pre">torch::leaky_relu_out</span></code> of PyTorch Aten:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;string.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/extension.h&gt;</span><span class="c1"> // Header file reference section</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;ms_ext.h&quot;</span>

<span class="k">extern</span><span class="w"> </span><span class="s">&quot;C&quot;</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">LeakyRelu</span><span class="p">(</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">nparam</span><span class="p">,</span>
<span class="w">    </span><span class="kt">void</span><span class="o">**</span><span class="w"> </span><span class="n">params</span><span class="p">,</span>
<span class="w">    </span><span class="kt">int</span><span class="o">*</span><span class="w"> </span><span class="n">ndims</span><span class="p">,</span>
<span class="w">    </span><span class="kt">int64_t</span><span class="o">**</span><span class="w"> </span><span class="n">shapes</span><span class="p">,</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="o">**</span><span class="w"> </span><span class="n">dtypes</span><span class="p">,</span>
<span class="w">    </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">stream</span><span class="p">,</span>
<span class="w">    </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">extra</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">tensors</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">get_torch_tensors</span><span class="p">(</span><span class="n">nparam</span><span class="p">,</span><span class="w"> </span><span class="n">params</span><span class="p">,</span><span class="w"> </span><span class="n">ndims</span><span class="p">,</span><span class="w"> </span><span class="n">shapes</span><span class="p">,</span><span class="w"> </span><span class="n">dtypes</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">kCPU</span><span class="p">);</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">at_input</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">at_output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tensors</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">leaky_relu_out</span><span class="p">(</span><span class="n">at_output</span><span class="p">,</span><span class="w"> </span><span class="n">at_input</span><span class="p">);</span>
<span class="w">    </span><span class="c1">// If you use the version without output, the code is as follows:</span>
<span class="w">    </span><span class="c1">// torch::Tensor output = torch::leaky_relu(at_input);</span>
<span class="w">    </span><span class="c1">// at_output.copy_(output);</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>If <code class="docutils literal notranslate"><span class="pre">gpu</span></code> operator is called, taking <code class="docutils literal notranslate"><span class="pre">leaky_relu.cu</span></code> as an example:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;string.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/extension.h&gt;</span><span class="c1"> // Header file reference section</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;ms_ext.h&quot;</span>

<span class="k">extern</span><span class="w"> </span><span class="s">&quot;C&quot;</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">LeakyRelu</span><span class="p">(</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">nparam</span><span class="p">,</span>
<span class="w">    </span><span class="kt">void</span><span class="o">**</span><span class="w"> </span><span class="n">params</span><span class="p">,</span>
<span class="w">    </span><span class="kt">int</span><span class="o">*</span><span class="w"> </span><span class="n">ndims</span><span class="p">,</span>
<span class="w">    </span><span class="kt">int64_t</span><span class="o">**</span><span class="w"> </span><span class="n">shapes</span><span class="p">,</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="o">**</span><span class="w"> </span><span class="n">dtypes</span><span class="p">,</span>
<span class="w">    </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">stream</span><span class="p">,</span>
<span class="w">    </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">extra</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">custream</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">cudaStream_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">stream</span><span class="p">);</span>
<span class="w">    </span><span class="n">cudaStreamSynchronize</span><span class="p">(</span><span class="n">custream</span><span class="p">);</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">tensors</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">get_torch_tensors</span><span class="p">(</span><span class="n">nparam</span><span class="p">,</span><span class="w"> </span><span class="n">params</span><span class="p">,</span><span class="w"> </span><span class="n">ndims</span><span class="p">,</span><span class="w"> </span><span class="n">shapes</span><span class="p">,</span><span class="w"> </span><span class="n">dtypes</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">kCUDA</span><span class="p">);</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">at_input</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">at_output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tensors</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">leaky_relu_out</span><span class="p">(</span><span class="n">at_output</span><span class="p">,</span><span class="w"> </span><span class="n">at_input</span><span class="p">);</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Among them, PyTorch Aten provides a version of the operator function with output and a version of the operator function without output. The operator function with output has the <code class="docutils literal notranslate"><span class="pre">_out</span></code> suffix, and PyTorch Aten provides <code class="docutils literal notranslate"><span class="pre">api</span></code> for 300+ commonly-used operators.</p>
<p>When calling <code class="docutils literal notranslate"><span class="pre">torch::*_out</span></code>, <code class="docutils literal notranslate"><span class="pre">output</span></code> copy is not required. When calling the version without the <code class="docutils literal notranslate"><span class="pre">_out</span></code> suffix, calling the API <code class="docutils literal notranslate"><span class="pre">torch.Tensor.copy_</span></code> to make a copy of the result is required.</p>
<p>To see which functions of PyTorch Aten are supported, refer to the PyTorch installation path: <code class="docutils literal notranslate"><span class="pre">python*/psite-packages/torch/include/ATen/CPUFunctions_inl.h</span></code> for the <code class="docutils literal notranslate"><span class="pre">CPU</span></code> version and <code class="docutils literal notranslate"><span class="pre">python*/</span> <span class="pre">site-packages/torch/include/ATen/CUDAFunctions_inl.h</span></code> for the <code class="docutils literal notranslate"><span class="pre">GPU</span></code> version.</p>
<p>The above use case uses the api provided by ms_ext.h, which is described here:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Transform inputs/outputs of MindSpore kernel as Tensor of PyTorch Aten</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">get_torch_tensors</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">nparam</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="o">**</span><span class="w"> </span><span class="n">params</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="o">*</span><span class="w"> </span><span class="n">ndims</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="o">**</span><span class="w"> </span><span class="n">shapes</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="o">**</span><span class="w"> </span><span class="n">dtypes</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">Device</span><span class="w"> </span><span class="n">device</span><span class="p">)</span><span class="w"> </span><span class="p">;</span>
</pre></div>
</div>
</section>
<section id="3-using-the-compile-script-setup-py-to-generate-so">
<h3>3. Using the Compile Script <code class="docutils literal notranslate"><span class="pre">setup.py</span></code> to Generate so<a class="headerlink" href="#3-using-the-compile-script-setup-py-to-generate-so" title="Permalink to this headline"></a></h3>
<p>setup.py compiles the above <code class="docutils literal notranslate"><span class="pre">c++/cuda</span></code> source code into a <code class="docutils literal notranslate"><span class="pre">so</span></code> file by using the <code class="docutils literal notranslate"><span class="pre">cppextension</span></code> provided by PyTorch Aten.</p>
<p>You need to make sure PyTorch is installed before executing it.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>torch
</pre></div>
</div>
<p>Add <code class="docutils literal notranslate"><span class="pre">lib</span></code> of PyTorch to <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="k">$(</span>python3<span class="w"> </span>-c<span class="w"> </span><span class="s1">&#39;import torch, os; print(os.path.dirname(torch.__file__))&#39;</span><span class="k">)</span>/lib:<span class="nv">$LD_LIBRARY_PATH</span>
</pre></div>
</div>
<p>Execute:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cpu:<span class="w"> </span>python<span class="w"> </span>setup.py<span class="w"> </span>leaky_relu.cpp<span class="w"> </span>leaky_relu_cpu.so
gpu:<span class="w"> </span>python<span class="w"> </span>setup.py<span class="w"> </span>leaky_relu.cu<span class="w"> </span>leaky_relu_gpu.so
</pre></div>
</div>
<p>Get so file we need.</p>
</section>
<section id="4-using-customized-operators">
<h3>4. Using Customized Operators<a class="headerlink" href="#4-using-customized-operators" title="Permalink to this headline"></a></h3>
<p>Taking the CPU as an example, the above PyTorch Aten operator is called by using the Custom operator. The code can be found in test_cpu_op.py:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">Cell</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">LeakyRelu</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="s2">&quot;./leaky_relu_cpu.so:LeakyRelu&quot;</span><span class="p">,</span> <span class="n">out_shape</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">func_type</span><span class="o">=</span><span class="s2">&quot;aot&quot;</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">leaky_relu</span> <span class="o">=</span> <span class="n">LeakyRelu</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">leaky_relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x0</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<p>Execute:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>test_cpu_op.py
</pre></div>
</div>
<p>The result is:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[ 0.    -0.001]
 [-0.002  1.   ]]
</pre></div>
</div>
<p>Note:</p>
<p>If you are using the PyTorch Aten <code class="docutils literal notranslate"><span class="pre">GPU</span></code> operator, <code class="docutils literal notranslate"><span class="pre">device_target</span></code> needs to be set to <code class="docutils literal notranslate"><span class="pre">&quot;GPU&quot;</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">)</span>
<span class="n">op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="s2">&quot;./leaky_relu_gpu.so:LeakyRelu&quot;</span><span class="p">,</span> <span class="n">out_shape</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">func_type</span><span class="o">=</span><span class="s2">&quot;aot&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>If the PyTorch Aten <code class="docutils literal notranslate"><span class="pre">CPU</span></code> operator is used and the <code class="docutils literal notranslate"><span class="pre">device_target</span></code> is set to <code class="docutils literal notranslate"><span class="pre">&quot;GPU&quot;</span></code>, you need to add the following settings:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">)</span>
<span class="n">op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="s2">&quot;./leaky_relu_cpu.so:LeakyRelu&quot;</span><span class="p">,</span> <span class="n">out_shape</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">func_type</span><span class="o">=</span><span class="s2">&quot;aot&quot;</span><span class="p">)</span>
<span class="n">op</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>
</pre></div>
</div>
<blockquote>
<div><ol class="arabic simple">
<li><p>To compile so with cppextension, you need to meet the compiler version required by the tool and check if gcc/clang/nvcc exists.</p></li>
<li><p>Using cppextension to compile so will generate a build folder in the script path, which stores so. The script will copy so outside build, but cppextension will skip the compilation if it finds that there is already so in the build, so remember to clear newly-compiled so under build.</p></li>
<li><p>The above tests are based on <a class="reference external" href="https://download.pytorch.org/whl/cu111/torch-1.9.1%2Bcu111-cp37-cp37m-linux_x86_64.whl">PyTorch 1.9.1，cuda11.1，python3.7</a>. The cuda version supported by PyTorch Aten should be the same as the local cuda version, and whether other versions is supported should be explored by the user.</p></li>
</ol>
</div></blockquote>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="faq.html" class="btn btn-neutral float-left" title="FAQs" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="tensorflow2mindspore.html" class="btn btn-neutral float-right" title="Method for Converting TensorFlow Models to MindSpore Models" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>