<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Distributed Parallel Native &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script src="../_static/js/mermaid-9.3.0.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="High Performance Data Processing Engine" href="data_engine.html" />
    <link rel="prev" title="Combination of Dynamic and Static Graphs" href="dynamic_graph_and_static_graph.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Design</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">MindSpore Design Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_view.html">TENSOR VIEWS</a></li>
<li class="toctree-l1"><a class="reference internal" href="programming_paradigm.html">Functional and Object-Oriented Fusion Programming Paradigm</a></li>
<li class="toctree-l1"><a class="reference internal" href="dynamic_graph_and_static_graph.html">Combination of Dynamic and Static Graphs</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Distributed Parallel Native</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#background">Background</a></li>
<li class="toctree-l2"><a class="reference internal" href="#data-parallelism">Data Parallelism</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#principle-of-data-parallelism">Principle of Data Parallelism</a></li>
<li class="toctree-l3"><a class="reference internal" href="#data-parallel-code">Data Parallel Code</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#semi-automatic-parallelism">Semi-automatic Parallelism</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#principle-of-semi-automatic-parallelism">Principle of Semi-automatic Parallelism</a></li>
<li class="toctree-l3"><a class="reference internal" href="#semi-automatic-parallel-code">Semi-automatic Parallel Code</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#fully-automatic-parallelism">Fully Automatic Parallelism</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#feature-design">Feature Design</a></li>
<li class="toctree-l3"><a class="reference internal" href="#three-search-algorithms">Three Search Algorithms</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fully-automatic-parallelism-code">Fully Automatic Parallelism Code</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#heterogeneous-parallelism">Heterogeneous Parallelism</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#computational-process">Computational Process</a></li>
<li class="toctree-l3"><a class="reference internal" href="#optimizer-heterogeneity">Optimizer Heterogeneity</a></li>
<li class="toctree-l3"><a class="reference internal" href="#embedding-heterogeneity">Embedding Heterogeneity</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ps-heterogeneity">PS Heterogeneity</a></li>
<li class="toctree-l3"><a class="reference internal" href="#constraints">Constraints</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="data_engine.html">High Performance Data Processing Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="all_scenarios.html">Full-scenarios Unified Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="graph_fusion_engine.html">Graph-Kernel Fusion Acceleration Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="pluggable_device.html">Third-Party Hardware Interconnection</a></li>
<li class="toctree-l1"><a class="reference internal" href="glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/official_models.html">Official Models</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.hal.html">mindspore.hal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.multiprocessing.html">mindspore.multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.experimental.html">mindspore.experimental</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Mapping</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/pytorch_api_mapping.html">PyTorch and MindSpore API Mapping Table</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Migration Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/overview.html">Overview of Migration Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/enveriment_preparation.html">Environment Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/analysis_and_preparation.html">Model Analysis and Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/model_development/model_development.html">Network Constructing Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/debug_and_tune.html">Debugging and Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/sample_code.html">Network Migration Debugging Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/faq.html">FAQs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Syntax Support</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax_support.html">Static Graph Syntax Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax/operators.html">Static Graph Syntax - Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax/statements.html">Static Graph Syntax - Python Statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax/python_builtin_functions.html">Static Graph Syntax - Python Built-in Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/index_support.html">Tensor Index Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Environment Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/env_var_list.html">Environment Variables</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/data_processing.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/implement_problem.html">Implement Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/network_compilation.html">Network Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/operators_compile.html">Operators Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/precision_tuning.html">Precision Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/distributed_parallel.html">Distributed Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/feature_advice.html">Feature Advice</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Distributed Parallel Native</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/design/distributed_training_design.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section class="tex2jax_ignore mathjax_ignore" id="distributed-parallel-native">
<h1>Distributed Parallel Native<a class="headerlink" href="#distributed-parallel-native" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.3.q1/docs/mindspore/source_en/design/distributed_training_design.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3.q1/resource/_static/logo_source_en.svg" /></a></p>
<section id="background">
<h2>Background<a class="headerlink" href="#background" title="Permalink to this headline"></a></h2>
<p>With the rapid development of deep learning, the number of datasets and parameters are growing exponentially to improve the accuracy and generalization capability of neural networks. Parallel distributed training has become a development trend to resolve the performance bottleneck of ultra-large scale networks.</p>
<p>To cope with the problem of oversized datasets, MindSpore introduces data parallelism mode, which utilizes the computing resources of multiple devices to process more training data simultaneously and speed up model training. Also when the data is too large or the model is too large to be loaded on a single compute node for training, model parallelism needs to be introduced, where each compute node only needs to load part of the model and data, which can reduce memory usage and improve training efficiency. In the evolution of distributed parallelism programming paradigm, in traditional manual parallelism, users need to manually slice the model to parallelize multiple nodes based on communication primitives through coding, and need perceptual graph slicing, operator slicing, and cluster topology to achieve optimal performance. This programming paradigm has certain threshold requirements for engineers, so it evolved into semi-automatic parallelism: parallel logic and algorithmic logic decoupling. The users write algorithmic code in a single-card serial way, with parallel logic serving as the algorithmic configuration. Users only need to configure parallel strategies to achieve automatic parallel slicing without writing additional code and do not need to perceive the distribution of model slices and cluster topology. The fully automatic parallel training programming paradigm goes a step further, where the user only needs to write single-card serial algorithms that automatically generate a better shard strategy through a search algorithm.</p>
<p>MindSpore implements data communication and synchronization operations during parallel training by means of aggregate communication, which relies on Huawei collective communication library (HCCL) on Ascend chips and NVIDIA collective communication library (NCCL) on GPUs. MindSpore currently uses a synchronous training mode, which ensures that the parameters are consistent across all devices and are synchronized on all devices before each training iteration begins.</p>
<p>This design document will focus on the design principles of several parallel training methods and guide users in custom development.</p>
</section>
<section id="data-parallelism">
<h2>Data Parallelism<a class="headerlink" href="#data-parallelism" title="Permalink to this headline"></a></h2>
<p>This section describes how the data parallel mode <code class="docutils literal notranslate"><span class="pre">ParallelMode.DATA_PARALLEL</span></code> works in MindSpore.</p>
<section id="principle-of-data-parallelism">
<h3>Principle of Data Parallelism<a class="headerlink" href="#principle-of-data-parallelism" title="Permalink to this headline"></a></h3>
<p><img alt="Data Parallel Description" src="../_images/data_parallel.png" /></p>
<ol class="arabic">
<li><p>Environment dependencies</p>
<p>Each time before parallel training starts, the <code class="docutils literal notranslate"><span class="pre">mindspore.communication.init</span></code> API is called to initialize communication resources and the global communication group <code class="docutils literal notranslate"><span class="pre">WORLD_COMM_GROUP</span></code> is automatically created.</p>
</li>
<li><p>Data distribution</p>
<p>The key of data parallelism is to split datasets based on the sample dimension and deliver the split datasets to different devices. Each dataset loading API provided by the <code class="docutils literal notranslate"><span class="pre">mindspore.dataset</span></code> module has the <code class="docutils literal notranslate"><span class="pre">num_shards</span></code> and <code class="docutils literal notranslate"><span class="pre">shard_id</span></code> parameters. The parameters are used to split a dataset into multiple datasets, perform cyclic sampling, and collect data of the <code class="docutils literal notranslate"><span class="pre">batch</span></code> size to each device. When the data volume is insufficient, the sampling restarts from the beginning.</p>
</li>
<li><p>Network structure</p>
<p>The scripting method of data parallel network is the same as that of standalone network. This is because, although models of each device are executed independently during the forward and backward propagation processes, the same network structure is maintained. To ensure the synchronous training between devices, the initial values of corresponding network parameters must be the same. You are advised to enable <code class="docutils literal notranslate"><span class="pre">parameter_broadcast</span></code> to broadcast the values of weights in <code class="docutils literal notranslate"><span class="pre">DATA_PARALLEL</span></code> and <code class="docutils literal notranslate"><span class="pre">HYBRID_PARALLEL</span></code> modes. And in <code class="docutils literal notranslate"><span class="pre">AUTO_PARALLEL</span></code> and <code class="docutils literal notranslate"><span class="pre">SEMI_AUTO_PARALLEL</span></code> modes, the sharded dimensions of weights will be processed automatically by setting random seeds to ensure the initialization of weights are consistent on the devices which belongs to the same data parallel dimension.</p>
</li>
<li><p>Gradient aggregation</p>
<p>Theoretically, the training effect of data parallel network should be the same as that of the standalone network. To ensure the consistency of the calculation logic, the <code class="docutils literal notranslate"><span class="pre">AllReduce</span></code> operator is inserted after gradient calculation to implement the gradient aggregation operation between devices. You can enable <code class="docutils literal notranslate"><span class="pre">mean</span></code> to average the sum of gradient values, or regard <code class="docutils literal notranslate"><span class="pre">mean</span></code> as a hyperparameter. Enabling <code class="docutils literal notranslate"><span class="pre">mean</span></code> is equivalent to reducing the learning rate by multiple times.</p>
</li>
<li><p>Parameter update</p>
<p>Because the gradient aggregation operation is introduced, the models of each device perform parameter update with the same gradient value. Therefore, MindSpore implements a synchronous data parallel training mode. Theoretically, models trained by each device are the same. If the reduce operation on samples is involved on the network, the network output may be different. This is determined by the sharding attribute of data parallelism.</p>
</li>
</ol>
</section>
<section id="data-parallel-code">
<h3>Data Parallel Code<a class="headerlink" href="#data-parallel-code" title="Permalink to this headline"></a></h3>
<ol class="arabic simple">
<li><p>Collective communication</p>
<ul class="simple">
<li><p><a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.3.q1/mindspore/python/mindspore/communication/management.py">management.py</a>: This file covers the <code class="docutils literal notranslate"><span class="pre">helper</span></code> function APIs commonly used during the collective communication process, for example, the APIs for obtaining the number of clusters and device ID. When collective communication is executed on the Ascend chip, the framework loads the <code class="docutils literal notranslate"><span class="pre">libhccl.so</span></code> library file in the environment and uses it to call the communication APIs from the Python layer to the underlying layer.</p></li>
<li><p><a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.3.q1/mindspore/python/mindspore/ops/operations/comm_ops.py">comm_ops.py</a>: MindSpore encapsulates supported collective communication operations as operators and stores the operators in this file. The operators include <code class="docutils literal notranslate"><span class="pre">AllReduce</span></code>, <code class="docutils literal notranslate"><span class="pre">AllGather</span></code>, <code class="docutils literal notranslate"><span class="pre">ReduceScatter</span></code>, and <code class="docutils literal notranslate"><span class="pre">Broadcast</span></code>. <code class="docutils literal notranslate"><span class="pre">PrimitiveWithInfer</span></code> defines the attributes required by the operators, as well as the <code class="docutils literal notranslate"><span class="pre">shape</span></code> and <code class="docutils literal notranslate"><span class="pre">dtype</span></code> inference methods from the input to the output during graph composition.</p></li>
</ul>
</li>
<li><p>Gradient aggregation</p>
<ul class="simple">
<li><p><a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.3.q1/mindspore/python/mindspore/nn/wrap/grad_reducer.py">grad_reducer.py</a>: This file implements the gradient aggregation process. After the input parameter <code class="docutils literal notranslate"><span class="pre">grads</span></code> is expanded by using <code class="docutils literal notranslate"><span class="pre">HyperMap</span></code>, the <code class="docutils literal notranslate"><span class="pre">AllReduce</span></code> operator is inserted. The global communication group is used. You can also perform custom development by referring to this section based on your network requirements. In MindSpore, standalone and distributed execution shares a set of network encapsulation APIs. In the <code class="docutils literal notranslate"><span class="pre">Cell</span></code>, <code class="docutils literal notranslate"><span class="pre">ParallelMode</span></code> is used to determine whether to perform gradient aggregation.</p></li>
</ul>
</li>
</ol>
</section>
</section>
<section id="semi-automatic-parallelism">
<h2>Semi-automatic Parallelism<a class="headerlink" href="#semi-automatic-parallelism" title="Permalink to this headline"></a></h2>
<p>This subsection describes how the <code class="docutils literal notranslate"><span class="pre">ParallelMode.SEMI_AUTO_PARALLEL</span></code> semi-automatic parallel mode works in MindSpore.</p>
<section id="principle-of-semi-automatic-parallelism">
<h3>Principle of Semi-automatic Parallelism<a class="headerlink" href="#principle-of-semi-automatic-parallelism" title="Permalink to this headline"></a></h3>
<p><img alt="Automatic Parallel Description" src="../_images/auto_parallel.png" /></p>
<ol class="arabic">
<li><p>Distributed operators and tensor distribution models</p>
<p>In the above architecture diagram, the automatic parallel process traverses the forward computation graph (ANF Graph) of a single machine, modeling the tensor slice in terms of distributed operators, representing how the input and output tensor of an operator is distributed to each card of the cluster (Tensor Layout). This model adequately expresses the mapping relationship between tensor and device, and the user does not need to perceive the device where each slice of the model is running, and the framework will automatically schedule the assignment.</p>
<p>In order to obtain the tensor distribution model, each operator has a shard strategy, which represents the slice of each input of the operator in the corresponding dimension. In general, any dimension of the tensor can be sliced as long as it satisfies the principle of base-2 and uniform distribution. The following figure is an example of a three-dimensional matrix multiplication (BatchMatMul) operation, whose shard strategy consists of two tuples representing the shard forms of <code class="docutils literal notranslate"><span class="pre">input</span></code> and <code class="docutils literal notranslate"><span class="pre">weight</span></code>, respectively. The elements in the tuple correspond to the tensor dimension one by one: <code class="docutils literal notranslate"><span class="pre">2^N</span></code> is the number of slicing copies, and <code class="docutils literal notranslate"><span class="pre">1</span></code> means no slicing. When the user wants to represent a data parallel shard strategy, i.e. <code class="docutils literal notranslate"><span class="pre">batch</span></code> dimension of <code class="docutils literal notranslate"><span class="pre">input</span></code> is sliced and the other dimensions are not sliced, which can be expressed as <code class="docutils literal notranslate"><span class="pre">strategy=((2^N,</span> <span class="pre">1,</span> <span class="pre">1),(1,</span> <span class="pre">1,</span> <span class="pre">1))</span></code>. When representing a model parallel shard strategy, i.e. non-<code class="docutils literal notranslate"><span class="pre">batch</span></code> dimensions slice of <code class="docutils literal notranslate"><span class="pre">weight</span></code>, here <code class="docutils literal notranslate"><span class="pre">channel</span></code> dimensions slicing is an example, other dimensions are not sliced, which can be expressed as <code class="docutils literal notranslate"><span class="pre">strategy=((1,</span> <span class="pre">1,</span> <span class="pre">1),(1,</span> <span class="pre">1,</span> <span class="pre">2^N))</span></code>. When representing a mixed parallelism shard strategy, one of the shard strategies is <code class="docutils literal notranslate"><span class="pre">strategy=((2^N,</span> <span class="pre">1,</span> <span class="pre">1),(1,</span> <span class="pre">1,</span> <span class="pre">2^N))</span></code>.</p>
<p><img alt="Operator slice definition" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3.q1/docs/mindspore/source_en/design/images/operator_split.png" /></p>
<p>Based on the shard strategy, deriving the distribution model method of the input and output tensor of the operator is defined in the distributed operator. This distribution model consists of <code class="docutils literal notranslate"><span class="pre">device_matrix</span></code>, <code class="docutils literal notranslate"><span class="pre">tensor_shape</span></code> and <code class="docutils literal notranslate"><span class="pre">tensor_map</span></code>, which represent the device matrix shape, tensor shape, and the mapping relationship between device and tensor dimensions, respectively. The distributed operator will further determine whether to insert additional computation and communication operations in the graph according to the tensor distribution model, to ensure that the operator operation logic is correct.</p>
</li>
<li><p>Tensor distribution Transformation</p>
<p>When the output tensor model of the former operator and the input tensor model of the latter operator are not consistent, it is necessary to introduce computational and communication operations to realize the change between tensor arrangements. The automatic parallel process introduces the tensor redistribution algorithm, which can derive an arbitrary inter-tensor communication transition. The following three examples represent the parallel computation of the formula <code class="docutils literal notranslate"><span class="pre">Z=(X×W)×V</span></code>, i.e., two two-dimensional matrix multiplication operations, and show how to convert between different parallel approaches.
In Sample 1, the output of the first data parallel matrix multiplication has a slice in the row direction, while the input of the second model parallel matrix multiplication requires the full-volume tensor. The framework will automatically insert the <code class="docutils literal notranslate"><span class="pre">AllGather</span></code> operator to implement the distribution transformation.</p>
<p><img alt="tensor-redistribution1" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3.q1/docs/mindspore/source_en/design/images/tensor_redistribution1.png" /></p>
<p>In Sample 2, the output of the first model parallel matrix multiplication has a slice in the column direction, while the input of the second data parallel matrix multiplication has a slice in the row direction, and the framework will automatically insert the communication operator equivalent to the <code class="docutils literal notranslate"><span class="pre">AlltoAll</span></code> operation in set communication to implement the distribution transformation.</p>
<p><img alt="tensor-redistribution2" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3.q1/docs/mindspore/source_en/design/images/tensor_redistribution2.png" /></p>
<p>In Sample 3, the output slice of the first hybrid parallel matrix multiplication is the same as the input slice of the second hybrid parallel matrix multiplication, so there is no need to introduce the redistribution transformation. However, since there is a slice in the relevant dimensions of the two inputs in the second matrix multiplication operation, the <code class="docutils literal notranslate"><span class="pre">AllReduce</span></code> operator needs to be inserted to guarantee the correctness of the operation.</p>
<p><img alt="tensor-redistribution3" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3.q1/docs/mindspore/source_en/design/images/tensor_redistribution3.png" /></p>
<p>In summary, samples 1 and 2 are the basis for automatic parallelism implementation. Overall this distributed representation breaks the boundary between data parallelism and model parallelism and easily achieves hybrid parallelism. From the scripting level, the user only needs to construct a single-machine network to express the parallelism algorithm logic, and the framework will automatically achieve the slicing of the whole graph.</p>
</li>
<li><p>Distributed auto-differentiation</p>
<p>The traditional manual model slicing needs to pay attention to the forward network communication and the parallel operation of network reverse. MindSpore automatically generates the reverse communication operator by encapsulating the communication operation as an operator and using the original auto-differentiation operation of the framework, so even in the distributed training, the user only needs to focus on the forward propagation of the network, which truly realizes the fully automatic parallelism of training.</p>
</li>
<li><p>Support multi-dimensional hybrid parallelism</p>
<p>Semi-automatic parallelism supports the automatic mixing of multiple parallel modes, respectively:</p>
<p><strong>Operator-level parallelism</strong>: Operator parallelism takes the operators in a neural network and slices the input tensor to multiple devices for computation. In this way, data samples and model parameters can be distributed among different devices to train large-scale deep learning models and use cluster resources for parallel computing to improve the overall speed. The user can set the shard strategy for each operator, and the framework will model the slice of each operator and its input tensor according to the shard strategy of the operator to maintain mathematical equivalence. This approach can effectively reduce the load on individual devices and improve computational efficiency, and is suitable for training large-scale deep neural networks. For more details, please refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.3.0rc1/parallel/operator_parallel.html">operator-level parallelism</a>.</p>
<p><strong>Pipeline parallism</strong>: When the number of cluster devices is large, if only operator parallelism is used, communication is required over the communication domain of the entire cluster, which may make communication inefficient and thus reduce the overall performance. The pipeline parallelism can slice the neural network structure into multiple stages, and each stage is running in a part of the device, which limits the communication domain of the collective communication to this part of the device, while the inter-stage uses point-to-point communication. The advantages of pipeline parallelism are: improving communication efficiency, and easily handling neural network structures stacked by layers. The disadvantage is that some nodes may be idle at the same time. Foe detailed information, refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.3.0rc1/parallel/pipeline_parallel.html">pipeline parallelism</a>.</p>
<p><strong>MoE parallism</strong>: MoE is to distribute the experts to different workers and each worker takes on different batches of training data. For the non-MoE layer, expert parallelism is the same as data parallelism. In the MoE layer, the tokens in the sequence are sent to the workers corresponding to their matching experts via all-to-all communication. After completing the computation of the corresponding expert, it is then re-passed back to the original worker by all-to-all and organized into the original sequence for computation of the next layer. Since MoE models usually have a large number of experts, the expert parallelism increases more with the size of the model than the model parallelism.</p>
<p><img alt="MoE Parallelism" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3.q1/docs/mindspore/source_zh_cn/design/images/MoE.png" /></p>
<p><strong>Multi-Copy Parallelism</strong>: The data of the input model is sliced according to the batchsize dimension, thus modifying the existing single-copy form into a multi-copy form, so that the underlying layer communicates while the other copy performs the computation operation without waiting. This ensures that the computation and communication times of multiple copies complement each other to improve model performance, while splitting the data into multiple copies also reduces the number of parameters of the operator inputs, thus reducing the computation time of individual operators, which helps a lot to improve model performance.</p>
<p><img alt="multi-copy Parallelism" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3.q1/docs/mindspore/source_zh_cn/design/images/multi_copy.png" /></p>
<p><strong>Optimizer Parallelism</strong>: When training in data parallelism or operator parallelism, the same copy of the model parameters may exist on multiple devices, which allows the optimizer to have redundant computations across multiple devices when updating that weight. In this case, the computation of the optimizer can be spread over multiple devices by optimizer parallelism. Its advantages are: reducing static memory consumption, and the amount of computation within the optimizer. The disadvantages are: increasing communication overhead. For detailed information, refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.3.0rc1/parallel/optimizer_parallel.html">Optimizer Parallelism</a>.</p>
</li>
</ol>
</section>
<section id="semi-automatic-parallel-code">
<h3>Semi-automatic Parallel Code<a class="headerlink" href="#semi-automatic-parallel-code" title="Permalink to this headline"></a></h3>
<ol class="arabic simple">
<li><p>Tensor layout model</p>
<ul class="simple">
<li><p><a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r2.3.q1/mindspore/ccsrc/frontend/parallel/tensor_layout">tensor_layout</a>: This directory contains the definitions and implementation of functions related to the tensor distribution model. <code class="docutils literal notranslate"><span class="pre">tensor_layout.h</span></code> declares the member variables <code class="docutils literal notranslate"><span class="pre">tensor_map_origin_</span></code>, <code class="docutils literal notranslate"><span class="pre">tensor_shape_</span></code>, and <code class="docutils literal notranslate"><span class="pre">device_arrangement_</span></code> required by a tensor distribution model. In <code class="docutils literal notranslate"><span class="pre">tensor_redistribution.h</span></code>, the related methods for implementing the <code class="docutils literal notranslate"><span class="pre">from_origin_</span></code> and <code class="docutils literal notranslate"><span class="pre">to_origin_</span></code> transformation between tensor distributions are declared. The deduced redistribution operation is stored in <code class="docutils literal notranslate"><span class="pre">operator_list_</span></code> and returned, in addition, the communication cost <code class="docutils literal notranslate"><span class="pre">comm_cost_</span></code>,, memory cost <code class="docutils literal notranslate"><span class="pre">memory_cost_</span></code>, and calculation cost <code class="docutils literal notranslate"><span class="pre">computation_cost_</span></code> required for redistribution are calculated.</p></li>
</ul>
</li>
<li><p>Distributed operators</p>
<ul class="simple">
<li><p><a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r2.3.q1/mindspore/ccsrc/frontend/parallel/ops_info">ops_info</a>: This directory contains the implementation of distributed operators. In <code class="docutils literal notranslate"><span class="pre">operator_info.h</span></code>, the base class <code class="docutils literal notranslate"><span class="pre">OperatorInfo</span></code> of distributed operator implementation is defined. A distributed operator to be developed shall inherit the base class and explicitly implement related imaginary functions. The <code class="docutils literal notranslate"><span class="pre">InferTensorInfo</span></code>, <code class="docutils literal notranslate"><span class="pre">InferTensorMap</span></code>, and <code class="docutils literal notranslate"><span class="pre">InferDevMatrixShape</span></code> functions define the algorithms for deriving the input and output tensor distribution model of the operator. The <code class="docutils literal notranslate"><span class="pre">InferForwardCommunication</span></code> and <code class="docutils literal notranslate"><span class="pre">InferMirrorOps</span></code> functions define the extra calculation and communication operations to be inserted for operator sharding. The <code class="docutils literal notranslate"><span class="pre">CheckStrategy</span></code> and <code class="docutils literal notranslate"><span class="pre">GenerateStrategies</span></code> functions define the sharding strategy validation and generation for the operator. According to the sharding strategy <code class="docutils literal notranslate"><span class="pre">SetCostUnderStrategy</span></code>, the parallel cost <code class="docutils literal notranslate"><span class="pre">operator_cost_</span></code> of the distributed operator is generated.</p></li>
</ul>
</li>
<li><p>Device management</p>
<ul class="simple">
<li><p><a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.3.q1/mindspore/ccsrc/frontend/parallel/device_manager.h">device_manager.h</a>: This file is used to create and manage cluster device communication groups. The device matrix model is defined by <code class="docutils literal notranslate"><span class="pre">device_matrix.h</span></code>, and the communication domain is managed by <code class="docutils literal notranslate"><span class="pre">group_manager.h</span></code>.</p></li>
</ul>
</li>
<li><p>Entire graph sharding</p>
<ul class="simple">
<li><p><a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.3.q1/mindspore/ccsrc/frontend/parallel/step_auto_parallel.h">step_auto_parallel.h</a>, and <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.3.q1/mindspore/ccsrc/frontend/parallel/step_parallel.h">step_parallel.h</a>: The two files contain the core implementation of the automatic parallel process. <code class="docutils literal notranslate"><span class="pre">step_auto_parallel.h</span></code> calls the strategy search process and generates the <code class="docutils literal notranslate"><span class="pre">OperatorInfo</span></code> of the distributed operator. Then in <code class="docutils literal notranslate"><span class="pre">step_parallel.h</span></code>, processes such as operator sharding and tensor redistribution are processed to reconstruct the standalone computing graph in distributed mode.</p></li>
</ul>
</li>
<li><p>Backward propagation of communication operators</p>
<ul class="simple">
<li><p><a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.3.q1/mindspore/python/mindspore/ops/_grad_experimental/grad_comm_ops.py">grad_comm_ops.py</a>: This file defines the backward propagation of communication operators, such as <code class="docutils literal notranslate"><span class="pre">AllReduce</span></code> and <code class="docutils literal notranslate"><span class="pre">AllGather</span></code>.</p></li>
</ul>
</li>
</ol>
</section>
</section>
<section id="fully-automatic-parallelism">
<h2>Fully Automatic Parallelism<a class="headerlink" href="#fully-automatic-parallelism" title="Permalink to this headline"></a></h2>
<p>Semi-automatic parallelism frees users from the complexity of distributed code development and greatly reduces the difficulty of developing distributed AI macromodels. Although users no longer need to consider data storage and communication between devices, they still need to specify the appropriate shard strategy for each operator, because the training performance of different shard strategies varies greatly. Users still need to have appropriate parallel knowledge and computational analysis based on network structure, cluster topology, etc. in order to define appropriate parallel strategies in a huge search space. The reality is that the main users of AI frameworks are AI researchers and engineers, who may not have specialized parallelism knowledge. On the other hand, in the face of a huge search space, finding the right parallel strategy for a large model requires monthly manual tuning costs and still does not guarantee the optimal strategy. For example, DeepSpeed, Megatron, and other expert custom strategies for transformer-like networks still require user-defined configurations of dp, mp, pp, etc., not to mention that the network model has more than one transformer structure. For these two reasons, MindSpore provides a variety of automatic hybrid parallel strategy generation schemes to minimize the user’s perception of parallel configurations and allow users to train large models quickly, efficiently, and easily.</p>
<p>This subsection describes how the <code class="docutils literal notranslate"><span class="pre">ParallelMode.AUTO_PARALLEL</span></code> fully automatic parallel mode works in MindSpore.</p>
<section id="feature-design">
<h3>Feature Design<a class="headerlink" href="#feature-design" title="Permalink to this headline"></a></h3>
<p>Fully automatic parallelism is based on the MindSpore semi-automatic framework, replacing expert configuration of parallel strategies with automatic hybrid parallel strategy generation algorithms. The following figure shows the process of using MindSpore to train or inference about a neural network in a distributed manner. Users develop their own neural network models (or MindIR imports) using the Python language, which are parsed into computational graphs (ANF graphs) by MindSpore. The automatic hybrid parallel strategy generation module searches for a better strategy through the algorithm and passes it to the semi-automatic parallel module, which analyzes the tensor distribution, distributed operator analysis, device management, and performs whole graph slicing, and passes it to the back-end for computation.</p>
<p>In fact, the hybrid parallel strategy generation module is responsible for finding a suitable parallel shard strategy for a given neural network model and cluster configuration. The key technology used is a strategy search algorithm based on cost model, which constructs a cost model to describe the computation cost and communication cost in a distributed training scenario, and uses memory cost as a constraint to efficiently search for a better performance parallel strategy through the computational graph search algorithm.</p>
<p><img alt="Fully automatic Parallelism" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3.q1/docs/mindspore/source_zh_cn/design/images/auto.png" /></p>
</section>
<section id="three-search-algorithms">
<h3>Three Search Algorithms<a class="headerlink" href="#three-search-algorithms" title="Permalink to this headline"></a></h3>
<p>Fully automatic parallelism is very difficult to implement, and MindSpore divides the provided strategy generation algorithm into L1 level and L2 level according to the degree of user intervention required (here we assume that the manually configured full graph strategy SEMI_AUTO is L0 level, and the scheme that does not require user participation is L3 level).</p>
<p>The strategy generation algorithm at the L1 level is called Strategy Broadcast (Sharding Propagation). In this mode, the user only needs to manually define the strategies for a few key operators, and the strategies for the remaining operators in the computational graph are automatically generated by the algorithm. Because the strategy of the key operator has been defined, the cost model of the algorithm mainly describes the redistribution cost between the operators, and the optimization objective is to minimize the redistribution cost of the whole graph. Because the main operator strategy has been defined, which is equivalent to a compressed search space, the search time of this scheme is shorter and its strategy performance depends on the definition of the key operator strategy, so it still requires the user to have the ability to analyze the defined strategy. Refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.3.0rc1/parallel/sharding_propagation.html">Sharding Propagation</a> for detailed information.</p>
<p>There are two types of L2-level strategy generation algorithms, Dynamic Programming and Symbolic Automatic Parallel Planner (SAPP for short). Both methods have their advantages and disadvantages. The dynamic programming algorithm is able to search for the optimal strategy inscribed by the cost model, but it takes longer time to search for parallel strategies for huge networks. The SAPP algorithm is able to generate optimal strategies instantaneously for huge networks and large-scale cuts.
The core idea of the dynamic programming algorithm is to build a cost model of the full graph, including computation cost and communication cost, to describe the absolute time delay in the distributed training process, and to compress the search time using equivalent methods such as edge elimination and point elimination, but the search space actually grows exponentially with the number of devices and operators, so it is not efficient for large clusters with large models.
SAPP is modeled based on the parallelism principle by creating an abstract machine to describe the hardware cluster topology and optimizing the cost model by symbolic simplification. Its cost model compares not the predicted absolute latency, but the relative cost of different parallel strategies, so it can greatly compress the search space and guarantee minute search times for 100-card clusters. Refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.3.0rc1/parallel/overview.html">Distributed Parallel Training Mode</a></p>
<p>Sharding Propagation and SAPP currently support manual definition of Pipeline + automatic operator parallelism, and can be used in conjunction with optimizations such as recomputation, optimizer parallelism, etc. Dynamic Programming algorithms only support automatic operator parallelism.</p>
</section>
<section id="fully-automatic-parallelism-code">
<h3>Fully Automatic Parallelism Code<a class="headerlink" href="#fully-automatic-parallelism-code" title="Permalink to this headline"></a></h3>
<p><strong>Strategic Search Algorithm</strong>: The <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r2.3.q1/mindspore/ccsrc/frontend/parallel/auto_parallel">auto_parallel</a> directory implements the algorithm for strategy search. <code class="docutils literal notranslate"><span class="pre">graph_costmodel.h</span></code> defines the composition information, where each point represents an operator <code class="docutils literal notranslate"><span class="pre">OperatorInfo</span></code> and the directed edge <code class="docutils literal notranslate"><span class="pre">edge_costmodel.h</span></code> represents the input-output relation of the operator and the cost of redistribution. The cost model for each operator is defined in <code class="docutils literal notranslate"><span class="pre">operator_costmodel.h</span></code>, including the computation cost, communication cost and memory cost. The data structures for the cost and graph operations are defined in <code class="docutils literal notranslate"><span class="pre">costmodel.h</span></code>.</p>
<ul class="simple">
<li><p><strong>dynamic_programming</strong>: <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.3.q1/mindspore/ccsrc/frontend/parallel/auto_parallel/dp_algo_costmodel.cc">dp_algo_costmodel.cc</a> mainly describes the main flow of the dynamic programming algorithm and consists of a series of graph operations.</p></li>
<li><p><strong>sharding_propagation</strong>: <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.3.q1/mindspore/ccsrc/frontend/parallel/auto_parallel/graph_costmodel.cc">graph_costmodel.cc</a> implements the strategy broadcast (Sharding Propagation), which mainly uses the traversal method of BFS to propagate the strategy of several points, from points to interfaces, to the whole graph.</p></li>
<li><p><strong>symbolic_automatic_parallel_planner</strong>: The <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r2.3.q1/mindspore/ccsrc/frontend/parallel/auto_parallel/rec_core">rec_core</a> directory implements the symbolic automatic parallelplanner (SAPP).</p></li>
</ul>
</section>
</section>
<section id="heterogeneous-parallelism">
<h2>Heterogeneous Parallelism<a class="headerlink" href="#heterogeneous-parallelism" title="Permalink to this headline"></a></h2>
<p>The heterogeneous parallel training method is to analyze the memory occupation and computational intensity of the operators on the graph, and slice the operators with huge memory consumption or suitable for CPU logic processing to the CPU subgraph, and slice the computationally intensive operators with less memory consumption to the hardware accelerator subgraph. The framework cooperates with different subgraphs for network training, so that subgraphs in different hardware and without dependencies can perform the execution process in parallel.</p>
<section id="computational-process">
<h3>Computational Process<a class="headerlink" href="#computational-process" title="Permalink to this headline"></a></h3>
<p>A typical computational process for MindSpore heterogeneous parallel training is shown in the following figure:</p>
<p><img alt="heterogeneous-heter" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3.q1/docs/mindspore/source_zh_cn/design/images/heter.png" /></p>
<ol class="arabic">
<li><p>Users set backend for network execution</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Users set execution backend of specific operators</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span>

<span class="n">prim</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span>

<span class="n">prim</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>The framework is sliced according to the computational graph operator flag.</p></li>
<li><p>The framework schedules different back-end execution subgraphs.</p></li>
</ol>
<p>Current scenarios that typically use heterogeneous parallel computing are: optimizer heterogeneity, Embedding heterogeneity, and PS heterogeneity.</p>
</section>
<section id="optimizer-heterogeneity">
<h3>Optimizer Heterogeneity<a class="headerlink" href="#optimizer-heterogeneity" title="Permalink to this headline"></a></h3>
<p>During the training of a large model in PanGu or GPT3, the optimizer state takes up a large amount of memory, which in turn limits the size of the model that can be trained. Using optimizer heterogeneity, assigning optimizers to CPUs for execution can greatly scale the trainable models:</p>
<p><img alt="heterogeneous-heter-opt" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3.q1/docs/mindspore/source_zh_cn/design/images/heter-opt.png" /></p>
<p>As shown in the figure, configuring the Adam operator to CPU execution while specifying an accelerator for FP16 computation reduces the parameter memory footprint to 1/3 of the original.</p>
<ol class="arabic simple">
<li><p>Configure the optimizer operators to CPU execution</p></li>
<li><p>Initialize weight parameters of FP16 and optimizer state variables of FP32</p></li>
<li><p>Convert the gradient of the input optimizer to FP16 (if the gradient is FP16, you can ignore this step)</p></li>
<li><p>The weights and gradients are converted to FP32 to participate in the optimizer operation</p></li>
<li><p>The updated FP32 weights are assigned to the FP16 weights</p></li>
</ol>
<p>Sample code of the optimizer heterogeneity is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">mindspore.common.initializer</span> <span class="kn">import</span> <span class="n">initializer</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">Optimizer</span>
<span class="n">_adam_opt</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MultitypeFuncGraph</span><span class="p">(</span><span class="s2">&quot;adam_opt&quot;</span><span class="p">)</span>
<span class="n">host_assign</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Assign</span><span class="p">()</span>
<span class="n">host_assign</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>
<span class="n">host_cast</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>
<span class="n">host_cast</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>
<span class="n">device_cast</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>

<span class="nd">@_adam_opt</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;Function&quot;</span><span class="p">,</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;Number&quot;</span><span class="p">,</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;Bool&quot;</span><span class="p">,</span> <span class="s2">&quot;Bool&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_update_run_kernel</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">weight_decay</span><span class="p">,</span> <span class="n">param</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">decay_flags</span><span class="p">,</span> <span class="n">optim_filter</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Update parameters by AdamWeightDecay op.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">success</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">if</span> <span class="n">optim_filter</span><span class="p">:</span>
        <span class="n">param32</span> <span class="o">=</span> <span class="n">host_cast</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">gradient</span> <span class="o">=</span> <span class="n">device_cast</span><span class="p">(</span><span class="n">gradient</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">decay_flags</span><span class="p">:</span>
            <span class="n">next_param</span> <span class="o">=</span> <span class="n">opt</span><span class="p">(</span><span class="n">param32</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">weight_decay</span><span class="p">,</span> <span class="n">gradient</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">next_param</span> <span class="o">=</span> <span class="n">opt</span><span class="p">(</span><span class="n">param32</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">gradient</span><span class="p">)</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">host_assign</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">host_cast</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">param32</span><span class="p">,</span> <span class="n">next_param</span><span class="p">),</span> <span class="n">ops</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">param</span><span class="p">)))</span>
        <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">success</span><span class="p">,</span> <span class="n">ret</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">success</span>

<span class="k">class</span> <span class="nc">AdamWeightDecayOp</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AdamWeightDecayOp</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">weight_decay</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">beta1</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta2</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">beta2</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">eps</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">moments1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clone_param32</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;adam_m&quot;</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">moments2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clone_param32</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;adam_v&quot;</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">opt</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">AdamWeightDecay</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hyper_map</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">HyperMap</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">opt</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gradients</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;AdamWeightDecayOp&quot;&quot;&quot;</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_lr</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_group</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_group_lr</span><span class="p">:</span>
                <span class="n">optim_result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">map_reverse</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_adam_opt</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">opt</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">),</span>
                                                <span class="n">lr</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">moments1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">moments2</span><span class="p">,</span>
                                                <span class="n">gradients</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">decay_flags</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optim_filter</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">optim_result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">map_reverse</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_adam_opt</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">opt</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span> <span class="n">lr</span><span class="p">),</span>
                                                <span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">moments1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">moments2</span><span class="p">,</span>
                                                <span class="n">gradients</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">decay_flags</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optim_filter</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">optim_result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">map_reverse</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_adam_opt</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">opt</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span>
                                                        <span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">moments1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">moments2</span><span class="p">,</span>
                                            <span class="n">gradients</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">decay_flags</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optim_filter</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">optim_result</span>

    <span class="k">def</span> <span class="nf">clone_param32</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">new</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">old_param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">:</span>
            <span class="n">param_init</span> <span class="o">=</span> <span class="n">init</span>
            <span class="k">if</span> <span class="n">init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">param_init</span> <span class="o">=</span> <span class="n">old_param</span><span class="o">.</span><span class="n">init</span>
            <span class="n">new_state</span> <span class="o">=</span> <span class="n">old_param</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
            <span class="n">new_state</span><span class="o">.</span><span class="n">set_dtype</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
            <span class="n">new_state</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="n">param_init</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">old_param</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
            <span class="n">new_state</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">prefix</span> <span class="o">+</span> <span class="s1">&#39;.&#39;</span> <span class="o">+</span> <span class="n">new_state</span><span class="o">.</span><span class="n">name</span>
            <span class="n">new</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_state</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ms</span><span class="o">.</span><span class="n">ParameterTuple</span><span class="p">(</span><span class="n">new</span><span class="p">)</span>
</pre></div>
</div>
<p>Steps 4 and 5 can also be directly fused into the optimizer operator for further optimization. The complete optimizer heterogeneous training process can be found at: <a class="reference external" href="https://gitee.com/mindspore/models/tree/r2.3/official/nlp/Pangu_alpha">https://gitee.com/mindspore/models/tree/r2.3/official/nlp/Pangu_alpha</a>.</p>
</section>
<section id="embedding-heterogeneity">
<h3>Embedding Heterogeneity<a class="headerlink" href="#embedding-heterogeneity" title="Permalink to this headline"></a></h3>
<p>In some networks where large Embedding tables need to be checked, the Embedding tables are often hundreds of gigabytes in size, which is limited by the accelerator memory size and cannot be executed by loading the entire table directly onto the accelerator. By putting the operators connected to the weight table on the CPU for execution, we avoid the problem that the accelerator cannot train the network due to memory limitation.</p>
<p><img alt="heterogeneous-heter-embed" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3.q1/docs/mindspore/source_zh_cn/design/images/heter-embed.png" /></p>
<ol class="arabic">
<li><p>Configure EmbeddingLookup operator to CPU execution</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.common.initializer</span> <span class="kn">import</span> <span class="n">initializer</span>
<span class="k">class</span> <span class="nc">EmbeddingLookupNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="n">param_init</span><span class="o">=</span><span class="s1">&#39;normal&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EmbeddingLookupNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embeddinglookup</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">EmbeddingLookup</span><span class="p">()</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="s1">&#39;CPU&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_table</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="n">param_init</span><span class="p">,</span> <span class="p">[</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;embedding_table&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddinglookup</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_table</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
</li>
<li><p>Configure related sparse optimizer of EmbeddingLookup to CPU execution</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore.nn.optim</span> <span class="kn">import</span> <span class="n">LazyAdam</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">EmbeddingLookupNet</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">LazyAdam</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="s2">&quot;CPU&quot;</span>
</pre></div>
</div>
</li>
</ol>
<p>A sample code for setting up the EmbeddingLookup operator is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.common.initializer</span> <span class="kn">import</span> <span class="n">initializer</span>

<span class="k">class</span> <span class="nc">EmbeddingLookup</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="n">param_init</span><span class="o">=</span><span class="s1">&#39;normal&#39;</span><span class="p">,</span>
                 <span class="n">target</span><span class="o">=</span><span class="s1">&#39;CPU&#39;</span><span class="p">,</span> <span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize EmbeddingLookup.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EmbeddingLookup</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;sparse&#39;</span><span class="p">,</span> <span class="n">sparse</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="s1">&#39;vocab_size&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="n">target</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sparse</span> <span class="o">=</span> <span class="n">sparse</span>
        <span class="k">if</span> <span class="n">sparse</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gatherv2</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">SparseGatherV2</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gatherv2</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Gather</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embeddinglookup</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">EmbeddingLookup</span><span class="p">()</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="s1">&#39;CPU&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">embedding_size</span><span class="p">,</span> <span class="s1">&#39;embedding_size&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_table</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="n">param_init</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span><span class="p">]),</span>
                                            <span class="n">name</span><span class="o">=</span><span class="s1">&#39;embedding_table&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">target</span> <span class="o">==</span> <span class="s2">&quot;CPU&quot;</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddinglookup</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_table</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gatherv2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_table</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
<p>EmbeddingLookup, FTRL, LazyAdam and other operators in the current nn directory are encapsulated the heterogeneous interface, and the user only needs to set the target attribute to CPU or DEVICE to switch the execution backend.</p>
<p>For the overall calling process, refer to <a class="reference external" href="https://gitee.com/mindspore/models/tree/r2.3/official/recommend/Wide_and_Deep">https://gitee.com/mindspore/models/tree/r2.3/official/recommend/Wide_and_Deep</a>.</p>
</section>
<section id="ps-heterogeneity">
<h3>PS Heterogeneity<a class="headerlink" href="#ps-heterogeneity" title="Permalink to this headline"></a></h3>
<p>When the EmbeddingTable reaches T level and the single machine memory cannot be put down, Parameter Server is used to pull and update the weights by heterogeneous Pull/Push operators.</p>
<p><img alt="heterogeneous-heter-ps" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3.q1/docs/mindspore/source_zh_cn/design/images/heter-ps.png" /></p>
<p>Parameter Server encapsulates heterogeneous processes, and users only need to configure parameters to use PS. For the detailed configuration process, refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.3.0rc1/parallel/parameter_server_training.html">Parameter Server training process</a>.</p>
<p>In addition, the process of using PS is also available in the wide&amp;deep network and can be found at: <a class="reference external" href="https://gitee.com/mindspore/models/tree/r2.3/official/recommend/Wide_and_Deep">https://gitee.com/mindspore/models/tree/r2.3/official/recommend/Wide_and_Deep</a>.</p>
</section>
<section id="constraints">
<h3>Constraints<a class="headerlink" href="#constraints" title="Permalink to this headline"></a></h3>
<p>Currently the user needs to specify the back-end of the operator execution, and the back-end does not support automatic configuration based on the network.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="dynamic_graph_and_static_graph.html" class="btn btn-neutral float-left" title="Combination of Dynamic and Static Graphs" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="data_engine.html" class="btn btn-neutral float-right" title="High Performance Data Processing Engine" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>