<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore.numpy.math_ops &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script><script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/js/theme.js"></script><script src="../../../_static/underscore.js"></script><script src="../../../_static/doctools.js"></script><script src="../../../_static/js/mermaid-9.3.0.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../design/overview.html">MindSpore Design Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/programming_paradigm.html">Functional and Object-Oriented Fusion Programming Paradigm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/dynamic_graph_and_static_graph.html">Combination of Dynamic and Static Graphs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/distributed_training_design.html">Distributed Parallel Native</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/data_engine.html">High Performance Data Processing Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/all_scenarios.html">Full-scenarios Unified Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/graph_fusion_engine.html">Graph-Kernel Fusion Acceleration Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/pluggable_device.html">Third-Party Hardware Interconnection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../note/official_models.html">Official Models</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.hal.html">mindspore.hal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.multiprocessing.html">mindspore.multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.experimental.html">mindspore.experimental</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Mapping</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../note/api_mapping/pytorch_api_mapping.html">PyTorch and MindSpore API Mapping Table</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Migration Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../migration_guide/overview.html">Overview of Migration Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../migration_guide/enveriment_preparation.html">Environment Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../migration_guide/analysis_and_preparation.html">Model Analysis and Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../migration_guide/model_development/model_development.html">Network Constructing Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../migration_guide/debug_and_tune.html">Debugging and Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../migration_guide/sample_code.html">Network Migration Debugging Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../migration_guide/faq.html">FAQs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Syntax Support</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../note/static_graph_syntax_support.html">Static Graph Syntax Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../note/static_graph_syntax/operators.html">Static Graph Syntax - Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../note/static_graph_syntax/statements.html">Static Graph Syntax - Python Statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../note/static_graph_syntax/python_builtin_functions.html">Static Graph Syntax - Python Built-in Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../note/index_support.html">Tensor Index Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Environment Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../note/env_var_list.html">Environment Variables</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/data_processing.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/implement_problem.html">Implement Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/network_compilation.html">Network Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/operators_compile.html">Operators Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/usage_migrate_3rd.html">Migration from a Third-party Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/precision_tuning.html">Precision Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/distributed_parallel.html">Distributed Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/feature_advice.html">Feature Advice</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../index.html">Module code</a> &raquo;</li>
      <li>mindspore.numpy.math_ops</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for mindspore.numpy.math_ops</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2020-2021 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>
<span class="sd">&quot;&quot;&quot;math operations, the function docs are adapted from Numpy API.&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span>

<span class="kn">import</span> <span class="nn">operator</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">nptype</span>

<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">composite</span> <span class="k">as</span> <span class="n">C</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.primitive</span> <span class="kn">import</span> <span class="n">constexpr</span><span class="p">,</span> <span class="n">_primexpr</span>
<span class="kn">from</span> <span class="nn">mindspore.common</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="kn">from</span> <span class="nn">mindspore.common</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">typing</span>

<span class="kn">from</span> <span class="nn">mindspore.numpy.dtypes</span> <span class="kn">import</span> <span class="n">nan</span><span class="p">,</span> <span class="n">pi</span><span class="p">,</span> <span class="n">dtype_map</span><span class="p">,</span> <span class="n">inf</span>

<span class="kn">from</span> <span class="nn">mindspore.numpy.array_creations</span> <span class="kn">import</span> <span class="n">asarray_const</span><span class="p">,</span> <span class="n">ones</span><span class="p">,</span> <span class="n">zeros</span><span class="p">,</span> <span class="n">empty</span><span class="p">,</span> <span class="n">full</span><span class="p">,</span> <span class="n">full_like</span><span class="p">,</span> <span class="n">diag</span><span class="p">,</span> \
    <span class="n">arange</span><span class="p">,</span> <span class="n">histogram_bin_edges</span><span class="p">,</span> <span class="n">eye</span>
<span class="kn">from</span> <span class="nn">mindspore.numpy.array_ops</span> <span class="kn">import</span> <span class="n">where</span> <span class="k">as</span> <span class="n">where_</span>
<span class="kn">from</span> <span class="nn">mindspore.numpy.array_ops</span> <span class="kn">import</span> <span class="n">ravel</span><span class="p">,</span> <span class="n">expand_dims</span><span class="p">,</span> <span class="n">moveaxis</span><span class="p">,</span> <span class="n">concatenate</span><span class="p">,</span> <span class="n">flip</span><span class="p">,</span> <span class="n">stack</span><span class="p">,</span> <span class="n">atleast_1d</span><span class="p">,</span> \
    <span class="n">split</span>

<span class="kn">from</span> <span class="nn">mindspore.numpy.utils_const</span> <span class="kn">import</span> <span class="n">_infer_out_shape</span><span class="p">,</span> <span class="n">_check_axis_valid</span><span class="p">,</span> <span class="n">_get_device</span><span class="p">,</span> \
    <span class="n">_check_shape_aligned</span><span class="p">,</span> <span class="n">_raise_type_error</span><span class="p">,</span> <span class="n">_check_same_type</span><span class="p">,</span> <span class="n">_check_is_float</span><span class="p">,</span> \
    <span class="n">_raise_value_error</span><span class="p">,</span> <span class="n">_promote</span><span class="p">,</span> <span class="n">_check_axis_type</span><span class="p">,</span> <span class="n">_canonicalize_axis</span><span class="p">,</span> \
    <span class="n">_is_shape_empty</span><span class="p">,</span> <span class="n">_check_is_int</span><span class="p">,</span> <span class="n">_expanded_shape</span><span class="p">,</span> <span class="n">_check_axis_in_range</span><span class="p">,</span> \
    <span class="n">_check_dtype</span><span class="p">,</span> <span class="n">_list_comprehensions</span><span class="p">,</span> <span class="n">_tuple_setitem</span><span class="p">,</span> <span class="n">_add_unit_axes</span><span class="p">,</span> <span class="n">_seq_prod</span><span class="p">,</span> \
    <span class="n">_make_tensor</span><span class="p">,</span> <span class="n">_promote_for_trigonometric</span><span class="p">,</span> <span class="n">_raise_runtime_error</span><span class="p">,</span> <span class="n">_max</span><span class="p">,</span> <span class="n">_type_convert</span><span class="p">,</span> \
    <span class="n">_raise_unimplemented_error</span><span class="p">,</span> <span class="n">_abs</span><span class="p">,</span> <span class="n">_in</span><span class="p">,</span> <span class="n">_tuple_slice</span><span class="p">,</span> <span class="n">_check_is_inf</span>
<span class="kn">from</span> <span class="nn">mindspore.numpy.utils</span> <span class="kn">import</span> <span class="n">_expand</span><span class="p">,</span> <span class="n">_broadcast_to</span><span class="p">,</span> <span class="n">_broadcast_to_shape</span><span class="p">,</span> <span class="n">_check_input_tensor</span><span class="p">,</span> \
    <span class="n">_to_tensor</span><span class="p">,</span> <span class="n">_to_tensor_origin_dtype</span><span class="p">,</span> <span class="n">_isnan</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.composite.multitype_ops._compile_utils</span> <span class="kn">import</span> <span class="n">reduce_</span>


<span class="n">ZERO_TENSOR</span> <span class="o">=</span> <span class="n">asarray_const</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>


<span class="n">_mean_keepdims</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceMean</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">_matmul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">MatMul</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
<span class="n">_matmul_t</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">MatMul</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">_reduce_sum_default</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">()</span>
<span class="n">_reduce_sum_keepdims</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">_reduce_min_default</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceMin</span><span class="p">()</span>
<span class="n">_reduce_min_keepdims</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceMin</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">_reduce_max_default</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceMax</span><span class="p">()</span>
<span class="n">_reduce_max_keepdims</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceMax</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">_cumsum_default</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">CumSum</span><span class="p">()</span>
<span class="n">_concat</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">_cumprod_default</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">CumProd</span><span class="p">()</span>
<span class="n">_round</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Round</span><span class="p">()</span>
<span class="n">_rint</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Rint</span><span class="p">()</span>



<div class="viewcode-block" id="absolute"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.absolute.html#mindspore.numpy.absolute">[docs]</a><span class="k">def</span> <span class="nf">absolute</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the absolute value element-wise.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>
<span class="sd">        Currently the backend kernel only supports float calculation, if the input</span>
<span class="sd">        is not a `float`, then it will be casted to :class:`mstype.float32` and casted back.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Tensor to be used for calculation.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If input arguments have types not specified above.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.asarray([1, 2, 3, -4, -5], np.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = np.absolute(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 2. 3. 4. 5.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">original_dtype</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">allowed_types</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">_get_device</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;Ascend&quot;</span><span class="p">:</span>
        <span class="n">allowed_types</span> <span class="o">=</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">allowed_types</span> <span class="o">=</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">original_dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">allowed_types</span> <span class="ow">and</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">absolute</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">original_dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">absolute</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="count_nonzero"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.count_nonzero.html#mindspore.numpy.count_nonzero">[docs]</a><span class="k">def</span> <span class="nf">count_nonzero</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Counts the number of non-zero values in the tensor `x`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The tensor for which to count non-zeros.</span>
<span class="sd">        axis (Union[int,tuple], optional): Axis or tuple of axes along which to</span>
<span class="sd">            count non-zeros. Default is None, meaning that non-zeros will be counted</span>
<span class="sd">            along a flattened version of `x`. Default: ``None`` .</span>
<span class="sd">        keepdims (bool, optional): If this is set to True, the axes that are counted</span>
<span class="sd">            are left in the result as dimensions with size one. With this option,</span>
<span class="sd">            the result will broadcast correctly against `x`. Default: ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, indicating number of non-zero values in the `x` along a given axis.</span>
<span class="sd">        Otherwise, the total number of non-zero values in `x` is returned.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If axis is not int or tuple.</span>
<span class="sd">        ValueError: If axis is not in range [-x.ndim, x.ndim).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.asarray([1, 2, 3, -4, 0, 3, 2, 0])</span>
<span class="sd">        &gt;&gt;&gt; output = np.count_nonzero(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        6</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">_is_shape_empty</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">ZERO_TENSOR</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">count_nonzero</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">)</span></div>


<div class="viewcode-block" id="clip"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.clip.html#mindspore.numpy.clip">[docs]</a><span class="k">def</span> <span class="nf">clip</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Clips (limits) the values in an array.</span>

<span class="sd">    Given an interval, values outside the interval are clipped to the interval edges.</span>
<span class="sd">    For example, if an interval of :math:`[0, 1]` is specified, values smaller than 0 become 0,</span>
<span class="sd">    and values larger than 1 become 1.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Tensor containing elements to clip.</span>
<span class="sd">        xmin (Tensor, scalar, None): Minimum value. If None, clipping is not performed</span>
<span class="sd">            on lower interval edge. Not more than one of `xmin` and `xmax` may be None.</span>
<span class="sd">        xmax (Tensor, scalar, None): Maximum value. If None, clipping is not performed</span>
<span class="sd">            on upper interval edge. Not more than one of `xmin` and `xmax` may be None.</span>
<span class="sd">            If `xmin` or `xmax` are tensors, then the three tensors will be broadcasted</span>
<span class="sd">            to match their shapes.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, a tensor with the elements of `x`, but where values</span>
<span class="sd">        &lt; `xmin` are replaced with `xmin`, and those &gt; `xmax` with `xmax`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If inputs have types not specified above.</span>
<span class="sd">        ValueError: If the shapes of `x1` and `x2` cannot broadcast, or both `xmin` and `xmax` are `None`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.asarray([1, 2, 3, -4, 0, 3, 2, 0])</span>
<span class="sd">        &gt;&gt;&gt; output = np.clip(x, 0, 2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1 2 2 0 0 2 2 0]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">xmin</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">xmax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;One of max or min must be given.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">xmin</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">maximum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">xmin</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">xmax</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">minimum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">xmax</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span></div>


<div class="viewcode-block" id="deg2rad"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.deg2rad.html#mindspore.numpy.deg2rad">[docs]</a><span class="k">def</span> <span class="nf">deg2rad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts angles from degrees to radians.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Angles in degrees.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the corresponding angle in radians. This is a tensor scalar if `x`</span>
<span class="sd">        is a tensor scalar.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.asarray([1, 2, 3, -4, -5])</span>
<span class="sd">        &gt;&gt;&gt; output = np.deg2rad(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0.01745329  0.03490658  0.05235988 -0.06981317 -0.08726647]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">convert</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">pi</span> <span class="o">/</span> <span class="mf">180.0</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">convert</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="rad2deg"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.rad2deg.html#mindspore.numpy.rad2deg">[docs]</a><span class="k">def</span> <span class="nf">rad2deg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts angles from radians to degrees.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Angles in radians.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the corresponding angle in degrees. This is a tensor scalar if `x`</span>
<span class="sd">        is a tensor scalar.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.asarray([1, 2, 3, -4, -5])</span>
<span class="sd">        &gt;&gt;&gt; output = np.rad2deg(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [  57.295776  114.59155   171.88733  -229.1831   -286.47888 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">convert</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="mf">180.0</span> <span class="o">/</span> <span class="n">pi</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">convert</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="add"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.add.html#mindspore.numpy.add">[docs]</a><span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adds arguments element-wise.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Tensor): input to be added.</span>
<span class="sd">        x2 (Tensor): input to be added.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, the sum of `x1` and `x2`, element-wise. This is a scalar</span>
<span class="sd">        if both `x1` and `x2` are scalars.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x1 = np.full((3, 2), [1, 2])</span>
<span class="sd">        &gt;&gt;&gt; x2 = np.full((3, 2), [3, 4])</span>
<span class="sd">        &gt;&gt;&gt; output = np.add(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[4 6]</span>
<span class="sd">        [4 6]</span>
<span class="sd">        [4 6]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># broadcast is not fully supported in tensor_add on CPU,</span>
    <span class="c1"># so we use tensor_sub as a substitute solution</span>
    <span class="k">if</span> <span class="n">_get_device</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;CPU&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">subtract</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">neg_tensor</span><span class="p">(</span><span class="n">_to_tensor</span><span class="p">(</span><span class="n">x2</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">tensor_add</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="subtract"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.subtract.html#mindspore.numpy.subtract">[docs]</a><span class="k">def</span> <span class="nf">subtract</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Subtracts arguments, element-wise.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Tensor): The input to be subtracted from.</span>
<span class="sd">        x2 (Tensor): The input to be subtracted by.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, the difference of `x1` and `x2`, element-wise. This is a</span>
<span class="sd">        scalar if both `x1` and `x2` are scalars.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x1 = np.full((3, 2), [1, 2])</span>
<span class="sd">        &gt;&gt;&gt; x2 = np.full((3, 2), [3, 4])</span>
<span class="sd">        &gt;&gt;&gt; output = np.subtract(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-2 -2]</span>
<span class="sd">        [-2 -2]</span>
<span class="sd">        [-2 -2]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">tensor_sub</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="multiply"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.multiply.html#mindspore.numpy.multiply">[docs]</a><span class="k">def</span> <span class="nf">multiply</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multiplies arguments element-wise.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Tensor): input tensor to be multiplied.</span>
<span class="sd">        x2 (Tensor): input tensor to be multiplied.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, the product of `x1` and `x2`, element-wise. This is a scalar</span>
<span class="sd">        if both `x1` and `x2` are scalars.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x1 = np.full((3, 2), [1, 2])</span>
<span class="sd">        &gt;&gt;&gt; x2 = np.full((3, 2), [3, 4])</span>
<span class="sd">        &gt;&gt;&gt; output = np.multiply(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[3 8]</span>
<span class="sd">        [3 8]</span>
<span class="sd">        [3 8]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">_get_device</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;CPU&#39;</span><span class="p">:</span>
        <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
        <span class="c1"># broadcast is not fully supported on CPU backend,</span>
        <span class="c1"># and explicit broadcasting is performed</span>
        <span class="n">shape_out</span> <span class="o">=</span> <span class="n">_infer_out_shape</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x1</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x2</span><span class="p">))</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">_broadcast_to_shape</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">shape_out</span><span class="p">)</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">_broadcast_to_shape</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">shape_out</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">tensor_mul</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="divide"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.divide.html#mindspore.numpy.divide">[docs]</a><span class="k">def</span> <span class="nf">divide</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a true division of the inputs, element-wise.</span>

<span class="sd">    Instead of the Python traditional &quot;floor division&quot;, this returns a true</span>
<span class="sd">    division.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Tensor): the divident.</span>
<span class="sd">        x2 (Tensor): the divisor.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, this is a scalar if both `x1` and `x2` are scalars.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x1 = np.full((3, 2), [1, 2])</span>
<span class="sd">        &gt;&gt;&gt; x2 = np.full((3, 2), [3, 4])</span>
<span class="sd">        &gt;&gt;&gt; output = np.divide(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0.33333334 0.5       ]</span>
<span class="sd">        [0.33333334 0.5       ]</span>
<span class="sd">        [0.33333334 0.5       ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_is_float</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">x1</span><span class="p">))</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">_check_is_float</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">x2</span><span class="p">)):</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">tensor_div</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="true_divide"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.true_divide.html#mindspore.numpy.true_divide">[docs]</a><span class="k">def</span> <span class="nf">true_divide</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a true division of the inputs, element-wise.</span>

<span class="sd">    Instead of the Python traditional &quot;floor division&quot;, this returns a true</span>
<span class="sd">    division.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Tensor): the dividend.</span>
<span class="sd">        x2 (Tensor): the divisor.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, this is a scalar if both `x1` and `x2` are scalars.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x1 = np.full((3, 2), [1, 2])</span>
<span class="sd">        &gt;&gt;&gt; x2 = np.full((3, 2), [3, 4])</span>
<span class="sd">        &gt;&gt;&gt; output = np.true_divide(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0.33333334 0.5       ]</span>
<span class="sd">        [0.33333334 0.5       ]</span>
<span class="sd">        [0.33333334 0.5       ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">divide</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="power"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.power.html#mindspore.numpy.power">[docs]</a><span class="k">def</span> <span class="nf">power</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    First array elements raised to powers from second array, element-wise.</span>

<span class="sd">    Raises each base in `x1` to the positionally-corresponding power in `x2`.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>
<span class="sd">        On GPU, the supported dtypes are np.float16, and np.float32.</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Tensor): The bases.</span>
<span class="sd">        x2 (Tensor): The exponents.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, the bases in `x1` raised to the exponents in `x2`. This</span>
<span class="sd">        is a scalar if both `x1` and `x2` are scalars.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x1 = np.full((3, 2), [1, 2]).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; x2 = np.full((3, 2), [3, 4]).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = np.power(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 1. 16.]</span>
<span class="sd">        [ 1. 16.]</span>
<span class="sd">        [ 1. 16.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">tensor_pow</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="float_power"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.float_power.html#mindspore.numpy.float_power">[docs]</a><span class="k">def</span> <span class="nf">float_power</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    First array elements raised to powers from second array, element-wise.</span>

<span class="sd">    Raise each base in `x1` to the positionally-corresponding power in `x2`. `x1` and</span>
<span class="sd">    `x2` must be broadcastable to the same shape. This differs from the power</span>
<span class="sd">    function in that integers, float16, and float64 are promoted to floats with</span>
<span class="sd">    a minimum precision of float32 so that the result is always inexact. The</span>
<span class="sd">    intent is that the function will return a usable result for negative powers</span>
<span class="sd">    and seldom overflow for positive powers.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>
<span class="sd">        Integers and floats are promoted to float32 instead of float64.</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Tensor): the bases.</span>
<span class="sd">        x2 (Tensor): the exponents.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, the bases in `x1` raised to the exponents in `x2`. This</span>
<span class="sd">        is a scalar if both `x1` and `x2` are scalars.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x1 = np.arange(6)</span>
<span class="sd">        &gt;&gt;&gt; x2 = np.array(3)</span>
<span class="sd">        &gt;&gt;&gt; output = np.float_power(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [  0.   1.   8.  27.  64. 125.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">x1</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">x2</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">tensor_pow</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="minimum"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.minimum.html#mindspore.numpy.minimum">[docs]</a><span class="k">def</span> <span class="nf">minimum</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise minimum of tensor elements.</span>

<span class="sd">    Compares two tensors and returns a new tensor containing the element-wise minima.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>
<span class="sd">        On Ascend, input arrays containing inf or NaN are not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Tensor): first input tensor to be compared.</span>
<span class="sd">        x2 (Tensor): second input tensor to be compared.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">       Tensor, element-wise minimum of `x1` and `x2`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If inputs have types not specified above.</span>
<span class="sd">        ValueError: If the shapes of `x1` and `x2` cannot be broadcast.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.asarray([1, 2])</span>
<span class="sd">        &gt;&gt;&gt; b = np.asarray([[1, 3],[1, 4]])</span>
<span class="sd">        &gt;&gt;&gt; print(np.minimum(a, b))</span>
<span class="sd">        [[1 2]</span>
<span class="sd">        [1 2]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">asarray_const</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;Input x1 is expected to be array_like&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">asarray_const</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;Input x2 is expected to be array_like&quot;</span><span class="p">)</span>

    <span class="c1"># if both are scalars, expand x1 to 1d tensor, since cpu kernel doesn&#39;t support</span>
    <span class="c1"># comparisons with 2 scalars</span>
    <span class="k">if</span> <span class="n">x1</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">x2</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">expand_dims</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_prop_nan</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">minimum</span><span class="p">),</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">x1</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">x2</span><span class="o">.</span><span class="n">dtype</span>
    <span class="k">elif</span> <span class="n">x2</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">x1</span><span class="o">.</span><span class="n">dtype</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_prop_nan</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">minimum</span><span class="p">),</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="mean"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.mean.html#mindspore.numpy.mean">[docs]</a><span class="k">def</span> <span class="nf">mean</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the arithmetic mean along the specified axis.</span>

<span class="sd">    Returns the average of the array elements. The average is taken</span>
<span class="sd">    over the flattened array by default, otherwise over the specified</span>
<span class="sd">    axis.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out` is not supported.</span>
<span class="sd">        On GPU, the supported dtypes are np.float16, and np.float32.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): input tensor containing numbers whose mean is desired.</span>
<span class="sd">                    If a is not an array, a conversion is attempted.</span>
<span class="sd">        axis (None or int or tuple of integers, optional): Axis or axes along</span>
<span class="sd">                    which the means are computed. The default is to compute</span>
<span class="sd">                    the mean of the flattened array. If this is a tuple of</span>
<span class="sd">                    ints, a mean is performed over multiple axes.</span>
<span class="sd">        keepdims (bool, optional): If this is set to ``True`` , the axes which</span>
<span class="sd">                    are reduced are left in the result as dimensions with</span>
<span class="sd">                    size one. With this option, the result will broadcast</span>
<span class="sd">                    correctly against the input tensor.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, an array containing the mean values.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If axes are out of the range of ``[-a.ndim, a.ndim)``, or</span>
<span class="sd">            if the axes contain duplicates.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.arange(6, dtype=&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = np.mean(a, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        2.5</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_reduce</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceMean</span><span class="p">(</span><span class="n">keepdims</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="inner"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.inner.html#mindspore.numpy.inner">[docs]</a><span class="k">def</span> <span class="nf">inner</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the inner product of two tensors.</span>

<span class="sd">    Ordinary inner product of vectors for 1-D tensors (without complex</span>
<span class="sd">    conjugation), in higher dimensions a sum product over the last</span>
<span class="sd">    axes.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy argument `out` is not supported.</span>
<span class="sd">        On GPU, the supported dtypes are np.float16, and np.float32.</span>
<span class="sd">        On CPU, the supported dtypes are np.float16, np.float32, and</span>
<span class="sd">        np.float64.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): input tensor. If `a` and `b` are nonscalar, their last</span>
<span class="sd">                    dimensions must match.</span>
<span class="sd">        b (Tensor): input tensor. If `a` and `b` are nonscalar, their last</span>
<span class="sd">                    dimensions must match.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If ``x1.shape[-1] != x2.shape[-1]``.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.ones((5, 3))</span>
<span class="sd">        &gt;&gt;&gt; b = np.ones((2, 7, 3))</span>
<span class="sd">        &gt;&gt;&gt; output = np.inner(a, b)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[3. 3. 3. 3. 3. 3. 3.]</span>
<span class="sd">        [3. 3. 3. 3. 3. 3. 3.]]</span>
<span class="sd">        [[3. 3. 3. 3. 3. 3. 3.]</span>
<span class="sd">        [3. 3. 3. 3. 3. 3. 3.]]</span>
<span class="sd">        [[3. 3. 3. 3. 3. 3. 3.]</span>
<span class="sd">        [3. 3. 3. 3. 3. 3. 3.]]</span>
<span class="sd">        [[3. 3. 3. 3. 3. 3. 3.]</span>
<span class="sd">        [3. 3. 3. 3. 3. 3. 3.]]</span>
<span class="sd">        [[3. 3. 3. 3. 3. 3. 3.]</span>
<span class="sd">        [3. 3. 3. 3. 3. 3. 3.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_mul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

    <span class="n">_check_shape_aligned</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>
    <span class="n">aligned_shape_a</span> <span class="o">=</span> <span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape_mul</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">a</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">a</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">aligned_shape_b</span> <span class="o">=</span> <span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape_mul</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">b</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">a</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">a_aligned</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">aligned_shape_a</span><span class="p">)</span>
    <span class="n">b_aligned</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">aligned_shape_b</span><span class="p">)</span>

    <span class="n">res</span> <span class="o">=</span> <span class="n">_matmul_t</span><span class="p">(</span><span class="n">a_aligned</span><span class="p">,</span> <span class="n">b_aligned</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">a</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">b</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">res</span></div>


<div class="viewcode-block" id="dot"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.dot.html#mindspore.numpy.dot">[docs]</a><span class="k">def</span> <span class="nf">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the dot product of two arrays.</span>

<span class="sd">    Specifically,</span>
<span class="sd">    If both `a` and `b` are 1-D arrays, it is inner product of vectors</span>
<span class="sd">    (without complex conjugation).</span>
<span class="sd">    If both `a` and `b` are 2-D arrays, it is matrix multiplication.</span>
<span class="sd">    If either `a` or `b` is 0-D (scalar), it is equivalent to multiply.</span>
<span class="sd">    If `a` is an `N-D` array and `b` is a 1-D array, it is a sum product</span>
<span class="sd">    over the last axis of `a` and `b`.</span>
<span class="sd">    If `a` is an `N-D` array and `b` is an `M-D` array (where ``M&gt;=2``), it is a</span>
<span class="sd">    sum product over the last axis of `a` and the second-to-last axis of `b`:</span>
<span class="sd">    ``dot(a, b)[i,j,k,m] = sum(a[i,j,:] * b[k,:,m])``</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy argument `out` is not supported.</span>
<span class="sd">        On GPU, the supported dtypes are np.float16, and np.float32.</span>
<span class="sd">        On CPU, the supported dtypes are np.float16, np.float32, and</span>
<span class="sd">        np.float64.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): input tensor</span>
<span class="sd">        b (Tensor): input tensor</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, the dot product of `a` and `b`. If `a` and `b` are</span>
<span class="sd">        both scalars or both 1-D arrays then a scalar is returned;</span>
<span class="sd">        otherwise an array is returned</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the last dimension of `a` is not the same size</span>
<span class="sd">            as the second-to-last dimension of `b`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.full((1, 3), 7).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; b = np.full((2, 3, 4), 5).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = np.dot(a, b)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[105. 105. 105. 105.]</span>
<span class="sd">        [105. 105. 105. 105.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">_check</span><span class="p">(</span><span class="n">dim_a</span><span class="p">,</span> <span class="n">dim_b</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">dim_a</span> <span class="o">!=</span> <span class="n">dim_b</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;shapes are not aligned&#39;</span><span class="p">)</span>

    <span class="n">ndim_a</span><span class="p">,</span> <span class="n">ndim_b</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ndim_a</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">ndim_b</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_mul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ndim_a</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">ndim_b</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">perm</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">make_range</span><span class="p">(</span><span class="n">ndim_b</span><span class="p">)</span>
        <span class="n">perm</span> <span class="o">=</span> <span class="n">perm</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">perm</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],)</span> <span class="o">+</span> <span class="p">(</span><span class="n">perm</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],)</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">perm</span><span class="p">)</span>

    <span class="n">_check</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">a</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">b</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">a_aligned</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">a</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">b_aligned</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">b</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

    <span class="n">res</span> <span class="o">=</span> <span class="n">_matmul_t</span><span class="p">(</span><span class="n">a_aligned</span><span class="p">,</span> <span class="n">b_aligned</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">a</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">b</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">res</span></div>


<div class="viewcode-block" id="outer"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.outer.html#mindspore.numpy.outer">[docs]</a><span class="k">def</span> <span class="nf">outer</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the outer product of two vectors.</span>

<span class="sd">    Given two vectors, ``a = [a0, a1, ..., aM]`` and ``b = [b0, b1, ..., bN]``,</span>
<span class="sd">    the outer product is:</span>

<span class="sd">    ``[[a0*b0  a0*b1 ... a0*bN ]``</span>

<span class="sd">    ``[a1*b0    .              ]``</span>

<span class="sd">    ``[ ...          .         ]``</span>

<span class="sd">    ``[aM*b0            aM*bN ]]``</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy argument ``out`` is not supported.</span>
<span class="sd">        On GPU, the supported dtypes are np.float16, and np.float32.</span>
<span class="sd">        On CPU, the supported dtypes are np.float16, np.float32, and</span>
<span class="sd">        np.float64.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): first input vector. Input is flattened if not</span>
<span class="sd">                    already 1-dimensional.</span>
<span class="sd">        b (Tensor): second input vector. Input is flattened if not</span>
<span class="sd">                    already 1-dimensional.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, ``out[i, j] = a[i] * b[j]``.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input is not a tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.full(7, 2).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; b = np.full(4, 3).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = np.outer(a, b)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[6. 6. 6. 6.]</span>
<span class="sd">        [6. 6. 6. 6.]</span>
<span class="sd">        [6. 6. 6. 6.]</span>
<span class="sd">        [6. 6. 6. 6.]</span>
<span class="sd">        [6. 6. 6. 6.]</span>
<span class="sd">        [6. 6. 6. 6.]</span>
<span class="sd">        [6. 6. 6. 6.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">ravel</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">ravel</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">a</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">_expand</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span></div>


<div class="viewcode-block" id="tensordot"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.tensordot.html#mindspore.numpy.tensordot">[docs]</a><span class="k">def</span> <span class="nf">tensordot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes tensor dot product along specified axes.</span>

<span class="sd">    Given two tensors, `a` and `b`, and an array_like object containing two array_like</span>
<span class="sd">    objects, `(a_axes, b_axes)`, sum the products of `a`&#39;s and `b`&#39;s elements (components)</span>
<span class="sd">    over the axes specified by `a_axes` and `b_axes`. The third argument can be a single</span>
<span class="sd">    non-negative integer_like scalar, `N`; if it is such, then the last `N` dimensions of</span>
<span class="sd">    `a` and the first `N` dimensions of `b` are summed over.</span>
<span class="sd">    Three common use cases are:</span>

<span class="sd">    - ``axes = 0`` : tensor product</span>

<span class="sd">    - ``axes = 1`` : tensor dot product</span>

<span class="sd">    - ``axes = 2`` : (default) tensor double contraction</span>

<span class="sd">    When axes is integer_like, the sequence for evaluation will be: first the `-Nth`</span>
<span class="sd">    axis in `a` and 0th axis in `b`, and the -1th axis in `a` and `Nth` axis in `b` last.</span>
<span class="sd">    When there is more than one axis to sum over - and they are not the last (first)</span>
<span class="sd">    axes of `a` `(b)` - the argument axes should consist of two sequences of the same</span>
<span class="sd">    length, with the first axis to sum over given first in both sequences, the second</span>
<span class="sd">    axis second, and so forth.</span>
<span class="sd">    The shape of the result consists of the non-contracted axes of the first tensor,</span>
<span class="sd">    followed by the non-contracted axes of the second.</span>

<span class="sd">    Note:</span>
<span class="sd">        On CPU, the supported dypes are np.float16 and np.float32.</span>
<span class="sd">        On GPU, the supported dypes are np.float16 and np.float32.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): Tensor to &quot;dot&quot;.</span>
<span class="sd">        b (Tensor): Tensor to &quot;dot&quot;.</span>
<span class="sd">        axes (int or sequence of ints):</span>

<span class="sd">            integer_like: If an int `N`, sum over the last `N` axes of `a` and the first `N`</span>
<span class="sd">            axes of `b` in order. The sizes of the corresponding axes must match.</span>

<span class="sd">            sequence of ints: Or, a list of axes to be summed over, first sequence</span>
<span class="sd">            applying to `a`, second to `b`. Both elements `array_like` must be of the same</span>
<span class="sd">            length.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, or list of tensors, the tensor dot product of the input.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.ones((3, 4, 5))</span>
<span class="sd">        &gt;&gt;&gt; b = np.ones((4, 3, 2))</span>
<span class="sd">        &gt;&gt;&gt; output = np.tensordot(a, b, axes=([1,0],[0,1]))</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (5, 2)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="o">*</span><span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">axes</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_mul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">tensor_dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">axes</span><span class="p">)</span></div>


<div class="viewcode-block" id="std"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.std.html#mindspore.numpy.std">[docs]</a><span class="k">def</span> <span class="nf">std</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the standard deviation along the specified axis.</span>
<span class="sd">    The standard deviation is the square root of the average of the squared deviations</span>
<span class="sd">    from the mean, i.e., :math:`std = sqrt(mean(abs(x - x.mean())**2))`.</span>

<span class="sd">    Returns the standard deviation, which is computed for the flattened array by default,</span>
<span class="sd">    otherwise over the specified axis.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `dtype`, `out` and `where` are not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): A Tensor to be calculated.</span>
<span class="sd">        axis (Union[None, int, tuple(int)]): Axis or axes along which the standard</span>
<span class="sd">            deviation is computed. Default: ``None`` .</span>

<span class="sd">            If ``None`` , compute the standard deviation of the flattened array.</span>
<span class="sd">        ddof (int): Means Delta Degrees of Freedom. The divisor used in calculations is :math:`N - ddof`,</span>
<span class="sd">            where :math:`N` represents the number of elements. Default: 0.</span>
<span class="sd">        keepdims: If this is set to True, the axes which are reduced are left in the result as</span>
<span class="sd">            dimensions with size one. With this option, the result will broadcast correctly against the input tensor.</span>
<span class="sd">            If the default value is passed, then keepdims will not be passed through to the std method of</span>
<span class="sd">            sub-classes of tensor, however any non-default value will be. If the sub-class’ method does not</span>
<span class="sd">            implement keepdims any exceptions will be raised. Default: ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Standard deviation tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; input_x = np.array([1., 2., 3., 4.])</span>
<span class="sd">        &gt;&gt;&gt; output = np.std(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        1.118034</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">ddof</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">)</span></div>


<div class="viewcode-block" id="var"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.var.html#mindspore.numpy.var">[docs]</a><span class="k">def</span> <span class="nf">var</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the variance along the specified axis.</span>
<span class="sd">    The variance is the average of the squared deviations from the mean, i.e.,</span>
<span class="sd">    :math:`var = mean(abs(x - x.mean())**2)`.</span>

<span class="sd">    Returns the variance, which is computed for the flattened array by default,</span>
<span class="sd">    otherwise over the specified axis.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `dtype`, `out` and `where` are not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): A Tensor to be calculated.</span>
<span class="sd">        axis (Union[None, int, tuple(int)]): Axis or axes along which the variance is computed.</span>
<span class="sd">            The default is to compute the variance of the flattened array. Default: ``None`` .</span>
<span class="sd">        ddof (int): Means Delta Degrees of Freedom. Default: ``0`` .</span>
<span class="sd">            The divisor used in calculations is :math:`N - ddof`, where :math:`N` represents the number of elements.</span>
<span class="sd">        keepdims (bool): If this is set to True, the axes which are reduced are left in the result as</span>
<span class="sd">            dimensions with size one. With this option, the result will broadcast correctly against the input tensor.</span>
<span class="sd">            If the default value is passed, then keepdims will not be passed through to the var method of</span>
<span class="sd">            sub-classes of tensor, however any non-default value will be. If the sub-class method does not</span>
<span class="sd">            implement keepdims any exceptions will be raised. Default: ``False`` .</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Returns:</span>
<span class="sd">        Standard deviation tensor.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; input_x = np.array([1., 2., 3., 4.])</span>
<span class="sd">        &gt;&gt;&gt; output = np.var(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        1.25</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">ddof</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">)</span></div>


<div class="viewcode-block" id="ptp"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.ptp.html#mindspore.numpy.ptp">[docs]</a><span class="k">def</span> <span class="nf">ptp</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Range of values (maximum - minimum) along an axis.</span>
<span class="sd">    The name of the function comes from the acronym for &quot;peak to peak&quot;.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `dtype` and `out` are not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input tensor.</span>
<span class="sd">        axis (Union[None, int, tuple(int)]): Axis or axes along which the range is computed.</span>
<span class="sd">            The default is to compute the variance of the flattened array. Default: ``None``.</span>
<span class="sd">        keepdims (bool): If this is set to True, the axes which are reduced are left in the result as</span>
<span class="sd">            dimensions with size one. With this option, the result will broadcast correctly against the input tensor.</span>
<span class="sd">            If the default value is passed, then keepdims will not be passed through to the ptp method of</span>
<span class="sd">            sub-classes of tensor, however any non-default value will be. Default: ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If inputs have types not specified above.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.array([[4.0, 9.0, 2.0, 10.0], [6.0, 9.0, 7.0, 12.0]])</span>
<span class="sd">        &gt;&gt;&gt; print(np.ptp(x, axis=1))</span>
<span class="sd">        [8. 6.]</span>
<span class="sd">        &gt;&gt;&gt; print(np.ptp(x, axis=0))</span>
<span class="sd">        [2. 0. 5. 2.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">ptp</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">)</span></div>


<div class="viewcode-block" id="average"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.average.html#mindspore.numpy.average">[docs]</a><span class="k">def</span> <span class="nf">average</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">returned</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the weighted average along the specified axis.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): A Tensor to be averaged.</span>
<span class="sd">        axis (Union[None, int, tuple(int)]): Axis along which to average `x`. Default: ``None`` .</span>
<span class="sd">            If the axis is `None`, it will average over all of the elements of the tensor `x`.</span>
<span class="sd">            If the axis is negative, it counts from the last to the first axis.</span>
<span class="sd">        weights (Union[None, Tensor]): Weights associated with the values in `x`. Default: ``None`` .</span>
<span class="sd">            If `weights` is `None`, all the data in `x` are assumed to have a weight equal to one.</span>
<span class="sd">            If `weights` is 1-D tensor, the length must be the same as the given axis.</span>
<span class="sd">            Otherwise, `weights` should have the same shape as `x`.</span>
<span class="sd">        returned (bool): Default: ``False`` .</span>
<span class="sd">            If `True`, the tuple (average, sum_of_weights) is returned.</span>
<span class="sd">            If `False`, only the average is returned.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Averaged Tensor. If returned is `True`, return tuple.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; input_x = np.array([[1., 2.], [3., 4.]])</span>
<span class="sd">        &gt;&gt;&gt; output = np.average(input_x, axis=0, weights=input_x, returned=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2], dtype=Float32, value= [ 2.50000000e+00,  3.33333325e+00]),</span>
<span class="sd">         Tensor(shape=[2], dtype=Float32, value= [ 4.00000000e+00,  6.00000000e+00]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_check_axis_type</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="n">_canonicalize_axis</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>

    <span class="n">x_avg</span> <span class="o">=</span> <span class="n">full</span><span class="p">((),</span> <span class="n">nan</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">sum_of_weights</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">weights</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x_avg</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
        <span class="n">sum_of_weights</span> <span class="o">=</span> <span class="n">compute_weights_for_mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_avg</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="n">x_avg</span><span class="p">,</span> <span class="n">sum_of_weights</span> <span class="o">=</span> <span class="n">comput_avg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;Axis must be specified when shapes of x and weights differ.&quot;</span><span class="p">)</span>
            <span class="n">perm</span> <span class="o">=</span> <span class="n">_expanded_shape</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">axis</span><span class="p">)</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">perm</span><span class="p">)</span>
            <span class="n">x_avg</span><span class="p">,</span> <span class="n">sum_of_weights</span> <span class="o">=</span> <span class="n">comput_avg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;Weights should be None, 1-D or the same shape as input x.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">returned</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">x_avg</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">sum_of_weights</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="n">sum_of_weights</span> <span class="o">=</span> <span class="n">_broadcast_to</span><span class="p">(</span><span class="n">sum_of_weights</span><span class="p">,</span> <span class="n">sum_of_weights</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">x_avg</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">x_avg</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">x_avg</span><span class="p">,</span> <span class="n">sum_of_weights</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_avg</span></div>


<span class="k">def</span> <span class="nf">compute_weights_for_mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_avg</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Computes weights for np.average.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">sum_of_weights</span> <span class="o">=</span> <span class="n">full</span><span class="p">((),</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">fill_value</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="ow">and</span> <span class="n">F</span><span class="o">.</span><span class="n">tuple_len</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">fill_value</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
        <span class="k">elif</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">sh</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
                <span class="n">fill_value</span> <span class="o">*=</span> <span class="n">sh</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">axis</span><span class="p">:</span>
                <span class="n">fill_value</span> <span class="o">*=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">ax</span><span class="p">]</span>
        <span class="n">sum_of_weights</span> <span class="o">=</span> <span class="n">full_like</span><span class="p">(</span><span class="n">x_avg</span><span class="p">,</span> <span class="n">fill_value</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">sum_of_weights</span>


<span class="k">def</span> <span class="nf">comput_avg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Computes average value of input x with given parameters.&quot;&quot;&quot;</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">axis</span>
    <span class="n">x_mul</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_mul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
    <span class="n">x_sum</span> <span class="o">=</span> <span class="n">_reduce_sum_default</span><span class="p">(</span><span class="n">x_mul</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
    <span class="n">sum_of_weights</span> <span class="o">=</span> <span class="n">_reduce_sum_default</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
    <span class="n">x_avg</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_div</span><span class="p">(</span><span class="n">x_sum</span><span class="p">,</span> <span class="n">sum_of_weights</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_avg</span><span class="p">,</span> <span class="n">sum_of_weights</span>


<div class="viewcode-block" id="matmul"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.matmul.html#mindspore.numpy.matmul">[docs]</a><span class="k">def</span> <span class="nf">matmul</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the matrix product of two arrays.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>
<span class="sd">        On GPU, the supported dtypes are np.float16 and np.float32.</span>
<span class="sd">        On CPU, the supported dtypes are np.float16 and np.float32.</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Tensor): Input tensor, scalar not allowed.</span>
<span class="sd">        x2 (Tensor): Input tensor, scalar not allowed.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, the matrix product of the inputs. This is a scalar only</span>
<span class="sd">        when both `x1`, `x2` are 1-d vectors.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the last dimension of `x1` is not the same size as the</span>
<span class="sd">            second-to-last dimension of `x2`, or if a scalar value is passed in.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x1 = np.arange(2*3*4).reshape(2, 3, 4).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; x2 = np.arange(4*5).reshape(4, 5).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = np.matmul(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[  70.   76.   82.   88.   94.]</span>
<span class="sd">        [ 190.  212.  234.  256.  278.]</span>
<span class="sd">        [ 310.  348.  386.  424.  462.]]</span>
<span class="sd">        [[ 430.  484.  538.  592.  646.]</span>
<span class="sd">        [ 550.  620.  690.  760.  830.]</span>
<span class="sd">        [ 670.  756.  842.  928. 1014.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">C</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="square"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.square.html#mindspore.numpy.square">[docs]</a><span class="k">def</span> <span class="nf">square</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the element-wise square of the input.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>
<span class="sd">        On GPU, the supported dtypes are np.float16 and np.float32.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input data.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, element-wise ``x*x``, of the same shape and dtype as `x`.</span>
<span class="sd">        This is a scalar if `x` is a scalar..</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.square(np.arange(6).reshape(2, 3).astype(&#39;float32&#39;))</span>
<span class="sd">        &gt;&gt;&gt; print(x)</span>
<span class="sd">        [[ 0.  1.  4.]</span>
<span class="sd">        [ 9. 16. 25.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">square</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="sqrt"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.sqrt.html#mindspore.numpy.sqrt">[docs]</a><span class="k">def</span> <span class="nf">sqrt</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the non-negative square-root of an array, element-wise.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>
<span class="sd">        On GPU, the supported dtypes are np.float16 and np.float32.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The values whose square-roots are required.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, an array of the same shape as `x`, containing the positive</span>
<span class="sd">        square-root of each element in `x`. For negative elements, nan is returned.</span>
<span class="sd">        This is a scalar if `x` is a scalar.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.arange(6).reshape(2, 3).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; x_squared = np.square(x)</span>
<span class="sd">        &gt;&gt;&gt; output = np.sqrt(x_squared)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 0. 1. 2.]</span>
<span class="sd">        [ 3. 4. 5.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">sqrt</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="reciprocal"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.reciprocal.html#mindspore.numpy.reciprocal">[docs]</a><span class="k">def</span> <span class="nf">reciprocal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the reciprocal of the argument, element-wise.</span>

<span class="sd">    Calculates ``1/x``.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>
<span class="sd">        When `where` is provided, `out` must have a tensor value. `out` is not supported</span>
<span class="sd">        for storing the result, however it can be used in combination with `where` to set</span>
<span class="sd">        the value at indices for which `where` is set to False.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input array. For integer arguments with absolute value larger</span>
<span class="sd">            than 1 the result is always zero because of the way Python handles</span>
<span class="sd">            integer division. For integer zero the result is an overflow.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, this is a scalar if `x` is a scalar.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.arange(1, 7).reshape(2, 3).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = np.reciprocal(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1.         0.5        0.33333334]</span>
<span class="sd">        [0.25       0.2        0.16666667]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_div</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="log"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.log.html#mindspore.numpy.log">[docs]</a><span class="k">def</span> <span class="nf">log</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the natural logarithm, element-wise.</span>

<span class="sd">    The natural logarithm log is the inverse of the exponential function, so that</span>
<span class="sd">    ``log(exp(x)) = x``. The natural logarithm is logarithm in base e.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>
<span class="sd">        On GPU, the supported dtypes are np.float16, and np.float32.</span>
<span class="sd">        On CPU, the supported dtypes are np.float16, np.float32, and np.float64.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input array.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, the natural logarithm of `x`, element-wise. This is a</span>
<span class="sd">        scalar if `x` is a scalar.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.array([2, 3, 4]).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = np.log(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.69314575 1.09861    1.3862929 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">log</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_prop_nan</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Selects NaN if either element is NaN&quot;&quot;&quot;</span>
    <span class="n">has_nan</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">logical_or</span><span class="p">(</span><span class="n">_isnan</span><span class="p">(</span><span class="n">x1</span><span class="p">),</span> <span class="n">_isnan</span><span class="p">(</span><span class="n">x2</span><span class="p">))</span>
    <span class="n">nan_tensor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">_promote</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">x1</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">x2</span><span class="p">)),</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">has_nan</span><span class="p">),</span> <span class="n">nan</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">has_nan</span><span class="p">,</span> <span class="n">nan_tensor</span><span class="p">,</span> <span class="n">res</span><span class="p">)</span>


<div class="viewcode-block" id="maximum"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.maximum.html#mindspore.numpy.maximum">[docs]</a><span class="k">def</span> <span class="nf">maximum</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the element-wise maximum of array elements.</span>

<span class="sd">    Compares two arrays and returns a new array containing the element-wise maxima.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>
<span class="sd">        On Ascend, input arrays containing inf or NaN are not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Tensor): Input array</span>
<span class="sd">        x2 (Tensor): The array holding the elements to be compared. If</span>
<span class="sd">            ``x1.shape != x2.shape``, they must be broadcastable to a common shape</span>
<span class="sd">            (which becomes the shape of the output).</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, the maximum of `x1` and `x2`, element-wise. This is a scalar</span>
<span class="sd">        if both `x1` and `x2` are scalars.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; output = np.maximum(np.array([2, 3, 4]), np.array([1, 5, 2]))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2 5 4]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">asarray_const</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;Input x1 is expected to be array_like&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">asarray_const</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;Input x2 is expected to be array_like&quot;</span><span class="p">)</span>

    <span class="c1"># F.maximum does not support when both operands are scalar</span>
    <span class="k">if</span> <span class="n">x1</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">x2</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">expand_dims</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_prop_nan</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">maximum</span><span class="p">),</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">x1</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">x2</span><span class="o">.</span><span class="n">dtype</span>
    <span class="k">elif</span> <span class="n">x2</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">x1</span><span class="o">.</span><span class="n">dtype</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_prop_nan</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">maximum</span><span class="p">),</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="heaviside"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.heaviside.html#mindspore.numpy.heaviside">[docs]</a><span class="k">def</span> <span class="nf">heaviside</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Heaviside step function.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Tensor): Input values.</span>
<span class="sd">        x2 (Tensor): The value of the function when `x1` is 0. If</span>
<span class="sd">            ``x1.shape != x2.shape``, they must be broadcastable to a common shape</span>
<span class="sd">            (which becomes the shape of the output).</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, the output array, element-wise Heaviside step function</span>
<span class="sd">        of `x1`. This is a scalar if both `x1` and `x2` are scalars.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; output = np.heaviside(np.array([-1.5, 0, 2.0]), np.array(0.5))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.  0.5 1. ]</span>
<span class="sd">        &gt;&gt;&gt; output = np.heaviside(np.array([-1.5, 0, 2.0]), np.array(1))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0. 1. 1.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_heaviside</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Computes heaviside without passing keyword arguments&quot;&quot;&quot;</span>
        <span class="c1"># performs type promotion</span>
        <span class="n">dtype1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
        <span class="n">dtype2</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
        <span class="n">dtype_out</span> <span class="o">=</span> <span class="n">_promote</span><span class="p">(</span><span class="n">dtype1</span><span class="p">,</span> <span class="n">dtype2</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">dtype1</span><span class="p">,</span> <span class="n">dtype_out</span><span class="p">):</span>
            <span class="n">x1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">dtype_out</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">dtype2</span><span class="p">,</span> <span class="n">dtype_out</span><span class="p">):</span>
            <span class="n">x2</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">dtype_out</span><span class="p">)</span>

        <span class="c1"># performs broadcast</span>
        <span class="n">shape_out</span> <span class="o">=</span> <span class="n">_infer_out_shape</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x1</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x2</span><span class="p">))</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">_broadcast_to_shape</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">shape_out</span><span class="p">)</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">_broadcast_to_shape</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">shape_out</span><span class="p">)</span>

        <span class="n">x2</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">x1</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">zeros</span><span class="p">(</span><span class="n">shape_out</span><span class="p">,</span> <span class="n">dtype_out</span><span class="p">),</span> <span class="n">x2</span><span class="p">)</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">x1</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">ones</span><span class="p">(</span><span class="n">shape_out</span><span class="p">,</span> <span class="n">dtype_out</span><span class="p">),</span> <span class="n">x2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x2</span>

    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">_heaviside</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="amax"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.amax.html#mindspore.numpy.amax">[docs]</a><span class="k">def</span> <span class="nf">amax</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">initial</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the maximum of an array or maximum along an axis.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy argument `out` is not supported.</span>
<span class="sd">        On GPU, the supported dtypes are np.float16, and np.float32.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): Input data.</span>
<span class="sd">        axis (None or int or tuple of integers, optional): Default: ``None`` . Axis or</span>
<span class="sd">            axes along which to operate. By default, flattened input is used. If</span>
<span class="sd">            this is a tuple of integers, the maximum is selected over multiple axes,</span>
<span class="sd">            instead of a single axis or all the axes as before.</span>
<span class="sd">        keepdims (boolean, optional): Default: ``False`` .</span>
<span class="sd">            If this is set to True, the axes which are reduced are left in the</span>
<span class="sd">            result as dimensions with size one. With this option, the result will</span>
<span class="sd">            broadcast correctly against the input array.</span>
<span class="sd">        initial (scalar, optional): Default: ``None`` .</span>
<span class="sd">            The minimum value of an output element. Must be present to allow</span>
<span class="sd">            computation on empty slice.</span>
<span class="sd">        where (boolean Tensor, optional): Default: ``True`` .</span>
<span class="sd">            A boolean array which is broadcasted to match the dimensions of array,</span>
<span class="sd">            and selects elements to include in the reduction. If non-default value</span>
<span class="sd">            is passed, initial must also be provided.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, maximum of `a`. If `axis` is None, the result is a scalar</span>
<span class="sd">        value. If `axis` is given, the result is an array of dimension ``a.ndim - 1``.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input is not a tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.arange(4).reshape((2,2)).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = np.amax(a)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        3.0</span>
<span class="sd">        &gt;&gt;&gt; output = np.amax(a, axis=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2. 3.]</span>
<span class="sd">        &gt;&gt;&gt; output = np.amax(a, axis=1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 3.]</span>
<span class="sd">        &gt;&gt;&gt; output = np.amax(a, where=np.array([False, True]), initial=-1, axis=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-1.  3.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">reduce_</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceMax</span><span class="p">(</span><span class="n">keepdims</span><span class="p">),</span> <span class="n">cmp_fn</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">maximum</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">,</span>
                   <span class="n">initial</span><span class="o">=</span><span class="n">initial</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="n">where</span><span class="p">)</span></div>


<div class="viewcode-block" id="amin"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.amin.html#mindspore.numpy.amin">[docs]</a><span class="k">def</span> <span class="nf">amin</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">initial</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the minimum of an array or minimum along an axis.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy argument `out` is not supported.</span>
<span class="sd">        On GPU, the supported dtypes are np.float16, and np.float32.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): Input data.</span>
<span class="sd">        axis (None or int or tuple of integers, optional): Default: ``None`` . Axis or</span>
<span class="sd">            axes along which to operate. By default, flattened input is used. If</span>
<span class="sd">            this is a tuple of integers, the minimum is selected over multiple axes,</span>
<span class="sd">            instead of a single axis or all the axes as before.</span>
<span class="sd">        keepdims (bool, optional): Default: ``False`` .</span>
<span class="sd">            If this is set to True, the axes which are reduced are left in the</span>
<span class="sd">            result as dimensions with size one. With this option, the result will</span>
<span class="sd">            broadcast correctly against the input array.</span>
<span class="sd">        initial (Number, optional): Default: ``None`` .</span>
<span class="sd">            The maximum value of an output element. Must be present to allow</span>
<span class="sd">            computation on empty slice.</span>
<span class="sd">        where (bool Tensor, optional): Default: ``True`` .</span>
<span class="sd">            A boolean array which is broadcasted to match the dimensions of array,</span>
<span class="sd">            and selects elements to include in the reduction. If non-default value</span>
<span class="sd">            is passed, initial must also be provided.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, minimum of `a`. If axis is None, the result is a scalar</span>
<span class="sd">        value. If `axis` is given, the result is an array of dimension ``a.ndim - 1``.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input is not a tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.arange(4).reshape((2,2)).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = np.amin(a)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.0</span>
<span class="sd">        &gt;&gt;&gt; output = np.amin(a, axis=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0. 1.]</span>
<span class="sd">        &gt;&gt;&gt; output = np.amin(a, axis=1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0. 2.]</span>
<span class="sd">        &gt;&gt;&gt; output = np.amin(a, where=np.array([False, True]), initial=10, axis=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [10.  1.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">reduce_</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceMin</span><span class="p">(</span><span class="n">keepdims</span><span class="p">),</span> <span class="n">cmp_fn</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">minimum</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">,</span>
                   <span class="n">initial</span><span class="o">=</span><span class="n">initial</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="n">where</span><span class="p">)</span></div>


<div class="viewcode-block" id="hypot"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.hypot.html#mindspore.numpy.hypot">[docs]</a><span class="k">def</span> <span class="nf">hypot</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Given the &quot;legs&quot; of a right triangle, returns its hypotenuse.</span>

<span class="sd">    Equivalent to ``sqrt(x1**2 + x2**2)``, element-wise. If `x1` or `x2` is scalar_like</span>
<span class="sd">    (i.e., unambiguously cast-able to a scalar type), it is broadcast for use</span>
<span class="sd">    with each element of the other argument. (See Examples)</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>
<span class="sd">        On GPU, the supported dtypes are np.float16 and np.float32.</span>
<span class="sd">        On CPU, the supported dtypes are np.float16, np.float32, and np.float64.</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Tensor): Leg of the triangle(s).</span>
<span class="sd">        x2 (Tensor): Leg of the triangle(s). If ``x1.shape != x2.shape``, they</span>
<span class="sd">            must be broadcastable to a common shape (which becomes the shape of</span>
<span class="sd">            the output).</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, the hypotenuse of the triangle(s). This is a scalar if</span>
<span class="sd">        both `x1` and `x2` are scalars.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; output = np.hypot(3*np.ones((3, 3)), 4*np.ones((3, 3)))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[5. 5. 5.]</span>
<span class="sd">        [5. 5. 5.]</span>
<span class="sd">        [5. 5. 5.]]</span>
<span class="sd">        &gt;&gt;&gt; output = np.hypot(3*np.ones((3, 3)), np.array([4.0]))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[5. 5. 5.]</span>
<span class="sd">        [5. 5. 5.]</span>
<span class="sd">        [5. 5. 5.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_hypot</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Computes hypotenuse without passing keyword arguments&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">_get_device</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;CPU&#39;</span><span class="p">:</span>
            <span class="c1"># broadcast is not fully supported in tensor_add on CPU,</span>
            <span class="c1"># so we use tensor_sub as a substitute solution</span>
            <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">tensor_sub</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x1</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">neg_tensor</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x2</span><span class="p">))))</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">tensor_add</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x1</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x2</span><span class="p">)))</span>

    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">_hypot</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="floor"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.floor.html#mindspore.numpy.floor">[docs]</a><span class="k">def</span> <span class="nf">floor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the floor of the input, element-wise.</span>

<span class="sd">    The floor of the scalar `x` is the largest integer `i`, such that ``i &lt;= x``.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>
<span class="sd">        On GPU, the supported dtypes are np.float16 and np.float32.</span>
<span class="sd">        On CPU, the supported dtypes are np.float16, np.float32, and np.float64.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): input data.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, the floor of each element in `x`. This is a scalar if `x`</span>
<span class="sd">        is a scalar.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; output = np.floor(np.array([-1.7, -1.5, -0.2, 0.2, 1.5, 1.7, 2.0]))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-2. -2. -1.  0.  1.  1.  2.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">floor</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="floor_divide"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.floor_divide.html#mindspore.numpy.floor_divide">[docs]</a><span class="k">def</span> <span class="nf">floor_divide</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the largest integer smaller or equal to the division of the inputs.</span>
<span class="sd">    It is equivalent to the Python // operator and pairs with the</span>
<span class="sd">    Python % (remainder), function so that ``a = a % b + b * (a // b)`` up to roundoff.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Tensor): Input array.</span>
<span class="sd">        x2 (Tensor): Input array.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; output = np.floor_divide(np.array([1., 2., 3., 4.]), np.array(2.5))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0. 0. 1. 1.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">tensor_floordiv</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_remainder</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">c_style</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Computes remainder without applying keyword arguments.&quot;&quot;&quot;</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">_promote</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">x1</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">x2</span><span class="p">))</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_is_float</span><span class="p">(</span><span class="n">dtype</span><span class="p">):</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="n">quotient</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_div</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">c_style</span><span class="p">:</span>
        <span class="n">quotient</span> <span class="o">=</span> <span class="n">fix</span><span class="p">(</span><span class="n">quotient</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">quotient</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">quotient</span><span class="p">)</span>
    <span class="n">prod</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_mul</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">quotient</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_sub</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">prod</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_check_is_int</span><span class="p">(</span><span class="n">dtype</span><span class="p">):</span>
        <span class="n">zeros_tensor</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">quotient</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">quotient</span><span class="p">))</span>
        <span class="n">x2_zeros</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">zeros_tensor</span><span class="p">)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">x2_zeros</span><span class="p">,</span> <span class="n">zeros_tensor</span><span class="p">,</span> <span class="n">res</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">res</span><span class="p">),</span> <span class="n">dtype</span><span class="p">):</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span>


<div class="viewcode-block" id="remainder"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.remainder.html#mindspore.numpy.remainder">[docs]</a><span class="k">def</span> <span class="nf">remainder</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns element-wise remainder of division.</span>

<span class="sd">    Computes the remainder complementary to the floor_divide function. It is</span>
<span class="sd">    equivalent to the Python modulus operator ``x1 % x2`` and has the same sign</span>
<span class="sd">    as the divisor `x2`. The MATLAB function equivalent to np.remainder is mod.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Tensor): input array.</span>
<span class="sd">        x2 (Tensor): input array.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, the element-wise remainder of the quotient</span>
<span class="sd">        ``floor_divide(x1, x2)``. This is a scalar if both `x1` and `x2` are scalars.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; output = np.remainder(np.array([4, 7]), np.array([2, 3]))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0 1]</span>
<span class="sd">        &gt;&gt;&gt; output = np.remainder(np.arange(7), np.array(5))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0 1 2 3 4 0 1]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">_remainder</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="fix"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.fix.html#mindspore.numpy.fix">[docs]</a><span class="k">def</span> <span class="nf">fix</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Rounds to nearest integer towards zero.</span>

<span class="sd">    Rounds an array of floats element-wise to nearest integer towards zero. The</span>
<span class="sd">    rounded values are returned as floats.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy argument `out` is not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): An array of floats to be rounded.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input is not a tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; output = np.fix(np.array([2.1, 2.9, -2.1, -2.9]))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 2.  2. -2. -2.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_is_float</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">floored</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="c1"># change to F.ceil once supported on CPU.</span>
    <span class="n">ceiled</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">neg_tensor</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">neg_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
    <span class="n">is_neg</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_lt</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">zeros</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">is_neg</span><span class="p">,</span> <span class="n">ceiled</span><span class="p">,</span> <span class="n">floored</span><span class="p">)</span></div>


<div class="viewcode-block" id="fmod"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.fmod.html#mindspore.numpy.fmod">[docs]</a><span class="k">def</span> <span class="nf">fmod</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the element-wise remainder of division.</span>

<span class="sd">    This is the NumPy implementation of the C library function fmod, the remainder</span>
<span class="sd">    has the same sign as the dividend `x1`. It is equivalent to the Matlab(TM) rem</span>
<span class="sd">    function and should not be confused with the Python modulus operator ``x1 % x2``.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Tensor): the first input arrays.</span>
<span class="sd">        x2 (Tensor): the second input arrays.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, the remainder of the division of `x1` by `x2`. This is a</span>
<span class="sd">        scalar if both `x1` and `x2` are scalars.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; output = np.fmod(np.array([-3, -2, -1, 1, 2, 3]), np.array(2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-1  0 -1  1  0  1]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">:</span> <span class="n">_remainder</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">c_style</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="trunc"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.trunc.html#mindspore.numpy.trunc">[docs]</a><span class="k">def</span> <span class="nf">trunc</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the truncated value of the input, element-wise.</span>

<span class="sd">    The truncated value of the scalar `x` is the nearest integer `i` which is closer to zero</span>
<span class="sd">    than `x` is. In short, the fractional part of the signed number `x` is discarded.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): input data.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, the truncated value of each element in `x`. This is a scalar if `x` is</span>
<span class="sd">        a scalar.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; output = np.trunc(np.array([-1.7, -1.5, -0.2, 0.2, 1.5, 1.7, 2.0]))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-1. -1. -0.  0.  1.  1.  2.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">fix</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="exp"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.exp.html#mindspore.numpy.exp">[docs]</a><span class="k">def</span> <span class="nf">exp</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the exponential of all elements in the input array.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>
<span class="sd">        When `where` is provided, `out` must have a tensor value. `out` is not supported</span>
<span class="sd">        for storing the result, however it can be used in combination with `where` to set</span>
<span class="sd">        the value at indices for which `where` is set to False.</span>
<span class="sd">        On GPU, the supported dtypes are np.float16, and np.float32.</span>
<span class="sd">        On CPU, the supported dtypes are np.float16, np.float32, np.float64.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): input data.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, element-wise exponential of `x`. This is a scalar if both</span>
<span class="sd">        `x1` and `x2` are scalars.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; output = np.exp(np.arange(5).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1.         2.718282   7.3890557 20.085537  54.598145 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">tensor_exp</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="expm1"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.expm1.html#mindspore.numpy.expm1">[docs]</a><span class="k">def</span> <span class="nf">expm1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates ``exp(x) - 1`` for all elements in the array.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>
<span class="sd">        On GPU, the supported dtypes are np.float16, and np.float32.</span>
<span class="sd">        On CPU, the supported dtypes are np.float16, and np.float32.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): input data.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, element-wise exponential minus one, ``out = exp(x) - 1``.</span>
<span class="sd">        This is a scalar if both `x1` and `x2` are scalars.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; output = np.expm1(np.arange(5).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0.         1.7182819  6.389056  19.085537  53.59815  ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">tensor_expm1</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">divmod_</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns element-wise quotient and remainder simultaneously.</span>

<span class="sd">    Args:</span>
<span class="sd">        x1(Union[Tensor]): Dividend tensor.</span>
<span class="sd">        x2(Union[Tensor, int, float, bool]): Divisor. If ``x1.shape != x2.shape``,</span>
<span class="sd">            they must be broadcastable to a common shape.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Element-wise quotient and remainder from floor division, in format of (quotient, remainder)</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x1` and `x2` are not Tensor or scalar.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.array([1, 2, 3, 4, 5])</span>
<span class="sd">        &gt;&gt;&gt; print(np.divmod(a, 1.5))</span>
<span class="sd">        (Tensor(shape=[5], dtype=Float32,</span>
<span class="sd">         value= [ 0.00000000e+00,  1.00000000e+00,  2.00000000e+00,  2.00000000e+00,  3.00000000e+00]),</span>
<span class="sd">         Tensor(shape=[5], dtype=Float32,</span>
<span class="sd">         value= [ 1.00000000e+00,  5.00000000e-01,  0.00000000e+00,  1.00000000e+00,  5.00000000e-01]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_floordiv</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">remainder</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">r</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_handle_prepend_append</span><span class="p">(</span><span class="n">combined</span><span class="p">,</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">additional_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Concatenates prepend or append to tensor.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">additional_tensor</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">)):</span>
        <span class="n">additional_tensor</span> <span class="o">=</span> <span class="n">asarray_const</span><span class="p">(</span><span class="n">additional_tensor</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">additional_tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;prepend must be scalar or Tensor, but got &quot;</span><span class="p">,</span> <span class="n">additional_tensor</span><span class="p">)</span>
    <span class="n">additional_shape</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">additional_shape</span> <span class="o">=</span> <span class="n">_tuple_setitem</span><span class="p">(</span><span class="n">additional_shape</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">additional_tensor</span> <span class="o">=</span> <span class="n">_broadcast_to_shape</span><span class="p">(</span><span class="n">additional_tensor</span><span class="p">,</span> <span class="n">additional_shape</span><span class="p">)</span>
    <span class="n">combined</span> <span class="o">+=</span> <span class="p">(</span><span class="n">additional_tensor</span><span class="p">,)</span>
    <span class="k">return</span> <span class="n">combined</span>


<div class="viewcode-block" id="diff"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.diff.html#mindspore.numpy.diff">[docs]</a><span class="k">def</span> <span class="nf">diff</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">prepend</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">append</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the n-th discrete difference along the given axis.</span>

<span class="sd">    The first difference is given by :math:`out[i] = a[i+1] - a[i]` along the given axis,</span>
<span class="sd">    higher differences are calculated by using `diff` iteratively.</span>

<span class="sd">    Note:</span>
<span class="sd">        Since zero-shaped Tensor is not supported in MindSpore, a value error is raised if</span>
<span class="sd">        an empty Tensor is encountered.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): Input tensor.</span>
<span class="sd">        n (int, optional): The number of times values are differenced. If zero,</span>
<span class="sd">            the input is returned as-is. Default: ``1`` .</span>
<span class="sd">        axis (int, optional): The axis along which the difference is taken, default</span>
<span class="sd">            is the last axis. Default: ``-1`` .</span>
<span class="sd">        prepend/append (Tensor, optional): Values to prepend or append to a along</span>
<span class="sd">            `axis` prior to performing the difference. Scalar values are expanded to</span>
<span class="sd">            arrays with length 1 in the direction of `axis` and the shape of the input</span>
<span class="sd">            array in along all other axes. Otherwise the dimension and shape must</span>
<span class="sd">            match `a` except along axis. Default: ``None`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        The n-th differences. The shape of the output is the same as a except along</span>
<span class="sd">        `axis` where the dimension is smaller by `n`. The type of the output is the same</span>
<span class="sd">        as the type of the difference between any two elements of `a`. This is the same</span>
<span class="sd">        as the type of `a` in most cases.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If inputs have types not specified above.</span>
<span class="sd">        ValueError: If ``n &lt; 0``.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; arr = np.array([1, 3, -1, 0, 4])</span>
<span class="sd">        &gt;&gt;&gt; print(np.diff(arr, n=2))</span>
<span class="sd">        [-6  5  3]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># This implementation is inspired by jax.numpy</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">new_axis</span> <span class="o">=</span> <span class="n">_canonicalize_axis</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;Input n should be int, but got &quot;</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">n</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;Input n must &gt; 0.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">n</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">a</span>

    <span class="n">combined</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">if</span> <span class="n">prepend</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">combined</span> <span class="o">=</span> <span class="n">_handle_prepend_append</span><span class="p">(</span><span class="n">combined</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">prepend</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>

    <span class="n">combined</span> <span class="o">+=</span> <span class="p">(</span><span class="n">a</span><span class="p">,)</span>

    <span class="k">if</span> <span class="n">append</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">combined</span> <span class="o">=</span> <span class="n">_handle_prepend_append</span><span class="p">(</span><span class="n">combined</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">append</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">combined</span><span class="p">:</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">concatenate</span><span class="p">(</span><span class="n">combined</span><span class="p">,</span> <span class="n">new_axis</span><span class="p">)</span>

    <span class="c1"># if n &gt; maximum length allowed, the tensor is empty, and is not supported</span>
    <span class="k">if</span> <span class="n">n</span> <span class="o">&gt;=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">new_axis</span><span class="p">]:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;n is bigger then the specified dimension, this will result in an empty tensor.&quot;</span><span class="p">)</span>

    <span class="n">original_dtype</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">dtype</span>
    <span class="c1"># will change once F.tensor_slice supports types other than float32</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_is_float</span><span class="p">(</span><span class="n">original_dtype</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">moveaxis</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">new_axis</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">F</span><span class="o">.</span><span class="n">make_range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">slice_start</span> <span class="o">=</span> <span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="n">slice_size</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">a</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">a</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,)</span>
        <span class="n">minuend</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_slice</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">slice_start</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">slice_size</span><span class="p">)</span>
        <span class="n">subtrahend</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_slice</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">slice_start</span> <span class="o">+</span> <span class="p">(</span><span class="mi">0</span><span class="p">,),</span> <span class="n">slice_size</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_sub</span><span class="p">(</span><span class="n">minuend</span><span class="p">,</span> <span class="n">subtrahend</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_is_float</span><span class="p">(</span><span class="n">original_dtype</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">original_dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">moveaxis</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">new_axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="ediff1d"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.ediff1d.html#mindspore.numpy.ediff1d">[docs]</a><span class="k">def</span> <span class="nf">ediff1d</span><span class="p">(</span><span class="n">ary</span><span class="p">,</span> <span class="n">to_end</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">to_begin</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The differences between consecutive elements of a tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        ary (Tensor): If necessary, will be flattened before the differences are taken.</span>
<span class="sd">        to_end (Tensor or scalar, optional): Number(s) to append at the end of the</span>
<span class="sd">            returned differences.</span>
<span class="sd">        to_begin (Tensor or scalar, optional): Number(s) to prepend at the beginning</span>
<span class="sd">            of the returned differences.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The differences.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If inputs have types not specified above.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; arr = np.array([1, 3, -1, 0, 4])</span>
<span class="sd">        &gt;&gt;&gt; print(np.ediff1d(arr))</span>
<span class="sd">        [ 2 -4  1  4]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">ary</span><span class="p">)</span>
    <span class="n">combined</span> <span class="o">=</span> <span class="p">()</span>

    <span class="k">if</span> <span class="n">to_begin</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">to_begin</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="n">to_begin</span> <span class="o">=</span> <span class="n">to_begin</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">to_begin</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">to_begin</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="n">to_begin</span> <span class="o">=</span> <span class="n">to_begin</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">ary</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">combined</span> <span class="o">+=</span> <span class="p">(</span><span class="n">to_begin</span><span class="p">,)</span>

    <span class="n">combined</span> <span class="o">+=</span> <span class="p">(</span><span class="n">diff</span><span class="p">(</span><span class="n">ary</span><span class="o">.</span><span class="n">ravel</span><span class="p">()),)</span>

    <span class="k">if</span> <span class="n">to_end</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">to_end</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="n">to_end</span> <span class="o">=</span> <span class="n">to_end</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">to_end</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">to_end</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="n">to_end</span> <span class="o">=</span> <span class="n">to_end</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">ary</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">combined</span> <span class="o">+=</span> <span class="p">(</span><span class="n">to_end</span><span class="p">,)</span>

    <span class="k">return</span> <span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="mi">0</span><span class="p">)(</span><span class="n">combined</span><span class="p">)</span></div>


<div class="viewcode-block" id="trapz"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.trapz.html#mindspore.numpy.trapz">[docs]</a><span class="k">def</span> <span class="nf">trapz</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dx</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Integrates along the given axis using the composite trapezoidal rule.</span>

<span class="sd">    Integrates `y` (x) along given axis.</span>

<span class="sd">    Args:</span>
<span class="sd">        y (Tensor): Input array to integrate.</span>
<span class="sd">        x (Union[int, float, bool, list, tuple, Tensor], optional): The sample points</span>
<span class="sd">            corresponding to the `y` values. If `x` is None, the sample points are</span>
<span class="sd">            assumed to be evenly spaced `dx` apart. Default: ``None`` .</span>
<span class="sd">        dx (scalar, optional): The spacing between sample points when `x` is None. Default: ``1.0`` .</span>
<span class="sd">        axis (int, optional): The axis along which to integrate. Default: ``-1`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor of float, definite integral as approximated by trapezoidal rule.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If axis is out of range of ``[-y.ndim, y.ndim)``.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.arange(6).reshape(2, 3)</span>
<span class="sd">        &gt;&gt;&gt; output = np.trapz(a,  x=[-2, 1, 2], axis=1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 3. 15.]</span>
<span class="sd">        &gt;&gt;&gt; output = np.trapz(a,  dx=3, axis=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 4.5  7.5 10.5]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">_check_axis_in_range</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span> <span class="o">+</span> <span class="n">ndim</span> <span class="k">if</span> <span class="n">axis</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">axis</span>
    <span class="n">y_start_axis_left</span> <span class="o">=</span> <span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">y_start_axis_right</span> <span class="o">=</span> <span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">ndim</span> <span class="o">-</span> <span class="n">axis</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">y_slice_size</span> <span class="o">=</span> <span class="n">_tuple_setitem</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="n">diff</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">dx</span><span class="p">)</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">_expand</span><span class="p">(</span><span class="n">dx</span><span class="p">,</span> <span class="n">ndim</span> <span class="o">-</span> <span class="n">axis</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">_broadcast_to_shape</span><span class="p">(</span><span class="n">dx</span><span class="p">,</span> <span class="n">y_slice_size</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_is_float</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">y</span><span class="p">)):</span>
        <span class="c1"># trapz returns float</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">dx</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>

    <span class="c1"># product of dx and y with the last column removed</span>
    <span class="n">y_slice_left</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_slice</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_start_axis_left</span> <span class="o">+</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">+</span> <span class="n">y_start_axis_right</span><span class="p">,</span> <span class="n">y_slice_size</span><span class="p">)</span>
    <span class="n">prod_left</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_mul</span><span class="p">(</span><span class="n">y_slice_left</span><span class="p">,</span> <span class="n">dx</span><span class="p">)</span>
    <span class="c1"># product of dx and y with the first column removed</span>
    <span class="n">y_slice_right</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_slice</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_start_axis_left</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="n">y_start_axis_right</span><span class="p">,</span> <span class="n">y_slice_size</span><span class="p">)</span>
    <span class="n">prod_right</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_mul</span><span class="p">(</span><span class="n">y_slice_right</span><span class="p">,</span> <span class="n">dx</span><span class="p">)</span>
    <span class="n">prod_sum</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_div</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">tensor_add</span><span class="p">(</span><span class="n">prod_left</span><span class="p">,</span> <span class="n">prod_right</span><span class="p">),</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">y</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">prod_sum</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_gcd</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculates gcd without applying keyword arguments.&quot;&quot;&quot;</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">_promote</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">x1</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">x2</span><span class="p">))</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_is_float</span><span class="p">(</span><span class="n">dtype</span><span class="p">):</span>
        <span class="c1"># F.reduce_sum only supports float</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">absolute</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">absolute</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
    <span class="n">cond_ge</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_ge</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">where_</span><span class="p">(</span><span class="n">cond_ge</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">where_</span><span class="p">(</span><span class="n">cond_ge</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">x1</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">where_</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">ZERO_TENSOR</span><span class="p">),</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">_remainder</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="k">while</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_gt</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">r</span><span class="p">),</span> <span class="n">ZERO_TENSOR</span><span class="p">):</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">_remainder</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
        <span class="n">has_terminated</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">ZERO_TENSOR</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">where_</span><span class="p">(</span><span class="n">has_terminated</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">where_</span><span class="p">(</span><span class="n">has_terminated</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">b</span><span class="p">),</span> <span class="n">dtype</span><span class="p">):</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">b</span>


<div class="viewcode-block" id="gcd"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.gcd.html#mindspore.numpy.gcd">[docs]</a><span class="k">def</span> <span class="nf">gcd</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the greatest common divisor of ``|x1|`` and ``|x2|``.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Tensor): input data.</span>
<span class="sd">        x2 (Tensor): input data.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, the greatest common divisor of the absolute value of the inputs.</span>
<span class="sd">        This is a scalar if both `x1` and `x2` are scalars.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; output = np.gcd(np.arange(6), np.array(20))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [20  1  2  1  4  5]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">_gcd</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="lcm"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.lcm.html#mindspore.numpy.lcm">[docs]</a><span class="k">def</span> <span class="nf">lcm</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the lowest common multiple of ``|x1|`` and ``|x2|``.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Tensor): input data.</span>
<span class="sd">        x2 (Tensor): input data.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, the lowest common multiple of the absolute value of the inputs.</span>
<span class="sd">        This is a scalar if both `x1` and `x2` are scalars.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; output = np.lcm(np.arange(6), np.array(20))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0 20 20 60 20 20]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">_lcm</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculates lcm without applying keyword arguments&quot;&quot;&quot;</span>
        <span class="n">common_divisor</span> <span class="o">=</span> <span class="n">_gcd</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">_promote</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">x1</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">x2</span><span class="p">))</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">x1</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">x2</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">q1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_div</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">common_divisor</span><span class="p">)</span>
        <span class="n">q2</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_div</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">common_divisor</span><span class="p">)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_mul</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">tensor_mul</span><span class="p">(</span><span class="n">q1</span><span class="p">,</span> <span class="n">q2</span><span class="p">),</span> <span class="n">common_divisor</span><span class="p">)</span>
        <span class="n">has_zero</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">multiply</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">),</span> <span class="n">ZERO_TENSOR</span><span class="p">)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">where_</span><span class="p">(</span><span class="n">has_zero</span><span class="p">,</span> <span class="n">ZERO_TENSOR</span><span class="p">,</span> <span class="n">res</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">absolute</span><span class="p">(</span><span class="n">res</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">_lcm</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="convolve"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.convolve.html#mindspore.numpy.convolve">[docs]</a><span class="k">def</span> <span class="nf">convolve</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;full&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the discrete, linear convolution of two one-dimensional sequences.</span>

<span class="sd">    Note:</span>
<span class="sd">        If `v` is longer than `a`, the tensors are swapped before computation.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Union[list, tuple, Tensor]): First one-dimensional input tensor.</span>
<span class="sd">        v (Union[list, tuple, Tensor]): Second one-dimensional input tensor.</span>

<span class="sd">        mode (str, optional): By default, mode is `\&#39;full\&#39;`. This returns the</span>
<span class="sd">            convolution at each point of overlap, with an output shape of :math:`(N+M-1,)`.</span>
<span class="sd">            At the end-points of the convolution, the signals do not overlap completely,</span>
<span class="sd">            and boundary effects may be seen.</span>
<span class="sd">            If `mode` is `\&#39;same\&#39;`, it returns output of length :math:`max(M, N)`. Boundary</span>
<span class="sd">            effects are still visible.</span>
<span class="sd">            If `mode` is `\&#39;valid\&#39;`, it returns output of length :math:`max(M, N) - min(M, N) + 1`.</span>
<span class="sd">            The convolution product is only given for points where the signals overlap</span>
<span class="sd">            completely. Values outside the signal boundary have no effect.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, discrete, linear convolution of a and v.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the inputs have types not specified above.</span>
<span class="sd">        ValueError: If a and v are empty or have wrong dimensions</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; output = np.convolve([1., 2., 3., 4., 5.], [2., 3.], mode=&quot;valid&quot;)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 7. 12. 17. 22.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">asarray_const</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">asarray_const</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
    <span class="n">a_size</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape_mul</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">v_size</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape_mul</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">a_size</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">v_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;Inputs cannot be empty.&quot;</span><span class="p">)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">_expand</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">_expand</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">final_dtype</span> <span class="o">=</span> <span class="n">_promote</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">v</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;a and v must be 1-D tensor.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">a_size</span> <span class="o">&lt;</span> <span class="n">v_size</span><span class="p">:</span>
        <span class="n">a</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="p">,</span> <span class="n">a</span>
        <span class="n">a_size</span><span class="p">,</span> <span class="n">v_size</span> <span class="o">=</span> <span class="n">v_size</span><span class="p">,</span> <span class="n">a_size</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">_compute_1d_conv</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mode</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">final_dtype</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_handle_weights</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Checks fweight and aweight in np.cov.&quot;&quot;&quot;</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">asarray_const</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_is_int</span><span class="p">(</span><span class="n">weights</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;weights must be integer&quot;</span><span class="p">)</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">weights</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">_raise_runtime_error</span><span class="p">(</span><span class="s2">&quot;cannot handle multidimensional weights&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">num_samples</span><span class="p">:</span>
        <span class="n">_raise_runtime_error</span><span class="p">(</span><span class="s2">&quot;incompatible numbers of samples and weights&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">absolute</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_handle_inputs</span><span class="p">(</span><span class="n">cov_input</span><span class="p">,</span> <span class="n">rowvar</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Checks input arrays for np.cov.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cov_input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">cov_input</span> <span class="o">=</span> <span class="n">asarray_const</span><span class="p">(</span><span class="n">cov_input</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">cov_input</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;input array has dimension more than 2.&quot;</span><span class="p">)</span>
    <span class="n">cov_input</span> <span class="o">=</span> <span class="n">cov_input</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">cov_input</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;The value of cov_input should not be None, but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cov_input</span><span class="p">))</span>
    <span class="n">cov_input</span> <span class="o">=</span> <span class="n">_expand</span><span class="p">(</span><span class="n">cov_input</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">rowvar</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;input rowvar should be boolean.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">rowvar</span> <span class="ow">and</span> <span class="n">cov_input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">cov_input</span> <span class="o">=</span> <span class="n">cov_input</span><span class="o">.</span><span class="n">T</span>
    <span class="k">return</span> <span class="n">cov_input</span>


<span class="k">def</span> <span class="nf">_handle_facts</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">ddof</span><span class="p">,</span> <span class="n">aweights</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Computes facts for np.cov&quot;&quot;&quot;</span>
    <span class="n">fact</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">w</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">fact</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">ddof</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">w_sum</span> <span class="o">=</span> <span class="n">_reduce_sum_default</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">ddof</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">fact</span> <span class="o">=</span> <span class="n">w_sum</span>
        <span class="k">elif</span> <span class="n">aweights</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">fact</span> <span class="o">=</span> <span class="n">w_sum</span> <span class="o">-</span> <span class="n">ddof</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">fact</span> <span class="o">=</span> <span class="n">w_sum</span> <span class="o">-</span> <span class="n">ddof</span> <span class="o">*</span> <span class="n">F</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">w</span> <span class="o">*</span> <span class="n">aweights</span><span class="p">)</span> <span class="o">/</span> <span class="n">w_sum</span>
    <span class="k">return</span> <span class="n">fact</span>


<div class="viewcode-block" id="cov"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.cov.html#mindspore.numpy.cov">[docs]</a><span class="k">def</span> <span class="nf">cov</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">rowvar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fweights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">aweights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Estimates a covariance matrix, given data and weights.</span>

<span class="sd">    Covariance indicates the level to which two variables vary together. If we examine</span>
<span class="sd">    N-dimensional samples, :math:`X = [x_1, x_2, .. x_N]^T`, then the covariance matrix</span>
<span class="sd">    element :math:`C_{ij}` is the covariance of :math:`x_i` and :math:`x_j`. The element</span>
<span class="sd">    :math:`C_{ii}` is the variance of :math:`x_i`.</span>

<span class="sd">    Note:</span>
<span class="sd">        `fweights` and `aweights` must be all positive, in Numpy if negative values</span>
<span class="sd">        are detected, a value error will be raised, in MindSpore we converts all values</span>
<span class="sd">        to positive instead.</span>

<span class="sd">    Args:</span>
<span class="sd">        m (Union[Tensor, list, tuple]): A 1-D or 2-D tensor containing multiple variables</span>
<span class="sd">            and observations. Each row of `m` represents a variable, and each column</span>
<span class="sd">            represents a single observation of all those variables. Also see `rowvar` below.</span>
<span class="sd">        y (Union[Tensor, list, tuple], optional): An additional set of variables</span>
<span class="sd">            and observations. `y` has the same form as that of `m`, default is ``None``.</span>
<span class="sd">        rowvar(bool, optional): If `rowvar` is ``True`` (default), then each row represents</span>
<span class="sd">            a variable, with observations in the columns. Otherwise, the relationship</span>
<span class="sd">            is transposed: each column represents a variable, while the rows contain</span>
<span class="sd">            observations.</span>
<span class="sd">        bias (bool, optional): Default Normalization (``False``) is by :math:`(N - 1)`, where</span>
<span class="sd">            :math:`N` is the number of observations given (unbiased estimate). If bias is</span>
<span class="sd">            ``True``, then Normalization is by `N`. These values can be overridden by</span>
<span class="sd">            using the keyword `ddof`.</span>
<span class="sd">        ddof (int, optional): If not ``None``, the default value implied by `bias` is</span>
<span class="sd">            overridden. Note that :math:`ddof=1` will return the unbiased estimate, even</span>
<span class="sd">            if both fweights and aweights are specified, and :math:`ddof=0` will return</span>
<span class="sd">            the simple average. See the notes for the details. The default value</span>
<span class="sd">            is ``None``.</span>
<span class="sd">        fweights (Union[Tensor, list, tuple], optional): 1-D tensor of integer</span>
<span class="sd">            frequency weights; the number of times each observation vector should</span>
<span class="sd">            be repeated. The default value is ``None``.</span>
<span class="sd">        aweights (Union[Tensor, list, tuple], optional): 1-D tensor of observation</span>
<span class="sd">            vector weights. These relative weights are typically larger for observations</span>
<span class="sd">            considered more important and smaller for observations considered less</span>
<span class="sd">            important. If :math:`ddof=0` the tensor of weights can be used to assign probabilities</span>
<span class="sd">            to observation vectors. The default value is ``None``.</span>
<span class="sd">        dtype (Union[:class:`mindspore.dtype`, str], optional): Data-type of the</span>
<span class="sd">            result. By default, the return data-type will have mstype.float32 precision.</span>
<span class="sd">            Default is ``None``.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the covariance matrix of the variables.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the inputs have types not specified above.</span>
<span class="sd">        ValueError: If `m` and `y` have wrong dimensions.</span>
<span class="sd">        RuntimeError: If `aweights` and `fweights` have dimensions &gt; 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; output = np.cov([[2., 3., 4., 5.], [0., 2., 3., 4.], [7., 8., 9., 10.]])</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1.6666666 2.1666667 1.6666666]</span>
<span class="sd">        [2.1666667 2.9166667 2.1666667]</span>
<span class="sd">        [1.6666666 2.1666667 1.6666666]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># This implementation was inspired by original numpy implementation.</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">_handle_inputs</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">rowvar</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">empty</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">_handle_inputs</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">rowvar</span><span class="p">)</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">concatenate</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">ddof</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">bias</span><span class="p">:</span>
            <span class="n">ddof</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ddof</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># Handle fweights and aweights</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">_handle_weights</span><span class="p">(</span><span class="n">fweights</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">if</span> <span class="n">fweights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">aweights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">aweights</span> <span class="o">=</span> <span class="n">_handle_weights</span><span class="p">(</span><span class="n">aweights</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">aweights</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">w</span> <span class="o">*</span> <span class="n">aweights</span>

    <span class="n">avg</span> <span class="o">=</span> <span class="n">average</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">w</span><span class="p">)</span>

    <span class="c1"># Determine the Normalization</span>
    <span class="n">fact</span> <span class="o">=</span> <span class="n">_handle_facts</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">ddof</span><span class="p">,</span> <span class="n">aweights</span><span class="p">)</span>

    <span class="n">m</span> <span class="o">=</span> <span class="n">m</span> <span class="o">-</span> <span class="n">F</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">avg</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">w</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">m_t</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">T</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">m_t</span> <span class="o">=</span> <span class="p">(</span><span class="n">m</span> <span class="o">*</span> <span class="n">w</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">true_divide</span><span class="p">(</span><span class="n">dot</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">m_t</span><span class="p">),</span> <span class="n">fact</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">res</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_real_axes</span><span class="p">(</span><span class="n">ndim_orig</span><span class="p">,</span> <span class="n">ndim_out</span><span class="p">,</span> <span class="n">axes_orig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns the real axes to be reduced after performing broadcast&quot;&quot;&quot;</span>
    <span class="n">_diff</span> <span class="o">=</span> <span class="n">ndim_out</span> <span class="o">-</span> <span class="n">ndim_orig</span>
    <span class="n">axes</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">make_range</span><span class="p">(</span><span class="n">_diff</span><span class="p">)</span>
    <span class="n">axes_orig</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">operator</span><span class="o">.</span><span class="n">add</span><span class="p">,</span> <span class="n">_diff</span><span class="p">),</span> <span class="n">axes_orig</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">axes</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">axes_orig</span><span class="p">)</span>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_shape_reduced_keepdims</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">axes</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces dimensions corresponding to argument axes while</span>
<span class="sd">    keeping the number of dimensions unchanged.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ndim_out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tuple_len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">shape_out</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">ndim_out</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndim_out</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">:</span>
            <span class="n">shape_out</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">shape_out</span><span class="p">)</span>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_shape_reduced</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">axes</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Removes dimensions corresponding to argument axes&quot;&quot;&quot;</span>
    <span class="n">ndim_orig</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tuple_len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">ndim_out</span> <span class="o">=</span> <span class="n">ndim_orig</span> <span class="o">-</span> <span class="n">F</span><span class="o">.</span><span class="n">tuple_len</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span>
    <span class="n">shape_out</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">ndim_out</span>
    <span class="n">idx_out</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndim_orig</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">:</span>
            <span class="n">shape_out</span><span class="p">[</span><span class="n">idx_out</span><span class="p">]</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">idx_out</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">shape_out</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_reduce</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">reduce_fn</span><span class="p">,</span> <span class="n">cmp_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">initial</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies comparison based on cmp_fn and reduction based on reduce_fn.</span>
<span class="sd">    If cmp_fn is None, only reduction is performed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

    <span class="n">shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">axes</span> <span class="o">=</span> <span class="n">_check_axis_valid</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">initial</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">((</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">initial</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">initial</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">or</span>
                <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initial</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">))):</span>
            <span class="n">_raise_type_error</span><span class="p">(</span><span class="s1">&#39;initial should be scalar&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">_is_shape_empty</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">axes</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">a</span>
        <span class="k">if</span> <span class="n">keepdims</span><span class="p">:</span>
            <span class="n">shape_out</span> <span class="o">=</span> <span class="n">_shape_reduced_keepdims</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">axes</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">shape_out</span> <span class="o">=</span> <span class="n">_shape_reduced</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">axes</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">_is_shape_empty</span><span class="p">(</span><span class="n">shape_out</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">empty</span><span class="p">(</span><span class="n">shape_out</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">initial</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">cmp_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">initial</span> <span class="o">=</span> <span class="n">nan</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;initial value must be provided for zero-size arrays&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">full</span><span class="p">(</span><span class="n">shape_out</span><span class="p">,</span> <span class="n">initial</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">initial</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">initial</span> <span class="o">=</span> <span class="n">full</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">initial</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">cmp_fn</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">initial</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">where</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">initial</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;initial value must be provided for where masks&#39;</span><span class="p">)</span>
        <span class="n">ndim_orig</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">where_</span><span class="p">(</span><span class="n">where</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">initial</span><span class="p">)</span>
        <span class="n">axes</span> <span class="o">=</span> <span class="n">_real_axes</span><span class="p">(</span><span class="n">ndim_orig</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">axes</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">reduce_fn</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axes</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>


<div class="viewcode-block" id="nanmax"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.nanmax.html#mindspore.numpy.nanmax">[docs]</a><span class="k">def</span> <span class="nf">nanmax</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return the maximum of an array or maximum along an axis, ignoring any NaNs.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out` is not supported.</span>
<span class="sd">        For all NaN slices, a very small negative number is returned instead of NaN.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Union[int, float, list, tuple, Tensor]): Array containing numbers whose maximum</span>
<span class="sd">            is desired. If `a` is not an array, a conversion is attempted.</span>
<span class="sd">        axis (Union[int, tuple of int, None], optional): Axis or axes along which the maximum is</span>
<span class="sd">            computed. The default is to compute the maximum of the flattened array.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>
<span class="sd">        keepdims (boolean, optional): Default: ``False`` . If this is set to True, the axes which</span>
<span class="sd">            are reduced are left in the result as dimensions with size one. With this option,</span>
<span class="sd">            the result will broadcast correctly against the original `a`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If axes are out of the range of ``[-a.ndim, a.ndim)``, or</span>
<span class="sd">            if the axes contain duplicates.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.array([[1, 2], [3, np.nan]])</span>
<span class="sd">        &gt;&gt;&gt; output = np.nanmax(a)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        3.0</span>
<span class="sd">        &gt;&gt;&gt; output = np.nanmax(a, axis=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [3. 2.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">keepdims</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;integer argument expected, got&quot;</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">)</span>
    <span class="n">nan_mask</span> <span class="o">=</span> <span class="n">_isnan</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">nan_mask</span><span class="p">,</span> <span class="n">full</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="o">-</span><span class="n">sys</span><span class="o">.</span><span class="n">maxsize</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">a</span><span class="p">)),</span> <span class="n">a</span><span class="p">)</span>
    <span class="n">reduce_fn</span> <span class="o">=</span> <span class="n">_reduce_max_keepdims</span> <span class="k">if</span> <span class="n">keepdims</span> <span class="k">else</span> <span class="n">_reduce_max_default</span>
    <span class="k">return</span> <span class="n">_reduce</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">reduce_fn</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="nanmin"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.nanmin.html#mindspore.numpy.nanmin">[docs]</a><span class="k">def</span> <span class="nf">nanmin</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the minimum of array elements over a given axis, ignoring any NaNs.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out` is not supported.</span>
<span class="sd">        For all-NaN slices, a very large number is returned instead of NaN.</span>
<span class="sd">        On Ascend, since checking for NaN is currently not supported, it is not recommended to</span>
<span class="sd">        use np.nanmin. If the array does not contain NaN, np.min should be used instead.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Union[int, float, list, tuple, Tensor]): Array containing numbers whose minimum</span>
<span class="sd">            is desired. If `a` is not an array, a conversion is attempted.</span>
<span class="sd">        axis (Union[int, tuple of int, None], optional): Axis or axes along which the minimum is</span>
<span class="sd">            computed. The default is to compute the minimum of the flattened array.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>
<span class="sd">        keepdims (boolean, optional): Default: ``False`` . If this is set to True, the axes which</span>
<span class="sd">            are reduced are left in the result as dimensions with size one. With this option,</span>
<span class="sd">            the result will broadcast correctly against the original `a`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If axes are out of the range of ``[-a.ndim, a.ndim)``, or</span>
<span class="sd">            if the axes contain duplicates.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.array([[1, 2], [3, np.nan]])</span>
<span class="sd">        &gt;&gt;&gt; output = np.nanmin(a)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        1.0</span>
<span class="sd">        &gt;&gt;&gt; output = np.nanmin(a, axis=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 2.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">keepdims</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;integer argument expected, got&quot;</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">)</span>
    <span class="n">nan_mask</span> <span class="o">=</span> <span class="n">_isnan</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">nan_mask</span><span class="p">,</span> <span class="n">full</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">sys</span><span class="o">.</span><span class="n">maxsize</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">a</span><span class="p">)),</span> <span class="n">a</span><span class="p">)</span>
    <span class="n">reduce_fn</span> <span class="o">=</span> <span class="n">_reduce_min_keepdims</span> <span class="k">if</span> <span class="n">keepdims</span> <span class="k">else</span> <span class="n">_reduce_min_default</span>
    <span class="k">return</span> <span class="n">_reduce</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">reduce_fn</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_reduce_nansum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Computes reduce sum treating NaNs as zeros.&quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">_isnan</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">zeros</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">keepdims</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_reduce_sum_keepdims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_reduce_sum_default</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>


<div class="viewcode-block" id="nansum"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.nansum.html#mindspore.numpy.nansum">[docs]</a><span class="k">def</span> <span class="nf">nansum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the sum of array elements over a given axis treating Not a Numbers (NaNs) as zero.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out` is not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Union[int, float, list, tuple, Tensor]): Array containing numbers</span>
<span class="sd">            whose sum is desired. If `a` is not an array, a conversion is attempted.</span>
<span class="sd">        axis (Union[int, tuple of int, None], optional): Axis or axes along which the sum is</span>
<span class="sd">            computed. The default is to compute the sum of the flattened array.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>
<span class="sd">        keepdims (boolean, optional): Default: ``False`` . If this is set to True, the axes which</span>
<span class="sd">            are reduced are left in the result as dimensions with size one. With this option,</span>
<span class="sd">            the result will broadcast correctly against the original `a`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If axes are out of the range of ``[-a.ndim, a.ndim)``, or</span>
<span class="sd">            if the axes contain duplicates.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.array([[1, 1], [1, np.nan]])</span>
<span class="sd">        &gt;&gt;&gt; output = np.nansum(a)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        3.0</span>
<span class="sd">        &gt;&gt;&gt; output = np.nansum(a, axis=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2. 1.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">nan_mask</span> <span class="o">=</span> <span class="n">_isnan</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">nan_mask</span><span class="p">,</span> <span class="n">zeros</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">a</span><span class="p">)),</span> <span class="n">a</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_reduce</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_reduce_nansum</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span>
                   <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_count_nonnan</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Counts the number of elements excluding NaNs.&quot;&quot;&quot;</span>
    <span class="n">nonnan_mask</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">_isnan</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">zeros</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">a</span><span class="p">)),</span> <span class="n">ones</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">a</span><span class="p">)))</span>
    <span class="k">if</span> <span class="n">keepdims</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_reduce_sum_keepdims</span><span class="p">(</span><span class="n">nonnan_mask</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_reduce_sum_default</span><span class="p">(</span><span class="n">nonnan_mask</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>


<div class="viewcode-block" id="nanmean"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.nanmean.html#mindspore.numpy.nanmean">[docs]</a><span class="k">def</span> <span class="nf">nanmean</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the arithmetic mean along the specified axis, ignoring NaNs.</span>

<span class="sd">    Returns the average of the array elements. The average is taken over the flattened</span>
<span class="sd">    array by default, otherwise over the specified axis. float32 intermediate and</span>
<span class="sd">    return values are used for integer inputs.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out` is not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Union[int, float, list, tuple, Tensor]): Array containing numbers</span>
<span class="sd">            whose mean is desired. If `a` is not an array, a conversion is attempted.</span>
<span class="sd">        axis (Union[int, tuple of int, None], optional): Axis or axes along which the mean is</span>
<span class="sd">            computed. The default is to compute the mean of the flattened array.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>
<span class="sd">        keepdims (boolean, optional): Default: ``False`` . If this is set to True, the axes which</span>
<span class="sd">            are reduced are left in the result as dimensions with size one. With this option,</span>
<span class="sd">            the result will broadcast correctly against the original `a`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If axes are out of the range of ``[-a.ndim, a.ndim)``, or</span>
<span class="sd">            if the axes contain duplicates.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.array([[1, np.nan], [3, 4]])</span>
<span class="sd">        &gt;&gt;&gt; output = np.nanmean(a)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        2.6666667</span>
<span class="sd">        &gt;&gt;&gt; output = np.nanmean(a, axis=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2. 4.]</span>
<span class="sd">        &gt;&gt;&gt; output = np.nanmean(a, axis=1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.  3.5]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="n">_check_axis_valid</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>
    <span class="n">sum_a</span> <span class="o">=</span> <span class="n">nansum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_div</span><span class="p">(</span><span class="n">sum_a</span><span class="p">,</span> <span class="n">_count_nonnan</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">))</span></div>


<span class="k">def</span> <span class="nf">_nanvar</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Computes nanvar without applying keyword arguments.&quot;&quot;&quot;</span>
    <span class="n">mean_a</span> <span class="o">=</span> <span class="n">nanmean</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">pow_a</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_pow</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">tensor_sub</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">mean_a</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">sum_a</span> <span class="o">=</span> <span class="n">_reduce_nansum</span><span class="p">(</span><span class="n">pow_a</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">)</span>
    <span class="n">count</span> <span class="o">=</span> <span class="n">_count_nonnan</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">divide</span><span class="p">(</span><span class="n">sum_a</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_sub</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">ddof</span><span class="p">))</span>


<div class="viewcode-block" id="nanvar"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.nanvar.html#mindspore.numpy.nanvar">[docs]</a><span class="k">def</span> <span class="nf">nanvar</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the variance along the specified axis, while ignoring NaNs.</span>

<span class="sd">    Returns the variance of the array elements, a measure of the spread of a distribution. The</span>
<span class="sd">    variance is computed for the flattened array by default, otherwise over the specified axis.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out` is not supported.</span>
<span class="sd">        On GPU, the supported dtypes are np.float16, and np.float32.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Union[int, float, list, tuple, Tensor]): Array containing numbers</span>
<span class="sd">            whose variance is desired. If `a` is not an array, a conversion is attempted.</span>
<span class="sd">        axis (Union[int, tuple of int, None], optional): Axis or axes along which the variance is</span>
<span class="sd">            computed. The default is to compute the variance of the flattened array.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>
<span class="sd">        ddof (int, optional): &quot;Delta Degrees of Freedom&quot;: the divisor used in the calculation is</span>
<span class="sd">            ``N - ddof``, where `N` represents the number of non-NaN elements. By default `ddof`</span>
<span class="sd">            is zero.</span>
<span class="sd">        keepdims (boolean, optional): Default: ``False`` . If this is set to True, the axes which</span>
<span class="sd">            are reduced are left in the result as dimensions with size one. With this option,</span>
<span class="sd">            the result will broadcast correctly against the original `a`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If axes are out of the range of ``[-a.ndim, a.ndim)``, or</span>
<span class="sd">            if the axes contain duplicates.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.array([[1, np.nan], [3, 4]])</span>
<span class="sd">        &gt;&gt;&gt; output = np.nanvar(a)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        1.5555557</span>
<span class="sd">        &gt;&gt;&gt; output = np.nanvar(a, axis=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 0.]</span>
<span class="sd">        &gt;&gt;&gt; output = np.nanvar(a, axis=1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.   0.25]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span>
    <span class="k">return</span> <span class="n">_reduce</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_nanvar</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="n">ddof</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span>
                   <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="nanstd"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.nanstd.html#mindspore.numpy.nanstd">[docs]</a><span class="k">def</span> <span class="nf">nanstd</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the standard deviation along the specified axis, while ignoring NaNs.</span>

<span class="sd">    Returns the standard deviation, a measure of the spread of a distribution, of the non-NaN</span>
<span class="sd">    array elements. The standard deviation is computed for the flattened array by default,</span>
<span class="sd">    otherwise over the specified axis.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out` is not supported.</span>
<span class="sd">        On GPU, the supported dtypes are np.float16, and np.float32.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Union[int, float, list, tuple, Tensor]): Calculates the standard deviation of the non-NaN values.</span>
<span class="sd">        axis (Union[int, tuple of int, None], optional): Axis or axes along which the standard</span>
<span class="sd">            deviation is computed. The default is to compute the standard deviation of the</span>
<span class="sd">            flattened array.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>
<span class="sd">        ddof (int, optional): &quot;Delta Degrees of Freedom&quot;: the divisor used in the calculation is</span>
<span class="sd">            ``N - ddof``, where `N` represents the number of non-NaN elements. By default `ddof`</span>
<span class="sd">            is zero.</span>
<span class="sd">        keepdims (boolean, optional): Default: ``False`` . If this is set to True, the axes which</span>
<span class="sd">            are reduced are left in the result as dimensions with size one. With this option,</span>
<span class="sd">            the result will broadcast correctly against the original `a`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If axes are out of the range of ``[-a.ndim, a.ndim)``, or</span>
<span class="sd">            if the axes contain duplicates.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.array([[1, np.nan], [3, 4]])</span>
<span class="sd">        &gt;&gt;&gt; output = np.nanstd(a)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        1.2472192</span>
<span class="sd">        &gt;&gt;&gt; output = np.nanstd(a, axis=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 0.]</span>
<span class="sd">        &gt;&gt;&gt; output = np.nanstd(a, axis=1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.  0.5]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span>
    <span class="k">return</span> <span class="n">_reduce</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">F</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">_nanvar</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="n">ddof</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">)),</span>
                   <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="exp2"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.exp2.html#mindspore.numpy.exp2">[docs]</a><span class="k">def</span> <span class="nf">exp2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates ``2**p`` for all p in the input array.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>
<span class="sd">        On GPU, the supported dtypes are np.float16, and np.float32.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): input values.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Defaults to ``None``. Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, element-wise 2 to the power `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.array([2, 3]).astype(np.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = np.exp2(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [4. 8.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_pow</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="kron"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.kron.html#mindspore.numpy.kron">[docs]</a><span class="k">def</span> <span class="nf">kron</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Kronecker product of two arrays.</span>

<span class="sd">    Computes the Kronecker product, a composite array made of blocks of the second</span>
<span class="sd">    array scaled by the first.</span>

<span class="sd">    Note:</span>
<span class="sd">        Booleans are not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Union[int, float, list, tuple, Tensor]): input values.</span>
<span class="sd">        b (Union[int, float, list, tuple, Tensor]): input values.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; output = np.kron([1,10,100], [5,6,7])</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [  5   6   7  50  60  70 500 600 700]</span>
<span class="sd">        &gt;&gt;&gt; output = np.kron([5,6,7], [1,10,100])</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [  5  50 500   6  60 600   7  70 700]</span>
<span class="sd">        &gt;&gt;&gt; output = np.kron(np.eye(2), np.ones((2,2)))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 1. 0. 0.]</span>
<span class="sd">        [1. 1. 0. 0.]</span>
<span class="sd">        [0. 0. 1. 1.]</span>
<span class="sd">        [0. 0. 1. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="n">_max</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_mul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">_expand</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">_expand</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
    <span class="n">shape_a</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">shape_b</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

    <span class="c1"># scales a by the shape of b</span>
    <span class="n">kron_shape</span> <span class="o">=</span> <span class="n">_seq_prod</span><span class="p">(</span><span class="n">shape_a</span><span class="p">,</span> <span class="n">shape_b</span><span class="p">)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">_add_unit_axes</span><span class="p">(</span><span class="n">shape_a</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">ndim</span><span class="p">,</span> <span class="kc">True</span><span class="p">))</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">_add_unit_axes</span><span class="p">(</span><span class="n">shape_b</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">ndim</span><span class="p">,</span> <span class="kc">False</span><span class="p">))</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">moveaxis</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">make_range</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">ndim</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">make_range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">ndim</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">kron_shape</span><span class="p">)</span>
    <span class="c1"># scales b by the shape of a</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">shape_a</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_mul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span></div>


<div class="viewcode-block" id="cross"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.cross.html#mindspore.numpy.cross">[docs]</a><span class="k">def</span> <span class="nf">cross</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">axisa</span><span class="o">=-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">axisb</span><span class="o">=-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">axisc</span><span class="o">=-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the cross product of two (arrays of) vectors.</span>

<span class="sd">    The cross product of `a` and `b` in :math:`R^3` is a vector perpendicular to both</span>
<span class="sd">    `a` and `b`. If `a` and `b` are arrays of vectors, the vectors are defined by the</span>
<span class="sd">    last axis of `a` and `b` by default, and these axes can have dimensions 2 or 3.</span>
<span class="sd">    Where the dimension of either `a` or `b` is 2, the third component of the input</span>
<span class="sd">    vector is assumed to be zero and the cross product calculated accordingly. In cases</span>
<span class="sd">    where both input vectors have dimension 2, the z-component of the cross product is</span>
<span class="sd">    returned.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Union[list, tuple, Tensor]): Components of the first vector(s).</span>
<span class="sd">        b (Union[list, tuple, Tensor]): Components of the second vector(s).</span>
<span class="sd">        axisa (int, optional): Axis of `a` that defines the vector(s). By default, the last</span>
<span class="sd">            axis.</span>
<span class="sd">        axisb (int, optional): Axis of `b` that defines the vector(s). By default, the last</span>
<span class="sd">            axis.</span>
<span class="sd">        axisc (int, optional): Axis of `c` containing the cross product vector(s). Ignored</span>
<span class="sd">            if both input vectors have dimension 2, as the return is scalar. By default,</span>
<span class="sd">            the last axis.</span>
<span class="sd">        axis (int, optional): If defined, the axis of `a`, `b` and `c` that defines the</span>
<span class="sd">            vector(s) and cross product(s). Overrides `axisa`, `axisb` and `axisc`.</span>
<span class="sd">            Default: ``None`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, vector cross product(s).</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: when the dimensions of the vector(s) in `a` and/or `b` does not equal 2</span>
<span class="sd">            or 3.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.array([[1,2,3], [4,5,6]])</span>
<span class="sd">        &gt;&gt;&gt; y = np.array([[4,5,6], [1,2,3]])</span>
<span class="sd">        &gt;&gt;&gt; output = np.cross(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-3  6 -3]</span>
<span class="sd">        [ 3 -6  3]]</span>
<span class="sd">        &gt;&gt;&gt; output = np.cross(x, y, axisc=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-3  3]</span>
<span class="sd">        [ 6 -6]</span>
<span class="sd">        [-3  3]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axisa</span><span class="p">,</span> <span class="n">axisb</span><span class="p">,</span> <span class="n">axisc</span> <span class="o">=</span> <span class="n">axis</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">axis</span>

    <span class="n">_check_axis_in_range</span><span class="p">(</span><span class="n">axisa</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>
    <span class="n">_check_axis_in_range</span><span class="p">(</span><span class="n">axisb</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">moveaxis</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axisa</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">moveaxis</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">axisb</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">shape_a</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">shape_b</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">a</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="ow">or</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">b</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">):</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;incompatible dimensions for cross product (dimension must be 2 or 3)&#39;</span><span class="p">)</span>
    <span class="n">a_has_z</span> <span class="o">=</span> <span class="n">shape_a</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">3</span>
    <span class="n">b_has_z</span> <span class="o">=</span> <span class="n">shape_b</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">3</span>
    <span class="n">shape_out</span> <span class="o">=</span> <span class="n">_infer_out_shape</span><span class="p">(</span><span class="n">shape_a</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">shape_b</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">a_has_z</span> <span class="ow">or</span> <span class="n">b_has_z</span><span class="p">:</span>
        <span class="n">shape_out</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)</span>
    <span class="n">_check_axis_in_range</span><span class="p">(</span><span class="n">axisc</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape_out</span><span class="p">))</span>

    <span class="n">dtype</span> <span class="o">=</span> <span class="n">_promote</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">_get_device</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;CPU&#39;</span><span class="p">:</span>
        <span class="c1"># F.tensor_slice only supports float on CPU</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_is_float</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">a</span><span class="p">)):</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_is_float</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">b</span><span class="p">)):</span>
            <span class="n">b</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="n">a_slice_start</span> <span class="o">=</span> <span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">a_slice_size</span> <span class="o">=</span> <span class="n">shape_a</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span>
    <span class="n">b_slice_start</span> <span class="o">=</span> <span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">b_slice_size</span> <span class="o">=</span> <span class="n">shape_b</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span>

    <span class="k">def</span> <span class="nf">_get_slice_product</span><span class="p">(</span><span class="n">idx_a</span><span class="p">,</span> <span class="n">idx_b</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">multiply</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">tensor_slice</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">a_slice_start</span> <span class="o">+</span> <span class="p">(</span><span class="n">idx_a</span><span class="p">,),</span> <span class="n">a_slice_size</span><span class="p">),</span>
                        <span class="n">F</span><span class="o">.</span><span class="n">tensor_slice</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">b_slice_start</span> <span class="o">+</span> <span class="p">(</span><span class="n">idx_b</span><span class="p">,),</span> <span class="n">b_slice_size</span><span class="p">))</span>

    <span class="n">cz</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_sub</span><span class="p">(</span><span class="n">_get_slice_product</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">_get_slice_product</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span> <span class="c1"># ax*by - ay*bx</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">a_has_z</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">b_has_z</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">cz</span><span class="p">,</span> <span class="n">shape_out</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">a_has_z</span> <span class="ow">and</span> <span class="n">b_has_z</span><span class="p">:</span>
        <span class="n">cx</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_sub</span><span class="p">(</span><span class="n">_get_slice_product</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">_get_slice_product</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="c1"># ay*bz - az*by</span>
        <span class="n">cy</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_sub</span><span class="p">(</span><span class="n">_get_slice_product</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">_get_slice_product</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> <span class="c1"># az*bx - ax*bz</span>
    <span class="k">elif</span> <span class="n">a_has_z</span><span class="p">:</span>
        <span class="n">cx</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">neg_tensor</span><span class="p">(</span><span class="n">_get_slice_product</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="c1"># -az*by</span>
        <span class="n">cy</span> <span class="o">=</span> <span class="n">_get_slice_product</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>               <span class="c1"># az*bx</span>
    <span class="k">else</span><span class="p">:</span> <span class="c1"># b_has_z</span>
        <span class="n">cx</span> <span class="o">=</span> <span class="n">_get_slice_product</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>               <span class="c1"># ay*bz</span>
        <span class="n">cy</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">neg_tensor</span><span class="p">(</span><span class="n">_get_slice_product</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> <span class="c1"># -ax*bz</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">_concat</span><span class="p">((</span><span class="n">cx</span><span class="p">,</span> <span class="n">cy</span><span class="p">,</span> <span class="n">cz</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">shape_out</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">moveaxis</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">axisc</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="ceil"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.ceil.html#mindspore.numpy.ceil">[docs]</a><span class="k">def</span> <span class="nf">ceil</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the ceiling of the input, element-wise.</span>

<span class="sd">    The ceil of the scalar `x` is the smallest integer `i`, such that ``i &gt;= x``.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>
<span class="sd">        On GPU, the supported dtypes are np.float16, and np.float32.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): input values.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, the floor of each element in `x`. This is a scalar if `x` is a scalar.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.array([-1.7, -1.5, -0.2, 0.2, 1.5, 1.7, 2.0])</span>
<span class="sd">        &gt;&gt;&gt; output = np.ceil(a)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-1. -1. -0.  1.  2.  2.  2.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">F</span><span class="o">.</span><span class="n">neg_tensor</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">neg_tensor</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)))),</span>
                            <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_infer_shape_rem</span><span class="p">(</span><span class="n">shape1</span><span class="p">,</span> <span class="n">shape2</span><span class="p">,</span> <span class="n">ndim1</span><span class="p">,</span> <span class="n">ndim2</span><span class="p">,</span> <span class="n">transpose_b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Infers the shape of the last two dimensions after performing matmul.&quot;&quot;&quot;</span>
    <span class="n">shape_rem</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">if</span> <span class="n">ndim1</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">shape_rem</span> <span class="o">+=</span> <span class="p">(</span><span class="n">shape1</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],)</span>
    <span class="k">if</span> <span class="n">transpose_b</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">ndim2</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">shape_rem</span> <span class="o">+=</span> <span class="p">(</span><span class="n">shape2</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">ndim1</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">shape_rem</span> <span class="o">+=</span> <span class="p">(</span><span class="n">shape2</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],)</span>
    <span class="k">return</span> <span class="n">shape_rem</span>


<div class="viewcode-block" id="positive"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.positive.html#mindspore.numpy.positive">[docs]</a><span class="k">def</span> <span class="nf">positive</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Numerical positive, element-wise.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): Input tensor.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.asarray([1, -1]).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = np.positive(a)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. -1.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">neg_tensor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">neg_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">neg_tensor</span><span class="p">,</span> <span class="n">neg_tensor</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="negative"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.negative.html#mindspore.numpy.negative">[docs]</a><span class="k">def</span> <span class="nf">negative</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Numerical negative, element-wise.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): Input tensor.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.asarray([1, -1]).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = np.negative(a)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-1. 1.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">neg_tensor</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="cumsum"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.cumsum.html#mindspore.numpy.cumsum">[docs]</a><span class="k">def</span> <span class="nf">cumsum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the cumulative sum of the elements along a given axis.</span>

<span class="sd">    Note:</span>
<span class="sd">        If ``a.dtype`` is :class:`int8`, :class:`int16` or :class:`bool`, the result</span>
<span class="sd">        `dtype` will be elevated to :class:`int32`.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): Input tensor.</span>
<span class="sd">        axis (int, optional): Axis along which the cumulative sum is computed. The</span>
<span class="sd">            default ( ``None`` ) is to compute the cumsum over the flattened array.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): If not specified, stay the same as `a`,</span>
<span class="sd">            unless `a` has an integer dtype with a precision less than that of the</span>
<span class="sd">            default platform integer. In that case, the default platform integer</span>
<span class="sd">            is used. Default: ``None`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If input arguments have types not specified above.</span>
<span class="sd">        ValueError: If axis is out of range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; output = np.cumsum(np.ones((3,3)), axis=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 1. 1.]</span>
<span class="sd">         [2. 2. 2.]</span>
<span class="sd">         [3. 3. 3.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="nancumsum"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.nancumsum.html#mindspore.numpy.nancumsum">[docs]</a><span class="k">def</span> <span class="nf">nancumsum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return the cumulative sum of array elements over a given axis treating Not a Numbers (NaNs)</span>
<span class="sd">    as zero. The cumulative sum does not change when NaNs are encountered and leading NaNs are</span>
<span class="sd">    replaced by zeros.</span>

<span class="sd">    Zeros are returned for slices that are all-NaN or empty.</span>

<span class="sd">    Note:</span>
<span class="sd">        If ``a.dtype`` is :class:`int8`, :class:`int16` or :class:`bool`, the result</span>
<span class="sd">        `dtype` will be elevated to :class:`int32`.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): Input tensor.</span>
<span class="sd">        axis (int, optional): Axis along which the cumulative sum is computed. The</span>
<span class="sd">            default (None) is to compute the cumsum over the flattened array.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): If not specified, stay the same as `a`,</span>
<span class="sd">            unless `a` has an integer dtype with a precision less than that of the</span>
<span class="sd">            default platform integer. In that case, the default platform integer</span>
<span class="sd">            is used.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If input arguments have types not specified above.</span>
<span class="sd">        ValueError: If axis is out of range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.array([[1, 2], [3, np.nan]])</span>
<span class="sd">        &gt;&gt;&gt; output = np.nancumsum(a)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 3. 6. 6.]</span>
<span class="sd">        &gt;&gt;&gt; output = np.nancumsum(a, axis=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 2.]</span>
<span class="sd">        [4. 2.]]</span>
<span class="sd">        &gt;&gt;&gt; output = np.nancumsum(a, axis=1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 3.]</span>
<span class="sd">        [3. 3.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">_isnan</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">zeros</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">a</span><span class="p">)),</span> <span class="n">a</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="cbrt"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.cbrt.html#mindspore.numpy.cbrt">[docs]</a><span class="k">def</span> <span class="nf">cbrt</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the cube-root of a tensor, element-wise.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input tensor.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.asarray([1, -1, 3, -8, 64])</span>
<span class="sd">        &gt;&gt;&gt; output = np.cbrt(a)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1.        -1.         1.4422495 -2.         4.       ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">_cbrt</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="n">compute_type</span> <span class="o">=</span> <span class="n">promote_types</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="s2">&quot;float32&quot;</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">compute_type</span><span class="p">)</span>
        <span class="c1"># TODO: use P.Sign() once gpu support is added</span>
        <span class="n">abs_x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">absolute</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">sign_x</span> <span class="o">=</span> <span class="n">abs_x</span> <span class="o">/</span> <span class="n">x</span>
        <span class="k">return</span> <span class="n">sign_x</span> <span class="o">*</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_pow</span><span class="p">(</span><span class="n">abs_x</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">/</span> <span class="mf">3.</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">_cbrt</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="log1p"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.log1p.html#mindspore.numpy.log1p">[docs]</a><span class="k">def</span> <span class="nf">log1p</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the natural logarithm of one plus the input array, element-wise.</span>

<span class="sd">    Calculates ``log(1 + x)``.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input array.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`): Default: ``None``. Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar. This is a scalar if `x` is a scalar.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.array([1, 2, 3]).astype(&#39;float16&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = np.log1p(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.6934 1.099 1.387 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">F</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="logaddexp"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.logaddexp.html#mindspore.numpy.logaddexp">[docs]</a><span class="k">def</span> <span class="nf">logaddexp</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Logarithm of the sum of exponentiations of the inputs.</span>

<span class="sd">    Calculates ``log(exp(x1) + exp(x2))``. This function is useful in statistics where the</span>
<span class="sd">    calculated probabilities of events may be so small as to exceed the range of normal</span>
<span class="sd">    floating point numbers. In such cases the logarithm of the calculated probability is</span>
<span class="sd">    stored. This function allows adding probabilities stored in such a fashion.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Tensor): Input array.</span>
<span class="sd">        x2 (Tensor): Input array. If ``x1.shape != x2.shape``, they must be broadcastable to</span>
<span class="sd">            a common shape (which becomes the shape of the output).</span>
<span class="sd">        dtype (:class:`mindspore.dtype`): Default: ``None``. Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar. This is a scalar if both `x1` and `x2` are scalars.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x1 = np.array([1, 2, 3]).astype(&#39;float16&#39;)</span>
<span class="sd">        &gt;&gt;&gt; x2 = np.array(2).astype(&#39;float16&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = np.logaddexp(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2.312 2.693 3.312]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">_logaddexp</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">tensor_add</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">tensor_exp</span><span class="p">(</span><span class="n">x1</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_exp</span><span class="p">(</span><span class="n">x2</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">_logaddexp</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="log2"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.log2.html#mindspore.numpy.log2">[docs]</a><span class="k">def</span> <span class="nf">log2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Base-2 logarithm of `x`.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input tensor.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None``. Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar. This is a scalar if `x` is a scalar.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.array([2, 4, 8]).astype(&#39;float16&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = np.log2(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 2. 3.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">tensor_2</span> <span class="o">=</span> <span class="n">_make_tensor</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_log2</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="n">F</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">tensor_2</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">_log2</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="logaddexp2"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.logaddexp2.html#mindspore.numpy.logaddexp2">[docs]</a><span class="k">def</span> <span class="nf">logaddexp2</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Logarithm of the sum of exponentiations of the inputs in base of 2.</span>

<span class="sd">    Calculates ``log2(2**x1 + 2**x2)``.</span>
<span class="sd">    This function is useful in machine learning when the calculated probabilities of events</span>
<span class="sd">    may be so small as to exceed the range of normal floating point numbers.</span>
<span class="sd">    In such cases the base-2 logarithm of the calculated probability can be used instead.</span>
<span class="sd">    This function allows adding probabilities stored in such a fashion.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Tensor): Input tensor.</span>
<span class="sd">        x2 (Tensor): Input tensor. If ``x1.shape != x2.shape``, they must be broadcastable to</span>
<span class="sd">            a common shape (which becomes the shape of the output).</span>
<span class="sd">        dtype (:class:`mindspore.dtype`): Default: ``None``. Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar. This is a scalar if both `x1` and `x2` are scalars.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x1 = np.array([2, 4, 8]).astype(&#39;float16&#39;)</span>
<span class="sd">        &gt;&gt;&gt; x2 = np.array(2).astype(&#39;float16&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = np.logaddexp2(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [3. 4.32 8.02]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
    <span class="n">add_exp</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_add</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">tensor_pow</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">x1</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_pow</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">x2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">log2</span><span class="p">(</span><span class="n">add_exp</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="log10"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.log10.html#mindspore.numpy.log10">[docs]</a><span class="k">def</span> <span class="nf">log10</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Base-10 logarithm of `x`.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input tensor.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None``. Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar. This is a scalar if `x` is a scalar.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.array([10, 100, 1000]).astype(&#39;float16&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = np.log10(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 2. 3.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">tensor_10</span> <span class="o">=</span> <span class="n">_make_tensor</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_log10</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="n">F</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">tensor_10</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">_log10</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_cast_type_for_trigonometric</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span> <span class="ow">or</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span> <span class="ow">or</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">:</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">_promote_for_trigonometric</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>


<div class="viewcode-block" id="sin"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.sin.html#mindspore.numpy.sin">[docs]</a><span class="k">def</span> <span class="nf">sin</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Trigonometric sine, element-wise.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input tensor.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None``. Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar. This is a scalar if `x` is a scalar.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.array([-5, -1, 0, 2, 4, 100]).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = np.sin(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0.9589243  -0.84147096  0.   0.9092974  -0.7568025  -0.50636566]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">_cast_type_for_trigonometric</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">sin</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="cos"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.cos.html#mindspore.numpy.cos">[docs]</a><span class="k">def</span> <span class="nf">cos</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Cosine element-wise.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input tensor.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None``. Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar. This is a scalar if `x` is a scalar.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.arange(5).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(np.cos(x))</span>
<span class="sd">        [ 1.          0.5403023  -0.41614684 -0.9899925  -0.6536436 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">_cast_type_for_trigonometric</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">cos</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="tan"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.tan.html#mindspore.numpy.tan">[docs]</a><span class="k">def</span> <span class="nf">tan</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes tangent element-wise.</span>

<span class="sd">    Equivalent to :math:`np.sin(x)/np.cos(x)` element-wise.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input tensor.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None``. Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar. This is a scalar if `x` is a scalar.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input is not a tensor or is :class:`tensor.dtype` is :class:`mindspore.float64`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.array([-5, -1, 0, 2, 4, 100]).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(np.tan(x))</span>
<span class="sd">        [ 3.380515   -1.5574077   0.         -2.1850398   1.1578213  -0.58721393]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">_cast_type_for_trigonometric</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">tan</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="arcsin"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.arcsin.html#mindspore.numpy.arcsin">[docs]</a><span class="k">def</span> <span class="nf">arcsin</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Inverse sine, element-wise.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input tensor. y-coordinate on the unit circle.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None``. Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Output Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input is not a tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.asarray([1, -1], np.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = np.arcsin(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1.5707964 -1.5707964]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">_cast_type_for_trigonometric</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">asin</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="arccos"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.arccos.html#mindspore.numpy.arccos">[docs]</a><span class="k">def</span> <span class="nf">arccos</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Trigonometric inverse cosine, element-wise.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input tensor. x-coordinate on the unit circle.</span>
<span class="sd">            For real arguments, the domain is :math:`[-1, 1]`.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None``. Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input is not a tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; input = np.asarray([1, -1], np.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = np.arccos(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.        3.1415927]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">_cast_type_for_trigonometric</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">acos</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="arctan"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.arctan.html#mindspore.numpy.arctan">[docs]</a><span class="k">def</span> <span class="nf">arctan</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Trigonometric inverse tangent, element-wise.</span>

<span class="sd">    The inverse of tan, so that if :math:`y = tan(x)` then :math:`x = arctan(y)`.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input tensor.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None``. Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar. This is a scalar if `x` is a scalar.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.arange(5).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(np.arctan(x))</span>
<span class="sd">        [0.        0.7853982 1.1071488 1.2490457 1.3258177]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">_cast_type_for_trigonometric</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">atan</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="sinh"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.sinh.html#mindspore.numpy.sinh">[docs]</a><span class="k">def</span> <span class="nf">sinh</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hyperbolic sine, element-wise.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input tensor.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None``. Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar. This is a scalar if `x` is a scalar.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.arange(5).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(np.sinh(x))</span>
<span class="sd">        [ 0.         1.1752012  3.6268604 10.017875  27.289917 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">_cast_type_for_trigonometric</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">sinh</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="cosh"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.cosh.html#mindspore.numpy.cosh">[docs]</a><span class="k">def</span> <span class="nf">cosh</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hyperbolic cosine, element-wise.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input tensor.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None``. Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar. This is a scalar if `x` is a scalar.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.arange(5).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(np.cosh(x))</span>
<span class="sd">        [ 1.         1.5430807  3.7621956 10.067662  27.308233 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">_cast_type_for_trigonometric</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">cosh</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="tanh"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.tanh.html#mindspore.numpy.tanh">[docs]</a><span class="k">def</span> <span class="nf">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes hyperbolic tangent element-wise.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input tensor.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None``. Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar. This is a scalar if `x` is a scalar.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.arange(5).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(np.tanh(x))</span>
<span class="sd">        [0.        0.7615942 0.9640276 0.9950548 0.9993293]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">_cast_type_for_trigonometric</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="arcsinh"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.arcsinh.html#mindspore.numpy.arcsinh">[docs]</a><span class="k">def</span> <span class="nf">arcsinh</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Inverse hyperbolic sine element-wise.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input tensor.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None``. Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar. This is a scalar if `x` is a scalar.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.array([1., 2., 3., 4.], dtype=np.float32)</span>
<span class="sd">        &gt;&gt;&gt; print(np.arcsinh(x))</span>
<span class="sd">        [0.8813736 1.4436355 1.8184465 2.0947125]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">_cast_type_for_trigonometric</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">asinh</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="arccosh"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.arccosh.html#mindspore.numpy.arccosh">[docs]</a><span class="k">def</span> <span class="nf">arccosh</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Inverse hyperbolic cosine, element-wise.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input tensor.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None``. Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar. This is a scalar if `x` is a scalar.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.arange(1, 5).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(np.arccosh(x))</span>
<span class="sd">        [0.        1.316958  1.7627472 2.063437 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">_cast_type_for_trigonometric</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">acosh</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="arctanh"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.arctanh.html#mindspore.numpy.arctanh">[docs]</a><span class="k">def</span> <span class="nf">arctanh</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Inverse hyperbolic tangent element-wise.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input tensor.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None``. Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar. This is a scalar if `x` is a scalar.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.array([-0.99, -0.75, -0.5, 0, 0.5]).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(np.arctanh(x))</span>
<span class="sd">        [-2.646653   -0.97295505 -0.54930615  0.          0.54930615]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">_cast_type_for_trigonometric</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">atanh</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="arctan2"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.arctan2.html#mindspore.numpy.arctan2">[docs]</a><span class="k">def</span> <span class="nf">arctan2</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise arc tangent of :math:`x1/x2` choosing the quadrant correctly.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Tensor): input tensor.</span>
<span class="sd">        x2 (Tensor): input tensor.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, the sum of `x1` and `x2`, element-wise. This is a scalar</span>
<span class="sd">        if both `x1` and `x2` are scalars.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x1 = np.array([-1, +1, +1, -1])</span>
<span class="sd">        &gt;&gt;&gt; x2 = np.array([-1, -1, +1, +1])</span>
<span class="sd">        &gt;&gt;&gt; output = np.arctan2(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-2.3561945   2.3561945   0.78539819 -0.78539819]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">_cast_type_for_trigonometric</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="n">_cast_type_for_trigonometric</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">atan2</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="promote_types"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.promote_types.html#mindspore.numpy.promote_types">[docs]</a><span class="k">def</span> <span class="nf">promote_types</span><span class="p">(</span><span class="n">type1</span><span class="p">,</span> <span class="n">type2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the data type with the smallest size and smallest scalar kind.</span>

<span class="sd">    Note:</span>
<span class="sd">        The promotion rule is slightly different from original Numpy, but more like</span>
<span class="sd">        jax, due to the preference on ``32-bit`` over ``64-bit`` data types.</span>

<span class="sd">    Args:</span>
<span class="sd">        type1 (Union[:class:`mindspore.dtype`, str]): First data type.</span>
<span class="sd">        type2 (Union[:class:`mindspore.dtype`, str]): Second data type.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The promoted data type.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input are not valid :class:`mindspore.dtype` input.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; output = np.promote_types(np.float32, np.float64)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        Float64</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">type1</span> <span class="o">=</span> <span class="n">_check_dtype</span><span class="p">(</span><span class="n">type1</span><span class="p">)</span>
    <span class="n">type2</span> <span class="o">=</span> <span class="n">_check_dtype</span><span class="p">(</span><span class="n">type2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_promote</span><span class="p">(</span><span class="n">type1</span><span class="p">,</span> <span class="n">type2</span><span class="p">)</span></div>


<div class="viewcode-block" id="corrcoef"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.corrcoef.html#mindspore.numpy.corrcoef">[docs]</a><span class="k">def</span> <span class="nf">corrcoef</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">rowvar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns Pearson product-moment correlation coefficients.</span>

<span class="sd">    Please refer to the documentation for cov for more detail. The relationship</span>
<span class="sd">    between the correlation coefficient matrix, R, and the covariance matrix, C, is</span>
<span class="sd">    :math:`R_{ij} = \frac{ C_{ij} } { \sqrt{ C_{ii} * C_{jj} } }`</span>
<span class="sd">    The values of R are between -1 and 1, inclusive.</span>

<span class="sd">    Note:</span>
<span class="sd">        Currently, complex numbers are not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[int, float, bool, tuple, list, Tensor]): A 1-D or 2-D array containing</span>
<span class="sd">            multiple variables and observations. Each row of `x` represents a variable,</span>
<span class="sd">            and each column a single observation of all those variables. Also see rowvar below.</span>
<span class="sd">        y (Union[int, float, bool, tuple, list, Tensor], optional): An additional set</span>
<span class="sd">            of variables and observations. Default: ``None`` .</span>
<span class="sd">        rowvar (bool, optional): If rowvar is ``True`` (default), then each row represents</span>
<span class="sd">            a variable, with observations in the columns. Otherwise, the relationship</span>
<span class="sd">            is transposed: each column represents a variable, while the rows contain observations.</span>
<span class="sd">            Default: ``True`` .</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Data-type of the result. By default,</span>
<span class="sd">            the return data-type will have at least float32 precision. Default: ``None`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, The correlation coefficient matrix of the variables.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the inputs have types not specified above.</span>
<span class="sd">        ValueError: If `x` and `y` have wrong dimensions.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; output = np.corrcoef([[2., 3., 4., 5.], [0., 2., 3., 4.], [7., 8., 9., 10.]])</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1.         0.9827076  1.        ]</span>
<span class="sd">        [0.9827077  0.99999994 0.9827077 ]</span>
<span class="sd">        [1.         0.9827076  1.        ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># This implementation was adapted from original Numpy.</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">cov</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">rowvar</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">c</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_div</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">diag</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
    <span class="n">stddev</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">/=</span> <span class="n">F</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">stddev</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">/=</span> <span class="n">F</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">stddev</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">clip</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">c</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">c</span></div>


<span class="k">def</span> <span class="nf">_slice_along_axis</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">slice_start</span><span class="p">,</span> <span class="n">slice_end</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Slice a tensor along a given axis, a helper function for gradient</span>

<span class="sd">    Args:</span>
<span class="sd">        f (Tensor): Input Tensor.</span>
<span class="sd">        axis (int): Specified axis.</span>
<span class="sd">        slice_start (int): The start of the slice.</span>
<span class="sd">        slice_end (int): The end of the int.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Sliced tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">slice_size</span> <span class="o">=</span> <span class="n">slice_end</span> <span class="o">-</span> <span class="n">slice_start</span>
    <span class="n">index_start</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="n">f</span><span class="o">.</span><span class="n">ndim</span>
    <span class="n">index_end</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">index_start</span> <span class="o">=</span> <span class="n">_tuple_setitem</span><span class="p">(</span><span class="n">index_start</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">slice_start</span><span class="p">)</span>
    <span class="n">index_end</span> <span class="o">=</span> <span class="n">_tuple_setitem</span><span class="p">(</span><span class="n">index_end</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">slice_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_slice</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">index_start</span><span class="p">,</span> <span class="n">index_end</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_gradient_along_axis</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;compute the gradients of `f` along a given axis, a helper function of gradient.&quot;&quot;&quot;</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span>
    <span class="n">upper_edge</span> <span class="o">=</span> <span class="n">_slice_along_axis</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="n">_slice_along_axis</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">lower_edge</span> <span class="o">=</span> <span class="n">_slice_along_axis</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">end</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">end</span><span class="p">)</span> <span class="o">-</span> <span class="n">_slice_along_axis</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">end</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="n">end</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">end</span> <span class="o">&lt;=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">a_grad</span> <span class="o">=</span> <span class="n">concatenate</span><span class="p">((</span><span class="n">upper_edge</span><span class="p">,</span> <span class="n">lower_edge</span><span class="p">),</span> <span class="n">axis</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">middle</span> <span class="o">=</span> <span class="p">(</span><span class="n">_slice_along_axis</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">end</span><span class="p">)</span> <span class="o">-</span> <span class="n">_slice_along_axis</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">end</span><span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="o">*</span> <span class="mf">0.5</span>
        <span class="n">a_grad</span> <span class="o">=</span> <span class="n">concatenate</span><span class="p">((</span><span class="n">upper_edge</span><span class="p">,</span> <span class="n">middle</span><span class="p">,</span> <span class="n">lower_edge</span><span class="p">),</span> <span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">a_grad</span> <span class="o">/</span> <span class="n">h</span>


<span class="k">def</span> <span class="nf">check_gradient_arguments</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">edge_order</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;check arguments for gradient&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">edge_order</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">_raise_unimplemented_error</span><span class="p">(</span><span class="s2">&quot;edge_order != 1 not implemented&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">f</span> <span class="o">=</span> <span class="n">asarray_const</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">f</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">:</span>
        <span class="n">f</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">make_range</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">_check_axis_type</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="n">_canonicalize_axis</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="p">(</span><span class="n">axis</span><span class="p">,)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="n">axis</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">edge_order</span>


<div class="viewcode-block" id="gradient"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.gradient.html#mindspore.numpy.gradient">[docs]</a><span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="o">*</span><span class="n">varargs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">edge_order</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the gradient of a N-dimensional array.</span>
<span class="sd">    The gradient is computed using second order accurate central differences</span>
<span class="sd">    in the interior points and either first or second order accurate one-sides</span>
<span class="sd">    (forward or backwards) differences at the boundaries.</span>
<span class="sd">    The returned gradient hence has the same shape as the input array.</span>

<span class="sd">    Note:</span>
<span class="sd">        Currently we only support `edge_order` =1 and uniform spacing of `varargs`.</span>

<span class="sd">    Args:</span>
<span class="sd">        f (Union[tuple, list, Tensor]): An N-dimensional array containing samples of</span>
<span class="sd">            a scalar function.</span>
<span class="sd">        varargs (Union[tuple[number], tuple[tensor scalar]], optional)</span>
<span class="sd">            Spacing between f values. Default unitary spacing for all dimensions.</span>
<span class="sd">            Spacing can be specified using:</span>
<span class="sd">            1. single scalar to specify a sample distance for all dimensions.</span>
<span class="sd">            2. N scalars to specify a constant sample distance for each dimension.</span>
<span class="sd">        axis (Union[None, int, tuple(int), list(int)], optional): Gradient is calculated</span>
<span class="sd">            only along the given axis or axes. The default :class:`(axis = None)` is to calculate</span>
<span class="sd">            the gradient for all the axes of the input tensor. `axis` may be negative,</span>
<span class="sd">            in which case it counts from the last to the first `axis`.</span>
<span class="sd">        edge_order (int): Gradient is calculated using N-th order accurate differences</span>
<span class="sd">            at the boundaries. Default: ``1`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        gradient, a list of tensors (or a single tensor if there is only one dimension</span>
<span class="sd">        to be calculated). Each derivative has the same shape as f.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the inputs have types not specified above.</span>
<span class="sd">        ValueError: If `axis` values out of bounds, or shape of `f` has entries &lt; 1.</span>
<span class="sd">        NotImplementedError: If `edge_order` != 1, or `varargs` contains non-scalar entries.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; output = np.gradient([[1, 2, 6], [3, 4, 5]], axis=-1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1.  2.5 4. ]</span>
<span class="sd">        [1.  1.  1. ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># This implementation was adapted from Numpy and jax.numpy</span>
    <span class="n">f</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">edge_order</span> <span class="o">=</span> <span class="n">check_gradient_arguments</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">edge_order</span><span class="p">)</span>

    <span class="n">len_axes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">varargs</span><span class="p">)</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># check varargs and make varags the same length as axis</span>
    <span class="k">if</span> <span class="n">n</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">varargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># no spacing</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="n">len_axes</span>
    <span class="k">elif</span> <span class="n">n</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># single value for all axes</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="n">varargs</span> <span class="o">*</span> <span class="n">len_axes</span>
    <span class="k">elif</span> <span class="n">n</span> <span class="o">==</span> <span class="n">len_axes</span><span class="p">:</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="n">varargs</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;Invalid number of arguments&quot;</span><span class="p">)</span>

    <span class="n">a_grad</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">F</span><span class="o">.</span><span class="n">make_range</span><span class="p">(</span><span class="n">len_axes</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">dx</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">axis</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">f</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">ax</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;Shape of array too small to calculate a numerical gradient, &quot;</span>
                               <span class="s2">&quot;at least 2 elements are required.&quot;</span><span class="p">)</span>
        <span class="c1"># if h is not scalar</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">))</span> <span class="ow">or</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">h</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)):</span>
            <span class="n">_raise_unimplemented_error</span><span class="p">(</span><span class="s2">&quot;Non-constant spacing not implemented&quot;</span><span class="p">)</span>

        <span class="n">a_grad</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">_gradient_along_axis</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">ax</span><span class="p">))</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">a_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">a_grad</span></div>


<span class="k">def</span> <span class="nf">sum_</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">initial</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns sum of array elements over a given axis.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and</span>
<span class="sd">        `extobj` are not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[int, float, bool, list, tuple, Tensor]): Elements to sum.</span>
<span class="sd">        axis (Union[None, int, tuple(int)]): Axis or axes along which a sum is performed. Default: `None`.</span>
<span class="sd">            If `None`, sum all of the elements of the input array.</span>
<span class="sd">            If axis is negative it counts from the last to the first axis.</span>
<span class="sd">            If axis is a tuple of integers, a sum is performed on all of the axes specified in the tuple</span>
<span class="sd">            instead of a single axis or all the axes as before.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Defaults to `None`. Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>
<span class="sd">        keepdims (bool): If this is set to True, the axes which are reduced are left in the result as</span>
<span class="sd">            dimensions with size one. With this option, the result will broadcast correctly against the input array.</span>
<span class="sd">            If the default value is passed, then keepdims will not be passed through to the sum method of</span>
<span class="sd">            sub-classes of ndarray, however any non-default value will be. If the sub-class method does not</span>
<span class="sd">            implement keepdims any exceptions will be raised. Default: `False`.</span>
<span class="sd">        initial (scalar): Starting value for the sum, if `None`, which refers to the first element of the reduction.</span>
<span class="sd">            Default: `None`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor. An array with the same shape as a, with the specified axis removed.</span>
<span class="sd">        If a is a 0-d array, or if axis is `None`, a scalar is returned.</span>
<span class="sd">        If an output array is specified, a reference to out is returned.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If input is not array_like or `axis` is not int or tuple of integers or</span>
<span class="sd">            `keepdims` is not integer or `initial` is not scalar.</span>
<span class="sd">        ValueError: If any axis is out of range or duplicate axes exist.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; print(np.sum([0.5, 1.5]))</span>
<span class="sd">        2.0</span>
<span class="sd">        &gt;&gt;&gt; x = np.arange(10).reshape(2, 5).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(np.sum(x, axis=1))</span>
<span class="sd">        [10. 35.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span> <span class="n">initial</span><span class="p">)</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_min_cost_chain_matmul</span><span class="p">(</span><span class="n">dims</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns indices of splits that has the minimal cost for matmul.</span>
<span class="sd">    s[i, j] holds the index of the split with minimal cost for arrays[i, i + 1, ... j]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dims</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">dims</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dims</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">m</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">n</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span>
    <span class="n">s</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">n</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">pos</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">pos</span><span class="p">):</span>
            <span class="n">j</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="n">pos</span>
            <span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">maxsize</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">):</span>
                <span class="n">cost</span> <span class="o">=</span> <span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">m</span><span class="p">[</span><span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="n">dims</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">dims</span><span class="p">[</span><span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">dims</span><span class="p">[</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">cost</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]:</span>
                    <span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">cost</span>
                    <span class="n">s</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">k</span>
    <span class="k">return</span> <span class="n">s</span>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_get_dims</span><span class="p">(</span><span class="n">shapes</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the chain of the dimensions in arrays.</span>
<span class="sd">    dims[i] == arrays[i - 1].shape[1] == arrays[i].shape[0]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">shapes</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">shapes</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">shape</span> <span class="ow">in</span> <span class="n">shapes</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Array must be 2 dimensional&#39;</span><span class="p">)</span>
    <span class="n">dims</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">operator</span><span class="o">.</span><span class="n">itemgetter</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">shapes</span><span class="p">))</span>
    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">dim</span> <span class="k">for</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dim</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">shapes</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">:])):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Shapes</span><span class="si">{}</span><span class="s1"> are not aligned&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">shapes</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">dims</span> <span class="o">+</span> <span class="p">(</span><span class="n">shapes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">],)</span>


<span class="k">def</span> <span class="nf">_multi_dot</span><span class="p">(</span><span class="n">arrays</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">order</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Computes multi dot recursively using minimal cost.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">j</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">arrays</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">dot</span><span class="p">(</span><span class="n">_multi_dot</span><span class="p">(</span><span class="n">arrays</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">order</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">],</span> <span class="n">order</span><span class="p">),</span>
               <span class="n">_multi_dot</span><span class="p">(</span><span class="n">arrays</span><span class="p">,</span> <span class="n">order</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">order</span><span class="p">))</span>


<div class="viewcode-block" id="multi_dot"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.multi_dot.html#mindspore.numpy.multi_dot">[docs]</a><span class="k">def</span> <span class="nf">multi_dot</span><span class="p">(</span><span class="n">arrays</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the dot product of two or more arrays in a single function call, while automatically</span>
<span class="sd">    selecting the fastest evaluation order.</span>
<span class="sd">    multi_dot chains numpy.dot and uses optimal parenthesization of the matrices. For more</span>
<span class="sd">    information, refer to the `wiki page &lt;https://en.wikipedia.org/wiki/Matrix_chain_multiplication&gt;`_.</span>
<span class="sd">    Depending on the shapes of the matrices, this can speed up the multiplication a lot.</span>
<span class="sd">    If the first argument is 1-D, it is treated as a row vector. If the last argument is 1-D, it</span>
<span class="sd">    is treated as a column vector. The other arguments must be 2-D.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy argument `out` is not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        arrays (sequence of array_like): If the first argument is 1-D, it is treated as row</span>
<span class="sd">            vector. If the last argument is 1-D, it is treated as column vector. The other</span>
<span class="sd">            arguments must be 2-D.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the dot product of the supplied arrays.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: arrays are not 2-D.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; A = np.ones((10000, 100))</span>
<span class="sd">        &gt;&gt;&gt; B = np.ones((100, 1000))</span>
<span class="sd">        &gt;&gt;&gt; C = np.ones((1000, 5))</span>
<span class="sd">        &gt;&gt;&gt; D = np.ones((5, 333))</span>
<span class="sd">        &gt;&gt;&gt; output = np.multi_dot([A, B, C, D])</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[500000. 500000. 500000. ... 500000. 500000. 500000.]</span>
<span class="sd">        [500000. 500000. 500000. ... 500000. 500000. 500000.]</span>
<span class="sd">        [500000. 500000. 500000. ... 500000. 500000. 500000.]</span>
<span class="sd">        ...</span>
<span class="sd">        [500000. 500000. 500000. ... 500000. 500000. 500000.]</span>
<span class="sd">        [500000. 500000. 500000. ... 500000. 500000. 500000.]</span>
<span class="sd">        [500000. 500000. 500000. ... 500000. 500000. 500000.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">arrays</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;Expecting at least 2 arrays&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arrays</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
        <span class="n">arrays</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="o">*</span><span class="n">arrays</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">arrays</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">arrays</span><span class="p">)</span>
        <span class="n">num</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">arrays</span><span class="p">)</span>
        <span class="n">arrays</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">arrays</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="n">_tuple_slice</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">arrays</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
        <span class="n">arrays</span> <span class="o">=</span> <span class="n">split</span><span class="p">(</span><span class="n">arrays</span><span class="p">,</span> <span class="n">num</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">arrays</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">dot</span><span class="p">(</span><span class="o">*</span><span class="n">arrays</span><span class="p">)</span>

    <span class="n">shape_out</span> <span class="o">=</span> <span class="p">()</span>
    <span class="n">arrs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">arr</span> <span class="ow">in</span> <span class="n">arrays</span><span class="p">:</span>
        <span class="n">arrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">arrs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">arrs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">arrs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">arrs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">shape_out</span> <span class="o">+=</span> <span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">arrs</span><span class="p">[</span><span class="mi">0</span><span class="p">])[</span><span class="mi">0</span><span class="p">],)</span>
    <span class="k">if</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">arrs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">arrs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">arrs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span><span class="n">arrs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">shape_out</span> <span class="o">+=</span> <span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">arrs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])[</span><span class="mi">1</span><span class="p">],)</span>

    <span class="n">shapes</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">arr</span> <span class="ow">in</span> <span class="n">arrs</span><span class="p">:</span>
        <span class="n">shapes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">arr</span><span class="p">))</span>
    <span class="n">dims</span> <span class="o">=</span> <span class="n">_get_dims</span><span class="p">(</span><span class="n">shapes</span><span class="p">)</span>
    <span class="n">order</span> <span class="o">=</span> <span class="n">_min_cost_chain_matmul</span><span class="p">(</span><span class="n">dims</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">_multi_dot</span><span class="p">(</span><span class="n">arrs</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">arrs</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">order</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">shape_out</span><span class="p">)</span></div>


<div class="viewcode-block" id="argmax"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.argmax.html#mindspore.numpy.argmax">[docs]</a><span class="k">def</span> <span class="nf">argmax</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the indices of the maximum values along an axis.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy argument `out` is not supported.</span>
<span class="sd">        On Ascend, in case of multiple occurrences of the maximum values, the return</span>
<span class="sd">        indices may not necessarily correspond to the first occurrence.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Union[int, float, bool, list, tuple, Tensor]): Input array.</span>
<span class="sd">        axis (int, optional): By default, the index is into</span>
<span class="sd">            the flattened array, otherwise along the specified axis.</span>
<span class="sd">            Default: ``None`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, array of indices into the array. It has the same</span>
<span class="sd">        shape as a.shape with the dimension along axis removed.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If axis is out of range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.arange(10, 16).reshape(2, 3)</span>
<span class="sd">        &gt;&gt;&gt; print(np.argmax(a))</span>
<span class="sd">        5</span>
<span class="sd">        &gt;&gt;&gt; print(np.argmax(a, axis=0))</span>
<span class="sd">        [1 1 1]</span>
<span class="sd">        &gt;&gt;&gt; print(np.argmax(a, axis=1))</span>
<span class="sd">        [2 2]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">:</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="argmin"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.argmin.html#mindspore.numpy.argmin">[docs]</a><span class="k">def</span> <span class="nf">argmin</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the indices of the minimum values along an axis.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy argument `out` is not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Union[int, float, bool, list, tuple, Tensor]): Input array.</span>
<span class="sd">        axis (int, optional): By default, the index is into</span>
<span class="sd">            the flattened array, otherwise along the specified axis.</span>
<span class="sd">            Default: ``None`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, array of indices into the array. It has the same</span>
<span class="sd">        shape as a.shape with the dimension along axis removed.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If axis is out of range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.arange(10, 16).reshape(2, 3)</span>
<span class="sd">        &gt;&gt;&gt; print(np.argmin(a))</span>
<span class="sd">        0</span>
<span class="sd">        &gt;&gt;&gt; print(np.argmin(a, axis=0))</span>
<span class="sd">        [0 0 0]</span>
<span class="sd">        &gt;&gt;&gt; print(np.argmin(a, axis=1))</span>
<span class="sd">        [0 0]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_get_sort_range</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns the range for number of searches (log2(size)) on a sorted array with the given size.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">ceil</span><span class="p">(</span><span class="n">log2</span><span class="p">(</span><span class="n">_to_tensor</span><span class="p">(</span><span class="n">size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)))</span>


<div class="viewcode-block" id="searchsorted"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.searchsorted.html#mindspore.numpy.searchsorted">[docs]</a><span class="k">def</span> <span class="nf">searchsorted</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">side</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="n">sorter</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Finds indices where elements should be inserted to maintain order.</span>
<span class="sd">    Finds the indices into a sorted array `a` such that, if the corresponding elements</span>
<span class="sd">    in `v` were inserted before the indices, the order of `a` would be preserved.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Union[list, tuple, Tensor]): 1-D input array. If `sorter` is</span>
<span class="sd">            None, then it must be sorted in ascending order, otherwise `sorter` must be</span>
<span class="sd">            an array of indices that sort it.</span>
<span class="sd">        v (Union[int, float, bool, list, tuple, Tensor]): Values to insert into `a`.</span>
<span class="sd">        side (&#39;left&#39;, &#39;right&#39;, optional): If ``&#39;left&#39;`` , the index of the first suitable</span>
<span class="sd">            location found is given. If ``&#39;right&#39;`` , return the last such index. If there is</span>
<span class="sd">            no suitable index, return either 0 or N (where N is the length of `a`).</span>
<span class="sd">        sorter (Union[int, float, bool, list, tuple, Tensor]): 1-D optional array of</span>
<span class="sd">            integer indices that sort array `a` into ascending order. They are typically</span>
<span class="sd">            the result of argsort.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, array of insertion points with the same shape as `v`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If argument for `side` or `sorter` is invalid.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; print(np.searchsorted([1,2,3,4,5], 3))</span>
<span class="sd">        2</span>
<span class="sd">        &gt;&gt;&gt; print(np.searchsorted([1,2,3,4,5], 3, side=&#39;right&#39;))</span>
<span class="sd">        3</span>
<span class="sd">        &gt;&gt;&gt; print(np.searchsorted([1,2,3,4,5], [-10, 10, 2, 3]))</span>
<span class="sd">        [0 5 1 2]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">side</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="s1">&#39;right&#39;</span><span class="p">):</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;invalid value for keyword &quot;side&quot;&#39;</span><span class="p">)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;`a` should be 1-D array&#39;</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">sorter</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">sorter</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">sorter</span><span class="o">.</span><span class="n">size</span> <span class="o">!=</span> <span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
            <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;sorter must be 1-D array with the same size as `a`&#39;</span><span class="p">)</span>
        <span class="n">sorter</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">sorter</span><span class="p">)</span>
        <span class="n">sorter</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">sorter</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">gather_nd</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">sorter</span><span class="p">)</span>
    <span class="n">less_op</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_le</span> <span class="k">if</span> <span class="n">side</span> <span class="o">==</span> <span class="s1">&#39;left&#39;</span> <span class="k">else</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_lt</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">j</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
    <span class="n">two</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">_get_sort_range</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">):</span>
        <span class="n">mid</span> <span class="o">=</span> <span class="n">floor_divide</span><span class="p">(</span><span class="n">add</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">),</span> <span class="n">two</span><span class="p">)</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">less_op</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">gather_nd</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">mid</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">mid</span><span class="p">)</span>
        <span class="n">j</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">mid</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">j</span></div>


<div class="viewcode-block" id="interp"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.interp.html#mindspore.numpy.interp">[docs]</a><span class="k">def</span> <span class="nf">interp</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">xp</span><span class="p">,</span> <span class="n">fp</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    One-dimensional linear interpolation for monotonically increasing sample points.</span>
<span class="sd">    Returns the one-dimensional piecewise linear interpolant to a function with given</span>
<span class="sd">    discrete data points `(xp, fp)`, evaluated at `x`.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy argument `period` is not supported.</span>
<span class="sd">        Complex values are not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[int, float, bool, list, tuple, Tensor]): The x-coordinates at which</span>
<span class="sd">            to evaluate the interpolated values.</span>
<span class="sd">        xp (Union[int, float, bool, list, tuple, Tensor]): 1-D sequence of floats, the</span>
<span class="sd">            x-coordinates of the data points, must be increasing.</span>
<span class="sd">        fp (Union[int, float, bool, list, tuple, Tensor]): 1-D sequence of floats, the</span>
<span class="sd">            y-coordinates of the data points, same length as `xp`.</span>
<span class="sd">        left (float, optional): Value to return for ``x &lt; xp[0]``, default is ``fp[0]``</span>
<span class="sd">            once obtained.</span>
<span class="sd">        right (float, optional): Value to return for ``x &gt; xp[-1]``, default is ``fp[-1]``</span>
<span class="sd">            once obtained.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the interpolated values, same shape as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If `xp` or `fp` is not one-dimensional, or if `xp` and `fp` do not have</span>
<span class="sd">            the same length.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; xp = [1, 2, 3]</span>
<span class="sd">        &gt;&gt;&gt; fp = [3, 2, 0]</span>
<span class="sd">        &gt;&gt;&gt; print(np.interp([0, 1, 1.5, 2.72, 3.14], xp, fp))</span>
<span class="sd">        [3.         3.         2.5        0.55999994 0.        ]</span>
<span class="sd">        &gt;&gt;&gt; UNDEF = -99.0</span>
<span class="sd">        &gt;&gt;&gt; print(np.interp(3.14, xp, fp, right=UNDEF))</span>
<span class="sd">        -99.0</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># implement period once sort is supported</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">xp</span><span class="p">,</span> <span class="n">fp</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">xp</span><span class="p">,</span> <span class="n">fp</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">xp</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">fp</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;xp and fp must be 1-d sequences&#39;</span><span class="p">)</span>
    <span class="n">size</span> <span class="o">=</span> <span class="n">xp</span><span class="o">.</span><span class="n">size</span>
    <span class="k">if</span> <span class="n">fp</span><span class="o">.</span><span class="n">size</span> <span class="o">!=</span> <span class="n">size</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;the y-coordinates must have the same length as `xp`&#39;</span><span class="p">)</span>

    <span class="n">xp</span> <span class="o">=</span> <span class="n">xp</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">fp</span> <span class="o">=</span> <span class="n">fp</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="n">indices_1</span> <span class="o">=</span> <span class="n">clip</span><span class="p">(</span><span class="n">searchsorted</span><span class="p">(</span><span class="n">xp</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">indices_0</span> <span class="o">=</span> <span class="n">clip</span><span class="p">(</span><span class="n">indices_1</span> <span class="o">-</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">indices_0</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">indices_0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">indices_1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">indices_1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x_0</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">gather_nd</span><span class="p">(</span><span class="n">xp</span><span class="p">,</span> <span class="n">indices_0</span><span class="p">)</span>
    <span class="n">x_1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">gather_nd</span><span class="p">(</span><span class="n">xp</span><span class="p">,</span> <span class="n">indices_1</span><span class="p">)</span>
    <span class="n">y_0</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">gather_nd</span><span class="p">(</span><span class="n">fp</span><span class="p">,</span> <span class="n">indices_0</span><span class="p">)</span>
    <span class="n">y_1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">gather_nd</span><span class="p">(</span><span class="n">fp</span><span class="p">,</span> <span class="n">indices_1</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_0</span><span class="o">*</span><span class="p">(</span><span class="n">x_1</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">y_1</span><span class="o">*</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x_0</span><span class="p">))</span><span class="o">/</span><span class="p">(</span><span class="n">x_1</span> <span class="o">-</span> <span class="n">x_0</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">x_0</span><span class="p">,</span> <span class="n">x_1</span><span class="p">),</span> <span class="n">y_0</span><span class="p">,</span> <span class="n">res</span><span class="p">)</span>

    <span class="n">idx_0</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">idx_last</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">([</span><span class="n">size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">left</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">left</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">gather_nd</span><span class="p">(</span><span class="n">fp</span><span class="p">,</span> <span class="n">idx_0</span><span class="p">)</span>
    <span class="n">left</span> <span class="o">=</span> <span class="n">full</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">left</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">right</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">right</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">gather_nd</span><span class="p">(</span><span class="n">fp</span><span class="p">,</span> <span class="n">idx_last</span><span class="p">)</span>
    <span class="n">right</span> <span class="o">=</span> <span class="n">full</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">right</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">tensor_lt</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">gather_nd</span><span class="p">(</span><span class="n">xp</span><span class="p">,</span> <span class="n">idx_0</span><span class="p">)),</span> <span class="n">left</span><span class="p">,</span> <span class="n">res</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">tensor_gt</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">gather_nd</span><span class="p">(</span><span class="n">xp</span><span class="p">,</span> <span class="n">idx_last</span><span class="p">)),</span> <span class="n">right</span><span class="p">,</span> <span class="n">res</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span></div>


<span class="k">def</span> <span class="nf">_apply_tensor_op</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Applies tensor operations based on fn&quot;&quot;&quot;</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">res</span><span class="p">),</span> <span class="n">dtype</span><span class="p">):</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span>


<div class="viewcode-block" id="sign"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.sign.html#mindspore.numpy.sign">[docs]</a><span class="k">def</span> <span class="nf">sign</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns an element-wise indication of the sign of a number.</span>

<span class="sd">    The sign function returns `-1 if x &lt; 0, 0 if x == 0, 1 if x &gt; 0`. nan is returned for nan inputs.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>
<span class="sd">        Complex inputs are not supported now.</span>
<span class="sd">        On Ascend, integer inputs are not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[int, float, list, tuple, Tensor]): Input values.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The sign of x. This is a tensor or a scalar when x is a scalar.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of the input is not in the given types or</span>
<span class="sd">            the input can not be converted to tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; output = np.sign(np.array([-1., 0., 1., 1.2]))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-1.  0.  1.  1.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s1">&#39;integer, float, list, tuple or Tensor are expected, but got&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;sign does not accept dtype bool.&quot;</span><span class="p">)</span>

    <span class="n">_non_zero_sign</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="n">absolute</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">_zero</span> <span class="o">=</span> <span class="n">_broadcast_to_shape</span><span class="p">(</span><span class="n">_make_tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">is_zero</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">is_zero</span><span class="p">,</span> <span class="n">_zero</span><span class="p">,</span> <span class="n">_non_zero_sign</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">res</span><span class="p">),</span> <span class="n">dtype</span><span class="p">):</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span></div>


<div class="viewcode-block" id="copysign"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.copysign.html#mindspore.numpy.copysign">[docs]</a><span class="k">def</span> <span class="nf">copysign</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Changes the sign of `x1` to that of `x2`, element-wise.</span>

<span class="sd">    If `x2` is a scalar, its sign will be copied to all elements of `x1`.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>
<span class="sd">        Complex inputs are not supported now.</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Union[int, float, list, tuple, Tensor]): Values to change the sign of.</span>
<span class="sd">        x2 (Union[int, float, list, tuple, Tensor]): The sign of x2 is copied to x1. If `x1.shape != x2.shape`,</span>
<span class="sd">            they must be broadcastable to a common shape (which becomes the shape of the output).</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar. The values of `x1` with the sign of `x2`. This is a scalar if both `x1` and `x2` are scalars.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of the input is not in the given types or</span>
<span class="sd">            the input can not be converted to tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; output = np.copysign(np.array([1, -1, -1]), np.array([-1, 1, -1]))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-1  1 -1]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s1">&#39;integer, float, list, tuple or Tensor are expected, but got&#39;</span><span class="p">,</span> <span class="n">x1</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s1">&#39;integer, float, list, tuple or Tensor are expected, but got&#39;</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
    <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
    <span class="n">shape_out</span> <span class="o">=</span> <span class="n">_infer_out_shape</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x1</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x2</span><span class="p">))</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">_broadcast_to_shape</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">shape_out</span><span class="p">)</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="n">_broadcast_to_shape</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">shape_out</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">x1</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span> <span class="ow">or</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">x2</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;sign does not accept dtype bool.&quot;</span><span class="p">)</span>

    <span class="n">original_dtype</span> <span class="o">=</span> <span class="n">x1</span><span class="o">.</span><span class="n">dtype</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_is_float</span><span class="p">(</span><span class="n">original_dtype</span><span class="p">):</span>
        <span class="n">pos_tensor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">absolute</span><span class="p">(</span><span class="n">x1</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">original_dtype</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">pos_tensor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">absolute</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>

    <span class="n">neg_tensor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">neg_tensor</span><span class="p">(</span><span class="n">pos_tensor</span><span class="p">)</span>
    <span class="n">less_zero</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">less</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">less_zero</span><span class="p">,</span> <span class="n">neg_tensor</span><span class="p">,</span> <span class="n">pos_tensor</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">res</span><span class="p">),</span> <span class="n">dtype</span><span class="p">):</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span></div>


<div class="viewcode-block" id="digitize"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.digitize.html#mindspore.numpy.digitize">[docs]</a><span class="k">def</span> <span class="nf">digitize</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">bins</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the indices of the bins to which each value in input array belongs.</span>
<span class="sd">    If values in `x` are beyond the bounds of `bins`, 0 or ``len(bins)`` is returned</span>
<span class="sd">    as appropriate.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[int, float, bool, list, tuple, Tensor]): Input array to be binned.</span>
<span class="sd">        bins (Union[list, tuple, Tensor]): Array of bins. It has to</span>
<span class="sd">            be 1-dimensional and monotonic.</span>
<span class="sd">        right (boolean, optional): Indicating whether the intervals include the right</span>
<span class="sd">            or the left bin edge. Default behavior is ``(right==False)`` indicating</span>
<span class="sd">            that the interval does not include the right edge. The left bin end is</span>
<span class="sd">            open in this case, i.e., ``bins[i-1] &lt;= x &lt; bins[i]`` is the default</span>
<span class="sd">            behavior for monotonically increasing bins.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor of ints, output array of indices, of same shape as `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.array([1.2, 10.0, 12.4, 15.5, 20.])</span>
<span class="sd">        &gt;&gt;&gt; bins = np.array([0, 5, 10, 15, 20])</span>
<span class="sd">        &gt;&gt;&gt; inds = np.digitize(x, bins)</span>
<span class="sd">        &gt;&gt;&gt; print(inds)</span>
<span class="sd">        [1 3 3 4 5]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">bins</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">bins</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">bins</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;bins should be 1-dimensional&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span>
    <span class="k">if</span> <span class="n">bins</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">zeros</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">side</span> <span class="o">=</span> <span class="s1">&#39;left&#39;</span> <span class="k">if</span> <span class="n">right</span> <span class="k">else</span> <span class="s1">&#39;right&#39;</span>
    <span class="n">first_bin</span> <span class="o">=</span> <span class="n">bins</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">last_bin</span> <span class="o">=</span> <span class="n">bins</span><span class="p">[</span><span class="n">_type_convert</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">bins</span><span class="o">.</span><span class="n">size</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">cond</span> <span class="o">=</span> <span class="n">first_bin</span> <span class="o">&lt;=</span> <span class="n">last_bin</span>
    <span class="n">incr</span> <span class="o">=</span> <span class="n">searchsorted</span><span class="p">(</span><span class="n">bins</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">side</span><span class="p">)</span>
    <span class="n">decr</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">bins</span><span class="o">.</span><span class="n">size</span><span class="p">)</span> <span class="o">-</span> <span class="n">searchsorted</span><span class="p">(</span><span class="n">flip</span><span class="p">(</span><span class="n">bins</span><span class="p">),</span> <span class="n">x</span><span class="p">,</span> <span class="n">side</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">where_</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="n">incr</span><span class="p">,</span> <span class="n">decr</span><span class="p">)</span></div>


<div class="viewcode-block" id="bincount"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.bincount.html#mindspore.numpy.bincount">[docs]</a><span class="k">def</span> <span class="nf">bincount</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">minlength</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Count number of occurrences of each value in array of non-negative ints.</span>
<span class="sd">    The number of bins (of size 1) is one larger than the largest value in `x`.</span>
<span class="sd">    If `minlength` is specified, there will be at least this number of bins in the</span>
<span class="sd">    output array (though it will be longer if necessary, depending on the contents</span>
<span class="sd">    of `x`). Each bin gives the number of occurrences of its index value in `x`. If</span>
<span class="sd">    `weights` is specified the input array is weighted by it, i.e. if a value `n`</span>
<span class="sd">    is found at position `i`, ``out[n] += weight[i]`` instead of ``out[n] += 1``.</span>

<span class="sd">    Note:</span>
<span class="sd">        The additional argument `length` specifies the number of bins (overriding</span>
<span class="sd">        ``x.max() + 1``), which must be provided in graph mode.</span>
<span class="sd">        If `x` contains negative values, no error will be raised, and negative values</span>
<span class="sd">        are treated as zeros instead.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[list, tuple, Tensor]): 1-d input array.</span>
<span class="sd">        weights (Union[int, float, bool, list, tuple, Tensor], optional): Weights,</span>
<span class="sd">            array of the same shape as `x`. Default: ``None`` .</span>
<span class="sd">        minlength (int, optional): A minimum number of bins for the output array.</span>
<span class="sd">            Default: ``0`` .</span>
<span class="sd">        length (int, optional): Number of bins. Default: ``None`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the result of binning the input array. The length of out is equal to</span>
<span class="sd">        ``np.amax(x)+1``.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If `x` is not one-dimensional, or if `x` and `weights` do not have</span>
<span class="sd">            the same shape.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; print(np.bincount(np.arange(5)))</span>
<span class="sd">        [1. 1. 1. 1. 1.]</span>
<span class="sd">        &gt;&gt;&gt; print(np.bincount(np.array([0, 1, 1, 3, 2, 1, 7])))</span>
<span class="sd">        [1. 3. 1. 1. 0. 0. 0. 1.]</span>
<span class="sd">        &gt;&gt;&gt; w = np.array([0.3, 0.5, 0.2, 0.7, 1., -0.6]) # weights</span>
<span class="sd">        &gt;&gt;&gt; x = np.array([0, 1, 1, 2, 2, 2])</span>
<span class="sd">        &gt;&gt;&gt; print(np.bincount(x,  weights=w))</span>
<span class="sd">        [0.3 0.7 1.1]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;`x` should be one-dimensional&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_is_int</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s1">&#39;`x` should be an array of ints&#39;</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">clip</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">length</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">F</span><span class="o">.</span><span class="n">isconstant</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="n">length</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">maximum</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">minlength</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;argument `length` must be provided in graph mode&#39;</span><span class="p">)</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">arange</span><span class="p">(</span><span class="n">length</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">length</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">idx_mapping</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">!=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">weights</span><span class="p">):</span>
            <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;`x` and `weights` must have the same length&#39;</span><span class="p">)</span>
        <span class="n">idx_mapping</span> <span class="o">*=</span> <span class="n">weights</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">idx_mapping</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span></div>


<div class="viewcode-block" id="histogram"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.histogram.html#mindspore.numpy.histogram">[docs]</a><span class="k">def</span> <span class="nf">histogram</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="nb">range</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span> <span class="c1"># pylint: disable=redefined-builtin</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the histogram of a dataset.</span>

<span class="sd">    Note:</span>
<span class="sd">        String values for `bins` is not supported.</span>
<span class="sd">        Deprecated numpy argument `normed` is not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Union[int, float, bool, list, tuple, Tensor]): Input data. The histogram</span>
<span class="sd">            is computed over the flattened array.</span>
<span class="sd">        bins (Union[int, tuple, list, Tensor], optional): If `bins` is an int, it</span>
<span class="sd">            defines the number of equal-width bins in the given range (10, by</span>
<span class="sd">            default). If `bins` is a sequence, it defines the bin edges, including</span>
<span class="sd">            the rightmost edge, allowing for non-uniform bin widths.</span>
<span class="sd">        range((float, float), optional): The lower and upper range of the bins. If</span>
<span class="sd">            not provided, `range` is simply ``(a.min(), a.max())``. Values outside</span>
<span class="sd">            the range are ignored. The first element of the range must be less than</span>
<span class="sd">            or equal to the second.</span>
<span class="sd">        weights (Union[int, float, bool, list, tuple, Tensor], optional): An array</span>
<span class="sd">            of weights, of the same shape as `a`. If density is True, the weights</span>
<span class="sd">            are normalized, so that the integral of the density over the range</span>
<span class="sd">            remains 1.</span>
<span class="sd">        density (boolean, optional): If False, the result will contain the number of</span>
<span class="sd">            samples in each bin. If True, the result is the value of the probability</span>
<span class="sd">            density function at the bin, normalized such that the integral over the</span>
<span class="sd">            range is 1. Note that the sum of the histogram values will not be equal</span>
<span class="sd">            to 1 unless bins of unity width are chosen; it is not a probability mass</span>
<span class="sd">            function.</span>

<span class="sd">    Returns:</span>
<span class="sd">        (Tensor, Tensor), the values of the histogram and the bin edges.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If `x` and `weights` do not have the same size.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; print(np.histogram([1, 2, 1], bins=[0, 1, 2, 3]))</span>
<span class="sd">        (Tensor(shape=[3], dtype=Float32, value= [ 0.00000000e+00,  2.00000000e+00,  1.00000000e+00]),</span>
<span class="sd">        Tensor(shape=[4], dtype=Int32, value= [0, 1, 2, 3]))</span>
<span class="sd">        &gt;&gt;&gt; print(np.histogram(np.arange(4), bins=np.arange(5), density=True))</span>
<span class="sd">        (Tensor(shape=[4], dtype=Float32, value=</span>
<span class="sd">        [ 2.50000000e-01,  2.50000000e-01,  2.50000000e-01,  2.50000000e-01]),</span>
<span class="sd">        Tensor(shape=[5], dtype=Int32, value= [0, 1, 2, 3, 4]))</span>
<span class="sd">        &gt;&gt;&gt; print(np.histogram([[1, 2, 1], [1, 0, 1]], bins=[0,1,2,3]))</span>
<span class="sd">        (Tensor(shape=[3], dtype=Float32, value= [ 1.00000000e+00,  4.00000000e+00,  1.00000000e+00]),</span>
<span class="sd">        Tensor(shape=[4], dtype=Int32, value= [0, 1, 2, 3]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">!=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">weights</span><span class="p">):</span>
            <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;weights should have the same shape as a&#39;</span><span class="p">)</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="n">bin_edges</span> <span class="o">=</span> <span class="n">histogram_bin_edges</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">bins</span><span class="p">,</span> <span class="nb">range</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
    <span class="n">data_to_bins</span> <span class="o">=</span> <span class="n">searchsorted</span><span class="p">(</span><span class="n">bin_edges</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="s1">&#39;right&#39;</span><span class="p">)</span>
    <span class="n">bin_size</span> <span class="o">=</span> <span class="n">_type_convert</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">bin_edges</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
    <span class="n">data_to_bins</span> <span class="o">=</span> <span class="n">where_</span><span class="p">(</span><span class="n">a</span> <span class="o">==</span> <span class="n">bin_edges</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">bin_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="n">data_to_bins</span><span class="p">)</span>
    <span class="n">count</span> <span class="o">=</span> <span class="n">bincount</span><span class="p">(</span><span class="n">data_to_bins</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="n">bin_size</span><span class="p">)[</span><span class="mi">1</span><span class="p">:]</span>
    <span class="k">if</span> <span class="n">count</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">count</span><span class="p">,</span> <span class="n">bin_edges</span>
    <span class="k">if</span> <span class="n">density</span><span class="p">:</span>
        <span class="n">count</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">count</span> <span class="o">=</span> <span class="n">count</span><span class="o">/</span><span class="n">diff</span><span class="p">(</span><span class="n">bin_edges</span><span class="p">)</span><span class="o">/</span><span class="n">F</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">count</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">count</span><span class="p">,</span> <span class="n">bin_edges</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_factor_flattened_hist</span><span class="p">(</span><span class="n">nbin</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns the factor that will be applied to the histogram to be flattened.&quot;&quot;&quot;</span>
    <span class="n">factor</span> <span class="o">=</span> <span class="nb">list</span><span class="p">((</span><span class="n">itertools</span><span class="o">.</span><span class="n">accumulate</span><span class="p">(</span><span class="n">nbin</span><span class="p">[</span><span class="mi">1</span><span class="p">:][::</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">operator</span><span class="o">.</span><span class="n">mul</span><span class="p">)))[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">factor</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">factor</span>


<span class="k">def</span> <span class="nf">_get_histogramdd_count</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">bin_edges</span><span class="p">,</span> <span class="n">sample</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns count for histogramdd.&quot;&quot;&quot;</span>
    <span class="n">data_indices</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">nbin</span> <span class="o">=</span> <span class="p">()</span>
    <span class="n">flattened_bin_size</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">F</span><span class="o">.</span><span class="n">make_range</span><span class="p">(</span><span class="n">ndim</span><span class="p">):</span>
        <span class="n">data_to_bins</span> <span class="o">=</span> <span class="n">searchsorted</span><span class="p">(</span><span class="n">bin_edges</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">sample</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="s1">&#39;right&#39;</span><span class="p">)</span>
        <span class="n">bin_size</span> <span class="o">=</span> <span class="n">_type_convert</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">bin_edges</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
        <span class="n">data_to_bins</span> <span class="o">=</span> <span class="n">where_</span><span class="p">(</span><span class="n">sample</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">bin_edges</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">bin_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="n">data_to_bins</span><span class="p">)</span>
        <span class="n">data_indices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data_to_bins</span><span class="p">)</span>
        <span class="n">nbin</span> <span class="o">+=</span> <span class="p">(</span><span class="n">bin_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,)</span>
        <span class="n">flattened_bin_size</span> <span class="o">*=</span> <span class="p">(</span><span class="n">bin_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">factor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">_to_tensor</span><span class="p">(</span><span class="n">_factor_flattened_hist</span><span class="p">(</span><span class="n">nbin</span><span class="p">)),</span> <span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">stacked_indices</span> <span class="o">=</span> <span class="n">stack</span><span class="p">(</span><span class="n">data_indices</span><span class="p">)</span> <span class="o">*</span> <span class="n">factor</span>
    <span class="k">if</span> <span class="n">_get_device</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;Ascend&#39;</span><span class="p">:</span>
        <span class="n">stacked_indices</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">stacked_indices</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">flattened_hist</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">stacked_indices</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">count</span> <span class="o">=</span> <span class="n">bincount</span><span class="p">(</span><span class="n">flattened_hist</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="n">weights</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="n">flattened_bin_size</span><span class="p">)</span>
    <span class="n">count</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">nbin</span><span class="p">)</span>
    <span class="n">slices</span> <span class="o">=</span> <span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">make_slice</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">count</span> <span class="o">=</span> <span class="n">count</span><span class="p">[</span><span class="n">slices</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">count</span>


<div class="viewcode-block" id="histogramdd"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.histogramdd.html#mindspore.numpy.histogramdd">[docs]</a><span class="k">def</span> <span class="nf">histogramdd</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="nb">range</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span> <span class="c1"># pylint: disable=redefined-builtin</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the multidimensional histogram of some data.</span>

<span class="sd">    Note:</span>
<span class="sd">        Deprecated numpy argument `normed` is not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        sample (Union[list, tuple, Tensor]): The data to be histogrammed, either `(N, D)`</span>
<span class="sd">            array, or `(D, N)` array_like. Note the unusual interpretation of sample</span>
<span class="sd">            when an array_like:</span>

<span class="sd">            When an array, each row is a coordinate in a `D-dimensional` space, such as</span>
<span class="sd">            ``histogramdd(np.array([p1, p2, p3]))``.</span>

<span class="sd">            When an array_like, each element is the list of values for single coordinate,</span>
<span class="sd">            such as ``histogramdd((X, Y, Z))``.</span>

<span class="sd">            The first form should be preferred.</span>
<span class="sd">        bins (Union[int, tuple, list], optional): The bin specification:</span>

<span class="sd">            A sequence of arrays describing the monotonically increasing bin edges along</span>
<span class="sd">            each dimension.</span>

<span class="sd">            The number of bins for each dimension ``(nx, ny, … =bins)``</span>

<span class="sd">            The number of bins for all dimensions ``(nx=ny=…=bins)``.</span>
<span class="sd">        range(Union[list, tuple], optional): A sequence of length `D`, each an optional</span>
<span class="sd">            ``(lower, upper)`` tuple giving the outer bin edges to be used if the edges</span>
<span class="sd">            are not given explicitly in bins. An entry of None in the sequence results in</span>
<span class="sd">            the minimum and maximum values being used for the corresponding dimension.</span>
<span class="sd">            The default, None, is equivalent to passing a tuple of `D` None values.</span>
<span class="sd">        weights (Union[list, tuple, Tensor], optional): An array with shape `(N,)` of values</span>
<span class="sd">            `w_i` weighing each sample ``(x_i, y_i, z_i, …)``.</span>
<span class="sd">        density (boolean, optional): If False, the default, returns the number of samples</span>
<span class="sd">            in each bin. If True, returns the probability density function at the bin,</span>
<span class="sd">            ``bin_count / sample_count / bin_volume``.</span>

<span class="sd">    Returns:</span>
<span class="sd">        (Tensor, list of Tensor), the values of the histogram and the bin edges.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If `range` does not have the same size as the number of samples.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; sample = np.arange(15).reshape(5, 3)</span>
<span class="sd">        &gt;&gt;&gt; print(sample)</span>
<span class="sd">        [[ 0  1  2]</span>
<span class="sd">        [ 3  4  5]</span>
<span class="sd">        [ 6  7  8]</span>
<span class="sd">        [ 9 10 11]</span>
<span class="sd">        [12 13 14]]</span>
<span class="sd">        &gt;&gt;&gt; print(np.histogramdd(sample, bins=(2, 3, 4)))</span>
<span class="sd">        (Tensor(shape=[2, 3, 4], dtype=Float32, value=</span>
<span class="sd">        [[[ 1.00000000e+00,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00],</span>
<span class="sd">        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],</span>
<span class="sd">        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],</span>
<span class="sd">        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],</span>
<span class="sd">        [ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00],</span>
<span class="sd">        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  2.00000000e+00]]]),</span>
<span class="sd">        [Tensor(shape=[3], dtype=Float32, value= [ 0.00000000e+00,  6.00000000e+00,  1.20000000e+01]),</span>
<span class="sd">        Tensor(shape=[4], dtype=Float32, value=</span>
<span class="sd">        [ 1.00000000e+00,  5.00000000e+00,  9.00000000e+00,  1.30000000e+01]),</span>
<span class="sd">        Tensor(shape=[5], dtype=Float32, value=</span>
<span class="sd">        [ 2.00000000e+00,  5.00000000e+00,  8.00000000e+00,  1.10000000e+01,  1.40000000e+01])])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="o">*</span><span class="n">sample</span><span class="p">)</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="n">stack</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s1">&#39;sample should be (N, D) array, or (D, N) array_like&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;when an array, sample should be 2-dimensional&#39;</span><span class="p">)</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">sample</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">bins</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">bins</span> <span class="o">=</span> <span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">bins</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">bins</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">bins</span><span class="p">)</span> <span class="o">!=</span> <span class="n">ndim</span><span class="p">:</span>
            <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;The dimension of bins must be equal to the dimension of the sample&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s1">&#39;bins should be int or sequence&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">range</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">range</span> <span class="o">=</span> <span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">range</span><span class="p">)</span> <span class="o">!=</span> <span class="n">ndim</span><span class="p">:</span>
            <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;range argument must have one entry per dimension&#39;</span><span class="p">)</span>

    <span class="n">bin_edges</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">dedges</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">F</span><span class="o">.</span><span class="n">make_range</span><span class="p">(</span><span class="n">ndim</span><span class="p">):</span>
        <span class="n">edges</span> <span class="o">=</span> <span class="n">histogram_bin_edges</span><span class="p">(</span><span class="n">sample</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">bins</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">range</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">weights</span><span class="p">)</span>
        <span class="n">bin_edges</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">edges</span><span class="p">)</span>
        <span class="n">dedges</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">diff</span><span class="p">(</span><span class="n">edges</span><span class="p">))</span>

    <span class="n">count</span> <span class="o">=</span> <span class="n">_get_histogramdd_count</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">bin_edges</span><span class="p">,</span> <span class="n">sample</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">density</span><span class="p">:</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">count</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">F</span><span class="o">.</span><span class="n">make_range</span><span class="p">(</span><span class="n">ndim</span><span class="p">):</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="n">_expanded_shape</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">dedges</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
            <span class="n">count</span> <span class="o">/=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">dedges</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">count</span> <span class="o">/=</span> <span class="n">s</span>
    <span class="k">return</span> <span class="n">count</span><span class="p">,</span> <span class="n">bin_edges</span></div>


<div class="viewcode-block" id="histogram2d"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.histogram2d.html#mindspore.numpy.histogram2d">[docs]</a><span class="k">def</span> <span class="nf">histogram2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="nb">range</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span> <span class="c1"># pylint: disable=redefined-builtin</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the multidimensional histogram of some data.</span>

<span class="sd">    Note:</span>
<span class="sd">        Deprecated numpy argument `normed` is not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[list, tuple, Tensor]): An array with shape `(N,)` containing the x</span>
<span class="sd">            coordinates of the points to be histogrammed.</span>
<span class="sd">        y (Union[list, tuple, Tensor]): An array with shape `(N,)` containing the y</span>
<span class="sd">            coordinates of the points to be histogrammed.</span>
<span class="sd">        bins (Union[int, tuple, list], optional): The bin specification:</span>

<span class="sd">            If int, the number of bins for the two dimensions ``(nx=ny=bins)``.</span>

<span class="sd">            If array_like, the bin edges for the two dimensions ``(x_edges=y_edges=bins)``.</span>

<span class="sd">            If [int, int], the number of bins in each dimension ``(nx, ny = bins)``.</span>

<span class="sd">            If [array, array], the bin edges in each dimension ``(x_edges, y_edges = bins)``.</span>

<span class="sd">            A combination [int, array] or [array, int], where int is the number of bins and</span>
<span class="sd">            array is the bin edges.</span>
<span class="sd">        range(Union[list, tuple], optional): has shape (2, 2), the leftmost and rightmost</span>
<span class="sd">            edges of the bins along each dimension (if not specified explicitly in the bins</span>
<span class="sd">            parameters): ``[[xmin, xmax], [ymin, ymax]]``. All values outside of this range</span>
<span class="sd">            will be considered outliers and not tallied in the histogram.</span>
<span class="sd">        weights (Union[list, tuple, Tensor], optional): An array with shape `(N,)` of values</span>
<span class="sd">            `w_i` weighing each sample `(x_i, y_i)`.</span>
<span class="sd">        density (boolean, optional): If False, the default, returns the number of samples</span>
<span class="sd">            in each bin. If True, returns the probability density function at the bin,</span>
<span class="sd">            ``bin_count / sample_count / bin_volume``.</span>

<span class="sd">    Returns:</span>
<span class="sd">        (Tensor, Tensor, Tensor), the values of the bi-directional histogram and the bin edges</span>
<span class="sd">        along the first and second dimensions.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If `range` does not have the same size as the number of samples.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.arange(5)</span>
<span class="sd">        &gt;&gt;&gt; y = np.arange(2, 7)</span>
<span class="sd">        &gt;&gt;&gt; print(np.histogram2d(x, y, bins=(2, 3)))</span>
<span class="sd">        (Tensor(shape=[2, 3], dtype=Float32, value=</span>
<span class="sd">        [[ 2.00000000e+00,  0.00000000e+00,  0.00000000e+00],</span>
<span class="sd">        [ 0.00000000e+00,  1.00000000e+00,  2.00000000e+00]]),</span>
<span class="sd">        Tensor(shape=[3], dtype=Float32, value= [ 0.00000000e+00,  2.00000000e+00,  4.00000000e+00]),</span>
<span class="sd">        Tensor(shape=[4], dtype=Float32, value=</span>
<span class="sd">        [ 2.00000000e+00,  3.33333349e+00,  4.66666698e+00,  6.00000000e+00]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">count</span><span class="p">,</span> <span class="n">bin_edges</span> <span class="o">=</span> <span class="n">histogramdd</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span> <span class="nb">range</span><span class="o">=</span><span class="nb">range</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="n">density</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">count</span><span class="p">,</span> <span class="n">bin_edges</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">bin_edges</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span></div>


<div class="viewcode-block" id="matrix_power"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.matrix_power.html#mindspore.numpy.matrix_power">[docs]</a><span class="k">def</span> <span class="nf">matrix_power</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Raises a square matrix to the (integer) power `n`.</span>

<span class="sd">    For positive integers `n`, the power is computed by repeated matrix squarings and</span>
<span class="sd">    matrix multiplications.</span>
<span class="sd">    If :math:`n == 0`, the identity matrix of the same shape as `M` is returned.</span>

<span class="sd">    Note:</span>
<span class="sd">        Stacks of object matrices are not currently supported and</span>
<span class="sd">        :math:`n &lt; 0` is not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Union[int, float, bool, list, tuple, Tensor]): Input matrix.</span>
<span class="sd">        n (int): The exponent can be any integer or long integer, positive or zero.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input can not be converted to a tensor or</span>
<span class="sd">            the exponent is not integer.</span>
<span class="sd">        ValueError: If the input includes less than 2 dimensions or</span>
<span class="sd">            the last 2 dimensions are not square.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.arange(16).reshape(4, 4).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(np.matrix_power(a, 2))</span>
<span class="sd">        [[ 56.  62.  68.  74.]</span>
<span class="sd">         [152. 174. 196. 218.]</span>
<span class="sd">         [248. 286. 324. 362.]</span>
<span class="sd">         [344. 398. 452. 506.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;exponent must be an integer&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;Array must be at least two-dimensional&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">!=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;Last 2 dimensions of the array must be square&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">n</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;n &lt; 0 is not supported now.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">n</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_broadcast_to_shape</span><span class="p">(</span><span class="n">eye</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">n</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">a</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">a</span>
    <span class="k">while</span> <span class="n">n</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
        <span class="n">n</span> <span class="o">=</span> <span class="n">n</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">res</span></div>


<div class="viewcode-block" id="around"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.around.html#mindspore.numpy.around">[docs]</a><span class="k">def</span> <span class="nf">around</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">decimals</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Evenly round to the given number of decimals.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy argument `out` is not supported.</span>
<span class="sd">        Complex numbers are not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Union[int, float, list, tuple, Tensor]): Input data.</span>
<span class="sd">        decimals (int): Number of decimal places to round to. Default: ``0`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor. A tensor of the same type as a, containing the rounded values.</span>
<span class="sd">        The result of rounding a float is a float.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input can not be converted to a tensor or</span>
<span class="sd">            the `decimals` argument is not integer.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.array([-1.3, 0.0, 0.5, 1.5, 2.5])</span>
<span class="sd">        &gt;&gt;&gt; print(np.around(a))</span>
<span class="sd">        [-1. 0. 0. 2. 2.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">_to_tensor_origin_dtype</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">decimals</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;decimals must be an integer&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">decimals</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;decimals &lt; 0 is not supported now.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">decimals</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_round</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_div</span><span class="p">(</span><span class="n">_round</span><span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="mi">10</span><span class="o">**</span><span class="n">decimals</span><span class="p">),</span> <span class="mi">10</span><span class="o">**</span><span class="n">decimals</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_to_poly1d</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">atleast_1d</span><span class="p">(</span><span class="n">_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;input array must be scalar or 1-d sequence&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>


<div class="viewcode-block" id="polyadd"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.polyadd.html#mindspore.numpy.polyadd">[docs]</a><span class="k">def</span> <span class="nf">polyadd</span><span class="p">(</span><span class="n">a1</span><span class="p">,</span> <span class="n">a2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Finds the sum of two polynomials.</span>
<span class="sd">    Returns the polynomial resulting from the sum of two input polynomials.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy object poly1d is currently not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        a1 (Union[int, float, list, tuple, Tensor): Input polynomial.</span>
<span class="sd">        a2 (Union[int, float, list, tuple, Tensor): Input polynomial.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the sum of the inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the input array has more than 1 dimensions.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; print(np.polyadd([1, 2], [9, 5, 4]))</span>
<span class="sd">        [9 6 6]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">a1</span> <span class="o">=</span> <span class="n">_to_poly1d</span><span class="p">(</span><span class="n">a1</span><span class="p">)</span>
    <span class="n">a2</span> <span class="o">=</span> <span class="n">_to_poly1d</span><span class="p">(</span><span class="n">a2</span><span class="p">)</span>
    <span class="n">diff_size</span> <span class="o">=</span> <span class="n">a1</span><span class="o">.</span><span class="n">size</span> <span class="o">-</span> <span class="n">a2</span><span class="o">.</span><span class="n">size</span>
    <span class="k">if</span> <span class="n">diff_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">add</span><span class="p">(</span><span class="n">a1</span><span class="p">,</span> <span class="n">a2</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">diff_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">concatenate</span><span class="p">((</span><span class="n">a1</span><span class="p">[:</span><span class="n">diff_size</span><span class="p">],</span> <span class="n">add</span><span class="p">(</span><span class="n">a1</span><span class="p">[</span><span class="n">diff_size</span><span class="p">:],</span> <span class="n">a2</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">concatenate</span><span class="p">((</span><span class="n">a2</span><span class="p">[:</span><span class="o">-</span><span class="n">diff_size</span><span class="p">],</span> <span class="n">add</span><span class="p">(</span><span class="n">a1</span><span class="p">,</span> <span class="n">a2</span><span class="p">[</span><span class="o">-</span><span class="n">diff_size</span><span class="p">:])))</span></div>


<div class="viewcode-block" id="polysub"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.polysub.html#mindspore.numpy.polysub">[docs]</a><span class="k">def</span> <span class="nf">polysub</span><span class="p">(</span><span class="n">a1</span><span class="p">,</span> <span class="n">a2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Difference (subtraction) of two polynomials.</span>
<span class="sd">    Given two polynomials `a1` and `a2`, returns ``a1 - a2``.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy object poly1d is currently not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        a1 (Union[int, float, list, tuple, Tensor): Minuend polynomial.</span>
<span class="sd">        a2 (Union[int, float, list, tuple, Tensor): Subtrahend polynomial.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the difference of the inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the input array has more than 1 dimensions.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; print(np.polysub([2, 10, -2], [3, 10, -4]))</span>
<span class="sd">        [-1  0  2]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">polyadd</span><span class="p">(</span><span class="n">a1</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">neg_tensor</span><span class="p">(</span><span class="n">_to_tensor</span><span class="p">(</span><span class="n">a2</span><span class="p">)))</span></div>


<div class="viewcode-block" id="polyval"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.polyval.html#mindspore.numpy.polyval">[docs]</a><span class="k">def</span> <span class="nf">polyval</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Evaluates a polynomial at specific values.</span>
<span class="sd">    If `p` is of length `N`, this function returns the value:</span>
<span class="sd">    ``p[0]*x**(N-1) + p[1]*x**(N-2) + ... + p[N-2]*x + p[N-1]``</span>
<span class="sd">    If `x` is a sequence, then ``p(x)`` is returned for each element of `x`. If `x`</span>
<span class="sd">    is another polynomial then the composite polynomial ``p(x(t))`` is returned.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy object poly1d is currently not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        p (Union[int, float, bool, list, tuple, Tensor): 1D array of polynomial</span>
<span class="sd">            coefficients (including coefficients equal to zero) from highest</span>
<span class="sd">            degree to the constant term.</span>
<span class="sd">        x (Union[int, float, bool, list, tuple, Tensor): A number, an array of</span>
<span class="sd">            numbers, at which to evaluate `p`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>
<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If `p` has more than 1 dimensions.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; print(np.polyval([3.,0.,1.], 5.))</span>
<span class="sd">        76.0</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">_to_poly1d</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">exp_p</span> <span class="o">=</span> <span class="n">arange</span><span class="p">(</span><span class="n">_type_convert</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">size</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">var_p</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">shape</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)))</span><span class="o">**</span><span class="n">exp_p</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">p</span><span class="o">*</span><span class="n">var_p</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span></div>


<div class="viewcode-block" id="polyder"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.polyder.html#mindspore.numpy.polyder">[docs]</a><span class="k">def</span> <span class="nf">polyder</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the derivative of the specified order of a polynomial.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy object poly1d is currently not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        p (Union[int, float, bool, list, tuple, Tensor): Polynomial to differentiate.</span>
<span class="sd">            A sequence is interpreted as polynomial coefficients.</span>
<span class="sd">        m (int, optional): Default: ``1`` , order of differentiation.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, a new polynomial representing the derivative.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If `p` has more than 1 dimensions.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; print(np.polyder([1, 1, 1, 1]))</span>
<span class="sd">        [3 2 1]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">_to_poly1d</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">m</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;Order of derivative must be positive&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">m</span> <span class="o">&gt;=</span> <span class="n">p</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_to_tensor</span><span class="p">([])</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">coeff</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">make_range</span><span class="p">(</span><span class="n">_type_convert</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">size</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">coeff</span>
    <span class="k">return</span> <span class="n">p</span></div>


<div class="viewcode-block" id="polymul"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.polymul.html#mindspore.numpy.polymul">[docs]</a><span class="k">def</span> <span class="nf">polymul</span><span class="p">(</span><span class="n">a1</span><span class="p">,</span> <span class="n">a2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Finds the product of two polynomials.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy object poly1d is currently not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        a1 (Union[int, float, bool, list, tuple, Tensor): Input polynomial.</span>
<span class="sd">        a2 (Union[int, float, bool, list, tuple, Tensor): Input polynomial.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, a new polynomial representing the derivative.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the input array has more than 1 dimensions.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; print(np.polymul([3, 1, 2], [2, 5]))</span>
<span class="sd">        [ 6 17  9 10]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">a1</span> <span class="o">=</span> <span class="n">_to_poly1d</span><span class="p">(</span><span class="n">a1</span><span class="p">)</span>
    <span class="n">a2</span> <span class="o">=</span> <span class="n">_to_poly1d</span><span class="p">(</span><span class="n">a2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">convolve</span><span class="p">(</span><span class="n">a1</span><span class="p">,</span> <span class="n">a2</span><span class="p">)</span></div>


<div class="viewcode-block" id="polyint"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.polyint.html#mindspore.numpy.polyint">[docs]</a><span class="k">def</span> <span class="nf">polyint</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns an antiderivative (indefinite integral) of a polynomial.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy object poly1d is currently not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        p (Union[int, float, bool, list, tuple, Tensor): Polynomial to integrate. A</span>
<span class="sd">            sequence is interpreted as polynomial coefficients.</span>
<span class="sd">        m (int, optional): Defaults to 1, Order of the antiderivative.</span>
<span class="sd">        k (Union[int, list of int]y, optinoal): Integration constants. They are given</span>
<span class="sd">            in the order of integration: those corresponding to highest-order terms</span>
<span class="sd">            come first. If None (default), all constants are assumed to be zero. If</span>
<span class="sd">            ``m = 1``, a single scalar can be given instead of a list.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, a new polynomial representing the antiderivative.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If `p` has more than 1 dimensions.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; print(np.polyint([1, 1, 1]))</span>
<span class="sd">        [0.33333334 0.5        1.         0.        ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">_to_poly1d</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">m</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;Order of derivative must be positive&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">m</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">p</span>
    <span class="k">if</span> <span class="n">k</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">atleast_1d</span><span class="p">(</span><span class="n">_to_tensor</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">k</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="p">(</span><span class="n">m</span><span class="p">,))</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">coeff</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">make_range</span><span class="p">(</span><span class="n">_type_convert</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">size</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">concatenate</span><span class="p">((</span><span class="n">true_divide</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">coeff</span><span class="p">),</span> <span class="n">k</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
    <span class="k">return</span> <span class="n">p</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_get_dtype</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns the dtype of x.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">typing</span><span class="o">.</span><span class="n">Number</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">dtype_map</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">t</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">t</span> <span class="o">=</span> <span class="n">dtype_map</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">nptype</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="k">return</span> <span class="n">t</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;data type not understood&#39;</span><span class="p">)</span>


<div class="viewcode-block" id="result_type"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.result_type.html#mindspore.numpy.result_type">[docs]</a><span class="k">def</span> <span class="nf">result_type</span><span class="p">(</span><span class="o">*</span><span class="n">arrays_and_dtypes</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the type that results from applying the type promotion rules to the arguments.</span>

<span class="sd">    Note:</span>
<span class="sd">        The promotion rule is slightly different from original Numpy, but more like</span>
<span class="sd">        jax, due to the preference on ``32-bit`` over ``64-bit`` data types.</span>
<span class="sd">        Complex dtypes are not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        *arrays_and_dtypes (Union[int, float, bool, list, tuple, Tensor, :class:`mindspore.dtype`, str]):</span>
<span class="sd">            The operands of some operation whose result type is needed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`mindspore.dtype`, the result type.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input is not a valid data type.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; print(np.result_type(&#39;i2&#39;, np.float32, True))</span>
<span class="sd">        Float32</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">get_dtype</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">_get_dtype</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">dtype_out</span> <span class="o">=</span> <span class="n">get_dtype</span><span class="p">(</span><span class="n">arrays_and_dtypes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">arrays_and_dtypes</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
        <span class="n">dtype_out</span> <span class="o">=</span> <span class="n">_promote</span><span class="p">(</span><span class="n">dtype_out</span><span class="p">,</span> <span class="n">get_dtype</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">dtype_out</span></div>


<div class="viewcode-block" id="unwrap"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.unwrap.html#mindspore.numpy.unwrap">[docs]</a><span class="k">def</span> <span class="nf">unwrap</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">discont</span><span class="o">=</span><span class="mf">3.141592653589793</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Unwraps by changing deltas between values to ``2*pi`` complement.</span>
<span class="sd">    Unwraps radian phase `p` by changing absolute jumps greater than `discont` to their</span>
<span class="sd">    ``2*pi`` complement along the given axis.</span>

<span class="sd">    Note:</span>
<span class="sd">        For absolute jumps that are within a very close range to pi, unwrapping may be done</span>
<span class="sd">        differently than numpy due to differences in round-off.</span>

<span class="sd">    Args:</span>
<span class="sd">        p (Union[int, float, bool, list, tuple, Tensor): Input array.</span>
<span class="sd">        discont (float, optional): Maximum discontinuity between values, default: ``pi`` .</span>
<span class="sd">        axis (int, optional): Axis along which unwrap will operate, default: ``-1`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the axis is out of range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; phase = np.add(np.linspace(0, np.pi, num=5), [0, 0, 0, np.pi, np.pi])</span>
<span class="sd">        &gt;&gt;&gt; print(phase)</span>
<span class="sd">        [0.        0.7853982 1.5707964 5.4977875 6.2831855]</span>
<span class="sd">        &gt;&gt;&gt; print(np.unwrap(phase))</span>
<span class="sd">        [ 0.0000000e+00  7.8539819e-01  1.5707964e+00 -7.8539848e-01 -4.7683716e-07]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">discont</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s1">&#39;discont should be a float&#39;</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="n">_check_axis_in_range</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
    <span class="n">dd</span> <span class="o">=</span> <span class="n">diff</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
    <span class="n">ddmod</span> <span class="o">=</span> <span class="n">remainder</span><span class="p">(</span><span class="n">add</span><span class="p">(</span><span class="n">dd</span><span class="p">,</span> <span class="n">pi</span><span class="p">),</span> <span class="mi">2</span><span class="o">*</span><span class="n">pi</span><span class="p">)</span> <span class="o">-</span> <span class="n">pi</span>
    <span class="n">ddmod</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">ddmod</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">ddmod</span> <span class="o">==</span> <span class="o">-</span><span class="n">pi</span><span class="p">,</span> <span class="n">dd</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">pi</span><span class="p">,</span> <span class="n">ddmod</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
    <span class="n">ph_correct</span> <span class="o">=</span> <span class="n">ddmod</span> <span class="o">-</span> <span class="n">dd</span>
    <span class="n">ph_correct</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">ph_correct</span><span class="p">,</span> <span class="n">absolute</span><span class="p">(</span><span class="n">dd</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">discont</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ph_correct</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
    <span class="n">slice_all</span> <span class="o">=</span> <span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">p</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">make_slice</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">slice0</span> <span class="o">=</span> <span class="n">_tuple_setitem</span><span class="p">(</span><span class="n">slice_all</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">make_slice</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
    <span class="n">slice1</span> <span class="o">=</span> <span class="n">_tuple_setitem</span><span class="p">(</span><span class="n">slice_all</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">make_slice</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
    <span class="n">head</span> <span class="o">=</span> <span class="n">p</span><span class="p">[</span><span class="n">slice0</span><span class="p">]</span>
    <span class="n">tail</span> <span class="o">=</span> <span class="n">add</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">slice1</span><span class="p">],</span> <span class="n">cumsum</span><span class="p">(</span><span class="n">ph_correct</span><span class="p">,</span> <span class="n">axis</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">concatenate</span><span class="p">((</span><span class="n">head</span><span class="p">,</span> <span class="n">tail</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="cumprod"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.cumprod.html#mindspore.numpy.cumprod">[docs]</a><span class="k">def</span> <span class="nf">cumprod</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the cumulative product of elements along a given axis.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy argument `out` is not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Union[int, float, bool, list, tuple, Tensor]): Input tensor.</span>
<span class="sd">        axis (int, optional): Axis along which the cumulative product is computed.</span>
<span class="sd">            By default the input is flattened. Default: ``None`` .</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input can not be converted to tensor or `axis` is not integer.</span>
<span class="sd">        ValueError: If axis is out of range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.array([1, 2, 3])</span>
<span class="sd">        &gt;&gt;&gt; print(np.cumprod(x))</span>
<span class="sd">        [1 2 6]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">_to_tensor_origin_dtype</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">original_dtype</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;integer axis is expected, but got&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">_check_axis_in_range</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>

    <span class="n">a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span> <span class="k">if</span> <span class="n">original_dtype</span> <span class="o">!=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span> <span class="k">else</span> <span class="n">a</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">original_dtype</span> <span class="ow">in</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">]:</span>
            <span class="n">dtype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span>
        <span class="k">elif</span> <span class="n">original_dtype</span> <span class="ow">in</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint16</span><span class="p">]:</span>
            <span class="n">dtype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint32</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dtype</span> <span class="o">=</span> <span class="n">original_dtype</span>
    <span class="k">return</span> <span class="n">_cumprod_default</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_process_index</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">dims</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;raise&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generates index (Tensor) according to different modes.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;raise&quot;</span><span class="p">:</span>
        <span class="n">_raise_unimplemented_error</span><span class="p">(</span><span class="s2">&quot;&#39;raise&#39; mode is not implemented&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;clip&#39;</span><span class="p">,</span> <span class="s1">&#39;wrap&#39;</span><span class="p">]:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;invalid mode. Expected &#39;wrap&#39; or &#39;clip&#39;&quot;</span><span class="p">)</span>
    <span class="n">ori_shape</span> <span class="o">=</span> <span class="n">index</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">tup</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">index</span><span class="p">):</span>
        <span class="n">d</span> <span class="o">=</span> <span class="n">dims</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;clip&quot;</span><span class="p">:</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">clip</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">d</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;wrap&quot;</span><span class="p">:</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">remainder</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="k">if</span> <span class="n">idx</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">idx</span>
        <span class="n">tup</span> <span class="o">+=</span> <span class="p">(</span><span class="n">idx</span><span class="p">,)</span>
    <span class="k">return</span> <span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="mi">0</span><span class="p">)(</span><span class="n">tup</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">ori_shape</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_get_strides</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s1">&#39;C&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generates strides (1-D tensor) according to `dims` (1-D tensor).&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">order</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;C&#39;</span><span class="p">,</span> <span class="s1">&#39;F&#39;</span><span class="p">]:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;invalid order. Expected &#39;C&#39; or &#39;F&#39;&quot;</span><span class="p">)</span>
    <span class="n">tup</span> <span class="o">=</span> <span class="p">(</span><span class="n">_to_tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">]),)</span>
    <span class="n">dims</span> <span class="o">=</span> <span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">:][::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">order</span> <span class="o">==</span> <span class="s1">&#39;C&#39;</span> <span class="k">else</span> <span class="n">dims</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">dims</span><span class="p">:</span>
        <span class="n">tensor</span> <span class="o">=</span> <span class="n">tup</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">d</span>
        <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">tup</span> <span class="o">+=</span> <span class="p">(</span><span class="n">tensor</span><span class="p">,)</span>
    <span class="n">tup</span> <span class="o">=</span> <span class="n">tup</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">order</span> <span class="o">==</span> <span class="s1">&#39;C&#39;</span> <span class="k">else</span> <span class="n">tup</span>
    <span class="k">return</span> <span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="mi">0</span><span class="p">)(</span><span class="n">tup</span><span class="p">)</span>


<div class="viewcode-block" id="ravel_multi_index"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.ravel_multi_index.html#mindspore.numpy.ravel_multi_index">[docs]</a><span class="k">def</span> <span class="nf">ravel_multi_index</span><span class="p">(</span><span class="n">multi_index</span><span class="p">,</span> <span class="n">dims</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;clip&#39;</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s1">&#39;C&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts a tuple of index arrays into an array of flat indices,</span>
<span class="sd">    applying boundary modes to the multi-index.</span>

<span class="sd">    Note:</span>
<span class="sd">        `raise` mode is not supported. Default mode is `clip`.</span>

<span class="sd">    Args:</span>
<span class="sd">        multi_index (tuple of array_like):</span>
<span class="sd">            A tuple of integer arrays, one array for each dimension.</span>
<span class="sd">        dims (Union[int, tuple of integers]): The shape of array into which the indices from multi_index apply.</span>
<span class="sd">        mode ({`wrap`, `clip`}): Specifies how out-of-bounds indices are handled. Default: ``clip&#39;&#39;``.</span>

<span class="sd">            - `wrap`: wrap around</span>
<span class="sd">            - `clip`: clip to the range</span>

<span class="sd">            In `clip` mode, a negative index which would normally wrap will clip to 0 instead.</span>
<span class="sd">        order ({`C`, `F`}): Determines whether the multi-index should be viewed as indexing in</span>
<span class="sd">            row-major (C-style) or column-major (Fortran-style) order.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Raveled_indices array. An array of indices into the flattened version of an array of dimensions dims.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `multi_index` or `dims` can not be converted to tensor or</span>
<span class="sd">            `dims` is not a sequence of integer values.</span>
<span class="sd">        ValueError: If the length of `multi_index` and that of `dims` are not equal.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; arr = np.array([[3, 6, 6], [4, 5, 1]])</span>
<span class="sd">        &gt;&gt;&gt; output = np.ravel_multi_index(arr, (7, 6))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [22. 41. 37.]</span>
<span class="sd">        &gt;&gt;&gt; output = np.ravel_multi_index((3, 1, 4, 1), (6, 7, 8, 9))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        1621.0</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">dims</span> <span class="o">=</span> <span class="p">(</span><span class="n">dims</span><span class="p">,)</span>
    <span class="n">dims</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">dims</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dims</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">dims</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;only 1-D integer arrays are accepted.&quot;</span><span class="p">)</span>
    <span class="n">multi_index</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">multi_index</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">multi_index</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dims</span><span class="p">):</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;parameter multi_index must be a sequence of length &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">dims</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">multi_index</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;only int indices permitted&quot;</span><span class="p">)</span>

    <span class="n">multi_index</span> <span class="o">=</span> <span class="n">_process_index</span><span class="p">(</span><span class="n">multi_index</span><span class="p">,</span> <span class="n">dims</span><span class="p">,</span> <span class="n">mode</span><span class="p">)</span>
    <span class="n">strides</span> <span class="o">=</span> <span class="n">_get_strides</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="n">order</span><span class="p">)</span>
    <span class="n">s_shape</span> <span class="o">=</span> <span class="n">strides</span><span class="o">.</span><span class="n">shape</span> <span class="o">+</span> <span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">multi_index</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">strides</span> <span class="o">=</span> <span class="n">_broadcast_to_shape</span><span class="p">(</span><span class="n">strides</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">s_shape</span><span class="p">),</span> <span class="n">multi_index</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sum_</span><span class="p">((</span><span class="n">multi_index</span> <span class="o">*</span> <span class="n">strides</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_vector_norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">_ord</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns norm of a vector.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">_in</span><span class="p">(</span><span class="n">_ord</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;fro&#39;</span><span class="p">,</span> <span class="s1">&#39;nuc&#39;</span><span class="p">)):</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;Frobenius norm and nuclear norm are only defined for vectors&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_ord</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_ord</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="k">if</span> <span class="n">_ord</span> <span class="o">==</span> <span class="n">inf</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceMax</span><span class="p">(</span><span class="n">keepdims</span><span class="p">)(</span><span class="n">absolute</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">axis</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">_ord</span> <span class="o">==</span> <span class="o">-</span><span class="n">inf</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceMin</span><span class="p">(</span><span class="n">keepdims</span><span class="p">)(</span><span class="n">absolute</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">axis</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">_ord</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">(</span><span class="n">keepdims</span><span class="p">)(</span><span class="n">F</span><span class="o">.</span><span class="n">not_equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">axis</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">power</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">(</span><span class="n">keepdims</span><span class="p">)(</span><span class="n">power</span><span class="p">(</span><span class="n">absolute</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">_ord</span><span class="p">),</span> <span class="n">axis</span><span class="p">),</span> <span class="mf">1.</span><span class="o">/</span><span class="n">_ord</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span>


<span class="k">def</span> <span class="nf">_matrix_norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">_ord</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns norm of a matrix.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">_ord</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;for 0 axis, norm is defined only for 2-D matrices&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_ord</span> <span class="o">==</span> <span class="s1">&#39;nuc&#39;</span><span class="p">:</span>
        <span class="n">_raise_unimplemented_error</span><span class="p">(</span><span class="s1">&#39;nuclear norm is not implemented&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_in</span><span class="p">(</span><span class="n">_ord</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)):</span>
        <span class="n">_raise_unimplemented_error</span><span class="p">(</span><span class="s1">&#39;2-norm is not implemented for matrices&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_in</span><span class="p">(</span><span class="n">_ord</span><span class="p">,</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;fro&#39;</span><span class="p">)):</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">(</span><span class="n">keepdims</span><span class="p">)(</span><span class="n">F</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">axis</span><span class="p">))</span>
    <span class="n">axis0</span><span class="p">,</span> <span class="n">axis1</span> <span class="o">=</span> <span class="n">axis</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">keepdims</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">_check_is_inf</span><span class="p">(</span><span class="n">_abs</span><span class="p">(</span><span class="n">_ord</span><span class="p">))</span> <span class="ow">and</span> <span class="n">axis0</span> <span class="o">&gt;</span> <span class="n">axis1</span><span class="p">:</span>
            <span class="n">axis0</span> <span class="o">-=</span> <span class="mi">1</span>
        <span class="k">elif</span> <span class="n">_abs</span><span class="p">(</span><span class="n">_ord</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">axis1</span> <span class="o">&gt;</span> <span class="n">axis0</span><span class="p">:</span>
            <span class="n">axis1</span> <span class="o">-=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">_check_is_inf</span><span class="p">(</span><span class="n">_ord</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceMax</span><span class="p">(</span><span class="n">keepdims</span><span class="p">)(</span><span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">(</span><span class="n">keepdims</span><span class="p">)(</span><span class="n">absolute</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">axis1</span><span class="p">),</span> <span class="n">axis0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_check_is_inf</span><span class="p">(</span><span class="n">_ord</span><span class="p">,</span> <span class="kc">True</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceMin</span><span class="p">(</span><span class="n">keepdims</span><span class="p">)(</span><span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">(</span><span class="n">keepdims</span><span class="p">)(</span><span class="n">absolute</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">axis1</span><span class="p">),</span> <span class="n">axis0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_ord</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceMax</span><span class="p">(</span><span class="n">keepdims</span><span class="p">)(</span><span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">(</span><span class="n">keepdims</span><span class="p">)(</span><span class="n">absolute</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">axis0</span><span class="p">),</span> <span class="n">axis1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_ord</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceMin</span><span class="p">(</span><span class="n">keepdims</span><span class="p">)(</span><span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">(</span><span class="n">keepdims</span><span class="p">)(</span><span class="n">absolute</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">axis0</span><span class="p">),</span> <span class="n">axis1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;invalid norm order for matrices&#39;</span><span class="p">)</span>


<div class="viewcode-block" id="norm"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.norm.html#mindspore.numpy.norm">[docs]</a><span class="k">def</span> <span class="nf">norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span> <span class="c1"># pylint: disable=redefined-builtin</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Matrix or vector norm.</span>
<span class="sd">    This function is able to return one of eight different matrix norms, or one of an</span>
<span class="sd">    infinite number of vector norms (described below), depending on the value of the</span>
<span class="sd">    ord parameter.</span>

<span class="sd">    Note:</span>
<span class="sd">        Nuclear norm and 2-norm are not supported for matrices.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[int, float, bool, list, tuple, Tensor]): Input array. If `axis` is None,</span>
<span class="sd">            `x` must be 1-D or 2-D, unless `ord` is None. If both `axis` and `ord` are None,</span>
<span class="sd">            the 2-norm of ``x.ravel`` will be returned.</span>
<span class="sd">        ord (Union[None, &#39;fro&#39;, &#39;nuc&#39;, inf, -inf, int, float], optional): Order of the norm.</span>
<span class="sd">            inf means numpy’s inf object. Default: ``None`` .</span>
<span class="sd">        axis (Union[None, int, 2-tuple of integers], optional): If `axis` is an integer, it</span>
<span class="sd">            specifies the axis of `x` along which to compute the vector norms. If `axis` is</span>
<span class="sd">            a 2-tuple, it specifies the axes that hold 2-D matrices, and the matrix norms of</span>
<span class="sd">            these matrices are computed. If `axis` is None then either a vector norm (when x</span>
<span class="sd">            is 1-D) or a matrix norm (when `x` is 2-D) is returned. The default is None.</span>
<span class="sd">        keepdims (boolean, optional): If this is set to True, the axes which are normed over</span>
<span class="sd">            are left in the result as dimensions with size one. With this option the result</span>
<span class="sd">            will broadcast correctly against the original `x`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, norm of the matrix or vector(s).</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the norm order is not defined.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; print(np.norm(np.arange(9).astype(np.float32)))</span>
<span class="sd">        14.282857</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">ord</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">))</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">_in</span><span class="p">(</span><span class="nb">ord</span><span class="p">,</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;fro&#39;</span><span class="p">,</span> <span class="s1">&#39;nuc&#39;</span><span class="p">,</span> <span class="n">inf</span><span class="p">,</span> <span class="o">-</span><span class="n">inf</span><span class="p">)):</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;invalid value for `ord`&#39;</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">ord</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span>
            <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;for None axis, array must a vector or a 2-D matrix&#39;</span><span class="p">)</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">make_range</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="n">_check_axis_valid</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">_vector_norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">ord</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">_matrix_norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">ord</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;invalid number of dimensions to norm&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">keepdims</span> <span class="ow">and</span> <span class="n">ndim</span> <span class="o">&gt;</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">res</span><span class="p">):</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">_expand</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span></div>


<div class="viewcode-block" id="bitwise_and"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.bitwise_and.html#mindspore.numpy.bitwise_and">[docs]</a><span class="k">def</span> <span class="nf">bitwise_and</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the bit-wise AND of two arrays element-wise.</span>
<span class="sd">    Computes the bit-wise AND of the underlying binary representation of the integers in</span>
<span class="sd">    the input arrays. This ufunc implements the C/Python operator &amp;.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Tensor): Input array.</span>
<span class="sd">        x2 (Tensor): Input array. Only integer and boolean types are handled. If</span>
<span class="sd">            ``x1.shape != x2.shape``, they must be broadcastable to a common shape (which becomes</span>
<span class="sd">            the shape of the output).</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, this is a scalar if both x1 and x2 are scalars.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; print(np.bitwise_and(13, 17))</span>
<span class="sd">        1</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">bitwise_and</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="bitwise_or"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.bitwise_or.html#mindspore.numpy.bitwise_or">[docs]</a><span class="k">def</span> <span class="nf">bitwise_or</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the bit-wise OR of two arrays element-wise.</span>
<span class="sd">    Computes the bit-wise OR of the underlying binary representation of the integers in</span>
<span class="sd">    the input arrays. This ufunc implements the C/Python operator \|.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Tensor): Input array.</span>
<span class="sd">        x2 (Tensor): Input array. Only integer and boolean types are handled. If</span>
<span class="sd">            ``x1.shape != x2.shape``, they must be broadcastable to a common shape (which becomes</span>
<span class="sd">            the shape of the output).</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, this is a scalar if both x1 and x2 are scalars.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; print(np.bitwise_or(13, 16))</span>
<span class="sd">        29</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">bitwise_or</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="bitwise_xor"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.bitwise_xor.html#mindspore.numpy.bitwise_xor">[docs]</a><span class="k">def</span> <span class="nf">bitwise_xor</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the bit-wise XOR of two arrays element-wise.</span>
<span class="sd">    Computes the bit-wise XOR of the underlying binary representation of the integers in</span>
<span class="sd">    the input arrays. This ufunc implements the C/Python operator ^.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Tensor): Input array.</span>
<span class="sd">        x2 (Tensor): Input array. Only integer and boolean types are handled. If</span>
<span class="sd">            ``x1.shape != x2.shape``, they must be broadcastable to a common shape (which becomes</span>
<span class="sd">            the shape of the output).</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, this is a scalar if both x1 and x2 are scalars.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; print(np.bitwise_xor(13, 17))</span>
<span class="sd">        28</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">bitwise_xor</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="invert"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.invert.html#mindspore.numpy.invert">[docs]</a><span class="k">def</span> <span class="nf">invert</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes bit-wise inversion, or bit-wise NOT, element-wise.</span>
<span class="sd">    Computes the bit-wise NOT of the underlying binary representation of the integers in</span>
<span class="sd">    the input arrays. This ufunc implements the C/Python operator ~.</span>
<span class="sd">    For signed integer inputs, the two&#39;s complement is returned. In a two&#39;s-complement system</span>
<span class="sd">    negative numbers are represented by the two&#39;s complement of the absolute value. This is</span>
<span class="sd">    the most common method of representing signed integers on computers</span>
<span class="sd">    `[1] &lt;https://en.wikipedia.org/wiki/Two&#39;s_complement&gt;`_. A N-bit two&#39;s-complement system</span>
<span class="sd">    can represent every integer in the range ``-2^{N-1}`` to ``+2^{N-1}-1``.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>
<span class="sd">        Supported dtypes on Ascend: np.int16, np.uint16.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Only integer and boolean types are handled.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; print(np.invert(np.array(13, dtype=np.uint16)))</span>
<span class="sd">        65522</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_apply_tensor_op</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">invert</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="rint"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.rint.html#mindspore.numpy.rint">[docs]</a><span class="k">def</span> <span class="nf">rint</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Rounds elements of the array to the nearest integer.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `where`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[float, list, tuple, Tensor]): Input tensor of any dimension.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Output tensor is same shape and type as x. This is a scalar if x is a scalar.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` can not be converted to tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.array([-1.7, -1.5, 0.2, 1.5, 1.7, 2.0])</span>
<span class="sd">        &gt;&gt;&gt; print(np.rint(x))</span>
<span class="sd">        [-2. -2. 0. 2. 2. 2.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">_to_tensor_origin_dtype</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">_rint</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">res</span><span class="p">),</span> <span class="n">dtype</span><span class="p">):</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span></div>


<div class="viewcode-block" id="correlate"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.correlate.html#mindspore.numpy.correlate">[docs]</a><span class="k">def</span> <span class="nf">correlate</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Cross-correlation of two 1-dimensional sequences.</span>

<span class="sd">    This function computes the correlation as generally defined in signal processing texts:</span>

<span class="sd">    :math:`c_{av}[k] = sum_n a[n+k] * conj(v[n])`</span>

<span class="sd">    with `a` and `v` sequences being zero-padded where necessary and conj being the conjugate.</span>

<span class="sd">    Note:</span>
<span class="sd">        Currently, complex numbers are not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Union[list, tuple, Tensor]): First input sequence.</span>
<span class="sd">        v (Union[list, tuple, Tensor]): Second input sequence.</span>
<span class="sd">        mode (str, optional): By default, mode is `\&#39;valid\&#39;`.</span>
<span class="sd">            If `mode` is `\&#39;valid\&#39;`, it returns output of length :math:`max(M, N) - min(M, N) + 1`.</span>
<span class="sd">            The convolution product is only given for points where the signals overlap</span>
<span class="sd">            completely. Values outside the signal boundary have no effect.</span>
<span class="sd">            If `mode` is `\&#39;full\&#39;`, it returns the convolution at each point of overlap, with</span>
<span class="sd">            an output shape of :math:`(N + M - 1,)`.</span>
<span class="sd">            At the end-points of the convolution, the signals do not overlap completely,</span>
<span class="sd">            and boundary effects may be seen.</span>
<span class="sd">            If `mode` is `\&#39;same\&#39;`, it returns output of length :math:`max(M, N)`. Boundary</span>
<span class="sd">            effects are still visible.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor. Discrete cross-correlation of `a` and `v`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the inputs can not be converted to tensor.</span>
<span class="sd">        ValueError: If `a` and `v` are empty or have wrong dimensions</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; output = np.correlate([1, 2, 3], [0, 1, 0.5])</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [3.5]</span>
<span class="sd">        &gt;&gt;&gt; output = np.correlate([1, 2, 3], [0, 1, 0.5], mode=&quot;same&quot;)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2.  3.5 3. ]</span>
<span class="sd">        &gt;&gt;&gt; output = np.correlate([1, 2, 3, 4, 5], [1, 2], mode=&quot;same&quot;)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 2.  5.  8. 11. 14.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">v</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;only support 1-dimensional inputs.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">v</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;Inputs cannot be empty.&quot;</span><span class="p">)</span>

    <span class="n">promote_dtype</span> <span class="o">=</span> <span class="n">_promote</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="c1"># P.Conv2D requires that the two tensors have the same data type.</span>
    <span class="c1"># If the promote data type is not supported, it will be converted to float32.</span>
    <span class="c1"># The supported dtype list may vary in the future.</span>
    <span class="k">if</span> <span class="n">promote_dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">]:</span>
        <span class="n">promote_dtype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">promote_dtype</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">promote_dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">size</span> <span class="o">&lt;</span> <span class="n">v</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
        <span class="n">a</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="p">,</span> <span class="n">a</span>
        <span class="k">return</span> <span class="n">_compute_1d_conv</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mode</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">_compute_1d_conv</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mode</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_compute_1d_conv</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mode</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns a 1-D sequence which is the cross-correlate of two 1-D sequences (`a` and `v`).&quot;&quot;&quot;</span>
    <span class="n">v_size</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape_mul</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="s1">&#39;full&#39;</span><span class="p">,</span> <span class="s1">&#39;valid&#39;</span><span class="p">):</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;mode must be one of [&#39;full&#39;, &#39;same&#39;, &#39;valid&#39;]&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">v_size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;same&#39;</span><span class="p">:</span>
            <span class="n">pad_left</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">v_size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="kc">True</span><span class="p">))</span>
            <span class="n">pad_right</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">v_size</span> <span class="o">-</span> <span class="n">v_size</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="kc">True</span><span class="p">))</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="mi">0</span><span class="p">)((</span><span class="n">pad_left</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">pad_right</span><span class="p">))</span>
        <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;full&#39;</span><span class="p">:</span>
            <span class="n">pad</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">v_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="kc">True</span><span class="p">))</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="mi">0</span><span class="p">)((</span><span class="n">pad</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">pad</span><span class="p">))</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
    <span class="n">_conv</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">size</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">_conv</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>


<div class="viewcode-block" id="radians"><a class="viewcode-back" href="../../../api_python/numpy/mindspore.numpy.radians.html#mindspore.numpy.radians">[docs]</a><span class="k">def</span> <span class="nf">radians</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts angles from degrees to radians.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Angles in degrees.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Default: ``None`` . Overrides the dtype of the</span>
<span class="sd">            output Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the corresponding radian values. This is a tensor scalar if `x`</span>
<span class="sd">        is a tensor scalar.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.asarray([1, 2, 3, -4, -5])</span>
<span class="sd">        &gt;&gt;&gt; output = np.radians(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0.01745329  0.03490658  0.05235988 -0.06981317 -0.08726647]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">deg2rad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>