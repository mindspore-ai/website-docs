<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore.ops.function.nn_func &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script><script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/js/theme.js"></script><script src="../../../../_static/underscore.js"></script><script src="../../../../_static/doctools.js"></script><script src="../../../../_static/js/mermaid-9.3.0.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/overview.html">MindSpore Design Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/tensor_view.html">TENSOR VIEWS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/programming_paradigm.html">Functional and Object-Oriented Fusion Programming Paradigm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/dynamic_graph_and_static_graph.html">Combination of Dynamic and Static Graphs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/distributed_training_design.html">Distributed Parallel Native</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/data_engine.html">High Performance Data Processing Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/all_scenarios.html">Full-scenarios Unified Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/graph_fusion_engine.html">Graph-Kernel Fusion Acceleration Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/pluggable_device.html">Third-Party Hardware Interconnection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/official_models.html">Official Models</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.hal.html">mindspore.hal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.multiprocessing.html">mindspore.multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.experimental.html">mindspore.experimental</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Mapping</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/api_mapping/pytorch_api_mapping.html">PyTorch and MindSpore API Mapping Table</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Migration Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/overview.html">Overview of Migration Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/enveriment_preparation.html">Environment Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/analysis_and_preparation.html">Model Analysis and Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/model_development/model_development.html">Network Constructing Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/debug_and_tune.html">Debugging and Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/sample_code.html">Network Migration Debugging Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/faq.html">FAQs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Syntax Support</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/static_graph_syntax_support.html">Static Graph Syntax Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/static_graph_syntax/operators.html">Static Graph Syntax - Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/static_graph_syntax/statements.html">Static Graph Syntax - Python Statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/static_graph_syntax/python_builtin_functions.html">Static Graph Syntax - Python Built-in Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/index_support.html">Tensor Index Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Environment Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/env_var_list.html">Environment Variables</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/data_processing.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/implement_problem.html">Implement Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/network_compilation.html">Network Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/operators_compile.html">Operators Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/precision_tuning.html">Precision Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/distributed_parallel.html">Distributed Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/feature_advice.html">Feature Advice</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
      <li>mindspore.ops.function.nn_func</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for mindspore.ops.function.nn_func</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2023-2024 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>

<span class="sd">&quot;&quot;&quot;Defines nn operators with functional form.&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">pi</span><span class="p">,</span> <span class="n">log</span>

<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">log</span> <span class="k">as</span> <span class="n">logger</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations</span> <span class="kn">import</span> <span class="n">nn_ops</span> <span class="k">as</span> <span class="n">NN_OPS</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations</span> <span class="kn">import</span> <span class="n">_sequence_ops</span> <span class="k">as</span> <span class="n">seq</span>
<span class="kn">import</span> <span class="nn">mindspore.common.dtype</span> <span class="k">as</span> <span class="nn">mstype</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.function.math_func</span> <span class="kn">import</span> <span class="n">logsumexp</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.function.random_func</span> <span class="kn">import</span> <span class="n">_get_seed</span><span class="p">,</span> <span class="n">_set_prim_op_user_data</span>
<span class="kn">from</span> <span class="nn">mindspore.common.tensor</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">mindspore.common.parameter</span> <span class="kn">import</span> <span class="n">Parameter</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">Tensor</span> <span class="k">as</span> <span class="n">Tensor_</span>
<span class="kn">from</span> <span class="nn">mindspore.ops._primitive_cache</span> <span class="kn">import</span> <span class="n">_get_cache_prim</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">_checkparam</span> <span class="k">as</span> <span class="n">validator</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.composite.multitype_ops._constexpr_utils</span> <span class="kn">import</span> <span class="n">raise_value_error</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations.nn_ops</span> <span class="kn">import</span> <span class="n">MaxUnpool2D</span><span class="p">,</span> <span class="n">MaxUnpool3D</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations.nn_ops</span> <span class="kn">import</span> <span class="n">FractionalMaxPoolWithFixedKsize</span><span class="p">,</span> <span class="n">FractionalMaxPool3DWithFixedKsize</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations.nn_ops</span> <span class="kn">import</span> <span class="n">PadV3</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations.nn_ops</span> <span class="kn">import</span> <span class="n">ChannelShuffle</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations.nn_ops</span> <span class="kn">import</span> <span class="n">TripletMarginLoss</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations.nn_ops</span> <span class="kn">import</span> <span class="n">LayerNormExt</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations._sequence_ops</span> <span class="kn">import</span> <span class="n">TupleToTensor</span><span class="p">,</span> <span class="n">TensorToTuple</span><span class="p">,</span> <span class="n">ListToTensor</span>
<span class="kn">from</span> <span class="nn">mindspore.common.api</span> <span class="kn">import</span> <span class="n">_function_forbid_reuse</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.auto_generate</span> <span class="kn">import</span> <span class="n">log_softmax</span><span class="p">,</span> <span class="n">dense</span><span class="p">,</span> <span class="n">prelu</span><span class="p">,</span> <span class="n">celu</span><span class="p">,</span> <span class="n">relu</span><span class="p">,</span> <span class="n">fast_gelu</span><span class="p">,</span> <span class="n">silu</span><span class="p">,</span> <span class="n">elu</span><span class="p">,</span> <span class="n">sigmoid</span><span class="p">,</span> <span class="n">relu6</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.auto_generate.gen_ops_prim</span> <span class="kn">import</span> <span class="n">GroupNorm</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.auto_generate</span> <span class="kn">import</span> <span class="p">(</span><span class="n">reflection_pad_1d_op</span><span class="p">,</span> <span class="n">reflection_pad_2d_op</span><span class="p">,</span> <span class="n">reflection_pad_3d_op</span><span class="p">,</span>
                                         <span class="n">replication_pad_1d_op</span><span class="p">,</span> <span class="n">replication_pad_2d_op</span><span class="p">,</span> <span class="n">replication_pad_3d_op</span><span class="p">,</span>
                                         <span class="n">constant_pad_nd_op</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.auto_generate.gen_ops_prim</span> <span class="kn">import</span> <span class="n">embedding_op</span><span class="p">,</span> <span class="n">Convolution</span>

<span class="n">abs_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Abs</span><span class="p">()</span>
<span class="n">add_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span>
<span class="n">bias_add_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BiasAdd</span><span class="p">()</span>
<span class="n">cast_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>
<span class="n">div_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Div</span><span class="p">()</span>
<span class="n">dtype_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()</span>
<span class="n">equal_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Equal</span><span class="p">()</span>
<span class="n">erf_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Erf</span><span class="p">()</span>
<span class="n">exp_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Exp</span><span class="p">()</span>
<span class="n">expand_dims_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">()</span>
<span class="n">fillv2_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">FillV2</span><span class="p">()</span>
<span class="n">gather_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Gather</span><span class="p">()</span>
<span class="n">gather_d_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">GatherD</span><span class="p">()</span>
<span class="n">gelu_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">GeLU</span><span class="p">()</span>
<span class="n">greater_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Greater</span><span class="p">()</span>
<span class="n">hardswish_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">HSwish</span><span class="p">()</span>
<span class="n">less_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Less</span><span class="p">()</span>
<span class="n">list_to_tensor_</span> <span class="o">=</span> <span class="n">ListToTensor</span><span class="p">()</span>
<span class="n">log_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Log</span><span class="p">()</span>
<span class="n">matmul_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>
<span class="n">maximum_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Maximum</span><span class="p">()</span>
<span class="n">minimum_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Minimum</span><span class="p">()</span>
<span class="n">mish_</span> <span class="o">=</span> <span class="n">NN_OPS</span><span class="o">.</span><span class="n">Mish</span><span class="p">()</span>
<span class="n">mul_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span>
<span class="n">neg_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Neg</span><span class="p">()</span>
<span class="n">ones_like_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">OnesLike</span><span class="p">()</span>
<span class="n">reduce_mean_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceMean</span><span class="p">()</span>
<span class="n">reduce_sum_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">()</span>
<span class="n">reshape_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()</span>
<span class="n">scalar_to_tensor_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScalarToTensor</span><span class="p">()</span>
<span class="n">select_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Select</span><span class="p">()</span>
<span class="n">selu_</span> <span class="o">=</span> <span class="n">NN_OPS</span><span class="o">.</span><span class="n">SeLU</span><span class="p">()</span>
<span class="n">shape_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()</span>
<span class="n">sigmoid_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
<span class="n">sign_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Sign</span><span class="p">()</span>
<span class="n">slice_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Slice</span><span class="p">()</span>
<span class="n">softplus_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Softplus</span><span class="p">()</span>
<span class="n">softsign_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Softsign</span><span class="p">()</span>
<span class="n">sqrt_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Sqrt</span><span class="p">()</span>
<span class="n">square_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Square</span><span class="p">()</span>
<span class="n">sub_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Sub</span><span class="p">()</span>
<span class="n">tensor_shape_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">()</span>
<span class="n">tensor_to_tuple_</span> <span class="o">=</span> <span class="n">TensorToTuple</span><span class="p">()</span>
<span class="n">transpose_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Transpose</span><span class="p">()</span>
<span class="n">tuple_to_tensor_</span> <span class="o">=</span> <span class="n">TupleToTensor</span><span class="p">()</span>

<span class="n">check_positive_int_const</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span>
<span class="n">check_positive_int_sequence_const</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int_sequence</span>
<span class="n">check_positive_float_const</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_float</span>
<span class="n">check_positive_float_sequence_const</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_float_sequence</span>
<span class="n">check_bool_const</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_bool</span>
<span class="n">check_int_const</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_is_int</span>
<span class="n">check_non_negative_float_const</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_float</span>
<span class="n">check_string_const</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span>


<div class="viewcode-block" id="adaptive_avg_pool2d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.adaptive_avg_pool2d.html#mindspore.ops.adaptive_avg_pool2d">[docs]</a><span class="k">def</span> <span class="nf">adaptive_avg_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs 2D adaptive average pooling on a multi-plane input signal.</span>
<span class="sd">    That is, for any input size, the size of the specified output is H x W.</span>
<span class="sd">    The number of output features is equal to the number of input features.</span>

<span class="sd">    The input and output data format can be &quot;NCHW&quot; and &quot;CHW&quot;. N is the batch size, C is the number of channels,</span>
<span class="sd">    H is the feature height, and W is the feature width.</span>

<span class="sd">    For adaptive average pooling for 2D:</span>

<span class="sd">    ..  math::</span>
<span class="sd">        \begin{align}</span>
<span class="sd">        h_{start} &amp;= floor(i * H_{in} / H_{out})\\</span>
<span class="sd">        h_{end} &amp;= ceil((i + 1) * H_{in} / H_{out})\\</span>
<span class="sd">        w_{start} &amp;= floor(j * W_{in} / W_{out})\\</span>
<span class="sd">        w_{end} &amp;= ceil((j + 1) * W_{in} / W_{out})\\</span>
<span class="sd">        Output(i,j) &amp;= \frac{\sum Input[h_{start}:h_{end}, w_{start}:w_{end}]}{(h_{end}- h_{start})</span>
<span class="sd">        * (w_{end}- w_{start})}</span>
<span class="sd">        \end{align}</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input of adaptive_avg_pool2d, which is a 3D or 4D tensor,</span>
<span class="sd">            with float16, float32 or float64 data type.</span>
<span class="sd">        output_size (Union[int, tuple]): The target output size. `output_size` can be a tuple :math:`(H, W)`,</span>
<span class="sd">            or an int H for :math:`(H, H)`. :math:`H` and :math:`W` can be int or None.</span>
<span class="sd">            If it is None, it means the output size is the same as the input size.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type as the `input`.</span>

<span class="sd">        Shape of the output is `input_shape[:len(input_shape) - len(out_shape)] + out_shape`.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out\_shape = \begin{cases}</span>
<span class="sd">        input\_shape[-2] + output\_size[1], &amp; \text{if } output\_size text{ is (None, w);}\\</span>
<span class="sd">        output\_size[0] + input\_shape[-1], &amp; \text{if } output\_size text{ is (h, None);}\\</span>
<span class="sd">        input\_shape[-2:], &amp; \text{if } output\_size text{ is (None, None);}\\</span>
<span class="sd">        (h, h), &amp; \text{if } output\_size text{ is h;}\\</span>
<span class="sd">        (h, w), &amp; \text{if } output\_size text{ is (h, w)}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If `output_size` is a tuple and the length of `output_size` is not 2.</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not float16, float32 or float64.</span>
<span class="sd">        ValueError: If the dimension of `input` is less than or equal to the dimension of `output_size`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; # case 1: output_size=(None, 2)</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]],</span>
<span class="sd">        ...                            [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]],</span>
<span class="sd">        ...                            [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.adaptive_avg_pool2d(input, (None, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1.5 2.5]</span>
<span class="sd">          [4.5 5.5]</span>
<span class="sd">          [7.5 8.5]]</span>
<span class="sd">         [[1.5 2.5]</span>
<span class="sd">          [4.5 5.5]</span>
<span class="sd">          [7.5 8.5]]</span>
<span class="sd">         [[1.5 2.5]</span>
<span class="sd">          [4.5 5.5]</span>
<span class="sd">          [7.5 8.5]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 2: output_size=2</span>
<span class="sd">        &gt;&gt;&gt; output = ops.adaptive_avg_pool2d(input, 2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[3. 4.]</span>
<span class="sd">          [6. 7.]]</span>
<span class="sd">         [[3. 4.]</span>
<span class="sd">          [6. 7.]]</span>
<span class="sd">         [[3. 4.]</span>
<span class="sd">          [6. 7.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: output_size=(1, 2)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.adaptive_avg_pool2d(input, (1, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[4.5 5.5]]</span>
<span class="sd">         [[4.5 5.5]]</span>
<span class="sd">         [[4.5 5.5]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">adaptive_avgpool2d_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">AdaptiveAvgPool2D</span><span class="p">)(</span><span class="n">output_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">adaptive_avgpool2d_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="adaptive_avg_pool3d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.adaptive_avg_pool3d.html#mindspore.ops.adaptive_avg_pool3d">[docs]</a><span class="k">def</span> <span class="nf">adaptive_avg_pool3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs 3D adaptive average pooling on a multi-plane input signal.</span>
<span class="sd">    That is, for any input size, the size of the specified output is :math:`(D, H, W)`.</span>
<span class="sd">    The number of output features is equal to the number of input planes.</span>

<span class="sd">    Suppose the last 3 dimension size of x is :math:`(inD, inH, inW)`, the last 3 dimension size of output is</span>
<span class="sd">    :math:`(outD, outH, outW)`.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            \forall \quad od \in [0,outD-1], oh \in [0,outH-1], ow \in [0,outW-1]\\</span>
<span class="sd">            output[od,oh,ow] = \\</span>
<span class="sd">            \qquad mean(x[istartD:iendD+1,istartH:iendH+1,istartW:iendW+1])\\</span>
<span class="sd">            where,\\</span>
<span class="sd">            \qquad istartD= \left\lceil \frac{od * inD}{outD} \right\rceil \\</span>
<span class="sd">            \qquad iendD=\left\lfloor \frac{(od+1)* inD}{outD} \right\rfloor \\</span>
<span class="sd">            \qquad istartH=\left\lceil \frac{oh * inH}{outH} \right\rceil \\</span>
<span class="sd">            \qquad iendH=\left\lfloor \frac{(oh+1) * inH}{outH} \right\rfloor \\</span>
<span class="sd">            \qquad istartW=\left\lceil \frac{ow * inW}{outW} \right\rceil \\</span>
<span class="sd">            \qquad iendW=\left\lfloor \frac{(ow+1) * inW}{outW} \right\rfloor</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input of adaptive_avg_pool3d, which is a 5D or 4D Tensor.</span>
<span class="sd">        output_size (Union[int, tuple]): The target output size. `output_size` can be a tuple :math:`(D, H, W)`,</span>
<span class="sd">            or an int D for :math:`(D, D, D)`. :math:`D`, :math:`H` and :math:`W` can be int or None</span>
<span class="sd">            which means the output size is the same as that of the input.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not float16, float32 or float64.</span>
<span class="sd">        ValueError: If the dimension of `input` is not 4D or 5D.</span>
<span class="sd">        ValueError: If `output_size` value is not positive.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; # case 1: output_size=(3, 3, 4)</span>
<span class="sd">        &gt;&gt;&gt; output_size=(3, 3, 4)</span>
<span class="sd">        &gt;&gt;&gt; input_val = np.random.randn(4, 3, 5, 6, 7)</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(input_val, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.adaptive_avg_pool3d(input, output_size)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (4, 3, 3, 3, 4)</span>
<span class="sd">        &gt;&gt;&gt; # case 2: output_size=4</span>
<span class="sd">        &gt;&gt;&gt; output_size=5</span>
<span class="sd">        &gt;&gt;&gt; input_val = np.random.randn(2, 3, 8, 6, 12)</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(input_val, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.adaptive_avg_pool3d(input, output_size)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 3, 5, 5, 5)</span>
<span class="sd">        &gt;&gt;&gt; # case 3: output_size=(None, 4, 5)</span>
<span class="sd">        &gt;&gt;&gt; output_size=(None, 4, 5)</span>
<span class="sd">        &gt;&gt;&gt; input_val = np.random.randn(4, 1, 9, 10, 8)</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(input_val, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.adaptive_avg_pool3d(input, output_size)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (4, 1, 9, 4, 5)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">adaptive_avg_pool3d_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">AdaptiveAvgPool3D</span><span class="p">)(</span><span class="n">output_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">adaptive_avg_pool3d_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_check_avgpool_1d_type_and_int</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">,</span> <span class="n">count_include_pad</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Checks the type of avgpool1d input&quot;&quot;&quot;</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="s1">&#39;avg_pool1d&#39;</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="s1">&#39;avg_pool1d&#39;</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;ceil_mode&#39;</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="s1">&#39;avg_pool1d&#39;</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;count_include_pad&#39;</span><span class="p">,</span> <span class="n">count_include_pad</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="s1">&#39;avg_pool1d&#39;</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="s1">&#39;avg_pool1d&#39;</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s2">&quot;stride&quot;</span><span class="p">,</span> <span class="s1">&#39;avg_pool1d&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">check_non_negative_int</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="n">arg_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prim_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check argument is non-negative integer, which mean arg_value &gt;= 0.&quot;&quot;&quot;</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="n">arg_name</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>


<div class="viewcode-block" id="avg_pool1d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.avg_pool1d.html#mindspore.ops.avg_pool1d">[docs]</a><span class="k">def</span> <span class="nf">avg_pool1d</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">count_include_pad</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a 1D average pooling over an input Tensor which can be regarded as a composition of 1D input planes.</span>

<span class="sd">    Typically the input is of shape :math:`(N_{in}, C_{in}, L_{in})`, avg_pool1d outputs regional average in the</span>
<span class="sd">    :math:`(L_{in})`-dimension. Given kernel size :math:`ks = l_{ker}` and `stride` :math:`s = s_0`, the</span>
<span class="sd">    operation is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(N_i, C_j, l) = \frac{1}{l_{ker}} \sum_{n=0}^{l_{ker}-1}</span>
<span class="sd">        \text{input}(N_i, C_j, s_0 \times l + n)</span>

<span class="sd">    .. warning::</span>
<span class="sd">        `kernel_size` is in the range `[1, 255]`. `stride` is in the range `[1, 63]`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Tensor of shape :math:`(N, C_{in}, L_{in})`.</span>
<span class="sd">        kernel_size (int): The size of kernel window used to take the average value. Default: ``1`` .</span>
<span class="sd">        stride (Union(int, tuple[int])): The distance of kernel moving. `stride` can either be an int</span>
<span class="sd">            number or a tuple of one int number. Default: ``1`` .</span>
<span class="sd">        padding (Union(int, tuple[int])): The pad value to be filled. `padding` can either be an integer</span>
<span class="sd">            or a tuple of one integer. Default: ``0`` .</span>
<span class="sd">        ceil_mode (bool): If True, apply ceil instead of floor to compute the output shape. Default: ``False``.</span>
<span class="sd">        count_include_pad (bool): If True, include the zero-padding in the averaging calculation. Default: ``True`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor of shape :math:`(N, C_{out}, L_{out})`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        TypeError: If `kernel_size` or `stride` is not an int.</span>
<span class="sd">        TypeError: If `ceil_mode` or `count_include_pad` is not a bool.</span>
<span class="sd">        ValueError: If length of shape of `input_x` is not equal to `3`.</span>
<span class="sd">        ValueError: If `kernel_size` or `stride` is less than `1`.</span>
<span class="sd">        ValueError: If `padding` is not int nor a tuple whose length is equal to `2`.</span>
<span class="sd">        ValueError: If value(s) of `padding` is less than `0`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.random.randint(0, 10, [1, 3, 6]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.avg_pool1d(input_x, kernel_size=6, stride=1)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 3, 1)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For avg_pool1d, the input input_x must be tensor&quot;</span><span class="p">)</span>

    <span class="n">_check_avgpool_1d_type_and_int</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">,</span> <span class="n">count_include_pad</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="s1">&#39;padding&#39;</span><span class="p">,</span> <span class="s1">&#39;avg_pool1d&#39;</span><span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">padding</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For avg_pool1d, padding should be int or tuple of length 1.&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">padding</span><span class="p">:</span>
            <span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s1">&#39;padding&#39;</span><span class="p">,</span> <span class="s1">&#39;avg_pool1d&#39;</span><span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">padding</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">padding</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For avg_pool1d, padding should be int or tuple of length 1.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For avg_pool1d, stride should be int or tuple of length 1.&quot;</span><span class="p">)</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">squeeze_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Squeeze</span><span class="p">)((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="n">avg_pool_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">AvgPool3D</span><span class="p">)(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">),</span>
                                               <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="p">),</span>
                                               <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span>
                                               <span class="n">pad</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
                                               <span class="n">ceil_mode</span><span class="o">=</span><span class="n">ceil_mode</span><span class="p">,</span>
                                               <span class="n">count_include_pad</span><span class="o">=</span><span class="n">count_include_pad</span><span class="p">)</span>
    <span class="n">input_x</span> <span class="o">=</span> <span class="n">expand_dims_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">input_x</span> <span class="o">=</span> <span class="n">expand_dims_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">input_x</span> <span class="o">=</span> <span class="n">avg_pool_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
    <span class="n">input_x</span> <span class="o">=</span> <span class="n">squeeze_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">input_x</span></div>


<span class="k">def</span> <span class="nf">_check_avgpool_2d_kernel_size</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;check and calculate the avgpool2d kernel_size&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="s1">&#39;avg_pool2d&#39;</span><span class="p">)</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For avg_pool2d, kernel_size should be int or tuple of length 2.&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">kernel_size</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="s1">&#39;avg_pool2d&#39;</span><span class="p">)</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For avg_pool2d, kernel_size should be int or tuple of length 2.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">kernel_size</span>


<span class="k">def</span> <span class="nf">_check_avgpool_2d_stride</span><span class="p">(</span><span class="n">stride</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;check and calculate the avgpool2d stride&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s2">&quot;stride&quot;</span><span class="p">,</span> <span class="s1">&#39;avg_pool2d&#39;</span><span class="p">)</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">stride</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For avg_pool2d, stride should be int or tuple of length 2.&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">stride</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s2">&quot;stride&quot;</span><span class="p">,</span> <span class="s1">&#39;avg_pool2d&#39;</span><span class="p">)</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For avg_pool2d, stride should be int or tuple of length 2.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">stride</span>


<span class="k">def</span> <span class="nf">_check_avgpool_2d_padding</span><span class="p">(</span><span class="n">padding</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;check and calculate the avgpool2d padding&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="s1">&#39;padding&#39;</span><span class="p">,</span> <span class="s1">&#39;avg_pool2d&#39;</span><span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">padding</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For avg_pool2d, padding should be int or tuple of length 4.&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">padding</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s1">&#39;padding&#39;</span><span class="p">,</span> <span class="s1">&#39;avg_pool2d&#39;</span><span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">padding</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">padding</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">padding</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">padding</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For avg_pool2d, padding should be int or tuple of length 4.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">padding</span>


<span class="k">def</span> <span class="nf">_check_avg_pool2d_type_and_value</span><span class="p">(</span><span class="n">ceil_mode</span><span class="p">,</span> <span class="n">count_include_pad</span><span class="p">,</span> <span class="n">divisor_override</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;check the type of avgpool2d input&quot;&quot;&quot;</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;ceil_mode&#39;</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="s1">&#39;avg_pool2d&#39;</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;count_include_pad&#39;</span><span class="p">,</span> <span class="n">count_include_pad</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="s1">&#39;avg_pool2d&#39;</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">divisor_override</span><span class="p">,</span> <span class="s1">&#39;divisor_override&#39;</span><span class="p">,</span> <span class="s1">&#39;avg_pool2d&#39;</span><span class="p">)</span>


<div class="viewcode-block" id="avg_pool2d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.avg_pool2d.html#mindspore.ops.avg_pool2d">[docs]</a><span class="k">def</span> <span class="nf">avg_pool2d</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">count_include_pad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
               <span class="n">divisor_override</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a 2D average pooling over an input Tensor which can be regarded as a composition of 2D input planes.</span>
<span class="sd">    Typically the input is of shape :math:`(N_{in}, C_{in}, H_{in}, W_{in})`, outputs regional average in the</span>
<span class="sd">    :math:`(H_{in}, W_{in})`-dimension. Given kernel size :math:`(k_{h}, k_{w})` and `strides` , the operation</span>
<span class="sd">    is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(N_i, C_j, h, w) = \frac{1}{k_{h} * k_{w}} \sum_{m=0}^{k_{h}-1} \sum_{n=0}^{k_{w}-1}</span>
<span class="sd">        \text{input}(N_i, C_j, stride[0] \times h + m, stride[1] \times w + n)</span>

<span class="sd">    .. warning::</span>
<span class="sd">        `kernel_size` is in the range `[1, 255]`. `stride` is in the range `[1, 63]`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of kernel used to take the average value. It is an int number</span>
<span class="sd">            that represents height and width of the kernel, or a tuple of two int numbers that represent height and</span>
<span class="sd">            width respectively. Default: ``1`` .</span>
<span class="sd">        stride (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents the height and</span>
<span class="sd">            width of movement are both strides, or a tuple of two int numbers that represent height and width of</span>
<span class="sd">            movement respectively. Default: ``1`` .</span>
<span class="sd">        padding (Union(int, tuple[int])): The pad value to be filled. Default: 0. If `padding` is an integer, the</span>
<span class="sd">            paddings of top, bottom, left and right are the same, equal to pad. If `padding` is a tuple of `4` integers,</span>
<span class="sd">            the padding of top, bottom, left and right equal to `padding[0]`, `padding[1]`, `padding[2]` and</span>
<span class="sd">            `padding[3]` correspondingly. Default: ``0`` .</span>
<span class="sd">        ceil_mode (bool): If True, apply ceil instead of floor to compute the output shape. Default: ``False``.</span>
<span class="sd">        count_include_pad (bool): If True, include the zero-padding in the averaging calculation. Default: ``True`` .</span>
<span class="sd">        divisor_override (int): If specified, it will be used as divisor in the averaging calculation, otherwise</span>
<span class="sd">            `kernel_size` will be used. Default: ``0``, which means not specified.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with shape :math:`(N, C_{out}, H_{out}, W_{out})`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        TypeError: If `kernel_size` or `stride` is neither int nor tuple.</span>
<span class="sd">        TypeError: If `ceil_mode` or `count_include_pad` is not a bool.</span>
<span class="sd">        TypeError: If `divisor_override` is not an int.</span>
<span class="sd">        ValueError: If length of shape of `input_x` is not equal to `4`.</span>
<span class="sd">        ValueError: If `kernel_size` or `stride` is less than 1.</span>
<span class="sd">        ValueError: If `kernel_size` or `stride` is a tuple whose length is not equal to `2`.</span>
<span class="sd">        ValueError: If `padding` is not int nor a tuple whose length is equal to `4`.</span>
<span class="sd">        ValueError: If value(s) of `padding` is less than `0`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(1 * 3 * 3 * 4).reshape(1, 3, 3, 4), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.avg_pool2d(x, kernel_size=2, stride=1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[ 2.5   3.5   4.5]</span>
<span class="sd">           [ 6.5   7.5   8.5]]</span>
<span class="sd">          [[14.5  15.5  16.5]</span>
<span class="sd">           [18.5  19.5  20.5]]</span>
<span class="sd">          [[26.5  27.5  28.5]</span>
<span class="sd">           [30.5  31.5  32.5]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For avg_pool2d, the input input_x must be tensor&quot;</span><span class="p">)</span>

    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_avgpool_2d_kernel_size</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">_check_avgpool_2d_stride</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="n">_check_avgpool_2d_padding</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
    <span class="n">_check_avg_pool2d_type_and_value</span><span class="p">(</span><span class="n">ceil_mode</span><span class="p">,</span> <span class="n">count_include_pad</span><span class="p">,</span> <span class="n">divisor_override</span><span class="p">)</span>
    <span class="n">squeeze_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Squeeze</span><span class="p">)(</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">avg_pool_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">AvgPool3D</span><span class="p">)(</span><span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
                                               <span class="n">strides</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
                                               <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span>
                                               <span class="n">pad</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
                                               <span class="n">ceil_mode</span><span class="o">=</span><span class="n">ceil_mode</span><span class="p">,</span>
                                               <span class="n">count_include_pad</span><span class="o">=</span><span class="n">count_include_pad</span><span class="p">,</span>
                                               <span class="n">divisor_override</span><span class="o">=</span><span class="n">divisor_override</span><span class="p">)</span>
    <span class="n">input_x</span> <span class="o">=</span> <span class="n">expand_dims_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">input_x</span> <span class="o">=</span> <span class="n">avg_pool_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
    <span class="n">input_x</span> <span class="o">=</span> <span class="n">squeeze_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">input_x</span></div>


<span class="k">def</span> <span class="nf">_check_avg_pool3d_padding</span><span class="p">(</span><span class="n">padding</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check the padding value in avg_pool3d op.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="s1">&#39;padding&#39;</span><span class="p">,</span> <span class="s1">&#39;avg_pool3d&#39;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">6</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For avg_pool3d, padding should be int or tuple of length 6.&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">padding</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s1">&#39;padding&#39;</span><span class="p">,</span> <span class="s1">&#39;avg_pool3d&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For avg_pool3d, padding should be int or tuple of length 6.&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="avg_pool3d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.avg_pool3d.html#mindspore.ops.avg_pool3d">[docs]</a><span class="k">def</span> <span class="nf">avg_pool3d</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">count_include_pad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
               <span class="n">divisor_override</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a 3D average pooling over an input Tensor which can be regarded as a composition of 3D input planes.</span>
<span class="sd">    Typically the input is of shape :math:`(N, C, D_{in}, H_{in}, W_{in})`, avg_pool3d outputs regional average in the</span>
<span class="sd">    :math:`(D_{in}, H_{in}, W_{in})`-dimension. Given kernel size :math:`ks = (d_{ker}, h_{ker}, w_{ker})` and stride</span>
<span class="sd">    :math:`s = (s_0, s_1, s_2)`, the operation is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(N_i, C_j, d, h, w) =</span>
<span class="sd">        \frac{1}{d_{ker} * h_{ker} * w_{ker}} \sum_{l=0}^{d_{ker}-1} \sum_{m=0}^{h_{ker}-1} \sum_{n=0}^{w_{ker}-1}</span>

<span class="sd">        \text{input}(N_i, C_j, s_0 \times d + l, s_1 \times h + m, s_2 \times w + n)</span>

<span class="sd">    .. warning::</span>
<span class="sd">        `kernel_size` is in the range `[1, 255]`. `stride` is in the range `[1, 63]`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Tensor of shape :math:`(N, C, D_{in}, H_{in}, W_{in})`. Currently support float16 and</span>
<span class="sd">            float32 data type.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]], optional): The size of kernel used to take the average value, is an int</span>
<span class="sd">            number that represents depth, height and width are both `kernel_size`, or a tuple of three int numbers that</span>
<span class="sd">            represent depth, height and width respectively. Default: ``1`` .</span>
<span class="sd">        stride (Union[int, tuple[int]], optional): The distance of kernel moving, an int number that represents the</span>
<span class="sd">            depth, height and width of movement are both stride, or a tuple of three int numbers that represent depth,</span>
<span class="sd">            height and width of movement respectively. Default: ``1`` .</span>
<span class="sd">        padding (Union(int, tuple[int]), optional): The pad value to be filled. If `padding` is an integer, the addings</span>
<span class="sd">            of head, tail, top, bottom, left and right are the same, equal to pad. If `padding` is a tuple of six</span>
<span class="sd">            integers, the padding of head, tail, top, bottom, left and right equal to padding[0], padding[1],</span>
<span class="sd">            padding[2], padding[3], padding[4] and padding[5] correspondingly. Default: ``0`` .</span>
<span class="sd">        ceil_mode (bool, optional): If ``True`` , ceil instead of floor to</span>
<span class="sd">            compute the output shape. Default: ``False`` .</span>
<span class="sd">        count_include_pad (bool, optional): If ``True`` , averaging calculation</span>
<span class="sd">            will include the zero-padding. Default: ``True`` .</span>
<span class="sd">        divisor_override (int, optional): If specified, it will be used as divisor in the averaging calculation,</span>
<span class="sd">            otherwise `kernel_size` will be used. Default: ``0`` , which means not specified.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with shape :math:`(N, C, D_{out}, H_{out}, W_{out})`. Has the same data type with `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        TypeError: If `kernel_size`, `stride` or `padding` is neither an int not a tuple.</span>
<span class="sd">        TypeError: If `ceil_mode` or `count_include_pad` is not a bool.</span>
<span class="sd">        TypeError: If `divisor_override` is not an int.</span>
<span class="sd">        ValueError: If length of shape of `input_x` is not equal to `5`.</span>
<span class="sd">        ValueError: If numbers in `kernel_size` or `stride` are not positive.</span>
<span class="sd">        ValueError: If `kernel_size` or `stride` is a tuple whose length is not equal to `3`.</span>
<span class="sd">        ValueError: If `padding` is a tuple whose length is not equal to `6`.</span>
<span class="sd">        ValueError: If element of `padding` is less than `0`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.arange(1 * 2 * 2 * 2 * 3).reshape((1, 2, 2, 2, 3)), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.avg_pool3d(input_x, kernel_size=2, stride=1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[[ 5.  6.]]]</span>
<span class="sd">          [[[17. 18.]]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For avg_pool3d, the input input_x must be tensor&quot;</span><span class="p">)</span>

    <span class="n">_check_avg_pool3d_padding</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>

    <span class="n">avg_pool_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">AvgPool3D</span><span class="p">)(</span><span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
                                               <span class="n">strides</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
                                               <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span>
                                               <span class="n">pad</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
                                               <span class="n">ceil_mode</span><span class="o">=</span><span class="n">ceil_mode</span><span class="p">,</span>
                                               <span class="n">count_include_pad</span><span class="o">=</span><span class="n">count_include_pad</span><span class="p">,</span>
                                               <span class="n">divisor_override</span><span class="o">=</span><span class="n">divisor_override</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">avg_pool_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">is_ascend_backend</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check if the Ascend is used&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s1">&#39;device_target&#39;</span><span class="p">)</span> <span class="o">==</span> <span class="s1">&#39;Ascend&#39;</span>


<span class="k">def</span> <span class="nf">_check_adaptive_max_pool1d_output_size</span><span class="p">(</span><span class="n">output_size</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check the output_size value in adaptive_max_pool1d op.&quot;&quot;&quot;</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s2">&quot;output_size&quot;</span><span class="p">,</span> <span class="s1">&#39;adaptive_max_pool1d&#39;</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;output_size&#39;</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="s1">&#39;adaptive_max_pool1d&#39;</span><span class="p">)</span>


<div class="viewcode-block" id="adaptive_max_pool1d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.adaptive_max_pool1d.html#mindspore.ops.adaptive_max_pool1d">[docs]</a><span class="k">def</span> <span class="nf">adaptive_max_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a 1D adaptive maximum pooling over an input Tensor which can be regarded as</span>
<span class="sd">    a composition of 1D input planes.</span>

<span class="sd">    Typically, the input is of shape :math:`(N, C, L_{in})`,</span>
<span class="sd">    adaptive_max_pool1d outputs regional maximum in the :math:`L_{in}`-dimension. The output is of</span>
<span class="sd">    shape :math:`(N, C, L_{out})`, where :math:`L_{out}` is defined by `output_size`.</span>

<span class="sd">    Note:</span>
<span class="sd">        - :math:`L_{in}` must be divisible by `output_size`.</span>
<span class="sd">        - Ascend platform only supports float16 type for input.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Tensor of shape :math:`(N, C, L_{in})`, with float16 or float32 data type.</span>
<span class="sd">        output_size (int): the target output size :math:`L_{out}`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor of shape :math:`(N, C, L_{out})`, has the same type as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `output_size` is not an int.</span>
<span class="sd">        ValueError: If `output_size` is less than 1.</span>
<span class="sd">        ValueError: If the last dimension of `input` is smaller than `output_size`.</span>
<span class="sd">        ValueError: If the last dimension of `input` is not divisible by `output_size`.</span>
<span class="sd">        ValueError: If length of shape of `input` is not equal to 3.</span>


<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.random.randint(0, 10, [1, 3, 6]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.adaptive_max_pool1d(input, output_size=2)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 3, 2)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For adaptive_max_pool1d, the input input must be tensor&quot;</span><span class="p">)</span>

    <span class="n">_check_adaptive_max_pool1d_output_size</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span>

    <span class="n">x_in_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">x_dtype</span> <span class="o">=</span> <span class="n">dtype_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">x_in_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">output_size</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For adaptive_max_pool1d input&#39;s last dimension must be greater or equal to &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;output size </span><span class="si">{</span><span class="n">output_size</span><span class="si">}</span><span class="s2">, but got </span><span class="si">{</span><span class="n">x_in_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x_in_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">%</span> <span class="n">output_size</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For adaptive_max_pool1d input&#39;s last dimension must be divisible by &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;output size </span><span class="si">{</span><span class="n">output_size</span><span class="si">}</span><span class="s2">, but got </span><span class="si">{</span><span class="n">x_in_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_ascend_backend</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">x_dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For adaptive_max_pool1d in Ascend platform, the input dtype must be float16, &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">x_dtype</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">x_dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For adaptive_max_pool1d, the input dtype must be float16 or float32, &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">x_dtype</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="n">squeeze_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Squeeze</span><span class="p">)(</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">width</span> <span class="o">=</span> <span class="n">x_in_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">width</span> <span class="o">//</span> <span class="n">output_size</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">width</span> <span class="o">-</span> <span class="p">(</span><span class="n">output_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">width</span> <span class="o">//</span> <span class="n">output_size</span><span class="p">)</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">)</span>
    <span class="n">max_pool_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">MaxPool</span><span class="p">)(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">expand_dims_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">max_pool_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">squeeze_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

    <span class="k">return</span> <span class="nb">input</span></div>


<span class="k">def</span> <span class="nf">_check_adaptive_max_pool2d</span><span class="p">(</span><span class="n">return_indices</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;check the type of return_indices&quot;&quot;&quot;</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;return_indices&quot;</span><span class="p">,</span> <span class="n">return_indices</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="s2">&quot;adaptive_max_pool2d&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="adaptive_max_pool2d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.adaptive_max_pool2d.html#mindspore.ops.adaptive_max_pool2d">[docs]</a><span class="k">def</span> <span class="nf">adaptive_max_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This operator applies a 2D adaptive max pooling to an input signal composed of multiple input planes.</span>
<span class="sd">    That is, for any input size, the size of the specified output is H x W.</span>
<span class="sd">    The number of output features is equal to the number of input planes.</span>

<span class="sd">    The input and output data format can be &quot;NCHW&quot; and &quot;CHW&quot;. N is the batch size, C is the number of channels,</span>
<span class="sd">    H is the feature height, and W is the feature width.</span>

<span class="sd">    .. math::</span>

<span class="sd">        \begin{align}</span>
<span class="sd">        h_{start} &amp;= floor(i * H_{in} / H_{out})\\</span>
<span class="sd">        h_{end} &amp;= ceil((i + 1) * H_{in} / H_{out})\\</span>
<span class="sd">        w_{start} &amp;= floor(j * W_{in} / W_{out})\\</span>
<span class="sd">        w_{end} &amp;= ceil((j + 1) * W_{in} / W_{out})\\</span>
<span class="sd">        Output(i,j) &amp;= {\max Input[h_{start}:h_{end}, w_{start}:w_{end}]}</span>
<span class="sd">        \end{align}</span>

<span class="sd">    Note:</span>
<span class="sd">        Ascend platform only supports float16 type for input.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): A 3D or 4D tensor,</span>
<span class="sd">            with float16, float32 or float64 data type.</span>
<span class="sd">        output_size (Union[int, tuple]): The target output size. `output_size` can be a tuple :math:`(H, W)`,</span>
<span class="sd">            or an int H for :math:`(H, H)`. :math:`H` and :math:`W` can be int or None.</span>
<span class="sd">            If it is None, it means the output size is the same as the input size.</span>

<span class="sd">        return_indices (bool): If `return_indices` is ``True`` , the indices of max value would be output.</span>
<span class="sd">            Default: ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same shape and dtype as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `output_size` is not int or tuple.</span>
<span class="sd">        TypeError: If `input` is not a tensor.</span>
<span class="sd">        TypeError: If `return_indices` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `input` is not float16, float32 or float64.</span>
<span class="sd">        ValueError: If `output_size` is a tuple and the length of `output_size` is not 2.</span>
<span class="sd">        ValueError: If the data format of `input` is not &quot;NCHW&quot; or &quot;CHW&quot;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; # case 1: output_size=(None, 2)</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([[[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]],</span>
<span class="sd">        ...                             [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]],</span>
<span class="sd">        ...                             [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.adaptive_max_pool2d(input, (None, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[2. 3.]</span>
<span class="sd">           [5. 6.]</span>
<span class="sd">           [8. 9.]]</span>
<span class="sd">          [[2. 3.]</span>
<span class="sd">           [5. 6.]</span>
<span class="sd">           [8. 9.]]</span>
<span class="sd">          [[2. 3.]</span>
<span class="sd">           [5. 6.]</span>
<span class="sd">           [8. 9.]]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 2: output_size=2</span>
<span class="sd">        &gt;&gt;&gt; output = ops.adaptive_max_pool2d(input, 2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[5. 6.]</span>
<span class="sd">           [8. 9.]]</span>
<span class="sd">          [[5. 6.]</span>
<span class="sd">           [8. 9.]]</span>
<span class="sd">          [[5. 6.]</span>
<span class="sd">           [8. 9.]]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: output_size=(1, 2)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.adaptive_max_pool2d(input, (1, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[8. 9.]]</span>
<span class="sd">          [[8. 9.]]</span>
<span class="sd">          [[8. 9.]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_adaptive_max_pool2d</span><span class="p">(</span><span class="n">return_indices</span><span class="p">)</span>
    <span class="n">_adaptive_max_pool2d</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">AdaptiveMaxPool2D</span><span class="p">)(</span><span class="n">output_size</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">_adaptive_max_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">out</span> <span class="k">if</span> <span class="n">return_indices</span> <span class="k">else</span> <span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">output</span></div>


<span class="k">def</span> <span class="nf">adaptive_max_pool3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the 3D adaptive max pooling for an input Tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Tensor, with shape :math:`(C, D, H, W)` or :math:`(N, C, D, H, W)`.</span>
<span class="sd">        output_size (Union[int, tuple]): The specified output size, which is an int or Tuple(int) that</span>
<span class="sd">            represents depth, height and width, or a tuple of three int numbers that represent depth, height and</span>
<span class="sd">            width respectively. The value must be a positive integer. If it is None, the output size and</span>
<span class="sd">            input size of the corresponding dimension are the same.</span>
<span class="sd">        return_indices (bool, optional): If `return_indices` is `True`, the indices of max value would be output,</span>
<span class="sd">            Otherwise, it will not be output. Default: `False`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        - **y** (Tensor) - Tensor, with the same number of dims and data type as the `input`.</span>
<span class="sd">        - **argmax** (Tensor) - Tensor, the indices of max value, which has the same shape as the</span>
<span class="sd">          `y` and it&#39;s data type is int32. It will output only when `return_indices` is True.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        ValueError: If the dimensions number of `input` is not 4 or 5.</span>
<span class="sd">        TypeError: If dtype of `input` is not int or float.</span>
<span class="sd">        ValueError: If `output_size` is neither an int nor a tuple with shape (3,).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.arange(0,36).reshape((1, 3, 3, 4)).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output_size = (1, 1, 2)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.adaptive_max_pool3d(input, output_size, True)</span>
<span class="sd">        &gt;&gt;&gt; print(output[0].asnumpy())</span>
<span class="sd">        [[[[33. 35.]]]]</span>
<span class="sd">        &gt;&gt;&gt; print(output[1].asnumpy())</span>
<span class="sd">        [[[[33 35]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">output_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
    <span class="n">adaptive_max_pool3d_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">AdaptiveMaxPool3D</span><span class="p">)()</span>
    <span class="n">output_size_</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">adaptive_max_pool3d_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size_</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">out</span> <span class="k">if</span> <span class="n">return_indices</span> <span class="k">else</span> <span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">output</span>


<div class="viewcode-block" id="max_unpool1d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.max_unpool1d.html#mindspore.ops.max_unpool1d">[docs]</a><span class="k">def</span> <span class="nf">max_unpool1d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the inverse of `max_pool1d`.</span>

<span class="sd">    `max_unpool1d` keeps the maximal value and set all position of non-maximal values to zero.</span>
<span class="sd">    Typically the input is of shape :math:`(N, C, H_{in})` or :math:`(C, H_{in})`, and the output is of shape</span>
<span class="sd">    :math:`(N, C, H_{out})` or :math:`(C, H_{out})`. The operation is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">        H_{out} = (H_{in} - 1) \times stride[0] - 2 \times padding[0] + kernel\_size[0] \\</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input Tensor to invert. Tensor of shape :math:`(N, C, H_{in})` or :math:`(C, H_{in})`.</span>
<span class="sd">        indices (Tensor): Index of maximum value.</span>
<span class="sd">          Tensor of shape must be same with input &#39;x&#39;.</span>
<span class="sd">          Values of indices must belong to :math:`[0, H_{in} - 1]`.</span>
<span class="sd">          Data type must be in int32 or int64.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of kernel used to take the maximum value.</span>
<span class="sd">        stride (Union[int, tuple[int]]): The distance of kernel moving,</span>
<span class="sd">            If stride is 0, (0) or ``None`` , then stride equal to kernel_size.</span>
<span class="sd">            Default: ``None`` , which indicates the moving step is `kernel_size` .</span>
<span class="sd">        padding (Union[int, tuple[int]]): The pad value to be filled. Default: ``0`` .</span>
<span class="sd">        output_size (tuple[int], optional): The output shape. Default: ``None`` .</span>
<span class="sd">            If output_size == (), then the shape of output computed by `kernel_size`, `stride` and `padding`.</span>
<span class="sd">            If output_size != (), then output_size must be :math:`(N, C, H)` , :math:`(C, H)` or</span>
<span class="sd">            :math:`(H)` and output_size must belong to</span>
<span class="sd">            :math:`[(N, C, H_{out} - stride[0]), (N, C, H_{out} + stride[0])]`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with shape :math:`(N, C, H_{out})` or :math:`(C, H_{out})`,</span>
<span class="sd">        with the same data type with `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If data type of `x` or `indices` is not supported.</span>
<span class="sd">        TypeError: If `kernel_size`, `stride` or `padding` is neither an int nor a tuple.</span>
<span class="sd">        ValueError: If numbers in `stride`, `padding` (also support 0 and (0)) or `kernel_size` is not positive.</span>
<span class="sd">        ValueError: If the shapes of `x` and `indices` are not equal.</span>
<span class="sd">        ValueError: If `x` whose length is not 2 or 3.</span>
<span class="sd">        ValueError: If type of `output_size` is not tuple.</span>
<span class="sd">        ValueError: If `output_size` whose length is not 0, 2 or 3.</span>
<span class="sd">        ValueError: If `output_size` is not close to output size computed by attr `kernel_size`, `stride`, `padding`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[2, 4, 6, 8]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[1, 3, 5, 7]]).astype(np.int64))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.max_unpool1d(x, indices, kernel_size =2, stride=2, padding=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output.asnumpy())</span>
<span class="sd">        [[0. 2. 0. 4. 0. 6. 0. 8.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">kernel_size</span>

    <span class="n">x_shape</span> <span class="o">=</span> <span class="n">shape_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">output_size</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For max_unpool1d, output_size must be tuple, but type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For max_unpool1d, length of output_size with tuple must be 0, 1, 2, 3, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got type </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">output_size</span><span class="p">:</span>
            <span class="n">output_size</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">elif</span> <span class="n">x_dim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">output_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="n">x_shape</span><span class="p">[:</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">output_size</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">output_size</span> <span class="o">=</span> <span class="n">x_shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">output_size</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">kernel_size</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="n">padding</span> <span class="o">+</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="n">max_unpool_2d</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">MaxUnpool2D</span><span class="p">)(</span><span class="n">ksize</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
                                                 <span class="n">pads</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span> <span class="n">output_shape</span><span class="o">=</span><span class="n">output_size</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x_dim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">max_unpool_2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">max_unpool_2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="max_unpool2d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.max_unpool2d.html#mindspore.ops.max_unpool2d">[docs]</a><span class="k">def</span> <span class="nf">max_unpool2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the inverse of `max_pool2d`.</span>

<span class="sd">    `max_unpool2d` keeps the maximal value and set all position of non-maximal values to zero. Typically the input</span>
<span class="sd">    is of shape :math:`(N, C, H_{in}, W_{in})` or :math:`(C, H_{in}, W_{in})`, and the output is of</span>
<span class="sd">    shape :math:`(N, C, H_{out}, W_{out})` or :math:`(C, H_{out}, W_{out})`. The operation is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">        H_{out} = (H{in} - 1) \times stride[0] - 2 \times padding[0] + kernel\_size[0] \\</span>
<span class="sd">        W_{out} = (W{in} - 1) \times stride[1] - 2 \times padding[1] + kernel\_size[1] \\</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input Tensor to invert.</span>
<span class="sd">          Tensor of shape :math:`(N, C, H_{in}, W_{in})` or :math:`(C, H_{in}, W_{in})`.</span>
<span class="sd">        indices (Tensor): Max values&#39; index represented by the indices.</span>
<span class="sd">          Tensor of shape must be same with input &#39;x&#39;.</span>
<span class="sd">          Values of indices must belong to :math:`[0, H_{in} \times W_{in} - 1]`.</span>
<span class="sd">          Data type must be in int32 or int64.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of kernel used to take the maximum value,</span>
<span class="sd">            an int number that represents height and width of the kernel, or a tuple</span>
<span class="sd">            of two int numbers that represent height and width respectively.</span>
<span class="sd">        stride (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the height and width of movement are both stride, or a tuple of two int numbers that</span>
<span class="sd">            represent height and width of movement respectively.</span>
<span class="sd">            Default: ``None`` , which indicates the moving step is `kernel_size` .</span>
<span class="sd">        padding (Union[int, tuple[int]]): The pad value to be filled. Default: ``0`` . If `padding` is an integer,</span>
<span class="sd">            the paddings of height and width are the same, equal to padding. If `padding` is a tuple of two</span>
<span class="sd">            integers, the padding of height and width equal to padding[0] and padding[1] correspondingly.</span>
<span class="sd">        output_size (tuple[int], optional): The target output size. Default: ``None`` .</span>
<span class="sd">            If output_size == (), then the shape of output computed by `kernel_size`, `stride` and `padding`.</span>
<span class="sd">            If output_size != (), then output_size must be :math:`(N, C, H, W)` , :math:`(C, H, W)` or :math:`(H, W)`</span>
<span class="sd">            and output_size must belong to</span>
<span class="sd">            :math:`[(N, C, H_{out} - stride[0], W_{out} - stride[1]),</span>
<span class="sd">            (N, C, H_{out} + stride[0], W_{out} + stride[1])]`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with shape :math:`(N, C, H_{out}, W_{out})` or :math:`(C, H_{out}, W_{out})`,</span>
<span class="sd">        with the same data type with `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If data type of `x` or `indices` is not supported.</span>
<span class="sd">        TypeError: If `kernel_size`, `stride` or `padding` is neither an int nor a tuple.</span>
<span class="sd">        ValueError: If numbers in `stride`, `padding` (also support 0 and (0, 0)) or `kernel_size` is not positive.</span>
<span class="sd">        ValueError: If the shape of `x` and `indices` are not equal.</span>
<span class="sd">        ValueError: If `kernel_size`, `stride` or `padding` is a tuple whose length is not equal to 2.</span>
<span class="sd">        ValueError: If `x` whose length is not 3 or 4.</span>
<span class="sd">        ValueError: If `output_size` whose type is not tuple.</span>
<span class="sd">        ValueError: If `output_size` whose length is not 0, 3 or 4.</span>
<span class="sd">        ValueError: If `output_size` is not close to output size computed by attr `kernel_size`, `stride`, `padding`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[[0, 1], [8, 9]]]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[[[0, 1], [2, 3]]]]).astype(np.int64))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.max_unpool2d(x, indices, kernel_size=1, stride=1, padding=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output.asnumpy())</span>
<span class="sd">        [[[[0. 1.]</span>
<span class="sd">           [8. 9.]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">kernel_size</span>

    <span class="n">x_shape</span> <span class="o">=</span> <span class="n">shape_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">output_size</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For max_unpool2d, output_size must be tuple, but type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For max_unpool2d, length of output_size with tuple must be 0, 2, 3, 4, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got type </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">output_size</span><span class="p">:</span>
            <span class="n">output_size</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">elif</span> <span class="n">x_dim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">output_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="n">x_shape</span><span class="p">[:</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">output_size</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">output_size</span> <span class="o">=</span> <span class="n">x_shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">output_size</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>

    <span class="n">max_unpool_2d</span> <span class="o">=</span> <span class="n">MaxUnpool2D</span><span class="p">(</span><span class="n">ksize</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">pads</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span> <span class="n">output_shape</span><span class="o">=</span><span class="n">output_size</span><span class="p">,</span>
                                <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x_dim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">max_unpool_2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">max_unpool_2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="max_unpool3d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.max_unpool3d.html#mindspore.ops.max_unpool3d">[docs]</a><span class="k">def</span> <span class="nf">max_unpool3d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the inverse of :func:`mindspore.ops.max_pool3d`.</span>

<span class="sd">    `max_unpool3d` keeps the maximal value and set all position of non-maximal values to zero.</span>
<span class="sd">    Typically the input is of shape :math:`(N, C, D_{in}, H_{in}, W_{in})` or :math:`(C, D_{in}, H_{in}, W_{in})`,</span>
<span class="sd">    and the output is of shape :math:`(N, C, D_{out}, H_{out}, W_{out})` or :math:`(C, D_{out}, H_{out}, W_{out})`.</span>
<span class="sd">    The operation is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">        D_{out} = (D{in} - 1) \times stride[0] - 2 \times padding[0] + kernel\_size[0] \\</span>
<span class="sd">        H_{out} = (H{in} - 1) \times stride[1] - 2 \times padding[1] + kernel\_size[1] \\</span>
<span class="sd">        W_{out} = (W{in} - 1) \times stride[2] - 2 \times padding[2] + kernel\_size[2] \\</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input Tensor to invert.</span>
<span class="sd">          Tensor of shape :math:`(N, C, D_{in}, H_{in}, W_{in})` or :math:`(C, D_{in}, H_{in}, W_{in})`.</span>
<span class="sd">        indices (Tensor): Max values&#39; index represented by the indices. Tensor of shape must be same with input &#39;x&#39;.</span>
<span class="sd">          Values of indices must belong to :math:`[0, D_{in} \times H_{in} \times W_{in} - 1]`.</span>
<span class="sd">          Data type must be in int32 or int64.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of kernel used to take the maximum value,</span>
<span class="sd">            an int number that represents depth, height and width of the kernel, or a tuple</span>
<span class="sd">            of three int numbers that represent depth, height and width respectively.</span>
<span class="sd">        stride (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the depth, height and width of movement are both stride, or a tuple of three int numbers that</span>
<span class="sd">            represent depth, height and width of movement respectively.</span>
<span class="sd">            Default: ``None`` , which indicates the moving step is `kernel_size` .</span>
<span class="sd">        padding (Union[int, tuple[int]]): The pad value to be filled. Default: ``0`` . If `padding` is an integer,</span>
<span class="sd">            the paddings of depth, height and width are the same, equal to padding. If `padding` is a tuple of three</span>
<span class="sd">            integers, the padding of depth, height and width equal to padding[0], padding[1] and padding[2]</span>
<span class="sd">            correspondingly.</span>
<span class="sd">        output_size (tuple[int], optional): The output size. Default: ``None`` . If output_size == (), then the shape of</span>
<span class="sd">            output computed by `kernel_size`, `stride` and `padding`. If output_size != (), then output_size must be</span>
<span class="sd">            :math:`(N, C, D, H, W)` or :math:`(C, D, H, W)` or :math:`(D, H, W)` and output_size must belong to</span>
<span class="sd">            :math:`[(N, C, D_{out} - stride[0], H_{out} - stride[1], W_{out} - stride[2]),</span>
<span class="sd">            (N, C, D_{out} + stride[0], H_{out} + stride[1], W_{out} + stride[2])]`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with shape :math:`(N, C, D_{out}, H_{out}, W_{out})` or :math:`(C, D_{out}, H_{out}, W_{out})`,</span>
<span class="sd">        with the same data type with `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If data type of `x` or `indices` is not supported.</span>
<span class="sd">        TypeError: If `kernel_size`, `stride` or `padding` is neither an int nor a tuple.</span>
<span class="sd">        ValueError: If numbers in `stride` or `padding` (also support 0 and (0, 0, 0)) or `kernel_size` is not positive.</span>
<span class="sd">        ValueError: If the shape of `x` and `indices` are not equal.</span>
<span class="sd">        ValueError: If `kernel_size`, `stride` or `padding` is a tuple whose length is not equal to 3.</span>
<span class="sd">        ValueError: If `x` whose length is not 4 or 5.</span>
<span class="sd">        ValueError: If `output_size` whose length is not 0, 4 or 5.</span>
<span class="sd">        ValueError: If `output_size` whose type is not tuple.</span>
<span class="sd">        ValueError: If `output_size` is not close to output size computed by attr `kernel_size`, `stride`, `padding`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[[[0, 1], [8, 9]]]]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices= Tensor(np.array([[[[[0, 1], [2, 3]]]]]).astype(np.int64))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.max_unpool3d(x, indices, kernel_size=2, stride=1, padding=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[[0. 1. 8.]</span>
<span class="sd">            [9. 0. 0.]</span>
<span class="sd">            [0. 0. 0.]]</span>
<span class="sd">           [[0. 0. 0.]</span>
<span class="sd">            [0. 0. 0.]</span>
<span class="sd">            [0. 0. 0.]]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">kernel_size</span>

    <span class="n">x_shape</span> <span class="o">=</span> <span class="n">shape_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">output_size</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For max_unpool3d, output_size must be tuple, but type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For max_unpool3d, length of output_size with tuple must be 0, 3, 4, 5, &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;but got type </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">output_size</span><span class="p">:</span>
        <span class="n">output_size</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">elif</span> <span class="n">x_dim</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
        <span class="n">output_size</span> <span class="o">=</span> <span class="n">x_shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">output_size</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">:]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">output_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="n">x_shape</span><span class="p">[:</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">output_size</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">:]</span>
    <span class="n">max_unpool_3d</span> <span class="o">=</span> <span class="n">MaxUnpool3D</span><span class="p">(</span><span class="n">ksize</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">pads</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span> <span class="n">output_shape</span><span class="o">=</span><span class="n">output_size</span><span class="p">,</span>
                                <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCDHW&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">x_dim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">max_unpool_3d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">max_unpool_3d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="binary_cross_entropy_with_logits"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.binary_cross_entropy_with_logits.html#mindspore.ops.binary_cross_entropy_with_logits">[docs]</a><span class="k">def</span> <span class="nf">binary_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">pos_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adds sigmoid activation function to input `logits`, and uses the given logits to compute binary cross entropy</span>
<span class="sd">    between the logits and the label.</span>

<span class="sd">    Sets input logits as :math:`X`, input label as :math:`Y`, input weight as :math:`W`, output as :math:`L`. Then,</span>

<span class="sd">    .. math::</span>

<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            p_{ij} = sigmoid(X_{ij}) = \frac{1}{1 + e^{-X_{ij}}} \\</span>
<span class="sd">            L_{ij} = -[Y_{ij}log(p_{ij}) + (1 - Y_{ij})log(1 - p_{ij})]</span>
<span class="sd">        \end{array}</span>

<span class="sd">    :math:`i` indicates the :math:`i^{th}` sample, :math:`j` indicates the category. Then,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = \begin{cases}</span>
<span class="sd">        L, &amp; \text{if reduction} = \text{&#39;none&#39;;}\\</span>
<span class="sd">        \operatorname{mean}(L), &amp; \text{if reduction} = \text{&#39;mean&#39;;}\\</span>
<span class="sd">        \operatorname{sum}(L),  &amp; \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    :math:`\ell` indicates the method of calculating the loss. There are three methods:</span>
<span class="sd">    the first method is to provide the loss value directly,</span>
<span class="sd">    the second method is to calculate the average value of all losses,</span>
<span class="sd">    and the third method is to calculate the sum of all losses.</span>

<span class="sd">    This operator will multiply the output by the corresponding weight.</span>
<span class="sd">    The tensor :math:`weight` assigns different weights to each piece of data in the batch,</span>
<span class="sd">    and the tensor :math:`pos\_weight` adds corresponding weights to the positive examples of each category.</span>

<span class="sd">    In addition, it can trade off recall and precision by adding weights to positive examples.</span>
<span class="sd">    In the case of multi-label classification the loss can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            p_{ij,c} = sigmoid(X_{ij,c}) = \frac{1}{1 + e^{-X_{ij,c}}} \\</span>
<span class="sd">            L_{ij,c} = -[P_{c}Y_{ij,c} * log(p_{ij,c}) + (1 - Y_{ij,c})log(1 - p_{ij,c})]</span>
<span class="sd">        \end{array}</span>

<span class="sd">    where c is the class number (c&gt;1 for multi-label binary classification, c=1 for single-label binary classification),</span>
<span class="sd">    n is the number of the sample in the batch and :math:`P_c` is the weight of the positive answer for the class c.</span>
<span class="sd">    :math:`P_c&gt;1` increases the recall, :math:`P_c&lt;1` increases the precision.</span>

<span class="sd">    Args:</span>
<span class="sd">        logits (Tensor): Input logits. Data type must be float16 or float32.</span>
<span class="sd">        label (Tensor): Ground truth label, has the same shape as `logits`.</span>
<span class="sd">          Data type must be float16 or float32.</span>
<span class="sd">        weight (Tensor, optional): A rescaling weight applied to the loss of each batch element. It can be</span>
<span class="sd">          broadcast to a tensor with shape of `logits`. Data type must be float16 or float32.</span>
<span class="sd">          Default: ``None``, `weight` is a Tensor whose value is ``1``.</span>
<span class="sd">        pos_weight (Tensor, optional): A weight of positive examples. Must be a vector with length equal to the</span>
<span class="sd">          number of classes. It can be broadcast to a tensor with shape of `logits`.</span>
<span class="sd">          Data type must be float16 or float32. Default: ``None``, `pos_weight` is a Tensor whose value is ``1``.</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the weighted mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the output elements will be summed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or Scalar, if `reduction` is ``&#39;none&#39;``, it&#39;s a tensor with the same shape and type as input `logits`.</span>
<span class="sd">        Otherwise, the output is a scalar.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If input `logits`, `label`, `weight`, `pos_weight` is not Tensor.</span>
<span class="sd">        TypeError: If data type of input `logits`, `label`, `weight`, `pos_weight` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If data type of input `reduction` is not string.</span>
<span class="sd">        ValueError: If `weight` or `pos_weight` can not be broadcast to a tensor with shape of `logits`.</span>
<span class="sd">        ValueError: If `reduction` is not one of ``&#39;none&#39;``, ``&#39;mean&#39;`` or ``&#39;sum&#39;``.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([[-0.8, 1.2, 0.7], [-0.1, -0.4, 0.7]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; label = Tensor(np.array([[0.3, 0.8, 1.2], [-0.6, 0.1, 2.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.array([1.0, 1.0, 1.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; pos_weight = Tensor(np.array([1.0, 1.0, 1.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.binary_cross_entropy_with_logits(logits, label, weight, pos_weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.3463612</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">pos_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pos_weight</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">bce_with_logits_loss_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">)(</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">bce_with_logits_loss_op</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">pos_weight</span><span class="p">)</span></div>


<div class="viewcode-block" id="dropout"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.dropout.html#mindspore.ops.dropout">[docs]</a><span class="nd">@_function_forbid_reuse</span>
<span class="k">def</span> <span class="nf">dropout</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    During training, randomly zeroes some of the elements of the input tensor</span>
<span class="sd">    with probability `p` from a Bernoulli distribution. It plays the role of reducing neuron correlation and</span>
<span class="sd">    avoid overfitting. And the return will be multiplied by :math:`\frac{1}{1-p}` during training.</span>
<span class="sd">    During the reasoning, this operation returns the same Tensor as the `x`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input Tensor of shape :math:`(*, N)`, with data type of float16, float32 or float64.</span>
<span class="sd">        p (float, optional): The dropping rate, between 0 and 1, e.g. p = 0.1,</span>
<span class="sd">            means dropping out 10% of input units. Default: ``0.5`` .</span>
<span class="sd">        training (bool): Apply dropout if is True. Default: ``True``.</span>
<span class="sd">        seed (int, optional): Seed is used as entropy source for Random number engines generating pseudo-random numbers.</span>
<span class="sd">            Default: ``None`` , which will be treated as ``0`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        - **output** (Tensor) - Zeroed tensor, with the same shape and data type as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `p` is not a float.</span>
<span class="sd">        TypeError: If dtype of `input` is not float16, float32 or float64.</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(((20, 16), (50, 50)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.dropout(input, p=0.5)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 2)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_bool_const</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="s2">&quot;training&quot;</span><span class="p">,</span> <span class="s2">&quot;dropout&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">training</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">input</span>
    <span class="n">keep_prob</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">p</span>
    <span class="n">seed0</span><span class="p">,</span> <span class="n">seed1</span> <span class="o">=</span> <span class="n">_get_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">,</span> <span class="s2">&quot;dropout&quot;</span><span class="p">)</span>
    <span class="n">dropout_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">keep_prob</span><span class="o">=</span><span class="n">keep_prob</span><span class="p">,</span> <span class="n">Seed0</span><span class="o">=</span><span class="n">seed0</span><span class="p">,</span> <span class="n">Seed1</span><span class="o">=</span><span class="n">seed1</span><span class="p">)</span>
    <span class="n">dropout_op</span> <span class="o">=</span> <span class="n">_set_prim_op_user_data</span><span class="p">(</span><span class="n">dropout_op</span><span class="p">,</span> <span class="s2">&quot;random_cache&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">dropout_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="dropout1d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.dropout1d.html#mindspore.ops.dropout1d">[docs]</a><span class="k">def</span> <span class="nf">dropout1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    During training, randomly zeroes some channels of the input tensor with probability `p`</span>
<span class="sd">    from a Bernoulli distribution(For a 3-dimensional tensor with a shape of :math:`NCL`,</span>
<span class="sd">    the channel feature map refers to a 1-dimensional feature map with the shape of :math:`L`).</span>

<span class="sd">    For example, the :math:`j\_th` channel of the :math:`i\_th` sample in the batched input is a to-be-processed</span>
<span class="sd">    `1D` tensor input[i,j].</span>
<span class="sd">    Each channel will be zeroed out independently on every forward call which based on Bernoulli distribution</span>
<span class="sd">    probability `p`.</span>

<span class="sd">    The parper `Dropout: A Simple Way to Prevent Neural Networks from Overfitting</span>
<span class="sd">    &lt;http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf&gt;`_ mentioned this technology, And it is proved that</span>
<span class="sd">    it can effectively reduce over fitting and prevent neuronal coadaptation.</span>
<span class="sd">    For more details, refer to `Improving neural networks by preventing co-adaptation of feature detectors</span>
<span class="sd">    &lt;https://arxiv.org/pdf/1207.0580.pdf&gt;`_ .</span>

<span class="sd">    `dropout1d` can improve the independence between channel feature maps.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): A tensor with shape :math:`(N, C, L)` or :math:`(C, L)`, where `N` is the batch size, `C` is the</span>
<span class="sd">            number of channels, `L` is the feature length. The data type must be int8, int16, int32, int64, float16,</span>
<span class="sd">            float32 or float64.</span>
<span class="sd">        p (float, optional): The dropping probability of a channel, between 0 and 1, e.g. `p` = 0.8,</span>
<span class="sd">            which means an 80% chance of clearing. Default: ``0.5`` .</span>
<span class="sd">        training (bool, optional): Apply dropout if is True. Default: ``True`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, output, with the same shape and data type as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If the data type of `p` is not float.</span>
<span class="sd">        ValueError: If `p` is out of the range `[0.0, 1.0]`.</span>
<span class="sd">        ValueError: If `input` shape is not `2D` or `3D`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.random.randn(4, 3), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.dropout1d(input_x, 0.5)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (4, 3)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For dropout1d, &#39;p&#39; must be float, but got type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">p</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For dropout1d, the &#39;p&#39; must be a number in range [0, 1], but got </span><span class="si">{</span><span class="n">p</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For dropout1d, &#39;input&#39; must be Tensor, but got type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">check_bool_const</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="s2">&quot;training&quot;</span><span class="p">,</span> <span class="s2">&quot;dropout1d&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">training</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">input</span>
    <span class="n">dropout_2d_op</span> <span class="o">=</span> <span class="n">NN_OPS</span><span class="o">.</span><span class="n">Dropout2D</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">dropout_2d_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">dropout_2d_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For dropout1d, input shape should be 2D or 3D, but got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="dropout2d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.dropout2d.html#mindspore.ops.dropout2d">[docs]</a><span class="k">def</span> <span class="nf">dropout2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    During training, randomly zeroes some channels of the input tensor with probability `p`</span>
<span class="sd">    from a Bernoulli distribution(For a 4-dimensional tensor with a shape of :math:`NCHW`,</span>
<span class="sd">    the channel feature map refers to a 2-dimensional feature map with the shape of :math:`HW`).</span>

<span class="sd">    For example, the :math:`j\_th` channel of the :math:`i\_th` sample in the batched input is a to-be-processed</span>
<span class="sd">    `2D` tensor input[i,j].</span>
<span class="sd">    Each channel will be zeroed out independently on every forward call which based on Bernoulli distribution</span>
<span class="sd">    probability `p`.</span>
<span class="sd">    The parper `Dropout: A Simple Way to Prevent Neural Networks from Overfitting</span>
<span class="sd">    &lt;http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf&gt;`_ mentioned this technology, And it is proved that</span>
<span class="sd">    it can effectively reduce over fitting and prevent neuronal coadaptation.</span>
<span class="sd">    For more details, refer to `Improving neural networks by preventing co-adaptation of feature detectors</span>
<span class="sd">    &lt;https://arxiv.org/pdf/1207.0580.pdf&gt;`_ .</span>

<span class="sd">    `dropout2d` can improve the independence between channel feature maps.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): A `4D` tensor with shape :math:`(N, C, H, W)`, where `N` is the batch size, `C` is the number</span>
<span class="sd">            of channels, `H` is the feature height, and `W` is the feature width. The data type must be int8,</span>
<span class="sd">            int16, int32, int64, float16, float32 or float64.</span>
<span class="sd">        p (float): The dropping probability of a channel, between 0 and 1, e.g. `p` = 0.8,</span>
<span class="sd">            which means dropping out 80% of channels. Default: ``0.5`` .</span>
<span class="sd">        training(bool): If `training` is True, applying dropout, otherwise, not applying. Default: ``True`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, output, with the same shape and data type as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not int8, int16, int32, int64, float16, float32 or float64.</span>
<span class="sd">        TypeError: If the data type of `p` is not float.</span>
<span class="sd">        ValueError: If `p` is out of the range `[0.0, 1.0]`.</span>
<span class="sd">        ValueError: If `input` shape is not `4D`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.ones([2, 1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.dropout2d(input, 0.5)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 1, 2, 3)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_bool_const</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="s2">&quot;training&quot;</span><span class="p">,</span> <span class="s2">&quot;dropout2d&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">training</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">input</span>
    <span class="n">dropout_2d_op</span> <span class="o">=</span> <span class="n">NN_OPS</span><span class="o">.</span><span class="n">Dropout2D</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">dropout_2d_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="dropout3d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.dropout3d.html#mindspore.ops.dropout3d">[docs]</a><span class="k">def</span> <span class="nf">dropout3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    During training, randomly zeroes some channels of the input tensor</span>
<span class="sd">    with probability `p` from a Bernoulli distribution(For a 5-dimensional tensor</span>
<span class="sd">    with a shape of :math:`NCDHW`, the channel feature map refers to a 3-dimensional</span>
<span class="sd">    feature map with a shape of :math:`DHW`).</span>

<span class="sd">    For example, the :math:`j\_th` channel of the :math:`i\_th` sample in the batched input is a to-be-processed</span>
<span class="sd">    `3D` tensor input[i,j].</span>
<span class="sd">    Each channel will be zeroed out independently on every forward call which based on Bernoulli distribution</span>
<span class="sd">    probability `p`.</span>

<span class="sd">    `dropout3d` can improve the independence between channel feature maps.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): A `5D` tensor with shape :math:`(N, C, D, H, W)`, where `N` is the batch size, `C` is the number</span>
<span class="sd">            of channels, `D` is the feature depth, `H` is the feature height, and `W` is the feature width.</span>
<span class="sd">            The data type must be int8, int16, int32, int64, float16, float32 or float64.</span>
<span class="sd">        p (float): The dropping probability of a channel, between 0 and 1, e.g. `p` = 0.8,</span>
<span class="sd">            which means dropping out 80% of channels. Default: ``0.5`` .</span>
<span class="sd">        training(bool): If `training` is True, applying dropout, otherwise, not applying. Default: ``True`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, output, with the same shape and data type as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not int8, int16, int32, int64, float16, float32 or float64.</span>
<span class="sd">        TypeError: If the data type of `p` is not float.</span>
<span class="sd">        ValueError: If `p` is out of the range `[0.0, 1.0]`.</span>
<span class="sd">        ValueError: If `input` shape is not 5D.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.ones([2, 1, 2, 1, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.dropout3d(input, 0.5)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 1, 2, 1, 2)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_bool_const</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="s2">&quot;training&quot;</span><span class="p">,</span> <span class="s2">&quot;dropout3d&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">training</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">input</span>
    <span class="n">dropout_3d_op</span> <span class="o">=</span> <span class="n">NN_OPS</span><span class="o">.</span><span class="n">Dropout3D</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">dropout_3d_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span></div>


<span class="k">def</span> <span class="nf">_check_float_range_inc_neither</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="n">lower_limit</span><span class="p">,</span> <span class="n">upper_limit</span><span class="p">,</span> <span class="n">arg_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prim_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Method for checking whether input value is in float range inc neither.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="n">lower_limit</span><span class="p">,</span> <span class="n">upper_limit</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">INC_NEITHER</span><span class="p">,</span> <span class="n">arg_name</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_fractional_output_size_ratio</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">output_ratio</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Internal function, used to check whether fractional_max_pool can specify the output shape.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">output_ratio</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For </span><span class="si">{</span><span class="n">cls_name</span><span class="si">}</span><span class="s2">, &#39;output_size&#39; and &#39;output_ratio&#39; can not be None&quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;at the same time, but got </span><span class="si">{</span><span class="n">output_ratio</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">output_size</span><span class="si">}</span><span class="s2"> .&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">fractional_max_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_ratio</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                          <span class="n">_random_samples</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies the 2D FractionalMaxPool operation over `input`. The output Tensor shape can be determined by either</span>
<span class="sd">    `output_size` or `output_ratio`, and the step size is determined by `_random_samples`. `output_size` will take</span>
<span class="sd">    effect when `output_size` and `output_ratio` are set at the same time.</span>
<span class="sd">    And `output_size` and `output_ratio` can not be ``None`` at the same time.</span>

<span class="sd">    Refer to the paper `Fractional MaxPooling by Ben Graham &lt;https://arxiv.org/abs/1412.6071&gt;`_  for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Tensor of shape :math:`(N, C, H_{in}, W_{in})` or :math:`(C, H_{in}, W_{in})`,</span>
<span class="sd">            with float16, float32, float64, int32, int64 data type.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of kernel used to take the maximum value,</span>
<span class="sd">            is an int number that represents height and width of the kernel, or a tuple</span>
<span class="sd">            of two int numbers that represent height and width respectively.</span>
<span class="sd">            The value must be a positive integer.</span>
<span class="sd">        output_size (Union[int, tuple[int]], optional): The shape of the target `output_size`,</span>
<span class="sd">            is an int number that represents height and width, or a tuple</span>
<span class="sd">            of two int numbers that represent height and width respectively.</span>
<span class="sd">            The value must be a positive integer.</span>
<span class="sd">            Default: ``None``.</span>
<span class="sd">        output_ratio (Union[float, tuple[float]], optional): The ratio of target output shape to input shape.</span>
<span class="sd">            Specifying the size of the output tensor by using a ratio of the input size.</span>
<span class="sd">            Data type: float16, float32, double, and value is between (0, 1).</span>
<span class="sd">            Default: ``None``.</span>
<span class="sd">        return_indices (bool, optional): Whether to return the indices of max value. Default: ``False``.</span>
<span class="sd">        _random_samples (Tensor, optional): The random step of fractional_max_pool2d, which is a 3D tensor.</span>
<span class="sd">            Tensor of data type: float16, float32, double, and value is between [0, 1).</span>
<span class="sd">            Supported shape :math:`(N, C, 2)` or :math:`(1, C, 2)`.</span>
<span class="sd">            Default: ``None``, the values of `_random_samples`</span>
<span class="sd">            will be randomly distributed using uniform distribution over an interval [0,1).</span>

<span class="sd">    Returns:</span>
<span class="sd">        - **y** (Tensor) - Has the same type as the `input`.</span>
<span class="sd">          Has the shape :math:`(N, C, H_{out}, W_{out})` or :math:`(C, H_{out}, W_{out})` ,</span>
<span class="sd">          where :math:`(H_{out}, W_{out})` = `output_size`</span>
<span class="sd">          or :math:`(H_{out}, W_{out})` = `output_ratio` * :math:`(H_{in}, W_{in})`.</span>

<span class="sd">        - **argmax** (Tensor) - The indices along with the outputs, which is a Tensor, with the same shape as the</span>
<span class="sd">          `y` and int64 data type. It will output only when `return_indices` is True.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If data type of `input` is not one of the following: float16, float32, float64, int32, int64.</span>
<span class="sd">        TypeError: If data type of `_random_samples` is not one of the following: float16, float32, float64.</span>
<span class="sd">        ValueError: If `kernel_size` is not a number and `kernel_size` is not a tuple of length 2.</span>
<span class="sd">        ValueError: If `output_size` is not a number and `output_size` is not a tuple of length 2.</span>
<span class="sd">        ValueError: If the sum of `kernel_size` , `output_size` and -1 is larger than the corresponding</span>
<span class="sd">                    dimension of `input`.</span>
<span class="sd">        ValueError: If the dimension of `_random_samples` is not 3.</span>
<span class="sd">        ValueError: if `output_size` and `output_ratio` are None at the same time.</span>
<span class="sd">        ValueError: If the first dimension size of `input` and `_random_samples` is not equal.</span>
<span class="sd">        ValueError: If the second dimension size of `input` and `_random_samples` is not equal.</span>
<span class="sd">        ValueError: If the third dimension size of `_random_samples` is not 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([0.3220, 0.9545, 0.7879, 0.0975, 0.3698,</span>
<span class="sd">        ...                            0.5135, 0.5740, 0.3435, 0.1895, 0.8764,</span>
<span class="sd">        ...                            0.9581, 0.4760, 0.9014, 0.8522, 0.3664,</span>
<span class="sd">        ...                            0.4980, 0.9673, 0.9879, 0.6988, 0.9022,</span>
<span class="sd">        ...                            0.9304, 0.1558, 0.0153, 0.1559, 0.9852]).reshape([1, 1, 5, 5]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; _random_samples = Tensor(np.array([[[0.8, 0.8]]]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; y, argmax = ops.fractional_max_pool2d(input, kernel_size=2, output_size=(2, 2),</span>
<span class="sd">        ...                                       _random_samples=_random_samples, return_indices=True)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[[[0.9545 0.8764]</span>
<span class="sd">           [0.9673 0.9852]]]]</span>
<span class="sd">        &gt;&gt;&gt; print(argmax)</span>
<span class="sd">        [[[[ 1  9]</span>
<span class="sd">           [16 24]]]]</span>
<span class="sd">        &gt;&gt;&gt; y, argmax = ops.fractional_max_pool2d(input, kernel_size=2, output_ratio=(0.5, 0.5),</span>
<span class="sd">        ...                                       _random_samples=_random_samples, return_indices=True)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[[[0.9545 0.8764]</span>
<span class="sd">           [0.9673 0.9852]]]]</span>
<span class="sd">        &gt;&gt;&gt; print(argmax)</span>
<span class="sd">        [[[[ 1  9]</span>
<span class="sd">           [16 24]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_fractional_output_size_ratio</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">output_ratio</span><span class="p">,</span> <span class="s2">&quot;fractional_max_pool2d&quot;</span><span class="p">)</span>
    <span class="n">_check_value_type</span><span class="p">(</span><span class="s2">&quot;return_indices&quot;</span><span class="p">,</span> <span class="n">return_indices</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="s2">&quot;fractional_max_pool2d&quot;</span><span class="p">)</span>
    <span class="n">dim_flag</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">dim_flag</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">if</span> <span class="n">_random_samples</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float_type</span><span class="p">:</span>
            <span class="n">_random_samples</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_random_samples</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_ratio</span><span class="p">,</span> <span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">)):</span>
            <span class="n">_check_value_type</span><span class="p">(</span><span class="s2">&quot;output_ratio&quot;</span><span class="p">,</span> <span class="n">output_ratio</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="s2">&quot;fractional_max_pool2d&quot;</span><span class="p">)</span>
            <span class="n">output_ratio</span> <span class="o">=</span> <span class="p">(</span><span class="n">output_ratio</span><span class="p">,</span> <span class="n">output_ratio</span><span class="p">)</span>
        <span class="n">_check_float_range_inc_neither</span><span class="p">(</span><span class="n">output_ratio</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s2">&quot;output_ratio[0]&quot;</span><span class="p">,</span> <span class="s2">&quot;fractional_max_pool2d&quot;</span><span class="p">)</span>
        <span class="n">_check_float_range_inc_neither</span><span class="p">(</span><span class="n">output_ratio</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s2">&quot;output_ratio[1]&quot;</span><span class="p">,</span> <span class="s2">&quot;fractional_max_pool2d&quot;</span><span class="p">)</span>
        <span class="n">output_size</span> <span class="o">=</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">output_ratio</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">int</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">output_ratio</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">fractional_max_pool</span> <span class="o">=</span> <span class="n">FractionalMaxPoolWithFixedKsize</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">fractional_max_pool</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">_random_samples</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dim_flag</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">output</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">return_indices</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">output</span>
    <span class="k">return</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>


<div class="viewcode-block" id="fractional_max_pool3d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.fractional_max_pool3d.html#mindspore.ops.fractional_max_pool3d">[docs]</a><span class="k">def</span> <span class="nf">fractional_max_pool3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_ratio</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                          <span class="n">_random_samples</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies the 3D FractionalMaxPool operation over `input`. The output Tensor shape can be determined by either</span>
<span class="sd">    `output_size` or `output_ratio`, and the step size is determined by `_random_samples`. `output_size` will take</span>
<span class="sd">    effect when `output_size` and `output_ratio` are set at the same time.</span>
<span class="sd">    And `output_size` and `output_ratio` can not be ``None`` at the same time.</span>

<span class="sd">    Refer to the paper `Fractional MaxPooling by Ben Graham &lt;https://arxiv.org/abs/1412.6071&gt;`_  for more details.</span>

<span class="sd">    The input and output data format can be &quot;NCDHW&quot;. N is the batch size, C is the number of channels,</span>
<span class="sd">    D the feature depth, H is the feature height, and W is the feature width.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input of FractionalMaxPool3d, which is a 4D or 5D tensor.</span>
<span class="sd">            Tensor of data type: float16, float32, double.</span>
<span class="sd">            Supported shape :math:`(N, C, D_{in}, H_{in}, W_{in})` or :math:`(C, D_{in}, H_{in}, W_{in})`.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of kernel used to take the maximum value,</span>
<span class="sd">            is an int number that represents depth, height and width of the kernel, or a tuple</span>
<span class="sd">            of three int numbers that represent depth, height and width respectively.</span>
<span class="sd">            The value must be a positive integer.</span>
<span class="sd">        output_size (Union[int, tuple[int]], optional): The shape of the target `output_size`,</span>
<span class="sd">            is an int number that represents depth, height and width, or a tuple</span>
<span class="sd">            of three int numbers that represent depth, height and width respectively.</span>
<span class="sd">            The value must be a positive integer.</span>
<span class="sd">            Default: ``None`` .</span>
<span class="sd">        output_ratio (Union[float, tuple[float]], optional): The ratio of target output shape to input shape.</span>
<span class="sd">            Specifying the size of the output tensor by using a ratio of the input size.</span>
<span class="sd">            Data type: float16, float32, double, and value is between (0, 1).</span>
<span class="sd">            Default: ``None`` .</span>
<span class="sd">        return_indices (bool, optional): Whether to return the indices of max value. Default: ``False`` .</span>
<span class="sd">        _random_samples (Tensor, optional): The random step of fractional_max_pool3d, which is a 3D tensor.</span>
<span class="sd">            Tensor of data type: float16, float32, double, and value is between [0, 1).</span>
<span class="sd">            Supported shape :math:`(N, C, 3)` or :math:`(1, C, 3)` . Default: ``None``, the values of `_random_samples`</span>
<span class="sd">            will be randomly distributed using uniform distribution over an interval [0,1).</span>

<span class="sd">    Returns:</span>
<span class="sd">        - **y** (Tensor) - A tensor, the output of FractionalMaxPool3d.</span>
<span class="sd">          Has the same data type with `input`.</span>
<span class="sd">          Has the shape :math:`(N, C, D_{out}, H_{out}, W_{out})` or :math:`(C, D_{out}, H_{out}, W_{out})` ,</span>
<span class="sd">          where :math:`(D_{out}, H_{out}, W_{out})` = `output_size`</span>
<span class="sd">          or :math:`(D_{out}, H_{out}, W_{out})` = `output_ratio` * :math:`(D_{in}, H_{in}, W_{in})` .</span>

<span class="sd">        - **argmax** (Tensor) - The indices along with the outputs, which is a Tensor, with the same shape as the</span>
<span class="sd">          `y` and int32 data type. It will output only when `return_indices` is True.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a 4D or 5D tensor.</span>
<span class="sd">        TypeError: If `_random_samples` is not a 3D tensor.</span>
<span class="sd">        TypeError: If data type of `input` is not float16, float32, double, int32, int64.</span>
<span class="sd">        TypeError: If dtype of `_random_samples` is not float16, float32, double.</span>
<span class="sd">        TypeError: If dtype of `argmax` is not int32, int64.</span>
<span class="sd">        TypeError: if _random_samples to have the different dtypes as input.</span>
<span class="sd">        ValueError: If `output_size` is a tuple and if `output_size` length is not 3.</span>
<span class="sd">        ValueError: If `kernel_size` is a tuple and if `kernel_size` length is not 3.</span>
<span class="sd">        ValueError: If numbers in `output_size` or `kernel_size` is not positive.</span>
<span class="sd">        ValueError: if `output_size` and `output_ratio` are None at the same time.</span>
<span class="sd">        ValueError: If the first dimension size of `input` and `_random_samples` is not equal.</span>
<span class="sd">        ValueError: If the second dimension size of `input` and `_random_samples` is not equal.</span>
<span class="sd">        ValueError: If the third dimension size of `_random_samples` is not 3.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16])</span>
<span class="sd">        ...            .reshape([1, 1, 2, 2, 4]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; _random_samples = Tensor(np.array([0.7, 0.7, 0.7]).reshape([1, 1, 3]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; output, argmax = ops.fractional_max_pool3d(x, kernel_size=(1, 1, 1), output_size=(1, 1, 3),</span>
<span class="sd">        ...                                            _random_samples=_random_samples, return_indices=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[[13. 14. 16.]]]]]</span>
<span class="sd">        &gt;&gt;&gt; print(argmax)</span>
<span class="sd">        [[[[[12 13 15]]]]]</span>
<span class="sd">        &gt;&gt;&gt; output, argmax = ops.fractional_max_pool3d(x, kernel_size=(1, 1, 1), output_ratio=(0.5, 0.5, 0.5),</span>
<span class="sd">        ...                                            _random_samples=_random_samples, return_indices=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[[13. 16.]]]]]</span>
<span class="sd">        &gt;&gt;&gt; print(argmax)</span>
<span class="sd">        [[[[[12 15]]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_fractional_output_size_ratio</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">output_ratio</span><span class="p">,</span> <span class="s2">&quot;fractional_max_pool3d&quot;</span><span class="p">)</span>
    <span class="n">_check_value_type</span><span class="p">(</span><span class="s2">&quot;return_indices&quot;</span><span class="p">,</span> <span class="n">return_indices</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="s2">&quot;fractional_max_pool3d&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_random_samples</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">n</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">4</span> <span class="k">else</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float_type</span><span class="p">:</span>
            <span class="n">_random_samples</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">],</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_random_samples</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">],</span> <span class="mi">3</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="n">_random_samples</span> <span class="o">=</span> <span class="n">_random_samples</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_ratio</span><span class="p">,</span> <span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">)):</span>
            <span class="n">_check_value_type</span><span class="p">(</span><span class="s2">&quot;output_ratio&quot;</span><span class="p">,</span> <span class="n">output_ratio</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="s2">&quot;fractional_max_pool3d&quot;</span><span class="p">)</span>
            <span class="n">output_ratio</span> <span class="o">=</span> <span class="p">(</span><span class="n">output_ratio</span><span class="p">,</span> <span class="n">output_ratio</span><span class="p">,</span> <span class="n">output_ratio</span><span class="p">)</span>
        <span class="n">_check_float_range_inc_neither</span><span class="p">(</span><span class="n">output_ratio</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s2">&quot;output_ratio[0]&quot;</span><span class="p">,</span> <span class="s2">&quot;fractional_max_pool3d&quot;</span><span class="p">)</span>
        <span class="n">_check_float_range_inc_neither</span><span class="p">(</span><span class="n">output_ratio</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s2">&quot;output_ratio[1]&quot;</span><span class="p">,</span> <span class="s2">&quot;fractional_max_pool3d&quot;</span><span class="p">)</span>
        <span class="n">_check_float_range_inc_neither</span><span class="p">(</span><span class="n">output_ratio</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s2">&quot;output_ratio[2]&quot;</span><span class="p">,</span> <span class="s2">&quot;fractional_max_pool3d&quot;</span><span class="p">)</span>
        <span class="n">output_size</span> <span class="o">=</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="n">output_ratio</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">int</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">output_ratio</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                       <span class="nb">int</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">output_ratio</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">_random_samples</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;fractional_max_pool3d&#39;, &#39;input&#39; and &#39;_random_samples&#39; must be same dtype, &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;but got Tensor[</span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">] and Tensor[</span><span class="si">{</span><span class="n">_random_samples</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">].&quot;</span><span class="p">)</span>
    <span class="n">fractional_max_pool</span> <span class="o">=</span> <span class="n">FractionalMaxPool3DWithFixedKsize</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">fractional_max_pool</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">_random_samples</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">return_indices</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">output</span>
    <span class="k">return</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></div>


<div class="viewcode-block" id="kl_div"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.kl_div.html#mindspore.ops.kl_div">[docs]</a><span class="k">def</span> <span class="nf">kl_div</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Kullback-Leibler divergence between the logits and the labels.</span>

<span class="sd">    For input tensors :math:`x` and :math:`target` with the same shape, the updating formulas of KLDivLoss algorithm are</span>
<span class="sd">    as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        L(x, target) = target \cdot (\log target - x)</span>

<span class="sd">    Then,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, target) = \begin{cases}</span>
<span class="sd">        L(x, target), &amp; \text{if reduction} = \text{&#39;none&#39;;}\\</span>
<span class="sd">        \operatorname{mean}(L(x, target)), &amp; \text{if reduction} = \text{&#39;mean&#39;;}\\</span>
<span class="sd">        \operatorname{sum}(L(x, target)) / x.\operatorname{shape}[0], &amp; \text{if reduction} = \text{&#39;batchmean&#39;;}\\</span>
<span class="sd">        \operatorname{sum}(L(x, target)),  &amp; \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    where :math:`x` represents `logits`.</span>
<span class="sd">    :math:`target` represents `labels`.</span>
<span class="sd">    :math:`\ell(x, target)` represents `output`.</span>

<span class="sd">    Note:</span>
<span class="sd">        - Currently it does not support float64 input on `Ascend`.</span>
<span class="sd">        - The output aligns with the mathematical definition of Kullback-Leibler divergence</span>
<span class="sd">          only when `reduction` is set to ``&#39;batchmean&#39;``.</span>

<span class="sd">    Args:</span>
<span class="sd">        logits (Tensor): The input Tensor. The data type must be float16, float32 or float64.</span>
<span class="sd">        labels (Tensor): The label Tensor which has the same shape and data type as `logits`.</span>
<span class="sd">        reduction (str): Specifies the reduction to be applied to the output.</span>
<span class="sd">            Its value must be one of ``&#39;none&#39;`` , ``&#39;mean&#39;`` , ``&#39;batchmean&#39;`` or ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the output elements will be summed.</span>
<span class="sd">            - ``&#39;batchmean&#39;``: the summed output elements divided by batch size.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or Scalar, if `reduction` is ``&#39;none&#39;``, then output is a tensor and has the same shape as `logits`.</span>
<span class="sd">        Otherwise, it is a scalar.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `reduction` is not a str.</span>
<span class="sd">        TypeError: If neither `logits` nor `labels` is a Tensor.</span>
<span class="sd">        TypeError: If dtype of `logits` or `labels` is not the supported type.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([0.2, 0.7, 0.1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([0., 1., 0.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = mindspore.ops.kl_div(logits, labels, &#39;mean&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        -0.23333333</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">reduction</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;kl_div&#39;, the &#39;reduction&#39; must be str and must be in &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;&#39;[&#39;none&#39;, &#39;mean&#39;, &#39;batchmean&#39;, &#39;sum&#39;]&#39;, but got &#39;</span><span class="si">{</span><span class="n">reduction</span><span class="si">}</span><span class="s2">&#39;.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;batchmean&#39;</span><span class="p">:</span>
        <span class="n">kl_div_sum</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">KLDivLoss</span><span class="p">)(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">)(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">shape_</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">kl_div_sum</span> <span class="o">/</span> <span class="n">batch_size</span>

    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
        <span class="n">kl_div_sum</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">KLDivLoss</span><span class="p">)(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">)(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">shape_</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
        <span class="n">total_size</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="n">shape</span><span class="p">:</span>
            <span class="n">total_size</span> <span class="o">=</span> <span class="n">total_size</span> <span class="o">*</span> <span class="n">dim</span>
        <span class="k">return</span> <span class="n">kl_div_sum</span> <span class="o">/</span> <span class="n">total_size</span>

    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">KLDivLoss</span><span class="p">)(</span><span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span></div>


<div class="viewcode-block" id="hardshrink"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.hardshrink.html#mindspore.ops.hardshrink">[docs]</a><span class="k">def</span> <span class="nf">hardshrink</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hard Shrink activation function. Calculates the output according to the input elements.</span>

<span class="sd">    The formula is defined as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{HardShrink}(x) =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">        x, &amp; \text{ if } x &gt; \lambda \\</span>
<span class="sd">        x, &amp; \text{ if } x &lt; -\lambda \\</span>
<span class="sd">        0, &amp; \text{ otherwise }</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    HShrink Activation Function Graph:</span>

<span class="sd">    .. image:: ../images/HShrink.png</span>
<span class="sd">        :align: center</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input of Hard Shrink with data type of float16 or float32.</span>
<span class="sd">        lambd (float, optional): The threshold :math:`\lambda` defined by the Hard Shrink formula.</span>
<span class="sd">            Default: ``0.5`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same data type and shape as the input `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `lambd` is not a float.</span>
<span class="sd">        TypeError: If `x` is not a tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[ 0.5,  1,  2.0], [0.0533,0.0776,-2.1233]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.hardshrink(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 0.      1.      2.    ]</span>
<span class="sd">        [ 0.      0.     -2.1233]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">hshrink_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">HShrink</span><span class="p">)(</span><span class="n">lambd</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">hshrink_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_check_axis_in_range</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">ndim</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Checks axes are with the bounds of ndim&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;The dims must be integers, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="o">-</span><span class="n">ndim</span> <span class="o">&lt;=</span> <span class="n">axis</span> <span class="o">&lt;</span> <span class="n">ndim</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The &#39;axis&#39; must be in the range of [-</span><span class="si">{</span><span class="n">ndim</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ndim</span><span class="si">}</span><span class="s2">), but got </span><span class="si">{</span><span class="n">axis</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">axis</span> <span class="o">%</span> <span class="n">ndim</span>


<span class="k">def</span> <span class="nf">_check_axis_valid</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">ndim</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks axes are valid given ndim, and returns axes that can be passed</span>
<span class="sd">    to the built-in operator (non-negative, int or tuple)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The parameter dims can not be None.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
        <span class="n">axes</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">_check_axis_in_range</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ndim</span><span class="p">),</span> <span class="n">axes</span><span class="p">))</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">el</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">el</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The element of parameter &#39;dims&#39; can not be duplicate, but got </span><span class="si">{</span><span class="n">axes</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">axes</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The parameter dims must be tuple of ints, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_get_flip_start</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">axes</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate the start index of flip&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">([</span><span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">axes</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndim</span><span class="p">)])</span>


<span class="k">def</span> <span class="nf">_get_flip_end</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">axes</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate the end index of flip&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">([</span><span class="o">-</span><span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">axes</span> <span class="k">else</span> <span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndim</span><span class="p">)])</span>


<span class="k">def</span> <span class="nf">_get_flip_strides</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">axes</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate the strides of flip&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">axes</span> <span class="k">else</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndim</span><span class="p">)])</span>


<span class="k">def</span> <span class="nf">_is_shape_empty</span><span class="p">(</span><span class="n">shp</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check whether shape contains zero&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shp</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">shp</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">shape_mul</span><span class="p">(</span><span class="n">shp</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>


<span class="k">def</span> <span class="nf">_check_input_tensor</span><span class="p">(</span><span class="n">arg_name</span><span class="p">,</span> <span class="o">*</span><span class="n">tensors</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check whether the input is tensor&quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="n">arg_name</span><span class="si">}</span><span class="s2">&#39;, the input must be Tensor, but got </span><span class="si">{</span><span class="n">ops</span><span class="o">.</span><span class="n">typeof</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">True</span>


<div class="viewcode-block" id="flip"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.flip.html#mindspore.ops.flip">[docs]</a><span class="k">def</span> <span class="nf">flip</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dims</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reverses the order of elements in a tensor along the given axis.</span>

<span class="sd">    The shape of the tensor is preserved, but the elements are reordered.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input tensor.</span>
<span class="sd">        dims (Union[list[int], tuple[int]]): Axis or axes along which to flip over.</span>
<span class="sd">            Flipping is performed on all of the axes specified in the tuple,</span>
<span class="sd">            If `dims` is a tuple of integers contains negative, it counts from the last to the first axis.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the entries of `dims` reversed.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input is not a tensor.</span>
<span class="sd">        ValueError: If `dims` is None.</span>
<span class="sd">        ValueError: If `dims` is not a list/tuple of ints.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; input = ms.Tensor(np.arange(1, 9).reshape((2, 2, 2)))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.flip(input, (0, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[6 5]</span>
<span class="sd">          [8 7]]</span>
<span class="sd">         [[2 1]</span>
<span class="sd">          [4 3]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">ReverseV2</span><span class="p">)(</span><span class="n">axis</span><span class="o">=</span><span class="n">dims</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span></div>


<div class="viewcode-block" id="flipud"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.flipud.html#mindspore.ops.flipud">[docs]</a><span class="k">def</span> <span class="nf">flipud</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Flips the elements of each column in the up/down direction, while preserving the rows of the input tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input array.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor after the flip.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input is not a tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; input = ms.Tensor(np.arange(1, 9).reshape((2, 2, 2)))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.flipud(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[5 6]</span>
<span class="sd">          [7 8]]</span>
<span class="sd">         [[1 2]</span>
<span class="sd">          [3 4]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">flip</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,))</span></div>


<div class="viewcode-block" id="fliplr"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.fliplr.html#mindspore.ops.fliplr">[docs]</a><span class="k">def</span> <span class="nf">fliplr</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Flips the elements of each row in the left/right direction, while preserving the columns of the input tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor after the flip.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input is not a tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; input = ms.Tensor(np.arange(1, 9).reshape((2, 2, 2)))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.fliplr(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[3 4]</span>
<span class="sd">          [1 2]]</span>
<span class="sd">         [[7 8]</span>
<span class="sd">          [5 6]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">flip</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span></div>


<div class="viewcode-block" id="is_floating_point"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.is_floating_point.html#mindspore.ops.is_floating_point">[docs]</a><span class="k">def</span> <span class="nf">is_floating_point</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Judge whether the data type of `input` is a floating point data type i.e., one of mindspore.float64,</span>
<span class="sd">    mindspore.float32, mindspore.float16.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Bool. If the dtype of `input` is a floating point data type, return ``True`` . Otherwise, return ``False`` .</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; x = ms.Tensor([1, 2, 3], ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = ms.Tensor([1, 2, 3], ms.int64)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.is_floating_point(x)</span>
<span class="sd">        &gt;&gt;&gt; output2 = ops.is_floating_point(y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        True</span>
<span class="sd">        &gt;&gt;&gt; print(output2)</span>
<span class="sd">        False</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">]</span></div>


<div class="viewcode-block" id="hardswish"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.hardswish.html#mindspore.ops.hardswish">[docs]</a><span class="k">def</span> <span class="nf">hardswish</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies hswish-type activation element-wise. The input is a Tensor with any valid shape.</span>

<span class="sd">    Hard swish is defined as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{hswish}(x_{i}) = x_{i} * \frac{ReLU6(x_{i} + 3)}{6}</span>

<span class="sd">    where :math:`x_i` is an element of the input Tensor.</span>

<span class="sd">    HSwish Activation Function Graph:</span>

<span class="sd">    .. image:: ../images/HSwish.png</span>
<span class="sd">        :align: center</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input to compute the Hard Swish.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same data type and shape as the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not int or float.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-1, -2, 0, 2, 1]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.hardswish(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-0.3333  -0.3333  0  1.666  0.6665]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">hardswish_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_is_dim_unknown</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="ow">and</span> <span class="o">-</span><span class="mi">2</span> <span class="ow">in</span> <span class="n">shape</span>


<span class="k">def</span> <span class="nf">_interploate_make_tuple</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">tuple_to_tensor_</span><span class="p">((</span><span class="n">rank</span><span class="p">,),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">fillv2_</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">tensor_to_tuple_</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>


<span class="k">def</span> <span class="nf">_interpolate_scale_factor_convert_size</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">scale_factor</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tuple_to_tensor_</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">tuple_to_tensor_</span><span class="p">(</span><span class="n">scale_factor</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">TruncateDiv</span><span class="p">()(</span><span class="n">t</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tensor_to_tuple_</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_interpolate_size_check_with_rank</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">input_rank</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">size</span><span class="p">)</span> <span class="o">!=</span> <span class="n">input_rank</span> <span class="o">-</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;For &#39;interpolate&#39;, &#39;input&#39; and &#39;size&#39; must have the same spatial dimensions, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;but got &#39;input&#39; is </span><span class="si">{</span><span class="n">input_rank</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">2</span><span class="si">}</span><span class="s2">D, &#39;size&#39; is </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">size</span><span class="p">)</span><span class="si">}</span><span class="s2">D&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_interpolate_scale_factor_check_with_rank</span><span class="p">(</span><span class="n">scale_factor</span><span class="p">,</span> <span class="n">input_rank</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">scale_factor</span><span class="p">)</span> <span class="o">!=</span> <span class="n">input_rank</span> <span class="o">-</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;For &#39;interpolate&#39;, &#39;input&#39; and &#39;scale_factor&#39; must have the same spatial dimensions, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;but got &#39;input&#39; is </span><span class="si">{</span><span class="n">input_rank</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">2</span><span class="si">}</span><span class="s2">D, &#39;scale_factor&#39; is </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">scale_factor</span><span class="p">)</span><span class="si">}</span><span class="s2">D&quot;</span>
        <span class="p">)</span>


<span class="k">def</span> <span class="nf">_interpolate_mode_check</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="n">supported_dict</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">or</span> <span class="n">mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">supported_dict</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;For &#39;interpolate&#39;, &#39;mode&#39; must be in &#39;</span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">supported_dict</span><span class="p">)</span><span class="si">}</span><span class="s2">&#39;, but got </span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>


<span class="k">def</span> <span class="nf">_interpolate_rank_check</span><span class="p">(</span><span class="n">input_rank</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">supported_dict</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">input_rank</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">supported_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">mode</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;For &#39;interpolate&#39;, </span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s2"> only support &#39;</span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">supported_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span><span class="w"> </span><span class="p">{}))</span><span class="si">}</span><span class="s2">&#39;D, but got </span><span class="si">{</span><span class="n">input_rank</span><span class="si">}</span><span class="s2">D&quot;</span>
        <span class="p">)</span>


<span class="k">def</span> <span class="nf">_interpolate_scale_factor_check</span><span class="p">(</span><span class="n">scale_factor</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">supported_dict</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">scale_factor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="s2">&quot;scale_factor&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">supported_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="n">mode</span><span class="p">,</span> <span class="p">{})</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">rank</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;For &#39;interpolate&#39;, &#39;scale_factor&#39; option cannot currently be set with the &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;mode = </span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s2"> and dim = </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">D.&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_interpolate_align_corners_mode_check</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">supported_dict</span><span class="p">):</span>
    <span class="k">if</span> <span class="s2">&quot;align_corners&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">supported_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="p">{})</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">rank</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;For &#39;interpolate&#39;, &#39;align_corners&#39; option cannot currently be set with the &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;mode = </span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s2">, and dim = </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">D&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="interpolate"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.interpolate.html#mindspore.ops.interpolate">[docs]</a><span class="k">def</span> <span class="nf">interpolate</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span>
                <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;nearest&quot;</span><span class="p">,</span>
                <span class="n">align_corners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">recompute_scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Samples the input Tensor to the given size or scale_factor by using one of the interpolate algorithms.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Tensor to be resized.</span>
<span class="sd">            Input tensor must be a 3-D, 4-D, or 5-D tensor with shape</span>
<span class="sd">            :math:`(N, C, [optional D], [optional H], W)` , with data type of float.</span>
<span class="sd">        size (Union[int, tuple[int], list[int]], optional): The target size.</span>
<span class="sd">            If size is a tuple or list, its length should be the same as the number of dimensions in input</span>
<span class="sd">            after removing the first two dimensions N, C.</span>
<span class="sd">            One and only one of size and scale_factor can be set to None. Default: ``None`` .</span>
<span class="sd">        scale_factor (Union[float, tuple[float], list[float]], optional): The scale factor of new size of the tensor.</span>
<span class="sd">            If scale_factor is a tuple or list, its length should be the same as the number of dimensions in input</span>
<span class="sd">            after removing the first two dimensions N, C.</span>
<span class="sd">            One and only one of size and scale_factor can be set to None. Default: ``None`` .</span>
<span class="sd">        mode (str): The sampling algorithm.</span>
<span class="sd">            One of &#39;nearest&#39;, &#39;linear&#39; (3D only), &#39;bilinear&#39; (4D only), &#39;trilinear&#39; (5D only), &#39;bicubic&#39; (4D only),</span>
<span class="sd">            &#39;area&#39;, &#39;nearest-exact&#39;(matches Scikit-Image and PIL nearest neighbours interpolation algorithms and fixes</span>
<span class="sd">            knows issues with `nearest`, 3D and 4D). Default: ``&quot;nearest&quot;`` .</span>

<span class="sd">        align_corners (bool): Whether to use corner alignment for coordinate mapping. Assuming a transformation is</span>
<span class="sd">            applied to the input Tensor along the x-axis, the specific calculation formula is as follows:</span>

<span class="sd">            .. code-block::</span>

<span class="sd">                ori_i = new_length != 1 ? new_i * (ori_length - 1) / (new_length - 1) : 0   # &#39;align_corners&#39; = True</span>

<span class="sd">                ori_i = new_length &gt; 1 ? (new_i + 0.5) * ori_length / new_length - 0.5 : 0  # &#39;align_corners&#39; = False</span>

<span class="sd">            Among them, :math:`ori\_length` and :math:`new\_length` represent the length of the Tensor before and after</span>
<span class="sd">            transformation along the x-axis respectively; :math:`new\_i` represents the coordinate of the i-th element</span>
<span class="sd">            along the x-axis after transformation; :math:`ori\_i` represents</span>
<span class="sd">            the corresponding coordinate of the original</span>
<span class="sd">            data along the x-axis.</span>

<span class="sd">            This is only valid for ``&#39;linear&#39;``, ``&#39;bilinear&#39;``, or ``&#39;bicubic&#39;`` modes. Default: ``False`` .</span>
<span class="sd">        recompute_scale_factor (bool, optional): Recalculate `scale_factor`.</span>
<span class="sd">            If True, the parameter `size` will be calculated using the value of the `scale_factor`,</span>
<span class="sd">            and finally scaled using the value of `size`.</span>
<span class="sd">            If False, the value of `size` or `scale_factor` will be used for direct interpolation. Default: ``None`` .</span>

<span class="sd">    .. note::</span>
<span class="sd">        The &#39;nearest-exact&#39; mode is the same as the nearest-neighbor interpolation algorithm used in</span>
<span class="sd">        scikit-image and PIL. The &#39;nearest&#39; mode produces the same results as the INTER_NEAREST interpolation</span>
<span class="sd">        algorithm used in OpenCV.</span>

<span class="sd">    Args Support List and Supported Platforms:</span>

<span class="sd">    +---------------+-----------+---------------+--------------+----------------+</span>
<span class="sd">    | mode          | input.dim | align_corners | scale_factor | device         |</span>
<span class="sd">    +===============+===========+===============+==============+================+</span>
<span class="sd">    | nearest       | 3         | \-            |             | Ascend,GPU,CPU |</span>
<span class="sd">    +---------------+-----------+---------------+--------------+----------------+</span>
<span class="sd">    |               | 4         | \-            |             | Ascend,GPU,CPU |</span>
<span class="sd">    +---------------+-----------+---------------+--------------+----------------+</span>
<span class="sd">    |               | 5         | \-            |             | Ascend,GPU,CPU |</span>
<span class="sd">    +---------------+-----------+---------------+--------------+----------------+</span>
<span class="sd">    | linear        | 3         |              |             | Ascend,GPU,CPU |</span>
<span class="sd">    +---------------+-----------+---------------+--------------+----------------+</span>
<span class="sd">    | bilinear      | 4         |              |             | Ascend,GPU,CPU |</span>
<span class="sd">    +---------------+-----------+---------------+--------------+----------------+</span>
<span class="sd">    | bicubic       | 4         |              |             | Ascend,GPU,CPU |</span>
<span class="sd">    +---------------+-----------+---------------+--------------+----------------+</span>
<span class="sd">    | area          | 3         | \-            |             | Ascend,GPU,CPU |</span>
<span class="sd">    +---------------+-----------+---------------+--------------+----------------+</span>
<span class="sd">    |               | 4         | \-            |             | Ascend,GPU,CPU |</span>
<span class="sd">    +---------------+-----------+---------------+--------------+----------------+</span>
<span class="sd">    |               | 5         | \-            |             | Ascend,GPU,CPU |</span>
<span class="sd">    +---------------+-----------+---------------+--------------+----------------+</span>
<span class="sd">    | nearest-exact | 3         | \-            |             | Ascend,CPU     |</span>
<span class="sd">    +---------------+-----------+---------------+--------------+----------------+</span>
<span class="sd">    |               | 4         | \-            |             | Ascend,CPU     |</span>
<span class="sd">    +---------------+-----------+---------------+--------------+----------------+</span>
<span class="sd">    | trilinear     | 5         |              |             | Ascend,GPU,CPU |</span>
<span class="sd">    +---------------+-----------+---------------+--------------+----------------+</span>

<span class="sd">    - `-` indicates that there is no such parameter.</span>
<span class="sd">    - `` indicates that this parameter is not currently supported.</span>
<span class="sd">    - `` indicates that this parameter is supported.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, resized, whose dimensions and dtype are the same as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: `input` is not a Tensor.</span>
<span class="sd">        ValueError: Both `size` and `scale_factor` are not empty.</span>
<span class="sd">        ValueError: Both `size` and `scale_factor` are empty.</span>
<span class="sd">        ValueError: When `size` is a tuple or list, its length is not equal to `input.ndim - 2`.</span>
<span class="sd">        ValueError: When `scale_factor` is a tuple or list, its length is not equal to `input.ndim - 2`.</span>
<span class="sd">        ValueError: `mode` is not in the list of supported modes.</span>
<span class="sd">        ValueError: `input.ndim` is not in the list of supported dimensions for the corresponding mode.</span>
<span class="sd">        ValueError: `size` is not empty, `recompute_scale_factor` is not empty.</span>
<span class="sd">        ValueError: `scale_factor` is not in the corresponding list of supported values.</span>
<span class="sd">        ValueError: `align_corners` is not in the corresponding list of supported values.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor([[[1, 2, 3], [4, 5, 6]]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.interpolate(input, size=(6,), mode=&#39;nearest&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">            [[[1. 1. 2. 2. 3. 3.]</span>
<span class="sd">              [4. 4. 5. 5. 6. 6.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">run_nearest</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># 3D 4D use ResizeNearestNeighborV2, 5D use UpsampleNearest3D</span>
        <span class="n">x_rank</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">x_rank</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">t1</span> <span class="o">=</span> <span class="n">seq</span><span class="o">.</span><span class="n">TupleToTensor</span><span class="p">()(</span><span class="n">size</span><span class="p">[:</span><span class="mi">1</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
            <span class="n">t2</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
            <span class="n">size</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">t1</span><span class="p">,</span> <span class="n">t2</span><span class="p">])</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ResizeNearestNeighborV2</span><span class="p">)()(</span>
                <span class="n">x</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Squeeze</span><span class="p">)(</span><span class="o">-</span><span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">x_rank</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">size</span> <span class="o">=</span> <span class="n">seq</span><span class="o">.</span><span class="n">TupleToTensor</span><span class="p">()(</span><span class="n">size</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ResizeNearestNeighborV2</span><span class="p">)()(</span>
                <span class="n">x</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">UpsampleNearest3D</span><span class="p">)()(</span><span class="n">x</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">scale_factor</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">run_linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">coordinate_transformation_mode</span> <span class="o">=</span> <span class="s2">&quot;align_corners&quot;</span> <span class="k">if</span> <span class="n">align_corners</span> <span class="k">else</span> <span class="s2">&quot;half_pixel&quot;</span>
        <span class="n">resize</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span>
            <span class="n">P</span><span class="o">.</span><span class="n">image_ops</span><span class="o">.</span><span class="n">ResizeLinear1D</span><span class="p">)(</span><span class="n">coordinate_transformation_mode</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">resize</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">run_bilinear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">resize</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ResizeBilinearV2</span><span class="p">)(</span><span class="n">align_corners</span><span class="p">,</span>
                                                     <span class="ow">not</span> <span class="n">align_corners</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">resize</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">run_trilinear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">resize</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span>
            <span class="n">P</span><span class="o">.</span><span class="n">nn_ops</span><span class="o">.</span><span class="n">UpsampleTrilinear3D</span><span class="p">)(</span><span class="n">align_corners</span><span class="o">=</span><span class="n">align_corners</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">resize</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">scale_factor</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">run_bicubic</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">resize</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">image_ops</span><span class="o">.</span><span class="n">ResizeBicubic</span><span class="p">)(</span>
            <span class="n">align_corners</span><span class="o">=</span><span class="n">align_corners</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="o">=</span><span class="ow">not</span> <span class="n">align_corners</span><span class="p">)</span>
        <span class="n">size</span> <span class="o">=</span> <span class="n">seq</span><span class="o">.</span><span class="n">TupleToTensor</span><span class="p">()(</span><span class="n">size</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">resize</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">run_area</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">x_rank</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">x_rank</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">adaptive_avg_pool1d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">size</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">elif</span> <span class="n">x_rank</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">adaptive_avg_pool2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">size</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">adaptive_avg_pool3d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">size</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">run_nearest_exact</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">x_rank</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">x_rank</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">size</span> <span class="o">=</span> <span class="n">seq</span><span class="o">.</span><span class="n">TupleToTensor</span><span class="p">()((</span><span class="n">size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
            <span class="c1"># For impl of nearest 3D use 4D.</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">resize</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ResizeNearestNeighborV2</span><span class="p">)(</span>
                <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">half_pixel_centers</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">resize</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Squeeze</span><span class="p">)(</span><span class="o">-</span><span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">x_rank</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="n">size</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">scalar_to_tensor</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
                <span class="n">size</span> <span class="o">=</span> <span class="n">seq</span><span class="o">.</span><span class="n">TupleToTensor</span><span class="p">()(</span><span class="n">size</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">size</span> <span class="o">=</span> <span class="n">seq</span><span class="o">.</span><span class="n">ListToTensor</span><span class="p">()(</span><span class="n">size</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
            <span class="n">resize</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ResizeNearestNeighborV2</span><span class="p">)(</span>
                <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">half_pixel_centers</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">resize</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="n">supported_dict</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;nearest&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="mi">3</span><span class="p">:</span> <span class="p">(),</span>
            <span class="mi">4</span><span class="p">:</span> <span class="p">(),</span>
            <span class="mi">5</span><span class="p">:</span> <span class="p">(</span><span class="s2">&quot;scale_factor&quot;</span><span class="p">,)</span>
        <span class="p">},</span>
        <span class="s2">&quot;linear&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="mi">3</span><span class="p">:</span> <span class="p">(</span><span class="s2">&quot;align_corners&quot;</span><span class="p">,)</span>
        <span class="p">},</span>
        <span class="s2">&quot;bilinear&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="mi">4</span><span class="p">:</span> <span class="p">(</span><span class="s2">&quot;align_corners&quot;</span><span class="p">,)</span>
        <span class="p">},</span>
        <span class="s2">&quot;bicubic&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="mi">4</span><span class="p">:</span> <span class="p">(</span><span class="s2">&quot;align_corners&quot;</span><span class="p">,)</span>
        <span class="p">},</span>
        <span class="s2">&quot;area&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="mi">3</span><span class="p">:</span> <span class="p">(</span><span class="s2">&quot;scale_factor&quot;</span><span class="p">,),</span>
            <span class="mi">4</span><span class="p">:</span> <span class="p">(</span><span class="s2">&quot;scale_factor&quot;</span><span class="p">,),</span>
            <span class="mi">5</span><span class="p">:</span> <span class="p">(</span><span class="s2">&quot;scale_factor&quot;</span><span class="p">,)</span>
        <span class="p">},</span>
        <span class="s2">&quot;nearest-exact&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="mi">3</span><span class="p">:</span> <span class="p">(),</span>
            <span class="mi">4</span><span class="p">:</span> <span class="p">()</span>
        <span class="p">},</span>
        <span class="s2">&quot;trilinear&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="mi">5</span><span class="p">:</span> <span class="p">(</span>
                <span class="s2">&quot;align_corners&quot;</span><span class="p">,</span>
                <span class="s2">&quot;scale_factor&quot;</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">},</span>
    <span class="p">}</span>
    <span class="n">resize_func</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;nearest&quot;</span><span class="p">:</span> <span class="n">run_nearest</span><span class="p">,</span>
        <span class="s2">&quot;linear&quot;</span><span class="p">:</span> <span class="n">run_linear</span><span class="p">,</span>
        <span class="s2">&quot;bilinear&quot;</span><span class="p">:</span> <span class="n">run_bilinear</span><span class="p">,</span>
        <span class="s2">&quot;bicubic&quot;</span><span class="p">:</span> <span class="n">run_bicubic</span><span class="p">,</span>
        <span class="s2">&quot;trilinear&quot;</span><span class="p">:</span> <span class="n">run_trilinear</span><span class="p">,</span>
        <span class="s2">&quot;area&quot;</span><span class="p">:</span> <span class="n">run_area</span><span class="p">,</span>
        <span class="s2">&quot;nearest-exact&quot;</span><span class="p">:</span> <span class="n">run_nearest_exact</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;For &#39;interpolate&#39;, &#39;input&#39; must be a tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="n">size</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scale_factor</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="n">scale_factor</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">scale_factor</span><span class="p">)</span>

    <span class="n">rank</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">dim_unknown</span> <span class="o">=</span> <span class="n">_is_dim_unknown</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>

    <span class="c1"># check for size and scale_factor</span>
    <span class="k">if</span> <span class="n">size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">scale_factor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;For &#39;interpolate&#39;, &#39;size&#39; and &#39;scale_factor&#39; cannot be set simultaneously&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
            <span class="n">check_positive_int_sequence_const</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="s2">&quot;size&quot;</span><span class="p">,</span> <span class="s2">&quot;interpolate&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">dim_unknown</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
                <span class="n">_interpolate_size_check_with_rank</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">rank</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">check_positive_int_const</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="s2">&quot;size&quot;</span><span class="p">,</span> <span class="s2">&quot;interpolate&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">dim_unknown</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
                <span class="n">size</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">([</span><span class="n">size</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">rank</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">size</span> <span class="o">=</span> <span class="n">_interploate_make_tuple</span><span class="p">(</span><span class="n">rank</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">scale_factor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scale_factor</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
            <span class="n">check_positive_float_sequence_const</span><span class="p">(</span><span class="n">scale_factor</span><span class="p">,</span> <span class="s2">&quot;scale_factor&quot;</span><span class="p">,</span>
                                                <span class="s2">&quot;interpolate&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">dim_unknown</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
                <span class="n">_interpolate_scale_factor_check_with_rank</span><span class="p">(</span><span class="n">scale_factor</span><span class="p">,</span> <span class="n">rank</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">check_positive_float_const</span><span class="p">(</span><span class="n">scale_factor</span><span class="p">,</span> <span class="s2">&quot;scale_factor&quot;</span><span class="p">,</span>
                                       <span class="s2">&quot;interpolate&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">dim_unknown</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
                <span class="n">scale_factor</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">([</span><span class="n">scale_factor</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">rank</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">scale_factor</span> <span class="o">=</span> <span class="n">_interploate_make_tuple</span><span class="p">(</span><span class="n">rank</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span> <span class="n">scale_factor</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;For &#39;interpolate&#39;, &#39;size&#39; and &#39;scale_factor&#39; cannot be both empty&quot;</span>
        <span class="p">)</span>

    <span class="c1"># rank check</span>
    <span class="n">_interpolate_mode_check</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="n">supported_dict</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dim_unknown</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
        <span class="n">_interpolate_rank_check</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">supported_dict</span><span class="p">)</span>

    <span class="c1"># &quot;area&quot; mode always requires an explicit size rather than scale factor.</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;area&quot;</span> <span class="ow">and</span> <span class="n">size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">recompute_scale_factor</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="c1"># recompute_scale_factor</span>
    <span class="k">if</span> <span class="n">recompute_scale_factor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">recompute_scale_factor</span><span class="p">:</span>
        <span class="n">check_bool_const</span><span class="p">(</span><span class="n">recompute_scale_factor</span><span class="p">,</span> <span class="s2">&quot;recompute_scale_factor&quot;</span><span class="p">,</span>
                         <span class="s2">&quot;interpolate&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;For &#39;interpolate&#39;, it is incorrect to set &#39;recompute_scale_factor&#39; to True&quot;</span>
                <span class="s2">&quot; after specifying an explicit &#39;size&#39;.&quot;</span><span class="p">)</span>
        <span class="n">size</span> <span class="o">=</span> <span class="n">_interpolate_scale_factor_convert_size</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">scale_factor</span><span class="p">)</span>
        <span class="n">scale_factor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">dim_unknown</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="n">_interpolate_scale_factor_check</span><span class="p">(</span><span class="n">scale_factor</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span>
                                            <span class="n">supported_dict</span><span class="p">)</span>

    <span class="c1"># align_corners</span>
    <span class="k">if</span> <span class="n">align_corners</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">check_bool_const</span><span class="p">(</span><span class="n">align_corners</span><span class="p">,</span> <span class="s2">&quot;align_corners&quot;</span><span class="p">,</span> <span class="s2">&quot;interpolate&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">dim_unknown</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="n">_interpolate_align_corners_mode_check</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">supported_dict</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">align_corners</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">return</span> <span class="n">resize_func</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">mode</span><span class="p">)(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">,</span> <span class="n">scale_factor</span><span class="p">)</span></div>


<div class="viewcode-block" id="upsample"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.upsample.html#mindspore.ops.upsample">[docs]</a><span class="k">def</span> <span class="nf">upsample</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;nearest&quot;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">recompute_scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Alias for :func:`mindspore.ops.interpolate` .</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">interpolate</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">scale_factor</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">,</span> <span class="n">recompute_scale_factor</span><span class="p">)</span></div>


<div class="viewcode-block" id="softsign"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.softsign.html#mindspore.ops.softsign">[docs]</a><span class="k">def</span> <span class="nf">softsign</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    SoftSign activation function.</span>

<span class="sd">    The function is shown as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{SoftSign}(x) = \frac{x}{1 + |x|}</span>

<span class="sd">    Softsign Activation Function Graph:</span>

<span class="sd">    .. image:: ../images/Softsign.png</span>
<span class="sd">        :align: center</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Tensor of shape :math:`(N, *)`, where :math:`*` means, any number of</span>
<span class="sd">            additional dimensions, with float16 or float32 data type.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0, -1, 2, 30, -30]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.softsign(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0.        -0.5         0.6666667  0.9677419 -0.9677419]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">softsign_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="soft_margin_loss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.soft_margin_loss.html#mindspore.ops.soft_margin_loss">[docs]</a><span class="k">def</span> <span class="nf">soft_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the soft margin loss of input and target.</span>

<span class="sd">    Creates a criterion that optimizes a two-class classification</span>
<span class="sd">    logistic loss between input tensor :math:`x` and target tensor :math:`y`</span>
<span class="sd">    (containing 1 or -1).</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss}(x, y) = \sum_i \frac{\log(1 + \exp(-y[i]*x[i]))}{\text{x.nelement}()}</span>

<span class="sd">    where :math:`x.nelement()` is the number of elements of :math:`x`.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Predict data. Data type must be float16 or float32.</span>
<span class="sd">        target (Tensor): Ground truth data, with the same type and shape as `input`.</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the output elements will be summed.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor or Scalar. If `reduction` is ``&#39;none&#39;``, its shape is the same as `input`.</span>
<span class="sd">        Otherwise, a scalar value will be returned.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` or `target` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` or `target` is neither float16 nor float32.</span>
<span class="sd">        ValueError: If shape of `input` is not the same as that of `target`.</span>
<span class="sd">        ValueError: If `reduction` is not one of ``&#39;none&#39;``, ``&#39;mean&#39;`` or ``&#39;sum&#39;``.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([[0.3, 0.7], [0.5, 0.5]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([[-1, 1], [1, -1]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.soft_margin_loss(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.6764238</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">soft_margin_loss_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">SoftMarginLoss</span><span class="p">)(</span><span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">soft_margin_loss_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span></div>


<div class="viewcode-block" id="softmax"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.softmax.html#mindspore.ops.softmax">[docs]</a><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies the Softmax operation to the input tensor on the specified axis.</span>
<span class="sd">    Suppose a slice in the given axis :math:`axis`, then for each element :math:`input_i`,</span>
<span class="sd">    the Softmax function is shown as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(input_i) = \frac{\exp(input_i)}{\sum_{j = 0}^{N-1}\exp(input_j)},</span>

<span class="sd">    where :math:`N` is the length of the tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Tensor of shape :math:`(N, *)`, where :math:`*` means, any number of</span>
<span class="sd">          additional dimensions, with float16 or float32 data type.</span>
<span class="sd">        axis (int, optional): The axis to perform the Softmax operation. Default: ``-1`` .</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): When set, `input` will be converted to the specified type,</span>
<span class="sd">            `dtype`, before execution, and dtype of returned Tensor will also be `dtype`. Default: ``None`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([1, 2, 3, 4, 5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.softmax(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.01165623 0.03168492 0.08612854 0.23412167 0.6364086 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">type_axis</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot; the type of &#39;axis&#39; must be &#39;int&#39;, but got &#39;</span><span class="si">{</span><span class="n">axis</span><span class="si">}</span><span class="s2">&#39; with type &#39;</span><span class="si">{</span><span class="n">type_axis</span><span class="si">}</span><span class="s2">&#39;.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="n">softmax_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Softmax</span><span class="p">)(</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">softmax_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="softmin"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.softmin.html#mindspore.ops.softmin">[docs]</a><span class="k">def</span> <span class="nf">softmin</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies the Softmin operation to the input tensor on the specified axis.</span>
<span class="sd">    Suppose a slice in the given axis :math:`x`, then for each element :math:`x_i`,</span>
<span class="sd">    the Softmin function is shown as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(x_i) = \frac{\exp(-x_i)}{\sum_{j = 0}^{N-1}\exp(-x_j)},</span>

<span class="sd">    where :math:`N` is the length of the tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (Union[int, tuple[int]], optional): The axis to perform the Softmin operation. Default: ``-1`` .</span>
<span class="sd">        x (Tensor): Tensor of shape :math:`(N, *)`, where :math:`*` means, any number of</span>
<span class="sd">          additional dimensions, with float16 or float32 data type.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): When set, `x` will be converted to the specified type,</span>
<span class="sd">            `dtype`, before execution, and dtype of returned Tensor will also be `dtype`. Default: ``None`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is not an int or a tuple.</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 nor float32.</span>
<span class="sd">        ValueError: If `axis` is a tuple whose length is less than 1.</span>
<span class="sd">        ValueError: If `axis` is a tuple whose elements are not all in range [-len(logits.shape), len(logits.shape)).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-1, -2, 0, 2, 1]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.softmin(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.2341  0.636  0.0862  0.01165  0.03168 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="n">softmax_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Softmax</span><span class="p">)(</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">softmax_</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="o">*</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="softshrink"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.softshrink.html#mindspore.ops.softshrink">[docs]</a><span class="k">def</span> <span class="nf">softshrink</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies the Softshrink function element-wise.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{SoftShrink}(x) =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">        x - \lambda, &amp; \text{ if } x &gt; \lambda \\</span>
<span class="sd">        x + \lambda, &amp; \text{ if } x &lt; -\lambda \\</span>
<span class="sd">        0, &amp; \text{ otherwise }</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    SoftShrink Activation Function Graph:</span>

<span class="sd">    .. image:: ../images/Softshrink.png</span>
<span class="sd">        :align: center</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input of soft shrink with data type of float16 or float32.</span>
<span class="sd">        lambd (float): The :math:`\lambda` must be no less than zero. Default: ``0.5`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and data type as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `lambd` is not a float.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 nor float32.</span>
<span class="sd">        ValueError: If `lambd` is less than 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[ 0.5297,  0.7871,  1.1754], [ 0.7836,  0.6218, -1.1542]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.softshrink(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 0.02979  0.287    0.676  ]</span>
<span class="sd">         [ 0.2837   0.1216  -0.6543 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">soft_shrink_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">SoftShrink</span><span class="p">)(</span><span class="n">lambd</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">soft_shrink_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">soft_shrink</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    `soft_shrink` is deprecated, please use `softshrink` instead.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;`soft_shrink` is deprecated, please use `softshrink` instead.&quot;</span><span class="p">)</span>
    <span class="n">soft_shrink_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">SoftShrink</span><span class="p">)(</span><span class="n">lambd</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">soft_shrink_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">softplus</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span> <span class="c1"># pylint:disable=redefined-outer-name</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies softplus function to `input` element-wise.</span>

<span class="sd">    The softplus function is shown as follows, x is the element of `input` :</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{output} = \frac{1}{beta}\log(1 + \exp(\text{beta * x}))</span>

<span class="sd">    When :math:`input * beta &gt; threshold`, the implementation converts to the linear function</span>
<span class="sd">    to ensure numerical stability.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor) - Tensor of any dimension.</span>
<span class="sd">            Supported dtypes:</span>

<span class="sd">            - GPU/CPU: float16, float32, float64.</span>
<span class="sd">            - Ascend: float16, float32.</span>

<span class="sd">        beta (int, optional) - The :math:`\beta` value in softplus function. Default: ``1`` .</span>
<span class="sd">        threshold (int, optional) - When :math:`input * beta &gt; threshold`, converting softplus to a linear function.</span>
<span class="sd">            Default: ``20`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as the `input` .</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If the dtype of `input` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``  ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([0.1, 0.2, 30, 25]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.softplus(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.7443967 0.79813886 30. 25.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">scaling_input</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">*</span> <span class="nb">input</span>
    <span class="n">op_output</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">beta</span><span class="p">)</span> <span class="o">*</span> <span class="n">softplus_</span><span class="p">(</span><span class="n">scaling_input</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">input</span> <span class="o">*</span> <span class="n">beta</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">op_output</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">softplus_ext</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span> <span class="c1"># pylint:disable=redefined-outer-name</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies softplus function to `input` element-wise.</span>

<span class="sd">    The softplus function is shown as follows, x is the element of `input` :</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{output} = \frac{1}{beta}\log(1 + \exp(\text{beta * x}))</span>

<span class="sd">    When :math:`input * beta &gt; threshold`, the implementation converts to the linear function</span>
<span class="sd">    to ensure numerical stability.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor) - Tensor of any dimension.</span>
<span class="sd">            Supported dtypes:</span>

<span class="sd">            - Ascend: float16, float32, bfloat16</span>

<span class="sd">        beta (number, optional) - The :math:`\beta` value in softplus function. Default: ``1`` .</span>
<span class="sd">        threshold (number, optional) - When :math:`input * beta &gt; threshold`, converting softplus to a linear function.</span>
<span class="sd">            Default: ``20`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as the `input` .</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If the dtype of `input` is not float16, float32, bfloat16.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, mint</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([0.1, 0.2, 30, 25]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = mint.softplus(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.74439657 0.7981388 30. 25.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">auto_generate</span><span class="o">.</span><span class="n">SoftplusExt</span><span class="p">)()(</span><span class="nb">input</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">threshold</span><span class="p">)</span>


<div class="viewcode-block" id="selu"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.selu.html#mindspore.ops.selu">[docs]</a><span class="k">def</span> <span class="nf">selu</span><span class="p">(</span><span class="n">input_x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Activation function SeLU (Scaled exponential Linear Unit).</span>

<span class="sd">    The activation function is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        E_{i} =</span>
<span class="sd">        scale *</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">        x_{i}, &amp;\text{if } x_{i} \geq 0; \cr</span>
<span class="sd">        \text{alpha} * (\exp(x_i) - 1), &amp;\text{otherwise.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    where :math:`alpha` and :math:`scale` are pre-defined constants(:math:`alpha=1.67326324`</span>
<span class="sd">    and :math:`scale=1.05070098`).</span>

<span class="sd">    See more details in `Self-Normalizing Neural Networks &lt;https://arxiv.org/abs/1706.02515&gt;`_.</span>

<span class="sd">    SeLU Activation Function Graph:</span>

<span class="sd">    .. image:: ../images/SeLU.png</span>
<span class="sd">        :align: center</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Tensor of any dimension,</span>
<span class="sd">            the data type is int8, int32, float16, float32, or float64 (CPU, GPU only).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as the `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `input_x` is not int8, int32, float16, float32, or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.selu(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-1.1113307 4.202804 -1.7575096]</span>
<span class="sd">        [ 2.101402 -1.7462534 9.456309 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">selu_</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="logsigmoid"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.logsigmoid.html#mindspore.ops.logsigmoid">[docs]</a><span class="k">def</span> <span class="nf">logsigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies logsigmoid activation element-wise. The input is a Tensor with any valid shape.</span>

<span class="sd">    Logsigmoid is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{logsigmoid}(x_{i}) = \log(\frac{1}{1 + \exp(-x_i)}),</span>

<span class="sd">    where :math:`x_{i}` is the element of the input.</span>

<span class="sd">    LogSigmoid Activation Function Graph:</span>

<span class="sd">    .. image:: ../images/LogSigmoid.png</span>
<span class="sd">        :align: center</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input of LogSigmoid with data type of float16 or float32.</span>
<span class="sd">          The shape is :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``  ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.logsigmoid(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-0.31326166 -0.12692806 -0.04858734]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">sigmoid_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">log_</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span></div>


<span class="k">def</span> <span class="nf">_check_dense_add_bias_shape</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">,</span> <span class="n">bias_shape</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check that the output has the correct shape after adding bias.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">input_shape</span> <span class="o">!=</span> <span class="n">output_shape</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For dense, the bias shape </span><span class="si">{</span><span class="n">bias_shape</span><span class="si">}</span><span class="s2"> does not match the input shape </span><span class="si">{</span><span class="n">input_shape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">check_dense_inputs_same_shape</span><span class="p">(</span><span class="n">input1_shape</span><span class="p">,</span> <span class="n">input2_shape</span><span class="p">,</span> <span class="n">prim_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;check bidense input Tensors&#39; shape&quot;&quot;&quot;</span>
    <span class="n">msg_prefix</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="n">prim_name</span><span class="si">}</span><span class="s2">&#39;, the&quot;</span> <span class="k">if</span> <span class="n">prim_name</span> <span class="k">else</span> <span class="s2">&quot;The&quot;</span>
    <span class="k">if</span> <span class="n">input1_shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">input2_shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">msg_prefix</span><span class="si">}</span><span class="s2"> dimensions except the last of &#39;input1&#39; must be same as &#39;input2&#39;, but got &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">input1_shape</span><span class="si">}</span><span class="s2"> of &#39;input1&#39; and </span><span class="si">{</span><span class="n">input2_shape</span><span class="si">}</span><span class="s2"> of &#39;input2&#39;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="bidense"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bidense.html#mindspore.ops.bidense">[docs]</a><span class="k">def</span> <span class="nf">bidense</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies bilinear dense connected layer for `input1` and `input2`. The bilinear dense function is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        output = x_{1}^{T}Ax_{2} + b</span>

<span class="sd">    :math:`x_{1}` represents `input1` , :math:`x_{2}` represents `input2` , :math:`A` represents `weight` ,</span>
<span class="sd">    :math:`b` represents `bias` .</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        input1 (Tensor): Input Tensor of shape :math:`(*, in1\_channels)`,</span>
<span class="sd">            where :math:`*` means any number of additional dimensions. All but the last dimension</span>
<span class="sd">            should be the same with `input2`.</span>
<span class="sd">        input2 (Tensor): Input Tensor of shape :math:`(*, in2\_channels)`,</span>
<span class="sd">            where :math:`*` means any number of additional dimensions. All but the last dimension</span>
<span class="sd">            should be the same with `input1`.</span>
<span class="sd">        weight (Tensor): The weight applied to the input1 and input2.</span>
<span class="sd">            The shape is :math:`(out\_channels, in1\_channels, in2\_channels)`.</span>
<span class="sd">        bias (Tensor, optional): Additive biases to the output.</span>
<span class="sd">            The shape is :math:`(out\_channels)` or :math:`()`. Defaults: ``None`` , the `bias` is 0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, shape :math:`(*, out\_channels)`, where :math:`*` means any number of additional dimensions.</span>
<span class="sd">        All but the last dimension should be the same with the input Tensors.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input1` is not Tensor.</span>
<span class="sd">        TypeError: If `input2` is not Tensor.</span>
<span class="sd">        TypeError: If `weight` is not Tensor.</span>
<span class="sd">        TypeError: If `bias` is not Tensor.</span>
<span class="sd">        ValueError: If dimensions except the last of &#39;input1&#39; are different from &#39;input2&#39; .</span>


<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``  ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input1 = mindspore.Tensor([[-1.1283, 1.2603],</span>
<span class="sd">        ...                            [0.0214, 0.7801],</span>
<span class="sd">        ...                            [-1.2086, 1.2849]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input2 = mindspore.Tensor([[-0.4631, 0.3238, 0.4201],</span>
<span class="sd">        ...                            [0.6215, -1.0910, -0.5757],</span>
<span class="sd">        ...                            [-0.7788, -0.0706, -0.7942]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = mindspore.Tensor([[[-0.3132, 0.9271, 1.1010],</span>
<span class="sd">        ...                             [0.6555, -1.2162, -0.2987]],</span>
<span class="sd">        ...                            [[1.0458, 0.5886, 0.2523],</span>
<span class="sd">        ...                             [-1.3486, -0.8103, -0.2080]],</span>
<span class="sd">        ...                            [[1.1685, 0.5569, -0.3987],</span>
<span class="sd">        ...                             [-0.4265, -2.6295, 0.8535]],</span>
<span class="sd">        ...                            [[0.6948, -1.1288, -0.6978],</span>
<span class="sd">        ...                             [0.3511, 0.0609, -0.1122]]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bidense(input1, input2, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-2.0612743 0.5581219 0.22383511 0.8667302]</span>
<span class="sd">         [1.4476739 0.12626505 1.6552988 0.21297503]</span>
<span class="sd">         [0.6003161 2.912046 0.5590313 -0.35449564]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s2">&quot;input1&quot;</span><span class="p">,</span> <span class="n">input1</span><span class="p">,</span> <span class="s2">&quot;bidense&quot;</span><span class="p">)</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s2">&quot;input2&quot;</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="s2">&quot;bidense&quot;</span><span class="p">)</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="s2">&quot;bidense&quot;</span><span class="p">)</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s2">&quot;bias&quot;</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="s2">&quot;bidense&quot;</span><span class="p">)</span>
    <span class="n">input1_shape</span> <span class="o">=</span> <span class="n">input1</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">input2_shape</span> <span class="o">=</span> <span class="n">input2</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">check_dense_inputs_same_shape</span><span class="p">(</span><span class="n">input1_shape</span><span class="p">,</span> <span class="n">input2_shape</span><span class="p">,</span> <span class="s2">&quot;bidense&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input1_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">input1</span> <span class="o">=</span> <span class="n">input1</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">input1_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">input2</span> <span class="o">=</span> <span class="n">input2</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">input2_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">input1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">matmul_</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">weight</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">input1_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">input2_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">input2</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">input1</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">bias</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">or</span> <span class="n">input2</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">bias</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;bidense&#39;, the dtype of &#39;bias&#39;, &#39;input1&#39; and &#39;input2&#39; must be the same,&quot;</span>
                            <span class="sa">f</span><span class="s2">&quot; but got </span><span class="si">{</span><span class="n">bias</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">input1</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">input2</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">bias_add_</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">bias</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">bias</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input1_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">output_shape</span> <span class="o">=</span> <span class="n">input1_shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">output_shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span></div>


<div class="viewcode-block" id="deformable_conv2d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.deformable_conv2d.html#mindspore.ops.deformable_conv2d">[docs]</a><span class="k">def</span> <span class="nf">deformable_conv2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilations</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                      <span class="n">deformable_groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">modulated</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Given 4D tensor inputs `x`, `weight` and `offsets`, compute a 2D deformable convolution. The deformable convolution</span>
<span class="sd">    operation can be expressed as follow:</span>

<span class="sd">    Deformable Convolution v1:</span>

<span class="sd">    .. math::</span>
<span class="sd">        y(p)=\sum_{k=1}^{K}w_{k}\cdot x(p+p_{k}+\Delta{p_{k}})</span>

<span class="sd">    Deformable Convolution v2:</span>

<span class="sd">    .. math::</span>
<span class="sd">        y(p)=\sum_{k=1}^{K}w_{k}\cdot x(p+p_{k}+\Delta{p_{k}})\cdot \Delta{m_{k}}</span>

<span class="sd">    Where :math:`\Delta{p_{k}}` and :math:`\Delta{m_{k}}` are the learnable offset and modulation scalar for the k-th</span>
<span class="sd">    location. For details, please refer to `Deformable ConvNets v2: More Deformable, Better Results</span>
<span class="sd">    &lt;https://arxiv.org/abs/1811.11168&gt;`_ and `Deformable Convolutional Networks &lt;https://arxiv.org/abs/1703.06211&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): A 4D tensor of input image. With the format &quot;NCHW&quot;,</span>
<span class="sd">            the shape is :math:`(N, C_{in}, H_{in}, W_{in})`. Dtype: float16 or float32.</span>
<span class="sd">        weight (Tensor): A 4D tensor of learnable filters. Must have the same type as `x`.</span>
<span class="sd">            The shape is :math:`(C_{out}, C_{in} / groups, H_{f}, W_{f})`.</span>
<span class="sd">        offsets (Tensor): A 4D tensor of x-y coordinates offset and mask. With the format &quot;NCHW&quot;,</span>
<span class="sd">            the shape is :math:`(batch, 3 * deformable\_groups * H_{f} * W_{f}, H_{out}, W_{out})`. Note the C dimension</span>
<span class="sd">            is stored in the order of (offset_x, offset_y, mask). Must have the same type as `x`.</span>
<span class="sd">        kernel_size (tuple[int]): A tuple of 2 integers. The size of kernel.</span>
<span class="sd">        strides (tuple[int]): A tuple of 4 integers. The stride of the sliding window for each dimension of</span>
<span class="sd">            input. The dimension order is interpreted according to the data format of `x`. The N and C dimensions must</span>
<span class="sd">            be set to 1.</span>
<span class="sd">        padding (tuple[int]): A tuple of 4 integers. The number of pixels to add to each (top, bottom, left,</span>
<span class="sd">            right) side of the input.</span>
<span class="sd">        bias (Tensor, optional): An 1D tensor of additive biases to the filter outputs.</span>
<span class="sd">            The shape is :math:`(C_{out})`. Default: ``None`` .</span>
<span class="sd">        dilations (tuple[int], optional): A tuple of 4 integers. The dilation factor for each dimension of input. The</span>
<span class="sd">            dimension order is interpreted according to the data format of `x`. The N and C dimensions must be set</span>
<span class="sd">            to 1. Default: ``(1, 1, 1, 1)`` .</span>
<span class="sd">        groups (int, optional): An integer of type int32. The number of blocked connections from input channels</span>
<span class="sd">            to output channels. In_channels and out_channels must both be divisible by `groups`. Default: ``1`` .</span>
<span class="sd">        deformable_groups (int, optional): An integer of type int32. The number of deformable group partitions.</span>
<span class="sd">            In_channels must be divisible by `deformable_groups`. Default: ``1`` .</span>
<span class="sd">        modulated (bool, optional): Specifies version of DeformableConv2D, True means v2, False means v1, currently</span>
<span class="sd">            only supports v2. Default: ``True`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, A 4D Tensor of output feature map. With the same type as `x`. With the format &quot;NCHW&quot;,</span>
<span class="sd">        the shape is :math:`(N, C_{out}, H_{out}, W_{out})`.</span>

<span class="sd">        .. math::</span>
<span class="sd">            \begin{array}{ll} \\</span>
<span class="sd">                H_{out} = \left \lfloor{\frac{H_{in} + padding[0] + padding[1] - (H_{f} - 1) \times</span>
<span class="sd">                \text{dilations[2]} - 1 }{\text{stride[0]}} + 1} \right \rfloor \\</span>
<span class="sd">                W_{out} = \left \lfloor{\frac{W_{in} + padding[2] + padding[3] - (W_{f} - 1) \times</span>
<span class="sd">                \text{dilations[3]} - 1 }{\text{stride[1]}} + 1} \right \rfloor \\</span>
<span class="sd">            \end{array}</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `strides`, `padding`, `kernel_size` or `dilations` is not a tuple with integer elements.</span>
<span class="sd">        TypeError: If `modulated` is not a bool.</span>
<span class="sd">        ValueError: If the tuple size of `strides`, `padding`, `kernel_size` or `dilations` is not expected.</span>
<span class="sd">        ValueError: The N or C dimensions of `strides` or `dilations` is not set to 1.</span>
<span class="sd">        ValueError: If `modulated` is not set to True.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones((4, 3, 10, 10)), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; kh, kw = 3, 3</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones((5, 3, kh, kw)), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; offsets = Tensor(np.ones((4, 3 * kh * kw, 8, 8)), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.deformable_conv2d(x, weight, offsets, (kh, kw), (1, 1, 1, 1), (0, 0, 0, 0))</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (4, 5, 8, 8)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">deformable_offsets</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">DeformableOffsets</span><span class="p">)(</span><span class="n">strides</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">dilations</span><span class="p">,</span> <span class="s2">&quot;NCHW&quot;</span><span class="p">,</span>
                                                                   <span class="n">deformable_groups</span><span class="p">,</span>
                                                                   <span class="n">modulated</span><span class="p">)</span>
    <span class="n">fm_offset</span> <span class="o">=</span> <span class="n">deformable_offsets</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">offsets</span><span class="p">)</span>
    <span class="n">weight_shape</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">out_channel</span> <span class="o">=</span> <span class="n">weight_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">strides_conv</span> <span class="o">=</span> <span class="p">(</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">conv</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">)(</span><span class="n">out_channel</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">strides_conv</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">conv</span><span class="p">(</span><span class="n">fm_offset</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">bias_add_</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span></div>


<div class="viewcode-block" id="pdist"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.pdist.html#mindspore.ops.pdist">[docs]</a><span class="k">def</span> <span class="nf">pdist</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">2.0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the distance between every pair of row vectors in</span>
<span class="sd">    the input using the p-norm. If the input `input` is a 2D Tensor with shape :math:`(N, M)`,</span>
<span class="sd">    the `output` must be a 1D Tensor with shape :math:`(N * (N - 1) / 2,)`.</span>

<span class="sd">    .. math::</span>
<span class="sd">        y[n] = \sqrt[p]{{\mid x_{i} - x_{j} \mid}^p}</span>

<span class="sd">    where :math:`x_{i}, x_{j}` are two different row vectors in the input.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input tensor. dtype: float16, float32 or float64.</span>
<span class="sd">        p (float): The order of norm distance, :math:`p[0, )`. Default: ``2.0`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not float16, float32 or float64.</span>
<span class="sd">        TypeError: If `p` is not a float.</span>
<span class="sd">        ValueError: If `p` is a negative float.</span>
<span class="sd">        ValueError: If dimension of `input` is less than 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1.0, 1.0], [2.0, 2.0], [3.0, 3.0]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; y = ops.pdist(x, p=2.0)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [1.4142135 2.828427 1.4142135]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">pdist_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">Pdist</span><span class="p">)(</span><span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pdist_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_circular_pad</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">padding</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;circular pad&quot;&quot;&quot;</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="n">scalar_to_tensor_</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="n">const_arg</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">is_expand</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">padding</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">==</span> <span class="n">input_x</span><span class="o">.</span><span class="n">ndim</span><span class="p">:</span>
        <span class="n">input_x</span> <span class="o">=</span> <span class="n">input_x</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">is_expand</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">PadV3</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s2">&quot;circular&quot;</span><span class="p">,</span> <span class="n">paddings_contiguous</span><span class="o">=</span><span class="kc">True</span><span class="p">)(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_expand</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>


<span class="k">def</span> <span class="nf">pad_ext</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;constant&#39;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Pads the input tensor according to the pad.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Tensor of shape :math:`(N, *)`, where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        pad (tuple[int]): Filling position of pad.</span>
<span class="sd">        mode (str, optional): Pad filling mode, ``&#39;constant&#39;`` , ``&#39;reflect&#39;`` , ``&#39;replicate&#39;``  or ``&#39;circular&#39;`` .</span>
<span class="sd">            Default: ``&#39;constant&#39;`` .</span>
<span class="sd">        value (Union[int, float, None], optional): Valid only in ``&#39;constant&#39;`` mode.</span>
<span class="sd">            Set the pad value in ``&#39;constant&#39;`` mode. If the value is None, 0 is used as the default pad value.</span>
<span class="sd">            Default: ``None`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the tensor after pad.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `pad` is not an int of tuple.</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        ValueError: If length of `pad` is not even.</span>
<span class="sd">        ValueError: If length of `pad` is greater than 6.</span>
<span class="sd">        ValueError: If `mode` is not ``&#39;constant&#39;`` and `value` not ``None``.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.mint.nn.functional import pad</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = ms.Tensor(np.arange(1 * 2 * 2 * 2).reshape((1, 2, 2, 2)), dtype=ms.float64)</span>
<span class="sd">        &gt;&gt;&gt; output = pad(x, [1, 0, 0, 1], mode=&#39;constant&#39;, value=6.0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[6. 0. 1.]</span>
<span class="sd">           [6. 2. 3.]</span>
<span class="sd">           [6. 6. 6.]]</span>
<span class="sd">          [[6. 4. 5.]</span>
<span class="sd">           [6. 6. 7.]</span>
<span class="sd">           [6. 6. 6.]]]]</span>
<span class="sd">        &gt;&gt;&gt; output1 = ops.pad(x, (1, 0, 0, 1), mode=&#39;reflect&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(output1)</span>
<span class="sd">        [[[[1. 0. 1.]</span>
<span class="sd">           [3. 2. 3.]</span>
<span class="sd">           [1. 0. 1.]]</span>
<span class="sd">          [[5. 4. 5.]</span>
<span class="sd">           [7. 6. 7.]</span>
<span class="sd">           [5. 4. 5.]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;pad&#39;, the type of &#39;input&#39; must be Tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="nb">input</span>
    <span class="k">if</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">pad</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">out</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;constant&quot;</span><span class="p">:</span>
        <span class="n">value</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">value</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">constant_pad_nd_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;circular&quot;</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">_circular_pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;reflect&quot;</span><span class="p">:</span>
                <span class="n">out</span> <span class="o">=</span> <span class="n">reflection_pad_1d_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;replicate&quot;</span><span class="p">:</span>
                <span class="n">out</span> <span class="o">=</span> <span class="n">replication_pad_1d_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Pad filling mode must be &#39;constant&#39; &#39;circular&#39; &#39;reflect&#39; or &#39;replicate&#39;.&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;reflect&quot;</span><span class="p">:</span>
                <span class="n">out</span> <span class="o">=</span> <span class="n">reflection_pad_2d_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;replicate&quot;</span><span class="p">:</span>
                <span class="n">out</span> <span class="o">=</span> <span class="n">replication_pad_2d_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Pad filling mode must be &#39;constant&#39; &#39;circular&#39; &#39;reflect&#39; or &#39;replicate&#39;.&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;reflect&quot;</span><span class="p">:</span>
                <span class="n">out</span> <span class="o">=</span> <span class="n">reflection_pad_3d_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;replicate&quot;</span><span class="p">:</span>
                <span class="n">out</span> <span class="o">=</span> <span class="n">replication_pad_3d_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Pad filling mode must be &#39;constant&#39; &#39;circular&#39; &#39;reflect&#39; or &#39;replicate&#39;.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>


<span class="k">def</span> <span class="nf">_check_pad_inputs</span><span class="p">(</span><span class="n">padding</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;check the input of pad&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;pad&#39;, the size of padding must be divisible by 2, but got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;pad&#39;, the type of &#39;paddings&#39; must be a tuple of int or list of int or a Tensor,&quot;</span>
                        <span class="sa">f</span><span class="s2">&quot; but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">pd</span> <span class="ow">in</span> <span class="n">padding</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pd</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;pad&#39;, the paddings value must be tuple of int or list of int, but got </span><span class="si">{</span><span class="n">padding</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="pad"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.pad.html#mindspore.ops.pad">[docs]</a><span class="k">def</span> <span class="nf">pad</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;constant&#39;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Pads the input tensor according to the padding.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Tensor of shape :math:`(N, *)`, where :math:`*` means, any number of additional dimensions</span>
<span class="sd">            which is required to be no more than 5 in Ascend.</span>
<span class="sd">        padding (Union[tuple[int], list[int], Tensor]): Filling position of pad where the negative value is not</span>
<span class="sd">            supported while running in Ascend.</span>
<span class="sd">            :math:`\left\lfloor\frac{\text{len(padding)}}{2}\right\rfloor` dimensions</span>
<span class="sd">            of `input_x` will be padded.</span>

<span class="sd">            Example: to pad only the last dimension of the input tensor, then</span>
<span class="sd">            :attr:`padding` has the form</span>
<span class="sd">            :math:`(\text{padding_left}, \text{padding_right})`;</span>

<span class="sd">            Example: to pad the last 2 dimensions of the input tensor, then use</span>
<span class="sd">            :math:`(\text{padding_left}, \text{padding_right}, \text{padding_top}, \text{padding_bottom})`;</span>

<span class="sd">            Example: to pad the last 3 dimensions, use</span>
<span class="sd">            :math:`(\text{padding_left}, \text{padding_right}, \text{padding_top}, \text{padding_bottom},</span>
<span class="sd">            \text{padding_front}, \text{padding_back})` and so on.</span>

<span class="sd">        mode (str, optional): Pad filling mode, ``&#39;constant&#39;`` , ``&#39;reflect&#39;`` , ``&#39;replicate&#39;``  or ``&#39;circular&#39;`` .</span>
<span class="sd">            Default: ``&#39;constant&#39;`` .</span>

<span class="sd">            For ``&#39;constant&#39;`` mode, please refer to :class:`mindspore.nn.ConstantPad1d` as an example to understand</span>
<span class="sd">            this filling pattern and extend the padding pattern to n dimensions.</span>

<span class="sd">            For ``&#39;reflect&#39;`` mode, please refer to :class:`mindspore.nn.ReflectionPad1d` as an example to understand</span>
<span class="sd">            this filling pattern.</span>
<span class="sd">            The reflect mode is used to pad the last two dimensions of 3D or 4D input, or the last dimension of 2D or</span>
<span class="sd">            3D input.</span>

<span class="sd">            For ``&#39;replicate&#39;`` mode, please refer to :class:`mindspore.nn.ReplicationPad1d` as an example to understand</span>
<span class="sd">            this filling pattern.</span>
<span class="sd">            The replicate mode is used to pad the last three dimensions of 4D or 5D input, the last two dimensions of 3D</span>
<span class="sd">            or 4D input, or the last dimension of 2D or 3D input.</span>

<span class="sd">            For ``&#39;circular&#39;`` mode, the pixels from one edge of the image are wrapped around to the opposite edge,</span>
<span class="sd">            such that the pixel on the right edge of the image is replaced with the pixel on the left edge,</span>
<span class="sd">            and the pixel on the bottom edge is replaced with the pixel on the top edge.</span>
<span class="sd">            The circular mode is used to pad the last three dimensions of 4D or 5D input, the last two dimensions of 3D</span>
<span class="sd">            or 4D input, or the last dimension of 2D or 3D input.</span>

<span class="sd">        value (Union[int, float, None], optional): Valid only in ``&#39;constant&#39;`` mode.</span>
<span class="sd">            Set the padding value in ``&#39;constant&#39;`` mode. If the value is None, 0 is used as the default padding value.</span>
<span class="sd">            Default: ``None`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the tensor after padding.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `padding` is not an int of tuple or int of list.</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        ValueError: If length of `padding` is not even.</span>
<span class="sd">        ValueError: If length of `padding` is greater than 6.</span>
<span class="sd">        ValueError: If `mode` is not ``&#39;constant&#39;`` and `value` not ``None``.</span>
<span class="sd">        ValueError: If rank of `input_x` is more than 5 while running in Ascend.</span>
<span class="sd">        ValueError: If `paddings` contains negative value while running in Ascend.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = ms.Tensor(np.arange(1 * 2 * 2 * 2).reshape((1, 2, 2, 2)), dtype=ms.float64)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.pad(x, [1, 0, 0, 1], mode=&#39;constant&#39;, value=6.0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[6. 0. 1.]</span>
<span class="sd">           [6. 2. 3.]</span>
<span class="sd">           [6. 6. 6.]]</span>
<span class="sd">          [[6. 4. 5.]</span>
<span class="sd">           [6. 6. 7.]</span>
<span class="sd">           [6. 6. 6.]]]]</span>
<span class="sd">        &gt;&gt;&gt; output1 = ops.pad(x, (1, 0, 0, 1), mode=&#39;reflect&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(output1)</span>
<span class="sd">        [[[[1. 0. 1.]</span>
<span class="sd">           [3. 2. 3.]</span>
<span class="sd">           [1. 0. 1.]]</span>
<span class="sd">          [[5. 4. 5.]</span>
<span class="sd">           [7. 6. 7.]</span>
<span class="sd">           [5. 4. 5.]]]]</span>
<span class="sd">        &gt;&gt;&gt; output2 = ops.pad(x, (1, 1, 2, 1), mode=&#39;replicate&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(output2)</span>
<span class="sd">        [[[[0. 0. 1. 1.]</span>
<span class="sd">           [0. 0. 1. 1.]</span>
<span class="sd">           [0. 0. 1. 1.]</span>
<span class="sd">           [2. 2. 3. 3.]</span>
<span class="sd">           [2. 2. 3. 3.]]</span>
<span class="sd">          [[4. 4. 5. 5.]</span>
<span class="sd">           [4. 4. 5. 5.]</span>
<span class="sd">           [4. 4. 5. 5.]</span>
<span class="sd">           [6. 6. 7. 7.]</span>
<span class="sd">           [6. 6. 7. 7.]]]]</span>
<span class="sd">        &gt;&gt;&gt; output3 = ops.pad(x, (1, 1, 2, 1), mode=&#39;circular&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(output3)</span>
<span class="sd">        [[[[1. 0. 1. 0.]</span>
<span class="sd">           [3. 2. 3. 2.]</span>
<span class="sd">           [1. 0. 1. 0.]</span>
<span class="sd">           [3. 2. 3. 2.]</span>
<span class="sd">           [1. 0. 1. 0.]]</span>
<span class="sd">          [[5. 4. 5. 4.]</span>
<span class="sd">           [7. 6. 7. 6.]</span>
<span class="sd">           [5. 4. 5. 4.]</span>
<span class="sd">           [7. 6. 7. 6.]</span>
<span class="sd">           [5. 4. 5. 4.]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;pad&#39;, the type of &#39;input_x&#39; must be Tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">padding</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">padding</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)):</span>
        <span class="k">return</span> <span class="n">input_x</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">_check_pad_inputs</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
    <span class="n">is_expand</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;constant&quot;</span><span class="p">:</span>
        <span class="n">value</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">value</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">)):</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">scalar_to_tensor_</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">input_x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">6</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;pad&#39;, the padding must be less than or equal to 6, but got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;pad&#39;, the padding mode &#39;</span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s2">&#39; can not set value, but got value </span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;replicate&quot;</span><span class="p">:</span>
            <span class="n">mode</span> <span class="o">=</span> <span class="s2">&quot;edge&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">==</span> <span class="n">input_x</span><span class="o">.</span><span class="n">ndim</span><span class="p">:</span>
            <span class="n">input_x</span> <span class="o">=</span> <span class="n">input_x</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">is_expand</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">PadV3</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span> <span class="n">paddings_contiguous</span><span class="o">=</span><span class="kc">True</span><span class="p">)(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_expand</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="rrelu"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.rrelu.html#mindspore.ops.rrelu">[docs]</a><span class="k">def</span> <span class="nf">rrelu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="mf">1.0</span> <span class="o">/</span> <span class="mi">8</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">1.0</span> <span class="o">/</span> <span class="mi">3</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">    Randomized Leaky ReLU activation function.</span>

<span class="sd">    The activation function is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{rrelu}(input_{ji}) = \begin{cases}input_{ji}, &amp;\text{if } input_{ji} \geq 0; \cr</span>
<span class="sd">        {\alpha_{ji}} * input_{ji}, &amp;\text{otherwise.}\end{cases}</span>

<span class="sd">    where :math:`\alpha_{ji}` ~ :math:`U(l, u)`, :math:`l \le u`.</span>

<span class="sd">    Applies the rrelu function elementally, as described in the paper:</span>
<span class="sd">    `Empirical Evaluation of Rectified Activations in Convolution Network &lt;https://arxiv.org/pdf/1505.00853.pdf&gt;`_ .</span>

<span class="sd">    Args:</span>
<span class="sd">        input  (Tensor): The input of rrelu is a Tensor of any dimension.</span>
<span class="sd">        lower (Union[int, float]): Slope of the activation function at x &lt; 0. Default: ``1.0 / 8`` .</span>
<span class="sd">        upper (Union[int, float]): Slope of the activation function at x &lt; 0. Default: ``1.0 / 3`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, after rrelu, has the same type and shape as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `lower` is not a float or an int.</span>
<span class="sd">        TypeError: If `upper` is not a float or an int.</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If `input` is not a Tensor of mindspore.float16 or mindspore.float32.</span>
<span class="sd">        ValueError: If `lower` is greater than upper.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[-1.0, 4.0], [2.0, 0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.rrelu(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-0.31465699  4.        ]</span>
<span class="sd">         [ 2.          0.        ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">upper</span><span class="p">,</span> <span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;rrelu&#39;, &#39;upper&#39; must be an int or a float, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">upper</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">lower</span><span class="p">,</span> <span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;rrelu&#39;, &#39;lower&#39; must be an int or a float, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">lower</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">lower</span> <span class="o">&gt;</span> <span class="n">upper</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;rrelu&#39;, the value of &#39;upper&#39; must be greater than or equal to &#39;lower&#39;, &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;but got upper: </span><span class="si">{</span><span class="n">upper</span><span class="si">}</span><span class="s2">, lower: </span><span class="si">{</span><span class="n">lower</span><span class="si">}</span><span class="s2">. &quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;rrelu&#39;, the &#39;input&#39; must be a Tensor but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">_lower</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">lower</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">_upper</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">upper</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">_size</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">if</span> <span class="n">ops</span><span class="o">.</span><span class="n">is_sequence_value_unknown</span><span class="p">(</span><span class="n">_size</span><span class="p">):</span>
        <span class="n">_size</span> <span class="o">=</span> <span class="n">tensor_shape_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">sign_matrix</span> <span class="o">=</span> <span class="n">sign_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">negative_filter</span> <span class="o">=</span> <span class="n">sign_matrix</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">positive_filter</span> <span class="o">=</span> <span class="n">sign_matrix</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">input_dtype</span> <span class="o">=</span> <span class="n">dtype_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">_size</span><span class="p">,</span> <span class="n">_lower</span><span class="p">,</span> <span class="n">_upper</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">input_dtype</span><span class="p">)</span>
    <span class="n">negative_mask</span> <span class="o">=</span> <span class="n">negative_filter</span> <span class="o">*</span> <span class="n">mask</span> <span class="o">*</span> <span class="o">-</span><span class="mi">1</span>
    <span class="n">total_mask</span> <span class="o">=</span> <span class="n">negative_mask</span> <span class="o">+</span> <span class="n">positive_filter</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">total_mask</span> <span class="o">*</span> <span class="nb">input</span>
    <span class="k">return</span> <span class="n">out</span></div>


<span class="k">def</span> <span class="nf">mirror_pad</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">mode</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Pads the input tensor according to the paddings and mode.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Tensor of shape :math:`(N, *)`, where :math:`*` means, any number of</span>
<span class="sd">          additional dimensions.</span>
<span class="sd">        paddings (Tensor): Paddings requires constant tensor. The value of `paddings` is a</span>
<span class="sd">          matrix(list), and its shape is (N, 2). N is the rank of input data. All elements of paddings</span>
<span class="sd">          are int type. For the input in the `D` th dimension, paddings[D, 0] indicates how many sizes</span>
<span class="sd">          to be extended ahead of the input tensor in the `D` th dimension, and paddings[D, 1]</span>
<span class="sd">          indicates how many sizes to be extended behind the input tensor in the `D` th dimension. Both</span>
<span class="sd">          paddings[D, 0] and paddings[D, 1] must be no greater than input_x.dim_size(D)</span>
<span class="sd">          (or input_x.dim_size(D) - 1) if mode is SYMMETRIC (if REFLECT, respectively).</span>
<span class="sd">        mode (str): Specifies the padding mode. The optional values are &quot;REFLECT&quot; and &quot;SYMMETRIC&quot;.</span>
<span class="sd">            Default: &quot;REFLECT&quot;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the tensor after padding.</span>

<span class="sd">        - If `mode` is &quot;REFLECT&quot;, it uses a way of symmetrical copying through the axis of symmetry to fill in.</span>
<span class="sd">          If the `input_x` is [[1,2,3], [4,5,6], [7,8,9]] and `paddings` is [[1,1], [2,2]], then the</span>
<span class="sd">          `Outputs` is [[6,5,4,5,6,5,4], [3,2,1,2,3,2,1], [6,5,4,5,6,5,4], [9,8,7,8,9,8,7], [6,5,4,5,6,5,4]].</span>
<span class="sd">          For a more intuitive understanding, please see the example below.</span>
<span class="sd">        - If `mode` is &quot;SYMMETRIC&quot;, the filling method is similar to the &quot;REFLECT&quot;. It is also copied</span>
<span class="sd">          according to the symmetry axis, except that it includes the symmetry axis. If the `input_x`</span>
<span class="sd">          is [[1,2,3], [4,5,6], [7,8,9]] and `paddings` is [[1,1], [2,2]], then the `Outputs` is</span>
<span class="sd">          [[2,1,1,2,3,3,2], [2,1,1,2,3,3,2], [5,4,4,5,6,6,5], [8,7,7,8,9,9,8], [8,7,7,8,9,9,8]].</span>
<span class="sd">          For a more intuitive understanding, please see the example below.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` or `paddings` is not a Tensor.</span>
<span class="sd">        TypeError: If `mode` is not a str.</span>
<span class="sd">        ValueError: If paddings.size is not equal to 2 * rank of input_x.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor([[1,2,3], [4,5,6], [7,8,9]])</span>
<span class="sd">        &gt;&gt;&gt; mode = &quot;REFLECT&quot;</span>
<span class="sd">        &gt;&gt;&gt; paddings = Tensor([[1, 1], [2, 2]])</span>
<span class="sd">        &gt;&gt;&gt; output = ops.mirror_pad(input_x, paddings, mode)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[6 5 4 5 6 5 4]</span>
<span class="sd">         [3 2 1 2 3 2 1]</span>
<span class="sd">         [6 5 4 5 6 5 4]</span>
<span class="sd">         [9 8 7 8 9 8 7]</span>
<span class="sd">         [6 5 4 5 6 5 4]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_mirror_pad</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">MirrorPad</span><span class="p">)(</span><span class="n">mode</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_mirror_pad</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_innner_log_softmax</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;inner implementation of log_softmax, since the LogSoftmaxGrad op do not support inputs &gt; 2d&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">inputs</span> <span class="o">-</span> <span class="n">logsumexp</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_cross_entropy_inputs</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Check inputs for cross_entropy().</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;input&#39;</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;cross_entropy_loss&quot;</span><span class="p">)</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;target&#39;</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="s2">&quot;cross_entropy_loss&quot;</span><span class="p">)</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="s2">&quot;cross_entropy_loss&quot;</span><span class="p">)</span>
    <span class="n">check_int_const</span><span class="p">(</span><span class="n">ignore_index</span><span class="p">,</span> <span class="s1">&#39;ignore_index&#39;</span><span class="p">,</span> <span class="s2">&quot;cross_entropy_loss&quot;</span><span class="p">)</span>
    <span class="n">check_non_negative_float_const</span><span class="p">(</span><span class="n">label_smoothing</span><span class="p">,</span> <span class="s1">&#39;label_smoothing&#39;</span><span class="p">,</span> <span class="s2">&quot;cross_entropy_loss&quot;</span><span class="p">)</span>
    <span class="n">check_string_const</span><span class="p">(</span><span class="n">reduction</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">],</span> <span class="s1">&#39;reduction&#39;</span><span class="p">,</span> <span class="s2">&quot;cross_entropy_loss&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;For cross_entropy, the input dtype should be mstype.float64, mstype.float32 or&#39;</span>
                        <span class="sa">f</span><span class="s1">&#39;mstype.float16, but got dtype:</span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>


<div class="viewcode-block" id="cross_entropy"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.cross_entropy.html#mindspore.ops.cross_entropy">[docs]</a><span class="k">def</span> <span class="nf">cross_entropy</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The cross entropy loss between input and target.</span>

<span class="sd">    The cross entropy support two kind of targets:</span>

<span class="sd">    - Class indices (int) in the range :math:`[0, C)` where :math:`C` is the number of classes,</span>
<span class="sd">      the loss with reduction=none can be described as:</span>

<span class="sd">      .. math::</span>

<span class="sd">          \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">          l_n = - w_{y_n} \log \frac{\exp(x_{n,y_n})}{\sum_{c=1}^C \exp(x_{n,c})}</span>
<span class="sd">          \cdot \mathbb{1}\{y_n \not= \text{ignore_index}\}</span>

<span class="sd">      where :math:`x` is the inputs, :math:`y` is the target, :math:`w` is the weight, N is the batch size,</span>
<span class="sd">      :math:`c` belonging to :math:`[0, C-1]` is class index, where :math:`C` is the number of classes.</span>

<span class="sd">      If `reduction` is not ``None`` (default ``&#39;mean&#39;`` ), then</span>

<span class="sd">      .. math::</span>

<span class="sd">          \ell(x, y) = \begin{cases}</span>
<span class="sd">              \sum_{n=1}^N \frac{1}{\sum_{n=1}^N w_{y_n} \cdot \mathbb{1}\{y_n \not= \text{ignore_index}\}} l_n, &amp;</span>
<span class="sd">              \text{if reduction} = \text{&#39;mean&#39;,}\\</span>
<span class="sd">              \sum_{n=1}^N l_n,  &amp;</span>
<span class="sd">              \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">              \end{cases}</span>

<span class="sd">    - Probabilities (float) for each class, useful when labels beyond a single class per minibatch item</span>
<span class="sd">      are required, the loss with reduction=none can be described as:</span>

<span class="sd">      .. math::</span>

<span class="sd">          \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">          l_n = - \sum_{c=1}^C w_c \log \frac{\exp(x_{n,c})}{\sum_{i=1}^C \exp(x_{n,i})} y_{n,c}</span>

<span class="sd">      where :math:`x` is the inputs, :math:`y` is the target, :math:`w` is the weight, N is the batch size,</span>
<span class="sd">      :math:`c` belonging to :math:`[0, C-1]` is class index, where :math:`C` is the number of classes.</span>

<span class="sd">      If `reduction` is not ``None`` (default ``&#39;mean&#39;`` ), then</span>

<span class="sd">      .. math::</span>

<span class="sd">          \ell(x, y) = \begin{cases}</span>
<span class="sd">              \frac{\sum_{n=1}^N l_n}{N}, &amp;</span>
<span class="sd">              \text{if reduction} = \text{&#39;mean&#39;,}\\</span>
<span class="sd">              \sum_{n=1}^N l_n,  &amp;</span>
<span class="sd">              \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">              \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): :math:`(N)` or :math:`(N, C)` where `C = number of classes` or :math:`(N, C, H, W)`</span>
<span class="sd">            in case of 2D Loss, or :math:`(N, C, d_1, d_2, ..., d_K)`.</span>
<span class="sd">            `input` is expected to be log-probabilities, data type must be float16 or float32.</span>
<span class="sd">        target (Tensor): For class indices, tensor of shape :math:`()`, :math:`(N)` or</span>
<span class="sd">            :math:`(N, d_1, d_2, ..., d_K)` , data type must be int32. For probabilities, tensor of shape :math:`(C,)` ,</span>
<span class="sd">            :math:`(N, C)` or :math:`(N, C, d_1, d_2, ..., d_K)` , data type must be float16 or float32 or float64.</span>
<span class="sd">        weight (Tensor): A rescaling weight applied to the loss of each batch element.</span>
<span class="sd">            If not None, the shape is :math:`(C,)`, data type must be float16 or float32. Default: ``None`` .</span>
<span class="sd">        ignore_index (int): Specifies a target value that is ignored</span>
<span class="sd">            and does not contribute to the input gradient. Default: ``-100`` .</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the weighted mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the output elements will be summed.</span>

<span class="sd">        label_smoothing (float): Label smoothing values, a regularization tool used to prevent the model</span>
<span class="sd">            from overfitting when calculating Loss. The value range is [0.0, 1.0]. Default value: ``0.0`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the computed loss value.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; # Case 1: Indices labels</span>
<span class="sd">        &gt;&gt;&gt; inputs = ms.Tensor(np.random.randn(3, 5), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; target = ms.Tensor(np.array([1, 0, 4]), ms.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ms.ops.cross_entropy(inputs, target)</span>
<span class="sd">        &gt;&gt;&gt; # Case 2: Probability labels</span>
<span class="sd">        &gt;&gt;&gt; inputs = ms.Tensor(np.random.randn(3, 5), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; target = ms.Tensor(np.random.randn(3, 5), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ms.ops.cross_entropy(inputs, target)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_cross_entropy_inputs</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="p">)</span>
    <span class="n">class_dim</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">target</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">_cross_entropy</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">class_dim</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">nll_loss</span><span class="p">(</span><span class="n">_innner_log_softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">class_dim</span><span class="p">),</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_cross_entropy</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">target_dim</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;cross entropy inner function&quot;&quot;&quot;</span>
    <span class="n">class_dim</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">inputs</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">1</span>
    <span class="n">n_classes</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">class_dim</span><span class="p">]</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">_innner_log_softmax</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">class_dim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">label_smoothing</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">:</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">label_smoothing</span><span class="p">)</span> <span class="o">+</span> <span class="n">label_smoothing</span> <span class="o">/</span> <span class="n">n_classes</span>

    <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">ones_like_</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">inputs</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">broadcast_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">ndim</span><span class="p">)]</span>
        <span class="n">broadcast_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">broadcast_shape</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="n">inputs</span> <span class="o">*</span> <span class="n">target</span> <span class="o">*</span> <span class="n">weight</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">size</span> <span class="o">/</span> <span class="n">n_classes</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;sum&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="n">inputs</span> <span class="o">*</span> <span class="n">target</span> <span class="o">*</span> <span class="n">weight</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="n">inputs</span> <span class="o">*</span> <span class="n">target</span> <span class="o">*</span> <span class="n">weight</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">class_dim</span><span class="p">)</span>


<div class="viewcode-block" id="nll_loss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.nll_loss.html#mindspore.ops.nll_loss">[docs]</a><span class="k">def</span> <span class="nf">nll_loss</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets the negative log likelihood loss between inputs and target.</span>

<span class="sd">    The nll loss with reduction=none can be described as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \ell(x, t)=L=\left\{l_{1}, \ldots, l_{N}\right\}^{\top},</span>
<span class="sd">        \quad l_{n}=-w_{t_{n}} x_{n, t_{n}},</span>
<span class="sd">        \quad w_{c}=\text { weight }[c] \cdot \mathbb{1}</span>
<span class="sd">        \{c \not= \text{ignore_index}\},</span>

<span class="sd">    where :math:`x` is the inputs, :math:`t` is the target, :math:`w` is the weight,</span>
<span class="sd">    N is the batch size, :math:`c` belonging to :math:`[0, C-1]` is class index, where :math:`C` is the number of</span>
<span class="sd">    classes.</span>

<span class="sd">    If `reduction` is not ``None`` (default ``&#39;mean&#39;``), then</span>

<span class="sd">    .. math::</span>

<span class="sd">        \ell(x, t)=\left\{\begin{array}{ll}</span>
<span class="sd">        \sum_{n=1}^{N} \frac{1}{\sum_{n=1}^{N} w_{t n}} l_{n}, &amp; \text { if reduction }=\text { &#39;mean&#39;, } \\</span>
<span class="sd">        \sum_{n=1}^{N} l_{n}, &amp; \text { if reduction }=\text { &#39;sum&#39; }</span>
<span class="sd">        \end{array}\right.</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs (Tensor): :math:`(N, C)` where `C = number of classes` or :math:`(N, C, H, W)`</span>
<span class="sd">            in case of 2D Loss, or :math:`(N, C, d_1, d_2, ..., d_K)`.</span>
<span class="sd">            `inputs` is expected to be log-probabilities, data type must be float16 or float32.</span>
<span class="sd">        target (Tensor): :math:`(N)` or :math:`(N, d_1, d_2, ..., d_K)` for</span>
<span class="sd">            high-dimensional loss, data type must be int32.</span>
<span class="sd">        weight (Tensor): A rescaling weight applied to the loss of each batch element.</span>
<span class="sd">            If not None, the shape is :math:`(C,)`.</span>
<span class="sd">            The data type must be float16 or float32. Default: ``None`` .</span>
<span class="sd">        ignore_index (int): Specifies a target value that is ignored</span>
<span class="sd">            and does not contribute to the input gradient. Default: ``-100`` .</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the weighted mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the output elements will be summed.</span>

<span class="sd">        label_smoothing (float): Label smoothing values, a regularization tool used to prevent the model</span>
<span class="sd">            from overfitting when calculating Loss. The value range is [0.0, 1.0]. Default value: ``0.0`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the computed loss value.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; inputs = mindspore.Tensor(np.random.randn(3, 5), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; target = mindspore.Tensor(np.array([1, 0, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.nll_loss(inputs, target)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">ndim</span>
    <span class="k">if</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">_nll_loss</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">_nll_loss</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">_nll_loss</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">n</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">out_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">n</span><span class="p">,)</span> <span class="o">+</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">view</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">view</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">reduction</span> <span class="o">!=</span> <span class="s1">&#39;none&#39;</span><span class="p">:</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">_nll_loss</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">_nll_loss</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="o">=</span><span class="n">label_smoothing</span><span class="p">)</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">ret</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">out_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span></div>


<span class="k">def</span> <span class="nf">_nll_loss</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">target_dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;nll loss inner function&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">target</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="n">inputs</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">target_dim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ignore_index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">non_pad_mask</span> <span class="o">=</span> <span class="n">equal_</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">)</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">non_pad_mask</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">non_pad_mask</span> <span class="o">=</span> <span class="n">target</span>
    <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">loss_weights</span> <span class="o">=</span> <span class="n">gather_</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">orig_shape</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">if</span> <span class="n">inputs</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">orig_shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,))</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
        <span class="n">weighted_inputs</span> <span class="o">=</span> <span class="n">inputs</span> <span class="o">*</span> <span class="n">weight</span>
        <span class="n">weighted_inputs</span> <span class="o">=</span> <span class="n">weighted_inputs</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">orig_shape</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">neg_</span><span class="p">(</span><span class="n">gather_d_</span><span class="p">(</span><span class="n">weighted_inputs</span><span class="p">,</span> <span class="n">target_dim</span><span class="p">,</span> <span class="n">target</span><span class="p">))</span>
        <span class="n">smooth_loss</span> <span class="o">=</span> <span class="n">neg_</span><span class="p">(</span><span class="n">weighted_inputs</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">target_dim</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">neg_</span><span class="p">(</span><span class="n">gather_d_</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target_dim</span><span class="p">,</span> <span class="n">target</span><span class="p">))</span>
        <span class="n">smooth_loss</span> <span class="o">=</span> <span class="n">neg_</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">target_dim</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
        <span class="n">loss_weights</span> <span class="o">=</span> <span class="n">ones_like_</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ignore_index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">non_pad_mask</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
        <span class="n">loss_weights</span> <span class="o">=</span> <span class="n">loss_weights</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">non_pad_mask</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">loss_weights</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
        <span class="n">smooth_loss</span> <span class="o">=</span> <span class="n">smooth_loss</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">non_pad_mask</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">smooth_loss</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">target_dim</span><span class="p">)</span>
    <span class="n">smooth_loss</span> <span class="o">=</span> <span class="n">smooth_loss</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">target_dim</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;sum&#39;</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">smooth_loss</span> <span class="o">=</span> <span class="n">smooth_loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">loss_weights</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">smooth_loss</span> <span class="o">=</span> <span class="n">smooth_loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">loss_weights</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

    <span class="n">eps_i</span> <span class="o">=</span> <span class="n">label_smoothing</span> <span class="o">/</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">target_dim</span><span class="p">]</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">label_smoothing</span><span class="p">)</span> <span class="o">*</span> <span class="n">loss</span> <span class="o">+</span> <span class="n">eps_i</span> <span class="o">*</span> <span class="n">smooth_loss</span>

    <span class="k">return</span> <span class="n">loss</span>


<div class="viewcode-block" id="l1_loss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.l1_loss.html#mindspore.ops.l1_loss">[docs]</a><span class="k">def</span> <span class="nf">l1_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the mean absolute error between the `input` value and the `target` value.</span>

<span class="sd">    Assuming that the :math:`x` and :math:`y` (predicted and target value) are 1-D Tensor,</span>
<span class="sd">    length :math:`N`, `reduction` is set to ``&#39;none&#39;``, then calculate the loss of</span>
<span class="sd">    :math:`x` and :math:`y` without dimensionality reduction.</span>

<span class="sd">    The formula is as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad \text{with } l_n = \left| x_n - y_n \right|,</span>

<span class="sd">    where :math:`N` is the batch size.</span>

<span class="sd">    If `reduction` is ``&#39;mean&#39;`` or ``&#39;sum&#39;`` , then:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">            \operatorname{mean}(L), &amp; \text{if reduction} = \text{&#39;mean&#39;;}\\</span>
<span class="sd">            \operatorname{sum}(L),  &amp; \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Predicted value, Tensor of any dimension.</span>
<span class="sd">        target (Tensor): Target value, usually has the same shape as the `input`.</span>
<span class="sd">            If `input` and `target` have different shape, make sure they can broadcast to each other.</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the output elements will be summed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or Scalar, if `reduction` is ``&#39;none&#39;``, return a Tensor with same shape and dtype as `input`.</span>
<span class="sd">        Otherwise, a scalar value will be returned.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If `target` is not a Tensor.</span>
<span class="sd">        ValueError: If `reduction` is not one of ``&#39;none&#39;``, ``&#39;mean&#39;`` or ``&#39;sum&#39;``.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[1, 2, 3], [4, 5, 6]], mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; target = Tensor([[6, 5, 4], [3, 2, 1]], mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.l1_loss(x, target, reduction=&quot;mean&quot;)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        3.0</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;input&#39;</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;l1_loss&quot;</span><span class="p">)</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;target&#39;</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="s2">&quot;l1_loss&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="s1">&#39;none&#39;</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For l1_loss, the &#39;reduction&#39; must be in [&#39;mean&#39;, &#39;sum&#39;, &#39;none&#39;], but got </span><span class="si">{</span><span class="n">reduction</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">abs_</span><span class="p">(</span><span class="nb">input</span> <span class="o">-</span> <span class="n">target</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_get_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="s2">&quot;l1_loss&quot;</span><span class="p">)</span></div>


<div class="viewcode-block" id="smooth_l1_loss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.smooth_l1_loss.html#mindspore.ops.smooth_l1_loss">[docs]</a><span class="k">def</span> <span class="nf">smooth_l1_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes smooth L1 loss, a robust L1 loss.</span>

<span class="sd">    SmoothL1Loss is a Loss similar to MSELoss but less sensitive to outliers as described in the</span>
<span class="sd">    `Fast R-CNN &lt;https://arxiv.org/abs/1504.08083&gt;`_ by Ross Girshick.</span>

<span class="sd">    Given two input :math:`x,\  y` of length :math:`N`, the unreduced SmoothL1Loss can be described</span>
<span class="sd">    as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        L_{i} =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">        \frac{0.5 (x_i - y_i)^{2}}{\beta}, &amp; \text{if } |x_i - y_i| &lt; \beta \\</span>
<span class="sd">        |x_i - y_i| - 0.5 * \beta, &amp; \text{otherwise. }</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    If `reduction` is not `none`, then:</span>

<span class="sd">    .. math::</span>
<span class="sd">        L =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">            \operatorname{mean}(L_{i}), &amp;  \text{if reduction} = \text{&#39;mean&#39;;}\\</span>
<span class="sd">            \operatorname{sum}(L_{i}),  &amp;  \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Here :math:`\text{beta}` controls the point where the loss function changes from quadratic to linear.</span>
<span class="sd">    :math:`\text{beta}&gt;0` , its default value is ``1.0`` . :math:`N` is the batch size.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Tensor of shape :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">            Data type is float16, float32 or float64.</span>
<span class="sd">        target (Tensor): Ground truth data, tensor of shape :math:`(N, *)`, same shape and dtype as the `input`.</span>
<span class="sd">        beta (float): A parameter used to control the point where the function will change between</span>
<span class="sd">            L1 to L2 loss. The value should be greater than zero. Default: ``1.0`` .</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;none&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the output elements will be summed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, if `reduction` is ``&#39;none&#39;``, then output is a tensor with the same shape as `input`.</span>
<span class="sd">        Otherwise, the shape of output tensor is :math:`(1,)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `beta` is not a float.</span>
<span class="sd">        ValueError: If `reduction` is not one of ``&#39;none&#39;``, ``&#39;mean&#39;``, ``&#39;sum&#39;``.</span>
<span class="sd">        TypeError: If dtype of `input` or `target` is not one of float16, float32, float64.</span>
<span class="sd">        ValueError: If `beta` is less than or equal to 0.</span>
<span class="sd">        ValueError: If shape of `input` is not the same as `target`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([1, 2, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.smooth_l1_loss(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.  0.  0.5]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_smooth_l1_loss</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">SmoothL1Loss</span><span class="p">)(</span><span class="n">beta</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_smooth_l1_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span></div>


<div class="viewcode-block" id="threshold"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.threshold.html#mindspore.ops.threshold">[docs]</a><span class="k">def</span> <span class="nf">threshold</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">thr</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns each element of `input` after thresholding by `thr` as a Tensor.</span>

<span class="sd">    The formula is defined as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        y =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">        input, &amp;\text{ if } input &gt; \text{thr} \\</span>
<span class="sd">        \text{value}, &amp;\text{ otherwise }</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input of threshold with data type of float16 or float32.</span>
<span class="sd">        thr (Union[int, float]): The value of the threshold.</span>
<span class="sd">        value (Union[int, float]): The value to replace with when element is less than threshold.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the same shape and data type as the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If `thr` is not a float or an int.</span>
<span class="sd">        TypeError: If `value` is not a float or an int.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; inputs = mindspore.Tensor([0.0, 2, 3], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; outputs = ops.threshold(inputs, 1, 100)</span>
<span class="sd">        &gt;&gt;&gt; print(outputs)</span>
<span class="sd">        [100.   2.   3.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;input&#39;</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;threshold&quot;</span><span class="p">)</span>
    <span class="n">_check_value_type</span><span class="p">(</span><span class="s2">&quot;thr&quot;</span><span class="p">,</span> <span class="n">thr</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="s2">&quot;threshold&quot;</span><span class="p">)</span>
    <span class="n">_check_value_type</span><span class="p">(</span><span class="s2">&quot;value&quot;</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="s2">&quot;threshold&quot;</span><span class="p">)</span>
    <span class="n">cond</span> <span class="o">=</span> <span class="n">greater_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">thr</span><span class="p">)</span>
    <span class="n">input_type</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">input_type</span><span class="p">)</span>
    <span class="n">input_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">shape_tensor</span> <span class="o">=</span> <span class="n">tuple_to_tensor_</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">fillv2_</span><span class="p">(</span><span class="n">shape_tensor</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">select_</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span></div>


<div class="viewcode-block" id="leaky_relu"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.leaky_relu.html#mindspore.ops.leaky_relu">[docs]</a><span class="k">def</span> <span class="nf">leaky_relu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    leaky_relu activation function. The element of `input` less than 0 times `alpha` .</span>

<span class="sd">    The activation function is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{leaky_relu}(input) = \begin{cases}input, &amp;\text{if } input \geq 0; \cr</span>
<span class="sd">        {\alpha} * input, &amp;\text{otherwise.}\end{cases}</span>

<span class="sd">    where :math:`\alpha` represents the `alpha` parameter.</span>

<span class="sd">    For more details, see `Rectifier Nonlinearities Improve Neural Network Acoustic Models</span>
<span class="sd">    &lt;https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf&gt;`_.</span>

<span class="sd">    LeakyReLU Activation Function Graph:</span>

<span class="sd">    .. image:: ../images/LeakyReLU.png</span>
<span class="sd">        :align: center</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input of leaky_relu is a Tensor of any dimension.</span>
<span class="sd">        alpha (Union[int, float]): Slope of the activation function when the element of `input` is less than 0.</span>
<span class="sd">          Default: ``0.2`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same type and shape as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If `alpha` is not a float or an int.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; print(ops.leaky_relu(x, alpha=0.2))</span>
<span class="sd">        [[-0.2  4.  -1.6]</span>
<span class="sd">         [ 2.  -1.   9. ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;input&#39;</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;leaky_relu&quot;</span><span class="p">)</span>
    <span class="n">_check_value_type</span><span class="p">(</span><span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="s2">&quot;leaky_relu&quot;</span><span class="p">)</span>
    <span class="n">select_op</span> <span class="o">=</span> <span class="n">maximum_</span>
    <span class="k">if</span> <span class="n">alpha</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">select_op</span> <span class="o">=</span> <span class="n">minimum_</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">cast_</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">scalar_to_tensor</span><span class="p">(</span><span class="n">alpha</span><span class="p">),</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">select_op</span><span class="p">(</span><span class="n">alpha</span> <span class="o">*</span> <span class="nb">input</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="intopk"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.intopk.html#mindspore.ops.intopk">[docs]</a><span class="k">def</span> <span class="nf">intopk</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Determines whether the targets are in the top `k` predictions.</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Tensor): A 2D Tensor defines the predictions of a batch of samples with float16 or float32</span>
<span class="sd">          data type.</span>
<span class="sd">        x2 (Tensor): A 1D Tensor defines the labels of a batch of samples with int32 data type. The size of `x2`</span>
<span class="sd">          must be equal to the first dimension of `x1`. The values of `x2` can not be negative and</span>
<span class="sd">          must be equal to or less than index of x1&#39;s second dimension.</span>
<span class="sd">        k (int): Specifies the number of top elements to be used for computing precision along the last dimension.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor has 1 dimension of type bool and the same shape with `x2`. For labeling sample `i` in `x2`,</span>
<span class="sd">        if the label in the first `k` predictions for sample `i` is in `x1`, then the value is True, otherwise False.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `k` is not an int.</span>
<span class="sd">        TypeError: If `x1` or `x2` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x1` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([[1, 8, 5, 2, 7], [4, 9, 1, 3, 5]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([1, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.intopk(x1, x2, 3)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True  False]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_in_topk</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">InTopK</span><span class="p">)(</span><span class="n">k</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_in_topk</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span></div>

<div class="viewcode-block" id="lrn"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.lrn.html#mindspore.ops.lrn">[docs]</a><span class="k">def</span> <span class="nf">lrn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">depth_radius</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">norm_region</span><span class="o">=</span><span class="s2">&quot;ACROSS_CHANNELS&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Local Response Normalization.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        lrn is deprecated on Ascend due to potential accuracy problem. It&#39;s recommended to use other</span>
<span class="sd">        normalization methods, e.g. :class:`mindspore.ops.batch_norm`.</span>

<span class="sd">    .. math::</span>

<span class="sd">        b_{c} = a_{c}\left(k + \frac{\alpha}{n}</span>
<span class="sd">        \sum_{c&#39;=\max(0, c-n/2)}^{\min(N-1,c+n/2)}a_{c&#39;}^2\right)^{-\beta}</span>

<span class="sd">    where the :math:`a_{c}` indicates the specific value of the pixel corresponding to :math:`c` in feature map;</span>
<span class="sd">    where the :math:`n/2` indicates the `depth_radius`; where the :math:`k` indicates the `bias`;</span>
<span class="sd">    where the :math:`\alpha` indicates the `alpha`; where the :math:`\beta` indicates the `beta`.</span>

<span class="sd">    Args:</span>
<span class="sd">        depth_radius (int): Half-width of the 1-D normalization window with the shape of 0-D. Default: ``5`` .</span>
<span class="sd">        bias (float): An offset (usually positive to avoid dividing by 0). Default: ``1.0`` .</span>
<span class="sd">        alpha (float): A scale factor, usually positive. Default: ``1.0`` .</span>
<span class="sd">        beta (float): An exponent. Default: ``0.5`` .</span>
<span class="sd">        norm_region (str): Specifies normalization region. Options: ``&quot;ACROSS_CHANNELS&quot;`` .</span>
<span class="sd">            Default: ``&quot;ACROSS_CHANNELS&quot;`` .</span>
<span class="sd">        x (Tensor): A 4-D Tensor with float16 or float32 data type.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same shape and data type as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `depth_radius` is not an int.</span>
<span class="sd">        TypeError: If `bias`, `alpha` or `beta` is not a float.</span>
<span class="sd">        TypeError: If `norm_region` is not a str.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[[0.1], [0.2]],</span>
<span class="sd">        ...                       [[0.3], [0.4]]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.lrn(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[0.09534626]</span>
<span class="sd">           [0.1825742 ]]</span>
<span class="sd">          [[0.2860388 ]</span>
<span class="sd">           [0.3651484 ]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">lrn_op</span> <span class="o">=</span> <span class="n">NN_OPS</span><span class="o">.</span><span class="n">LRN</span><span class="p">(</span><span class="n">depth_radius</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">norm_region</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">lrn_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="mish"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.mish.html#mindspore.ops.mish">[docs]</a><span class="k">def</span> <span class="nf">mish</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes MISH(A Self Regularized Non-Monotonic Neural Activation Function) of input tensors element-wise.</span>

<span class="sd">    The function is shown as follows:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{output} = x * \tanh(\log(1 + \exp(\text{x})))</span>

<span class="sd">    See more details in `A Self Regularized Non-Monotonic Neural Activation Function</span>
<span class="sd">    &lt;https://arxiv.org/abs/1908.08681&gt;`_.</span>

<span class="sd">    Mish Activation Function Graph:</span>

<span class="sd">    .. image:: ../images/Mish.png</span>
<span class="sd">        :align: center</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input Tensor.</span>
<span class="sd">            Supported dtypes:</span>

<span class="sd">            - GPU/CPU: float16, float32, float64.</span>
<span class="sd">            - Ascend: float16, float32.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.mish(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-3.0340147e-01  3.9974129e+00 -2.68311895e-03]</span>
<span class="sd">         [ 1.9439590e+00  -3.3576239e-02 8.99999990e+00]]</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(2.1, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.mish(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        2.050599</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">mish_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_check_value_type</span><span class="p">(</span><span class="n">arg_name</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">,</span> <span class="n">valid_types</span><span class="p">,</span> <span class="n">prim_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Checks whether a value is instance of some types.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="n">arg_name</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">,</span> <span class="n">valid_types</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_is_tensor</span><span class="p">(</span><span class="n">param_name</span><span class="p">,</span> <span class="n">input_data</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Internal function, used to check whether the input data is Tensor.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">input_data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">typeof</span><span class="p">(</span><span class="n">input_data</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">TensorType</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="n">cls_name</span><span class="si">}</span><span class="s2">&#39;, the &#39;</span><span class="si">{</span><span class="n">param_name</span><span class="si">}</span><span class="s2">&#39; must be a Tensor, &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;but got &#39;</span><span class="si">{</span><span class="n">ops</span><span class="o">.</span><span class="n">typeof</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_number_gt_value</span><span class="p">(</span><span class="n">arg_name</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Internal function, used to judge whether arg_value is greater than or equal to value.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number</span><span class="p">(</span><span class="n">arg_name</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GT</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_get_axis</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get a range of axis for input.&quot;&quot;&quot;</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">length</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">tuple_len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">perm</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">make_range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">length</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">perm</span>


<span class="k">def</span> <span class="nf">_get_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate the loss with reduction and weights.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="s1">&#39;none&#39;</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="n">cls_name</span><span class="si">}</span><span class="s2">&#39;, the &#39;reduction&#39; must be in [&#39;mean&#39;, &#39;sum&#39;, &#39;none&#39;], &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">reduction</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">input_dtype</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">cast_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">cast_</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">mul_</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">reduce_mean_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">_get_axis</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;sum&#39;</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">reduce_sum_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">_get_axis</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">cast_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>


<span class="k">def</span> <span class="nf">check_input_dtype</span><span class="p">(</span><span class="n">param_name1</span><span class="p">,</span> <span class="n">input_data1</span><span class="p">,</span> <span class="n">param_name2</span><span class="p">,</span> <span class="n">input_data2</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check the type of input1 and input2.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">input_data1</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">input_data2</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;For </span><span class="si">{</span><span class="n">cls_name</span><span class="si">}</span><span class="s1">, the </span><span class="si">{</span><span class="n">param_name1</span><span class="si">}</span><span class="s1"> dtype should be equal to </span><span class="si">{</span><span class="n">param_name2</span><span class="si">}</span><span class="s1"> dtype, &#39;</span>
                        <span class="sa">f</span><span class="s1">&#39;but got </span><span class="si">{</span><span class="n">param_name1</span><span class="si">}</span><span class="s1"> dtype:</span><span class="si">{</span><span class="n">input_data1</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">param_name2</span><span class="si">}</span><span class="s1"> dtype:</span><span class="si">{</span><span class="n">input_data2</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>


<div class="viewcode-block" id="margin_ranking_loss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.margin_ranking_loss.html#mindspore.ops.margin_ranking_loss">[docs]</a><span class="k">def</span> <span class="nf">margin_ranking_loss</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    MarginRankingLoss creates a criterion that measures the loss.</span>

<span class="sd">    Given two tensors :math:`input1`, :math:`input2` and a Tensor label :math:`target` with values 1 or -1,</span>
<span class="sd">    the operation is as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss}(input1, input2, target) = \max(0, -target * (input1 - input2) + \text{margin})</span>

<span class="sd">    Args:</span>
<span class="sd">        input1 (Tensor): Tensor of shape :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        input2 (Tensor): Tensor of shape :math:`(N, *)`, same shape and dtype as `input1`.</span>
<span class="sd">        target (Tensor): Contains value 1 or -1. Suppose the shape of `input1` is</span>
<span class="sd">          :math:`(x_1, x_2, x_3, ..., x_R)`, then the shape of `target` must be :math:`(x_1, x_2, x_3, ..., x_R)`.</span>
<span class="sd">        margin (float, optional): Specify the adjustment factor of the operation. Default: ``0.0`` .</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the output elements will be summed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or Scalar. if `reduction` is ``&#39;none&#39;``, its shape is the same as `input1`.</span>
<span class="sd">        Otherwise, a scalar value will be returned.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `margin` is not a float.</span>
<span class="sd">        TypeError: If `input1`, `input2` or `target` is not a Tensor.</span>
<span class="sd">        TypeError: If the types of `input1` and `input2` are inconsistent.</span>
<span class="sd">        TypeError: If the types of `input1` and `target` are inconsistent.</span>
<span class="sd">        ValueError: If the shape of `input1` and `input2` are inconsistent.</span>
<span class="sd">        ValueError: If the shape of `input1` and `target` are inconsistent.</span>
<span class="sd">        ValueError: If `reduction` is not one of ``&#39;none&#39;``, ``&#39;mean&#39;`` , ``&#39;sum&#39;``.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; input1 = Tensor(np.array([0.3864, -2.4093, -1.4076]), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; input2 = Tensor(np.array([-0.6012, -1.6681, 1.2928]), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; target = ops.Sign()(Tensor(np.array([-2, -2, 3]), ms.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.margin_ranking_loss(input1, input2, target)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        1.2293333</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">margin</span> <span class="o">=</span> <span class="n">_check_value_type</span><span class="p">(</span><span class="s2">&quot;margin&quot;</span><span class="p">,</span> <span class="n">margin</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="s2">&quot;margin_ranking_loss&quot;</span><span class="p">)</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;input1&#39;</span><span class="p">,</span> <span class="n">input1</span><span class="p">,</span> <span class="s2">&quot;margin_ranking_loss&quot;</span><span class="p">)</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;input2&#39;</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="s2">&quot;margin_ranking_loss&quot;</span><span class="p">)</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;target&#39;</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="s2">&quot;margin_ranking_loss&quot;</span><span class="p">)</span>
    <span class="n">check_input_dtype</span><span class="p">(</span><span class="s1">&#39;input1&#39;</span><span class="p">,</span> <span class="n">input1</span><span class="p">,</span> <span class="s1">&#39;input2&#39;</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="s1">&#39;margin_ranking_loss&#39;</span><span class="p">)</span>
    <span class="n">check_input_dtype</span><span class="p">(</span><span class="s1">&#39;target&#39;</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="s1">&#39;input1&#39;</span><span class="p">,</span> <span class="n">input1</span><span class="p">,</span> <span class="s1">&#39;margin_ranking_loss&#39;</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">maximum_</span><span class="p">(</span><span class="o">-</span><span class="n">target</span> <span class="o">*</span> <span class="p">(</span><span class="n">input1</span> <span class="o">-</span> <span class="n">input2</span><span class="p">)</span> <span class="o">+</span> <span class="n">margin</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_get_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="s2">&quot;margin_ranking_loss&quot;</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_check_reduced_shape_valid</span><span class="p">(</span><span class="n">ori_shape</span><span class="p">,</span> <span class="n">reduced_shape</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">,</span> <span class="n">arg_name1</span><span class="p">,</span> <span class="n">arg_name2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Internal function, used to check whether the reduced shape meets the requirements.&quot;&quot;&quot;</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_reduce_shape</span><span class="p">(</span><span class="n">ori_shape</span><span class="p">,</span> <span class="n">reduced_shape</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">,</span> <span class="n">arg_name1</span><span class="p">,</span> <span class="n">arg_name2</span><span class="p">)</span>


<div class="viewcode-block" id="cosine_embedding_loss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.cosine_embedding_loss.html#mindspore.ops.cosine_embedding_loss">[docs]</a><span class="k">def</span> <span class="nf">cosine_embedding_loss</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    CosineEmbeddingLoss creates a criterion to measure the similarity between two tensors using cosine distance.</span>

<span class="sd">    Given two tensors :math:`input1`, :math:`input2`, and a Tensor label :math:`target` with values 1 or -1:</span>

<span class="sd">    .. math::</span>
<span class="sd">        loss(input1, input2, target) = \begin{cases}</span>
<span class="sd">        1-cos(input1, input2), &amp; \text{if } target = 1\\</span>
<span class="sd">        max(0, cos(input1, input2)-margin), &amp; \text{if } target = -1\\</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        input1 (Tensor): Tensor of shape :math:`(N, *)` where :math:`*` means, any number</span>
<span class="sd">          of additional dimensions.</span>
<span class="sd">        input2 (Tensor): Tensor of shape :math:`(N, *)`, same shape and dtype as `input1`.</span>
<span class="sd">        target (Tensor): Contains value 1 or -1. Suppose the shape of `input1` is</span>
<span class="sd">          :math:`(x_1, x_2, x_3, ..., x_R)`, then the shape of `target` must be :math:`(x_1, x_3, x_4, ..., x_R)`.</span>
<span class="sd">        margin (float, optional): Should be in [-1.0, 1.0]. Default: ``0.0``.</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the output elements will be summed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or Scalar, if `reduction` is ``&quot;none&quot;``, its shape is the same as `target`.</span>
<span class="sd">        Otherwise, a scalar value will be returned.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `margin` is not a float.</span>
<span class="sd">        ValueError: If `reduction` is not one of ``&#39;none&#39;``, ``&#39;mean&#39;``, ``&#39;sum&#39;``.</span>
<span class="sd">        ValueError: If `margin` is not in range [-1.0, 1.0].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; intput1 = Tensor(np.array([[0.3, 0.8], [0.4, 0.3]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; intput2 = Tensor(np.array([[0.4, 1.2], [-0.4, -0.9]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; target = Tensor(np.array([1, -1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.cosine_embedding_loss(intput1, intput2, target)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.0003425479</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;input1&#39;</span><span class="p">,</span> <span class="n">input1</span><span class="p">,</span> <span class="s2">&quot;ops.cosine_embedding_loss&quot;</span><span class="p">)</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;input2&#39;</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="s2">&quot;ops.cosine_embedding_loss&quot;</span><span class="p">)</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;target&#39;</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="s2">&quot;ops.cosine_embedding_loss&quot;</span><span class="p">)</span>
    <span class="n">check_input_dtype</span><span class="p">(</span><span class="s1">&#39;input1&#39;</span><span class="p">,</span> <span class="n">input1</span><span class="p">,</span> <span class="s1">&#39;input2&#39;</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="s1">&#39;ops.cosine_embedding_loss&#39;</span><span class="p">)</span>
    <span class="n">_check_reduced_shape_valid</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">input1</span><span class="p">),</span> <span class="n">ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">target</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,),</span>
                               <span class="s2">&quot;ops.cosine_embedding_loss&quot;</span><span class="p">,</span> <span class="s2">&quot;input1&quot;</span><span class="p">,</span> <span class="s2">&quot;target&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">input1</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">):</span>
        <span class="n">input1</span> <span class="o">=</span> <span class="n">input1</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">input2</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">):</span>
        <span class="n">input2</span> <span class="o">=</span> <span class="n">input2</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">margin_f</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">margin</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">margin</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="n">margin</span>
    <span class="n">_check_value_type</span><span class="p">(</span><span class="s2">&quot;margin&quot;</span><span class="p">,</span> <span class="n">margin_f</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="s2">&quot;ops.cosine_embedding_loss&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">margin_f</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For ops.cosine_embedding_loss, &#39;margin&#39; must be float, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">margin_f</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">margin_f</span> <span class="o">&gt;</span> <span class="mf">1.0</span> <span class="ow">or</span> <span class="n">margin_f</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For ops.cosine_embedding_loss, the value of &#39;margin&#39; should be in [-1, 1],&quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">margin_f</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">prod_sum</span> <span class="o">=</span> <span class="n">reduce_sum_</span><span class="p">(</span><span class="n">input1</span> <span class="o">*</span> <span class="n">input2</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
    <span class="n">square1</span> <span class="o">=</span> <span class="n">reduce_sum_</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">input1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
    <span class="n">square2</span> <span class="o">=</span> <span class="n">reduce_sum_</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">input2</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
    <span class="n">denom</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">square1</span><span class="p">)</span> <span class="o">*</span> <span class="n">ops</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">square2</span><span class="p">)</span>
    <span class="n">cosine</span> <span class="o">=</span> <span class="n">prod_sum</span> <span class="o">/</span> <span class="n">denom</span>

    <span class="n">pos_value</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">cosine</span>
    <span class="n">neg_value</span> <span class="o">=</span> <span class="n">maximum_</span><span class="p">(</span><span class="n">cosine</span> <span class="o">-</span> <span class="n">margin_f</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
    <span class="n">zeros</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">cosine</span><span class="p">)</span>
    <span class="n">pos_part</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">target</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="n">pos_value</span><span class="p">,</span> <span class="n">zeros</span><span class="p">)</span>
    <span class="n">neg_part</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">target</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">neg_value</span><span class="p">,</span> <span class="n">zeros</span><span class="p">)</span>
    <span class="n">output_unreduced</span> <span class="o">=</span> <span class="n">pos_part</span> <span class="o">+</span> <span class="n">neg_part</span>

    <span class="k">return</span> <span class="n">_get_loss</span><span class="p">(</span><span class="n">output_unreduced</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="s2">&quot;cosine_embedding_loss&quot;</span><span class="p">)</span></div>


<div class="viewcode-block" id="max_pool3d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.max_pool3d.html#mindspore.ops.max_pool3d">[docs]</a><span class="k">def</span> <span class="nf">max_pool3d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs a 3D max pooling on the input Tensor.</span>

<span class="sd">    Typically the input is a Tensor with shape :math:`(N_{in}, C_{in}, D_{in}, H_{in}, W_{in})`, outputs</span>
<span class="sd">    regional maximum in the :math:`(D_{in}, H_{in}, W_{in})`-dimension. Given `kernel_size`</span>
<span class="sd">    :math:`ks = (d_{ker}, h_{ker}, w_{ker})` and `stride` :math:`s = (s_0, s_1, s_2)`, the operation is as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(N_i, C_j, d, h, w) =</span>
<span class="sd">        \max_{l=0, \ldots, d_{ker}-1} \max_{m=0, \ldots, h_{ker}-1} \max_{n=0, \ldots, w_{ker}-1}</span>
<span class="sd">        \text{input}(N_i, C_j, s_0 \times d + l, s_1 \times h + m, s_2 \times w + n)</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Tensor of shape :math:`(N_{in}, C_{in}, D_{in}, H_{in}, W_{in})` with data type of int8,</span>
<span class="sd">            int16, int32, int64, uint8, uint16, uint32, uint64, float16, float32 or float64.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of kernel used to take the maximum value and arg</span>
<span class="sd">            value, is an int number that represents depth, height and width of the kernel, or a tuple of</span>
<span class="sd">            three int numbers that represent depth, height and width respectively.</span>
<span class="sd">        stride (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the depth, height and width of movement are both stride, or a tuple of three int numbers that</span>
<span class="sd">            represent depth, height and width of movement respectively.</span>
<span class="sd">            Default: ``None`` , which indicates the moving step is `kernel_size` .</span>
<span class="sd">        padding (Union[int, tuple[int]]): An int number that represents the depth, height and width of movement are both</span>
<span class="sd">            strides, or a tuple of three int numbers that represent depth, height and width of movement respectively.</span>
<span class="sd">            Default: ``0`` .</span>
<span class="sd">        dilation (Union[int, tuple[int]]): Control the stride of elements in the kernel. Default: ``1`` .</span>
<span class="sd">        ceil_mode (bool): Whether to use ceil instead of floor to calculate output shape. Default: ``False`` .</span>
<span class="sd">        return_indices (bool): Whether to output the indices of max value. Default: ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        If `return_indices` is False, return a Tensor `output`, else return a tuple (`output`, `argmax`).</span>

<span class="sd">        - **output** (Tensor) - Maxpooling result, with shape :math:`(N_{out}, C_{out}, D_{out}, H_{out}, W_{out})`.</span>
<span class="sd">          It has the same data type as `x`.</span>

<span class="sd">        .. math::</span>
<span class="sd">            D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{dilation}[0] \times</span>
<span class="sd">            (\text{kernel_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor</span>

<span class="sd">        .. math::</span>
<span class="sd">            H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] - \text{dilation}[1] \times</span>
<span class="sd">            (\text{kernel_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor</span>

<span class="sd">        .. math::</span>
<span class="sd">            W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{dilation}[2] \times</span>
<span class="sd">            (\text{kernel_size}[2] - 1) - 1}{\text{stride}[2]} + 1\right\rfloor</span>

<span class="sd">        - **argmax** (Tensor) - Index corresponding to the maximum value. Data type is int64. It will be returned</span>
<span class="sd">          only when `return_indices` is ``True`` .</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        ValueError: If length of shape of `x` is not equal to 5.</span>
<span class="sd">        TypeError: If `kernel_size` , `stride` , `padding` or `dilation` is not int or tuple.</span>
<span class="sd">        ValueError: If `kernel_size` or `stride` is less than 1.</span>
<span class="sd">        ValueError: If `padding` is less than 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(2 * 1 * 2 * 2 * 2).reshape((2, 1, 2, 2, 2)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output_tensor, argmax = ops.max_pool3d(x, kernel_size=2, stride=1, padding=1, return_indices=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output_tensor.shape)</span>
<span class="sd">        (2, 1, 3, 3, 3)</span>
<span class="sd">        &gt;&gt;&gt; print(argmax.shape)</span>
<span class="sd">        (2, 1, 3, 3, 3)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">strides</span> <span class="o">=</span> <span class="n">stride</span> <span class="k">if</span> <span class="p">(</span><span class="n">stride</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="k">else</span> <span class="n">kernel_size</span>
    <span class="n">max_pool3d_with_argmax_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">MaxPool3DWithArgmax</span><span class="p">)(</span>
        <span class="n">kernel_size</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">)</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">max_pool3d_with_argmax_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">return_indices</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">indices</span>
    <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="grid_sample"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.grid_sample.html#mindspore.ops.grid_sample">[docs]</a><span class="k">def</span> <span class="nf">grid_sample</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Given an `input` and a flow-field `grid`, computes the `output` using `input` values and pixel locations from</span>
<span class="sd">    `grid`. Only spatial (4-D) and volumetric (5-D) `input` is supported.</span>

<span class="sd">    In the spatial (4-D) case, for `input` with shape :math:`(N, C, H_{in}, W_{in})` and `grid` with shape</span>
<span class="sd">    :math:`(N, H_{out}, W_{out}, 2)`, the `output` will have shape :math:`(N, C, H_{out}, W_{out})`.</span>

<span class="sd">    For each output location `output[n, :, h, w]`, the size-2 vector `grid[n, h, w]` specifies `input` pixel</span>
<span class="sd">    locations `x` and `y`, which are used to interpolate the output value `output[n, :, h, w]`. In the case of 5D</span>
<span class="sd">    inputs, `grid[n, d, h, w]`, specifies the `x`, `y`, `z` pixel locations for interpolating</span>
<span class="sd">    `output[n, :, d, h, w]`. And `mode` argument specifies &quot;nearest&quot; or &quot;bilinear&quot; (&quot;bicubic&quot; is not supported yet)</span>
<span class="sd">    interpolation method to sample the input pixels.</span>

<span class="sd">    `grid` specifies the sampling pixel locations normalized by the `input` spatial dimensions. Therefore, it should</span>
<span class="sd">    have most values in the range of :math:`[-1, 1]`.</span>

<span class="sd">    If `grid` has values outside the range of :math:`[-1, 1]`, the corresponding outputs are handled as defined by</span>
<span class="sd">    `padding_mode`. If `padding_mode` is set to be &quot;zeros&quot;, use :math:`0` for out-of-bound grid locations. If</span>
<span class="sd">    `padding_mode` is set to be &quot;border&quot;, use border values for out-of-bound grid locations. If `padding_mode` is set</span>
<span class="sd">    to be &quot;reflection&quot;, use values at locations reflected by the border for out-of-bound grid locations. For location</span>
<span class="sd">    far away from the border, it will keep being reflected until becoming in bound.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): input with shape of :math:`(N, C, H_{in}, W_{in})` (4-D case) or :math:`(N, C, D_{in},</span>
<span class="sd">            H_{in}, W_{in})` (5-D case) and dtype of float32 or float64.</span>
<span class="sd">        grid (Tensor): flow-field with shape of :math:`(N, H_{out}, W_{out}, 2)` (4-D case) or :math:`(N, D_{out},</span>
<span class="sd">            H_{out}, W_{out}, 3)` (5-D case) and same dtype as `input`.</span>
<span class="sd">        mode (str): An optional string specifying the interpolation method. The optional values are</span>
<span class="sd">            ``&#39;bilinear&#39;``, ``&#39;nearest&#39;``. Default: ``&#39;bilinear&#39;`` . Note: `bicubic` is not supported yet. When</span>
<span class="sd">            `mode=&quot;bilinear&quot;` and the input is 5-D, the interpolation mode used internally will actually</span>
<span class="sd">            be trilinear. However, when the input is 4-D, the interpolation mode will legistimately be bilinear.</span>
<span class="sd">            Default: ``&#39;bilinear&#39;`` .</span>

<span class="sd">            - ``&#39;nearest&#39;``: Nearest neighbor interpolation. Each output pixel is assigned the value of the</span>
<span class="sd">              nearest input pixel. This method is simple and fast but can result in blocky or pixelated outputs.</span>
<span class="sd">            - ``&#39;bilinear&#39;``: Bilinear interpolation. Each output pixel is a weighted average of the four nearest input</span>
<span class="sd">              pixels, computed using bilinear interpolation. This method produces smoother results compared</span>
<span class="sd">              to nearest neighbor interpolation.</span>
<span class="sd">            - ``&#39;trilinear&#39;``: Trilinear interpolation. This is an extension of bilinear interpolation to 3D data.</span>
<span class="sd">              It performs bilinear interpolation in the two spatial dimensions and linear interpolation along</span>
<span class="sd">              the third dimension. It is commonly used for volume or 3D image interpolation.</span>

<span class="sd">        padding_mode (str): An optional string specifying the pad method. The optional values are &quot;zeros&quot;, &quot;border&quot; or</span>
<span class="sd">            &quot;reflection&quot;. Default: ``&#39;zeros&#39;`` .</span>
<span class="sd">        align_corners (bool): If set to `True`, the extrema (-1 and 1) are considered as referring to</span>
<span class="sd">            the center points of the input&#39;s corner pixels. If set to `False`, they are instead considered as referring</span>
<span class="sd">            to the corner points of the input&#39;s corner pixels, making the sampling more resolution agnostic. Default:</span>
<span class="sd">            ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, dtype is the same as `input` and whose shape is :math:`(N, C, H_{out}, W_{out})` (4-D) and</span>
<span class="sd">        :math:`(N, C, D_{out}, H_{out}, W_{out})` (5-D).</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` or `grid` is not a Tensor.</span>
<span class="sd">        TypeError: If the dtypes of `input` and `grid` are inconsistent.</span>
<span class="sd">        TypeError: If the dtype of `input` or `grid` is not a valid type.</span>
<span class="sd">        TypeError: If `align_corners` is not a boolean value.</span>
<span class="sd">        ValueError: If the rank of `input` or `grid` is not equal to 4(4-D case) or 5(5-D case).</span>
<span class="sd">        ValueError: If the first dimension of `input` is not equal to that of `grid`.</span>
<span class="sd">        ValueError: If the last dimension of `grid` is not equal to 2(4-D case) or 3(5-D case).</span>
<span class="sd">        ValueError: If `mode` is not &quot;bilinear&quot;, &quot;nearest&quot; or a string value.</span>
<span class="sd">        ValueError: If `padding_mode` is not &quot;zeros&quot;, &quot;border&quot;, &quot;reflection&quot; or a string value.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.arange(16).reshape((2, 2, 2, 2)).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; grid = Tensor(np.arange(0.2, 1, 0.1).reshape((2, 2, 1, 2)).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.grid_sample(input_x, grid, mode=&#39;bilinear&#39;, padding_mode=&#39;zeros&#39;,</span>
<span class="sd">        ...                          align_corners=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[ 1.9      ]</span>
<span class="sd">           [ 2.1999998]]</span>
<span class="sd">          [[ 5.9      ]</span>
<span class="sd">           [ 6.2      ]]]</span>
<span class="sd">         [[[10.5      ]</span>
<span class="sd">           [10.8      ]]</span>
<span class="sd">          [[14.5      ]</span>
<span class="sd">           [14.8      ]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="n">_grid_sampler_2d</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">GridSampler2D</span><span class="p">)(</span><span class="n">mode</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">_grid_sampler_2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span>
    <span class="n">_grid_sampler_3d</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">GridSampler3D</span><span class="p">)(</span><span class="n">mode</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_grid_sampler_3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_check_ctc_loss_inputs</span><span class="p">(</span><span class="n">blank</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">zero_infinity</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">):</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;blank&quot;</span><span class="p">,</span> <span class="n">blank</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">prim_name</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;reduction&#39;</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">prim_name</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">reduction</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">],</span> <span class="s1">&#39;reduction&#39;</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;zero_infinity&quot;</span><span class="p">,</span> <span class="n">zero_infinity</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="n">prim_name</span><span class="p">)</span>


<div class="viewcode-block" id="ctc_loss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ctc_loss.html#mindspore.ops.ctc_loss">[docs]</a><span class="k">def</span> <span class="nf">ctc_loss</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">target_lengths</span><span class="p">,</span> <span class="n">blank</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="n">zero_infinity</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the CTC (Connectionist Temporal Classification) loss and the gradient.</span>

<span class="sd">    CTC is a loss function in sequence labeling problems, which is mainly used to deal with the alignment of input</span>
<span class="sd">    and output labels in sequence labeling problems.</span>
<span class="sd">    While traditional sequence labeling algorithms require the input and output symbols to be perfectly aligned at</span>
<span class="sd">    each moment, CTC expands the label collection and adds empty elements.</span>
<span class="sd">    After labeling the sequence using the extended label set, all the prediction sequences that can be converted</span>
<span class="sd">    into real sequences by the mapping function are correct prediction results, that is, the predicted sequence</span>
<span class="sd">    can be obtained without data alignment processing.</span>
<span class="sd">    Its objective function is to maximize the sum of probabilities of all correct prediction sequences.</span>

<span class="sd">    The CTC algorithm is proposed in `Connectionist Temporal Classification: Labeling Unsegmented Sequence Data with</span>
<span class="sd">    Recurrent Neural Networks &lt;http://www.cs.toronto.edu/~graves/icml_2006.pdf&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        log_probs (Tensor): A tensor of shape :math:`(T, N, C)`, where T is input length, N is batch size and C is</span>
<span class="sd">            number of classes (including blank).</span>
<span class="sd">        targets (Tensor): Target sequences. A tensor of shape :math:`(N, S)`, where S is max target length.</span>
<span class="sd">        input_lengths (Union(tuple, Tensor)): Lengths of the input. A tuple or Tensor of shape :math:`(N)`.</span>
<span class="sd">        target_lengths (Union(tuple, Tensor)): Lengths of the target. A tuple or Tensor of shape :math:`(N)`.</span>
<span class="sd">        blank (int, optional): The blank label. Default: ``0`` .</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the output elements will be summed.</span>

<span class="sd">        zero_infinity (bool, optional): Whether to set infinite loss and correlation gradient to 0. Default: ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        neg_log_likelihood (Tensor), A loss value with shape :math:`(N)` , which is differentiable with respect to</span>
<span class="sd">        each input node.</span>

<span class="sd">        log_alpha (Tensor), The probability of possible trace of input to target with shape :math:`(N, T, 2 * S + 1)` .</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `zero_infinity` is not a bool, `reduction` is not string.</span>
<span class="sd">        TypeError: If the dtype of `log_probs` is not float or double.</span>
<span class="sd">        TypeError: If the dtype of `targets`, `input_lengths` or `target_lengths` is not int32 or int64.</span>
<span class="sd">        ValueError: If the rank of `log_probs` is not 3.</span>
<span class="sd">        ValueError: If the rank of `targets` is not 2.</span>
<span class="sd">        ValueError: If the shape of `input_lengths` does not match N. N is batch size of `log_probs` .</span>
<span class="sd">        ValueError: If the shape of `target_lengths` does not match N. N is batch size of `log_probs` .</span>
<span class="sd">        ValueError: If the value of `blank` is not in range [0, num_labels|C). C is number of classes of `log_probs` .</span>
<span class="sd">        RuntimeError: If any value of `input_lengths` is larger than T. T is the length of `log_probs`.</span>
<span class="sd">        RuntimeError: If any target_lengths[i] is not in range [0, input_length[i]].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; log_probs = Tensor(np.array([[[0.3, 0.6, 0.6]],</span>
<span class="sd">        ...                              [[0.9, 0.4, 0.2]]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; targets = Tensor(np.array([[0, 1]]), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; input_lengths = Tensor(np.array([2]), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; target_lengths = Tensor(np.array([1]), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; loss, log_alpha = ops.ctc_loss(log_probs, targets, input_lengths,</span>
<span class="sd">        ...                                target_lengths, 0, &#39;mean&#39;, True)</span>
<span class="sd">        &gt;&gt;&gt; print(loss)</span>
<span class="sd">        -2.2986124</span>
<span class="sd">        &gt;&gt;&gt; print(log_alpha)</span>
<span class="sd">        [[[0.3       0.3            -inf      -inf      -inf]</span>
<span class="sd">          [1.2       1.8931472 1.2            -inf      -inf]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_ctc_loss_inputs</span><span class="p">(</span><span class="n">blank</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">zero_infinity</span><span class="p">,</span> <span class="s1">&#39;ctc_loss&#39;</span><span class="p">)</span>
    <span class="n">ctc_loss_op</span> <span class="o">=</span> <span class="n">NN_OPS</span><span class="o">.</span><span class="n">CTCLossV2</span><span class="p">(</span><span class="n">blank</span><span class="o">=</span><span class="n">blank</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span> <span class="n">zero_infinity</span><span class="o">=</span><span class="n">zero_infinity</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">log_alpha</span> <span class="o">=</span> <span class="n">ctc_loss_op</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">target_lengths</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;sum&#39;</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
        <span class="n">input_type</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">target_length_t</span> <span class="o">=</span> <span class="n">target_lengths</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">target_length_t</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">input_type</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">log_alpha</span><span class="p">)</span></div>


<div class="viewcode-block" id="gaussian_nll_loss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.gaussian_nll_loss.html#mindspore.ops.gaussian_nll_loss">[docs]</a><span class="k">def</span> <span class="nf">gaussian_nll_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">full</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gaussian negative log likelihood loss.</span>

<span class="sd">    The target values are considered to be samples from a Gaussian distribution, where the expectation and variance are</span>
<span class="sd">    predicted by a neural network. For `labels` modeled on a Gaussian distribution, `logits` to record expectations,</span>
<span class="sd">    and the variance `var` (elements are all positive), the calculated loss is:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss} = \frac{1}{2}\left(\log\left(\text{max}\left(\text{var},</span>
<span class="sd">        \ \text{eps}\right)\right) + \frac{\left(\text{x} - \text{target}\right)^2}</span>
<span class="sd">        {\text{max}\left(\text{var}, \ \text{eps}\right)}\right) + \text{const.}</span>

<span class="sd">    where :math:`eps` is used for stability of :math:`log`. When :math:`full=True`, a constant will be added to the</span>
<span class="sd">    loss. If the shape of :math:`var` and :math:`logits` are not the same (due to a homoscedastic assumption),</span>
<span class="sd">    their shapes must allow correct broadcasting.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Tensor of shape :math:`(N, *)` or :math:`(*)` where :math:`*` means any number of</span>
<span class="sd">            additional dimensions.</span>
<span class="sd">        target (Tensor): Tensor of shape :math:`(N, *)` or :math:`(*)`, same shape as the x, or same shape</span>
<span class="sd">            as the x but with one dimension equal to 1 (to allow broadcasting).</span>
<span class="sd">        var (Tensor): Tensor of shape :math:`(N, *)` or :math:`(*)`, same shape as x, or same shape as the x</span>
<span class="sd">            but with one dimension equal to 1, or same shape as the x but with one fewer dimension</span>
<span class="sd">            (to allow for broadcasting).</span>
<span class="sd">        full (bool, optional): Include the constant term in the loss calculation. When :math:`full=True`,</span>
<span class="sd">            the constant term will be :math:`const = 0.5*log(2\pi)`. Default: ``False``.</span>
<span class="sd">        eps (float, optional): Used to improve the stability of log function must be greater than 0. Default: ``1e-6`` .</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the output elements will be summed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or Tensor scalar, the computed loss depending on :math:`reduction`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x`, `target` or `var` is not a Tensor.</span>
<span class="sd">        TypeError: If `full` is not a bool.</span>
<span class="sd">        TypeError: If `eps` is not a float.</span>
<span class="sd">        ValueError: If `eps` is not a float within (0, inf).</span>
<span class="sd">        ValueError: If `reduction` is not one of ``&quot;none&quot;`` , ``&quot;mean&quot;`` , ``&quot;sum&quot;`` .</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.common.dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; arr1 = np.arange(8).reshape((4, 2))</span>
<span class="sd">        &gt;&gt;&gt; arr2 = np.array([2, 3, 1, 4, 6, 4, 4, 9]).reshape((4, 2))</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(arr1, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; var = Tensor(np.ones((4, 1)), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; target = Tensor(arr2, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.gaussian_nll_loss(x, target, var)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        1.4374993</span>

<span class="sd">    Reference:</span>
<span class="sd">        Nix, D. A. and Weigend, A. S., &quot;Estimating the mean and variance of the</span>
<span class="sd">        target probability distribution&quot;, Proceedings of 1994 IEEE International</span>
<span class="sd">        Conference on Neural Networks (ICNN&#39;94), Orlando, FL, USA, 1994, pp. 55-60</span>
<span class="sd">        vol.1, doi: 10.1109/ICNN.1994.374138.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;gaussian_nll_loss&#39;, &#39;x&#39; must be a tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;gaussian_nll_loss&#39;, &#39;target&#39; must be a tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">target</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;gaussian_nll_loss&#39;, &#39;var&#39; must be a tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">var</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">full</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;gaussian_nll_loss&#39;, &#39;full&#39; must be a bool, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">full</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">eps</span><span class="p">,</span> <span class="nb">float</span><span class="p">)</span> <span class="ow">or</span> <span class="n">eps</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;gaussian_nll_loss&#39;, &#39;eps&#39; must be a positive float, but got </span><span class="si">{</span><span class="n">eps</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;gaussian_nll_loss&#39;, &#39;reduction&#39; must be one of &#39;none&#39;, &#39;mean&#39;, or &#39;sum&#39;,</span><span class="se">\</span>
<span class="s2">        but got </span><span class="si">{</span><span class="n">reduction</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">var</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">var</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="n">var</span> <span class="o">=</span> <span class="n">var</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">maxima</span> <span class="o">=</span> <span class="n">maximum_</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
    <span class="n">logarithm</span> <span class="o">=</span> <span class="n">log_</span><span class="p">(</span><span class="n">maxima</span><span class="p">)</span>
    <span class="n">squared_loss</span> <span class="o">=</span> <span class="n">square_</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">target</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">full</span> <span class="k">else</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">pi</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">logarithm</span> <span class="o">+</span> <span class="n">squared_loss</span> <span class="o">/</span> <span class="n">maxima</span><span class="p">)</span> <span class="o">+</span> <span class="n">c</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="k">elif</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;sum&#39;</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">loss</span></div>


<span class="k">def</span> <span class="nf">_check_hinge_embedding_loss_type</span><span class="p">(</span><span class="n">inputs_dtype</span><span class="p">,</span> <span class="n">targets_dtype</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">margin</span><span class="p">,</span> <span class="n">reduction</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check hinge embedding loss type.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">margin</span><span class="p">,</span> <span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;HingeEmbeddingLoss&#39;, &#39;margin&#39; must be a float or int, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">margin</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;HingeEmbeddingLoss&#39;, &#39;reduction&#39; must be one of &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;,&quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">reduction</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;HingeEmbeddingLoss&#39;, the first input must be a Tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;HingeEmbeddingLoss&#39;, the second input must be a Tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">inputs_dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float_type</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;HingeEmbeddingLoss&#39;, the dtype of the first input must be float, but got &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">inputs_dtype</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">targets_dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float_type</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;HingeEmbeddingLoss&#39;, the dtype of the second input must be float, but got &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">targets_dtype</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="hinge_embedding_loss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.hinge_embedding_loss.html#mindspore.ops.hinge_embedding_loss">[docs]</a><span class="k">def</span> <span class="nf">hinge_embedding_loss</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Measures Hinge Embedding Loss given an input Tensor `intputs` and a labels Tensor `targets` (containing 1 or -1).</span>

<span class="sd">    The loss function for :math:`n`-th sample in the mini-batch is</span>

<span class="sd">    .. math::</span>
<span class="sd">        l_n = \begin{cases}</span>
<span class="sd">        x_n, &amp; \text{if}\; y_n = 1,\\</span>
<span class="sd">        \max \{0, \Delta - x_n\}, &amp; \text{if}\; y_n = -1,</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    and the total loss functions is</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = \begin{cases}</span>
<span class="sd">        \operatorname{mean}(L), &amp; \text{if reduction} = \text{&#39;mean&#39;;}\\</span>
<span class="sd">        \operatorname{sum}(L),  &amp; \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    where :math:`L = \{l_1,\dots,l_N\}^\top`.</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs (Tensor): Predicted values, represented as :math:`x` in the formula.</span>
<span class="sd">        targets (Tensor): Label values, represented as :math:`y` in the formula.</span>
<span class="sd">            Has the same shape as `inputs`, contains -1 or 1.</span>
<span class="sd">        margin (float, int): Threshold defined by Hinge Embedding Loss `margin`.</span>
<span class="sd">            Represented as :math:`\Delta` in the formula. Default: ``1.0`` .</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the output elements will be summed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or Tensor scalar, the computed loss depending on `reduction`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `inputs` is not a Tensor.</span>
<span class="sd">        TypeError: If `targets` is not a Tensor.</span>
<span class="sd">        TypeError: If `margin` is not a float or int.</span>
<span class="sd">        ValueError: If `targets` does not have the same shape as `inputs` or they could not broadcast to each other.</span>
<span class="sd">        ValueError: If `reduction` is not one of ``&#39;none&#39;``, ``&#39;mean&#39;``, ``&#39;sum&#39;``.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.common.dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; arr1 = np.array([0.9, -1.2, 2, 0.8, 3.9, 2, 1, 0, -1]).reshape((3, 3))</span>
<span class="sd">        &gt;&gt;&gt; arr2 = np.array([1, 1, -1, 1, -1, 1, -1, 1, 1]).reshape((3, 3))</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(arr1, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(arr2, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; loss = ops.hinge_embedding_loss(logits, labels, margin=1.0, reduction=&#39;mean&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(loss)</span>
<span class="sd">        0.16666666</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inputs_dtype</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">targets_dtype</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">_check_hinge_embedding_loss_type</span><span class="p">(</span><span class="n">inputs_dtype</span><span class="p">,</span> <span class="n">targets_dtype</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">margin</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>

    <span class="n">min_val</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">inputs_dtype</span><span class="p">)</span>
    <span class="n">pos_index</span> <span class="o">=</span> <span class="n">targets</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="n">neg_index</span> <span class="o">=</span> <span class="n">targets</span> <span class="o">&lt;</span> <span class="mi">0</span>
    <span class="n">pos</span> <span class="o">=</span> <span class="n">pos_index</span> <span class="o">*</span> <span class="n">inputs</span>
    <span class="n">neg</span> <span class="o">=</span> <span class="n">neg_index</span> <span class="o">*</span> <span class="n">inputs</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">margin</span><span class="p">,</span> <span class="n">inputs_dtype</span><span class="p">)</span>
    <span class="n">margin_matrix</span> <span class="o">=</span> <span class="n">m</span> <span class="o">*</span> <span class="n">neg_index</span>
    <span class="n">neg</span> <span class="o">=</span> <span class="n">margin_matrix</span> <span class="o">-</span> <span class="n">neg</span>
    <span class="n">neg</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">neg</span><span class="p">,</span> <span class="n">min_val</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">pos</span> <span class="o">+</span> <span class="n">neg</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="k">elif</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;sum&#39;</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">loss</span></div>


<div class="viewcode-block" id="ctc_greedy_decoder"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ctc_greedy_decoder.html#mindspore.ops.ctc_greedy_decoder">[docs]</a><span class="k">def</span> <span class="nf">ctc_greedy_decoder</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">merge_repeated</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs greedy decoding on the logits given in inputs.</span>

<span class="sd">    Note:</span>
<span class="sd">        On Ascend, &#39;merge_repeated&#39; can not be set to false.</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs (Tensor): The input Tensor must be a 3-D tensor whose shape is</span>
<span class="sd">            :math:`(max\_time, batch\_size, num\_classes)`. `num_classes` must be `num_labels + 1` classes,</span>
<span class="sd">            `num_labels` indicates the number of actual labels. Blank labels are reserved.</span>
<span class="sd">            Default blank label is `num_classes - 1`. Data type must be float32 or float64.</span>
<span class="sd">        sequence_length (Tensor): A tensor containing sequence lengths with the shape of :math:`(batch\_size, )`.</span>
<span class="sd">            The type must be int32. Each value in the tensor must be equal to or less than `max_time`.</span>
<span class="sd">        merge_repeated (bool): If ``true`` , merge repeated classes in output. Default: ``True`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        decoded_indices (Tensor), A tensor with shape of :math:`(total\_decoded\_outputs, 2)`.</span>
<span class="sd">        Data type is int64.</span>

<span class="sd">        decoded_values (Tensor), A tensor with shape of :math:`(total\_decoded\_outputs, )`,</span>
<span class="sd">        it stores the decoded classes. Data type is int64.</span>

<span class="sd">        decoded_shape (Tensor), A tensor with shape of :math:`(batch\_size, max\_decoded\_length)`.</span>
<span class="sd">        Data type is int64.</span>

<span class="sd">        log_probability (Tensor), A tensor with shape of :math:`(batch\_size, 1)`,</span>
<span class="sd">        containing sequence log-probability, has the same type as `inputs`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `merge_repeated` is not a bool.</span>
<span class="sd">        ValueError: If length of shape of `inputs` is not equal to 3.</span>
<span class="sd">        ValueError: If length of shape of `sequence_length` is not equal to 1.</span>
<span class="sd">        ValueError: If value in the `sequence_length` is larger than `max_time`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; inputs = Tensor(np.array([[[0.6, 0.4, 0.2], [0.8, 0.6, 0.3]],</span>
<span class="sd">        ...                           [[0.0, 0.6, 0.0], [0.5, 0.4, 0.5]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; sequence_length = Tensor(np.array([2, 2]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; decoded_indices, decoded_values, decoded_shape, log_probability = ops.ctc_greedy_decoder(inputs,</span>
<span class="sd">        ...                                                                                          sequence_length)</span>
<span class="sd">        &gt;&gt;&gt; print(decoded_indices)</span>
<span class="sd">        [[0 0]</span>
<span class="sd">         [0 1]</span>
<span class="sd">         [1 0]]</span>
<span class="sd">        &gt;&gt;&gt; print(decoded_values)</span>
<span class="sd">        [0 1 0]</span>
<span class="sd">        &gt;&gt;&gt; print(decoded_shape)</span>
<span class="sd">        [2 2]</span>
<span class="sd">        &gt;&gt;&gt; print(log_probability)</span>
<span class="sd">        [[-1.2]</span>
<span class="sd">         [-1.3]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_ctc_greedy_decoder</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">CTCGreedyDecoder</span><span class="p">)(</span><span class="n">merge_repeated</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_ctc_greedy_decoder</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">conv3d_transpose</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                     <span class="n">output_padding</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes a 3D transposed convolution, which is also known as a deconvolution</span>
<span class="sd">    (although it is not an actual deconvolution).</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs (Tensor): The gradients with respect to the output of the convolution.</span>
<span class="sd">           The shape conforms to the default.</span>
<span class="sd">           data_format :math:`(N, C_{in}, D_{out}, H_{out}, W_{out})`. Currently dout data type only supports float16</span>
<span class="sd">           and float32.</span>
<span class="sd">        weight (Tensor): Set size of kernel is :math:`(K_d, K_h, K_w)`, then the shape is</span>
<span class="sd">           :math:`(C_{in}, C_{out}//group, K_d, K_h, K_w)`. Where :math:`group` is the Args parameter,</span>
<span class="sd">           :math:`//` is the symbol for integer division.</span>
<span class="sd">           Currently weight data type only supports float16 and float32.</span>
<span class="sd">        pad_mode (str): Specifies padding mode. The optional values are</span>
<span class="sd">            &quot;same&quot;, &quot;valid&quot;, &quot;pad&quot;. Default: &quot;valid&quot;.</span>

<span class="sd">            - same: Adopts the way of completion. The depth, height and width of the output will be equal to</span>
<span class="sd">              the input `x` divided by stride. The padding will be evenly calculated in head and tail, top and bottom,</span>
<span class="sd">              left and right directions possibility.</span>
<span class="sd">              Otherwise, the last extra padding will be calculated from the tail, bottom and the right side.</span>
<span class="sd">              If this mode is set, `pad` must be 0.</span>

<span class="sd">            - valid: Adopts the way of discarding. The possible largest depth, height and width of output</span>
<span class="sd">              will be returned without padding. Extra pixels will be discarded. If this mode is set, `pad`</span>
<span class="sd">              and `output_padding` must be 0.</span>

<span class="sd">            - pad: Implicit paddings on both sides of the input in depth, height and width. The number of `pad` will</span>
<span class="sd">              be padded to the input Tensor borders. `pad` must be greater than or equal to 0.</span>

<span class="sd">        padding (Union(int, tuple[int])): The padding value to be filled. Default: 0. If `padding` is an integer, the</span>
<span class="sd">            paddings of head, tail, top, bottom, left and right are the same, equal to pad. If `padding` is a tuple of</span>
<span class="sd">            six integers, the padding of head, tail, top, bottom, left and right equal to padding[0], padding[1],</span>
<span class="sd">            padding[2], padding[3], padding[4] and padding[5] correspondingly.</span>
<span class="sd">        stride (Union(int, tuple[int])): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the depth, height and width of movement are both strides, or a tuple of three int numbers that</span>
<span class="sd">            represent depth, height and width of movement respectively. Default: 1.</span>
<span class="sd">        dilation (Union(int, tuple[int])): Specifies the space to use between kernel elements. Default: 1.</span>
<span class="sd">        group (int): Splits input into groups. Default: 1. Only 1 is currently supported.</span>
<span class="sd">        output_padding (Union(int, tuple[int])): Add extra size to each dimension of the output. Default: 0.</span>


<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the gradients with respect to the input of convolution 3D.</span>
<span class="sd">        Tensor of shape :math:`(N, C_{out}//group, D_{out}, H_{out}, W_{out})`,</span>
<span class="sd">        where :math:`group` is the Args parameter.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `group` is not an int.</span>
<span class="sd">        TypeError: If `stride`, `padding` , `dilation` or `output_padding` is neither an int not a tuple.</span>
<span class="sd">        ValueError: If the rank of `inputs`, `weight` is not equal to 5.</span>
<span class="sd">        ValueError: If `stride` or `dilation` is less than 1.</span>
<span class="sd">        ValueError: if inputs[1], weight[1] and weight[2:5] i.e. `in_channel`, `out_channel` and `kernel_size` is less</span>
<span class="sd">                    than 1.</span>
<span class="sd">        ValueError: If `padding` is less than 0.</span>
<span class="sd">        ValueError: If `pad_mode` is not one of &#39;same&#39;, &#39;valid&#39; nor &#39;pad&#39;.</span>
<span class="sd">        ValueError: If `padding` is a tuple whose length is not equal to 6.</span>
<span class="sd">        ValueError: If `pad_mode` is not equal to &#39;padding&#39; and `padding` is not equal to (0, 0, 0, 0, 0, 0).</span>
<span class="sd">        ValueError: If `data_format` is not &#39;NCDHW&#39;.</span>
<span class="sd">        TypeError: If data type of dout and weight is not float16.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; dout = Tensor(np.ones([32, 16, 10, 32, 32]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([16, 3, 4, 6, 2]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; output = conv3d_transpose(dout, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (32, 3, 13, 37, 33)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">5</span><span class="p">:</span>
        <span class="n">raise_value_error</span><span class="p">(</span><span class="s2">&quot;the rank of inputs tensor should be 5.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">5</span><span class="p">:</span>
        <span class="n">raise_value_error</span><span class="p">(</span><span class="s2">&quot;the rank of weight tensor should be 5.&quot;</span><span class="p">)</span>
    <span class="n">in_channel</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">out_channel</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">5</span><span class="p">]</span>
    <span class="n">_conv_3d_transpose</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">Conv3DTranspose</span><span class="p">)(</span><span class="n">in_channel</span><span class="p">,</span> <span class="n">out_channel</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span>
                                                                 <span class="n">padding</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="n">output_padding</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_conv_3d_transpose</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_manipulate_padding</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;convert padding to Conv2D padding&quot;&quot;&quot;</span>
    <span class="n">ms_padding</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;conv</span><span class="si">{</span><span class="n">dim</span><span class="si">}</span><span class="s2">d&#39;, &#39;padding&#39; must be a tuple, list or int, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span> <span class="o">!=</span> <span class="n">dim</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;conv</span><span class="si">{</span><span class="n">dim</span><span class="si">}</span><span class="s2">d&#39;, &#39;padding&#39; must be a tuple or list of </span><span class="si">{</span><span class="n">dim</span><span class="si">}</span><span class="s2"> integers, but got </span><span class="si">{</span><span class="n">padding</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dim</span><span class="p">):</span>
        <span class="n">ms_padding</span> <span class="o">+=</span> <span class="p">(</span><span class="n">padding</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">padding</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">ms_padding</span>


<span class="k">def</span> <span class="nf">_dim_manipulation</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;convert 1d dilation, stride, etc. to 2d&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;conv1d&#39;, </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> must be a positive int, but got </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="mi">1</span><span class="p">,</span> <span class="n">x</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;conv1d&#39;, </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> must be a tuple/list with 1 element or int, but got </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;conv1d&#39;, elements in </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> must be positive int, but got </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;conv1d&#39;, </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> must be an int or a tuple/list with 1 element, but got </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_conv_iterable_lengths</span><span class="p">(</span><span class="n">iterable</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">iter_name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;check iterables lengths used in conv functions&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">iterable</span><span class="p">)</span> <span class="o">!=</span> <span class="n">dim</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;conv</span><span class="si">{</span><span class="n">dim</span><span class="si">}</span><span class="s2">d&#39;, the </span><span class="si">{</span><span class="n">iter_name</span><span class="si">}</span><span class="s2"> must be a int or a tuple/list with length </span><span class="si">{</span><span class="n">dim</span><span class="si">}</span><span class="s2">, &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">iterable</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="conv1d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.conv1d.html#mindspore.ops.conv1d">[docs]</a><span class="k">def</span> <span class="nf">conv1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a 1D convolution over an input tensor. The input Tensor is typically</span>
<span class="sd">    of shape :math:`(N, C_{in}, L_{in})`,</span>
<span class="sd">    where :math:`N` is batch size, :math:`C` is channel number, :math:`L` is input sequence width.</span>

<span class="sd">    The output is calculated based on formula:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) +</span>
<span class="sd">        \sum_{k = 0}^{C_{in} - 1} \text{ccor}({\text{weight}(C_{\text{out}_j}, k), \text{X}(N_i, k)})</span>

<span class="sd">    where :math:`bias` is the output channel bias, :math:`ccor` is</span>
<span class="sd">    the `cross-correlation &lt;https://en.wikipedia.org/wiki/Cross-correlation&gt;`_,</span>
<span class="sd">    , :math:`weight` is the convolution kernel value and :math:`X` represents the input feature map.</span>

<span class="sd">    Here are the indices&#39; meanings:</span>

<span class="sd">    - :math:`i` corresponds to the batch number, the range is :math:`[0, N-1]`,</span>
<span class="sd">      where :math:`N` is the batch size of the input.</span>

<span class="sd">    - :math:`j` corresponds to the output channel, ranging from :math:`[0, C_{out}-1]`,</span>
<span class="sd">      where :math:`C_{out}` is the number of</span>
<span class="sd">      output channels, which is also equal to the number of kernels.</span>

<span class="sd">    - :math:`k` corresponds to the input channel, ranging from :math:`[0, C_{in}-1]`,</span>
<span class="sd">      where :math:`C_{in}` is the number of</span>
<span class="sd">      input channels, which is also equal to the number of channels in the convolutional kernels.</span>

<span class="sd">    Therefore, in the above formula, :math:`{bias}(C_{\text{out}_j})` represents the bias of the :math:`j`-th</span>
<span class="sd">    output channel, :math:`{weight}(C_{\text{out}_j}, k)` represents the slice of the :math:`j`-th convolutional</span>
<span class="sd">    kernel in the :math:`k`-th channel, and :math:`{X}(N_i, k)` represents the slice of the :math:`k`-th input</span>
<span class="sd">    channel in the :math:`i`-th batch of the input feature map.</span>

<span class="sd">    The shape of the convolutional kernel is given by :math:`(\text{kernel_size})`,</span>
<span class="sd">    where :math:`\text{kernel_size}` is the width of the kernel.</span>
<span class="sd">    If we consider the input and output channels as well as the `group` parameter, the complete kernel shape</span>
<span class="sd">    will be :math:`(C_{out}, C_{in} / \text{group}, \text{kernel_size})`,</span>
<span class="sd">    where `group` is the number of groups dividing `x`&#39;s input channel when applying group convolution.</span>

<span class="sd">    For more details about convolution layer, please refer to `Gradient Based Learning Applied to Document Recognition</span>
<span class="sd">    &lt;http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf&gt;`_</span>
<span class="sd">    and `ConvNets &lt;http://cs231n.github.io/convolutional-networks/&gt;`_ .</span>

<span class="sd">    Note:</span>
<span class="sd">        On Ascend platform, only group convolution in depthwise convolution scenarios is supported.</span>
<span class="sd">        That is, when `groups&gt;1`, condition :math:`C_{in}` = :math:`C_{out}` = `groups` must be satisfied.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input Tensor of shape :math:`(N, C_{in}, L_{in})`.</span>
<span class="sd">        weight (Tensor): The convolutional kernel value, it should has shape</span>
<span class="sd">            :math:`(N, C_{in} / \text{groups}, \text{kernel_size})`.</span>
<span class="sd">        bias (Tensor, optional): Bias Tensor with shape :math:`(C_{out})`.</span>
<span class="sd">            When bias is None, zeros will be used. Default: ``None`` .</span>
<span class="sd">        stride (Union(int, tuple[int]), optional): The distance of kernel moving, an int number or a tuple of one int</span>
<span class="sd">            that represents width of movement. Default: ``1``.</span>
<span class="sd">        pad_mode (str, optional): Specifies padding mode. The optional values are</span>
<span class="sd">            ``&quot;same&quot;`` , ``&quot;valid&quot;`` and ``&quot;pad&quot;`` . Default: ``&quot;valid&quot;`` .</span>

<span class="sd">            - ``&quot;same&quot;``: Adopts the way of completion. The height and width of the output will be equal to</span>
<span class="sd">              the input `x` divided by stride. The padding will be evenly calculated in left and right possiblily.</span>
<span class="sd">              Otherwise, the last extra padding will be calculated from the right side.</span>
<span class="sd">              If this mode is set, `padding` must be 0.</span>

<span class="sd">            - ``&quot;valid&quot;``: Adopts the way of discarding. The possible largest width of output will be returned</span>
<span class="sd">              without padding. Extra pixels will be discarded. If this mode is set, `padding` must be 0.</span>

<span class="sd">            - ``&quot;pad&quot;``: Implicit paddings on both sides of the input `x`.</span>
<span class="sd">              The number of `padding` will be padded to the input</span>
<span class="sd">              Tensor borders. `padding` must be greater than or equal to 0.</span>
<span class="sd">        padding (Union(int, tuple[int], list[int]), optional):  Specifies the amount of padding to apply on</span>
<span class="sd">            both side of `input` when `pad_mode` is set to ``&quot;pad&quot;``. The</span>
<span class="sd">            paddings of left and right are the same, equal to padding or padding[0] when padding is a tuple of</span>
<span class="sd">            1 integer. Default: ``0`` .</span>
<span class="sd">        dilation (Union(int, tuple[int]), optional): Specifies the dilation rate to use for dilated convolution.</span>
<span class="sd">            It can be a single int or a tuple of 1 integer.</span>
<span class="sd">            Assuming :math:`dilation=(d0,)`, the convolutional kernel samples the input with a</span>
<span class="sd">            spacing of :math:`d0-1` elements in the width direction.</span>
<span class="sd">            The value should be in the ranges [1, L].</span>
<span class="sd">            Default: ``1`` .</span>
<span class="sd">        groups (int, optional): Splits `input` into groups. Default: ``1`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the value that applied 1D convolution. The shape is :math:`(N, C_{out}, L_{out})`.</span>
<span class="sd">        To see how different pad modes affect the output shape, please refer to</span>
<span class="sd">        :class:`mindspore.nn.Conv1d` for more details.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `stride`, `padding` or `dilation` is neither an int nor a tuple.</span>
<span class="sd">        TypeError: `groups` is not an int.</span>
<span class="sd">        TypeError: If `bias` is not a Tensor.</span>
<span class="sd">        ValueError: If the shape of `bias` is not :math:`(C_{out})` .</span>
<span class="sd">        ValueError: If `stride` or `dilation` is less than 1.</span>
<span class="sd">        ValueError: If `pad_mode` is not one of &#39;same&#39;, &#39;valid&#39; or &#39;pad&#39;.</span>
<span class="sd">        ValueError: If `padding` is a tuple whose length is not equal to 1.</span>
<span class="sd">        ValueError: If `pad_mode` is not equal to &#39;pad&#39; and `padding` is greater than 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(64).reshape((4, 4, 4)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.arange(8).reshape((2, 2, 2)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; bias = Tensor([-0.12345, 2.7683], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.conv1d(x, weight, pad_mode=&#39;pad&#39;, padding=(1,), bias=bias, groups=2)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (4, 2, 5)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">expanded_input</span> <span class="o">=</span> <span class="n">expand_dims_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">sqz</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Squeeze</span><span class="p">)(</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">weight_shape</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">out_channel</span> <span class="o">=</span> <span class="n">weight_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">weight_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">expanded_weight</span> <span class="o">=</span> <span class="n">expand_dims_</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">padding</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;conv1d&#39;, padding must be a tuple or list with 1 element or int, but got </span><span class="si">{</span><span class="n">padding</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">padding</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">padding</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;conv1d&#39;, padding must be a tuple, list or int, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">input_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">in_channel</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">in_channel</span> <span class="o">%</span> <span class="n">groups</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">out_channel</span> <span class="o">%</span> <span class="n">groups</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The argument &#39;groups&#39; should be divisible by &#39;in_channel&#39; &quot;</span> \
                         <span class="sa">f</span><span class="s2">&quot;and &#39;out_channel&#39;, but got group:</span><span class="si">{</span><span class="n">groups</span><span class="si">}</span><span class="s2">, in_channel:</span><span class="si">{</span><span class="n">in_channel</span><span class="si">}</span><span class="s2">, &quot;</span> \
                         <span class="sa">f</span><span class="s2">&quot;out_channel:</span><span class="si">{</span><span class="n">out_channel</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">dilation</span> <span class="o">=</span> <span class="n">_dim_manipulation</span><span class="p">(</span><span class="n">dilation</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;dilation&#39;</span><span class="p">)</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">_dim_manipulation</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;stride&#39;</span><span class="p">)</span>
    <span class="n">conv</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">)(</span><span class="n">out_channel</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">groups</span><span class="p">,</span> <span class="s2">&quot;NCHW&quot;</span><span class="p">)</span>
    <span class="n">conv_res</span> <span class="o">=</span> <span class="n">conv</span><span class="p">(</span><span class="n">expanded_input</span><span class="p">,</span> <span class="n">expanded_weight</span><span class="p">)</span>
    <span class="n">squeezed_conv_res</span> <span class="o">=</span> <span class="n">sqz</span><span class="p">(</span><span class="n">conv_res</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">squeezed_conv_res</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;conv1d&#39;, the &#39;bias&#39; must be a Tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">bias</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">bias</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">out_channel</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;conv1d&#39;, given weight of size </span><span class="si">{</span><span class="n">weight_shape</span><span class="si">}</span><span class="s2">, expected bias to be 1-dimensional with &quot;</span> \
                        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">out_channel</span><span class="si">}</span><span class="s2"> elements, but got bias of size </span><span class="si">{</span><span class="n">bias</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> instead.&quot;</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">bias_add</span><span class="p">(</span><span class="n">squeezed_conv_res</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span></div>


<div class="viewcode-block" id="conv2d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.conv2d.html#mindspore.ops.conv2d">[docs]</a><span class="k">def</span> <span class="nf">conv2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a 2D convolution over an input tensor. The input tenor is typically of</span>
<span class="sd">    shape :math:`(N, C_{in}, H_{in}, W_{in})`, where :math:`N` is batch size, :math:`C` is</span>
<span class="sd">    channel number, :math:`H` is feature height, :math:`W` is feature width.</span>

<span class="sd">    The output is calculated based on formula:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) +</span>
<span class="sd">        \sum_{k = 0}^{C_{in} - 1} \text{ccor}({\text{weight}(C_{\text{out}_j}, k), \text{X}(N_i, k)})</span>

<span class="sd">    where :math:`bias` is the output channel bias, :math:`ccor` is</span>
<span class="sd">    the `cross-correlation &lt;https://en.wikipedia.org/wiki/Cross-correlation&gt;`_,</span>
<span class="sd">    , :math:`weight` is the convolution kernel value and :math:`X` represents the input feature map.</span>

<span class="sd">    Here are the indices&#39; meanings:</span>

<span class="sd">    - :math:`i` corresponds to the batch number, the range is :math:`[0, N-1]`,</span>
<span class="sd">      where :math:`N` is the batch size of the input.</span>

<span class="sd">    - :math:`j` corresponds to the output channel, the range is :math:`[0, C_{out}-1]`,</span>
<span class="sd">      where :math:`C_{out}` is the number of output channels, which is also equal to the number of kernels.</span>

<span class="sd">    - :math:`k` corresponds to the input channel, the range is :math:`[0, C_{in}-1]`,</span>
<span class="sd">      where :math:`C_{in}` is the number of</span>
<span class="sd">      input channels, which is also equal to the number of channels in the convolutional kernels.</span>

<span class="sd">    Therefore, in the above formula, :math:`{bias}(C_{out_j})` represents the bias of the :math:`j`-th</span>
<span class="sd">    output channel, :math:`{weight}(C_{out_j}, k)` represents the slice of the :math:`j`-th convolutional</span>
<span class="sd">    kernel in the :math:`k`-th channel, and :math:`{X}(N_i, k)` represents the slice of the :math:`k`-th input</span>
<span class="sd">    channel in the :math:`i`-th batch of the input feature map.</span>

<span class="sd">    The shape of the convolutional kernel is given by :math:`(\text{kernel_size[0]}, \text{kernel_size[1]})`,</span>
<span class="sd">    where :math:`\text{kernel_size[0]}` and :math:`\text{kernel_size[1]}` are the height and width of the kernel,</span>
<span class="sd">    respectively.</span>
<span class="sd">    If we consider the input and output channels as well as the `group` parameter, the complete kernel shape</span>
<span class="sd">    will be :math:`(C_{out}, C_{in} / \text{group}, \text{kernel_size[0]}, \text{kernel_size[1]})`,</span>
<span class="sd">    where `group` is the number of groups dividing `x`&#39;s input channel when applying group convolution.</span>

<span class="sd">    For more details about convolution layer, please refer to `Gradient Based Learning Applied to Document Recognition</span>
<span class="sd">    &lt;http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf&gt;`_ and</span>
<span class="sd">    `ConvNets &lt;http://cs231n.github.io/convolutional-networks/&gt;`_.</span>

<span class="sd">    Note:</span>
<span class="sd">        On Ascend platform, only group convolution in depthwise convolution scenarios is supported.</span>
<span class="sd">        That is, when `groups&gt;1`, condition :math:`C_{in}` = :math:`C_{out}` = `groups` must be satisfied.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.</span>
<span class="sd">        weight (Tensor): Tensor of shape</span>
<span class="sd">            :math:`(N, C_{in} / \text{groups}, \text{kernel_size[0]}, \text{kernel_size[1]})`, then the size of kernel</span>
<span class="sd">            is :math:`(\text{kernel_size[0]}, \text{kernel_size[1]})`.</span>
<span class="sd">        bias (Tensor, optional): Bias Tensor with shape :math:`(C_{out})`.</span>
<span class="sd">            When bias is ``None`` , zeros will be used. Default: ``None`` .</span>
<span class="sd">        stride (Union(int, tuple[int]), optional): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the height and width of movement are both strides, or a tuple of two int numbers that</span>
<span class="sd">            represent height and width of movement respectively. Default: ``1`` .</span>
<span class="sd">        pad_mode (str, optional): Specifies padding mode. The optional values are</span>
<span class="sd">            ``&quot;same&quot;`` , ``&quot;valid&quot;`` and ``&quot;pad&quot;`` . Default: ``&quot;valid&quot;`` .</span>

<span class="sd">            - same: Adopts the way of completion. The height and width of the output will be equal to</span>
<span class="sd">              the input `x` divided by stride. The padding will be evenly calculated in top and bottom,</span>
<span class="sd">              left and right possiblily. Otherwise, the last extra padding will be calculated from the bottom</span>
<span class="sd">              and the right side. If this mode is set, `padding` must be 0.</span>

<span class="sd">            - valid: Adopts the way of discarding. The possible largest height and width of output will be returned</span>
<span class="sd">              without padding. Extra pixels will be discarded. If this mode is set, `padding` must be 0.</span>

<span class="sd">            - pad: Implicit paddings on both sides of the input `x`. The number of `padding` will be padded to the input</span>
<span class="sd">              Tensor borders. `padding` must be greater than or equal to 0.</span>
<span class="sd">        padding (Union(int, tuple[int], list[int]), optional): Implicit paddings on both sides of the input `x`.</span>
<span class="sd">            If `padding` is one integer, the paddings of top, bottom, left and right are the same, equal to padding.</span>
<span class="sd">            If `padding` is a tuple/list with 2 integers, the padding of top adn bottom is padding[0],</span>
<span class="sd">            and the padding of left and right is padding[1]. Default: ``0`` .</span>
<span class="sd">        dilation (Union(int, tuple[int]), optional): Gaps between kernel elements.The data type is int or a tuple of</span>
<span class="sd">            2 integers. Specifies the dilation rate to use for dilated convolution. If set to be :math:`k &gt; 1`,</span>
<span class="sd">            there will be :math:`k - 1` pixels skipped for each sampling location. Its value must</span>
<span class="sd">            be greater than or equal to 1 and bounded by the height and width of the input `x`. Default: ``1`` .</span>
<span class="sd">        groups (int, optional): Splits `input` into groups. Default: ``1`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the value that applied 2D convolution. The shape is :math:`(N, C_{out}, H_{out}, W_{out})`.</span>
<span class="sd">        To see how different pad modes affect the output shape, please refer to</span>
<span class="sd">        :class:`mindspore.nn.Conv2d` for more details.</span>


<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `stride`, `padding` or `dilation` is neither an int nor a tuple.</span>
<span class="sd">        TypeError: `groups` is not an int.</span>
<span class="sd">        TypeError: If `bias` is not a Tensor.</span>
<span class="sd">        ValueError: If  the shape of `bias` is not :math:`(C_{out})` .</span>
<span class="sd">        ValueError: If `stride` or `dilation` is less than 1.</span>
<span class="sd">        ValueError: If `pad_mode` is not one of &#39;same&#39;, &#39;valid&#39; or &#39;pad&#39;.</span>
<span class="sd">        ValueError: If `padding` is a tuple/list whose length is not equal to 2.</span>
<span class="sd">        ValueError: If `pad_mode` is not equal to &#39;pad&#39; and `padding` is greater than 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([10, 32, 32, 32]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([32, 32, 3, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.conv2d(x, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (10, 32, 30, 30)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
        <span class="n">_check_conv_iterable_lengths</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">iter_name</span><span class="o">=</span><span class="s1">&#39;stride&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dilation</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
        <span class="n">_check_conv_iterable_lengths</span><span class="p">(</span><span class="n">dilation</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">iter_name</span><span class="o">=</span><span class="s1">&#39;dilation&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="n">_manipulate_padding</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">weight_shape</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">out_channel</span> <span class="o">=</span> <span class="n">weight_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">weight_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">4</span><span class="p">]</span>
    <span class="n">input_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">in_channel</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">in_channel</span> <span class="o">%</span> <span class="n">groups</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">out_channel</span> <span class="o">%</span> <span class="n">groups</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The argument &#39;groups&#39; should be divisible by &#39;in_channel&#39; &quot;</span> \
                         <span class="sa">f</span><span class="s2">&quot;and &#39;out_channel&#39;, but got group:</span><span class="si">{</span><span class="n">groups</span><span class="si">}</span><span class="s2">, in_channel:</span><span class="si">{</span><span class="n">in_channel</span><span class="si">}</span><span class="s2">, &quot;</span> \
                         <span class="sa">f</span><span class="s2">&quot;out_channel:</span><span class="si">{</span><span class="n">out_channel</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">conv</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">)(</span><span class="n">out_channel</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">groups</span><span class="p">,</span> <span class="s2">&quot;NCHW&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">conv</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;conv2d&#39;, the &#39;bias&#39; must be a Tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">bias</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">conv_result</span> <span class="o">=</span> <span class="n">conv</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">bias_add</span><span class="p">(</span><span class="n">conv_result</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span></div>


<span class="k">def</span> <span class="nf">conv_transpose2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">output_padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates a 2D transposed convolution, which can be regarded as Conv2d for the gradient of the input,</span>
<span class="sd">    also called deconvolution (although it is not an actual deconvolution).</span>

<span class="sd">    The input is typically of shape :math:`(N, C_{in}, H_{in}, W_{in})`,</span>
<span class="sd">    where :math:`N` is batch size, :math:`C_{in}` is space dimension,</span>
<span class="sd">    :math:`H_{in}, W_{in}` are the height and width of the feature layer respectively.</span>

<span class="sd">    When Conv2d and Conv2dTranspose are initialized with the same parameters, and `pad_mode` is set to &#39;pad&#39;,</span>
<span class="sd">    :math:`dilation * (kernel\_size - 1) - padding` amount of zero will be paded to the height and width</span>
<span class="sd">    directions of the input, they are inverses of each other in regard to the input and output shapes in this case.</span>
<span class="sd">    However, when `stride` &gt; 1, Conv2d maps multiple input shapes to the same output shape. Deconvolutional network</span>
<span class="sd">    can refer to `Deconvolutional Networks &lt;https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.</span>
<span class="sd">        weight (Tensor): Tensor of shape</span>
<span class="sd">            :math:`(N, C_{in} / \text{groups}, \text{kernel_size[0]}, \text{kernel_size[1]})`, then the size of kernel</span>
<span class="sd">            is :math:`(\text{kernel_size[0]}, \text{kernel_size[1]})`.</span>
<span class="sd">        bias (Tensor, optional): Bias Tensor with shape :math:`(C_{out})`.</span>
<span class="sd">            When bias is ``None`` , zeros will be used. Default: ``None`` .</span>
<span class="sd">        stride (Union(int, tuple[int]), optional): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the height and width of movement are both strides, or a tuple of two int numbers that</span>
<span class="sd">            represent height and width of movement respectively. Default: ``1`` .</span>
<span class="sd">        padding (Union(int, tuple[int], list[int]), optional): Implicit paddings on both sides of the input `x`.</span>
<span class="sd">            Can be an integer or a tuple/list with 2 integers.</span>
<span class="sd">        output_padding (Union[int, tuple[int]]): The number of padding on the height and width directions of the output.</span>
<span class="sd">            The data type is an integer or a tuple of two integers. If `output_padding` is an integer,</span>
<span class="sd">            then the bottom and right padding are all equal to `output_padding`. If `output_padding` is a tuple of</span>
<span class="sd">            2 integers, then the bottom and right padding is equal to `output_padding[0]`, `output_padding[1]`</span>
<span class="sd">            respectively.</span>
<span class="sd">        groups (int, optional): Splits `input` into groups. Default: ``1`` .</span>
<span class="sd">        dilation (Union(int, tuple[int]), optional): Gaps between kernel elements.The data type is int or a tuple of</span>
<span class="sd">            2 integers. Specifies the dilation rate to use for dilated convolution. If set to be :math:`k &gt; 1`,</span>
<span class="sd">            there will be :math:`k - 1` pixels skipped for each sampling location. Its value must</span>
<span class="sd">            be greater than or equal to 1 and bounded by the height and width of the input `x`. Default: ``1`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the value that applied 2D convolution. The shape is :math:`(N, C_{out}, H_{out}, W_{out})`.</span>
<span class="sd">        To see how different pad modes affect the output shape, please refer to</span>
<span class="sd">        :class:`mindspore.nn.Conv2dTranspose` for more details.</span>


<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `stride`, `padding` or `dilation` is neither an int nor a tuple.</span>
<span class="sd">        TypeError: `groups` is not an int.</span>
<span class="sd">        TypeError: If `bias` is not a Tensor.</span>
<span class="sd">        ValueError: If  the shape of `bias` is not :math:`(C_{out})` .</span>
<span class="sd">        ValueError: If `stride` or `dilation` is less than 1.</span>
<span class="sd">        ValueError: If `padding` is a tuple/list whose length is not equal to 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([1, 6, 32, 32]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([6, 3, 5, 5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.conv_transpose2d(x, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 3, 36, 36)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">conv</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Convolution</span><span class="p">)(</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">output_padding</span><span class="p">,</span> <span class="n">groups</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">conv</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>


<div class="viewcode-block" id="hardsigmoid"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.hardsigmoid.html#mindspore.ops.hardsigmoid">[docs]</a><span class="k">def</span> <span class="nf">hardsigmoid</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hard sigmoid activation function.</span>

<span class="sd">    Applies hard sigmoid activation element-wise. The input is a Tensor with any valid shape.</span>

<span class="sd">    Hard sigmoid is defined as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{hsigmoid}(x_{i}) = \max(0, \min(1, \frac{x_{i} + 3}{6}))</span>

<span class="sd">    where :math:`x_i` is an element of the input Tensor.</span>

<span class="sd">    HSigmoid Activation Function Graph:</span>

<span class="sd">    .. image:: ../images/HSigmoid.png</span>
<span class="sd">        :align: center</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A Tensor whose dtype and shape are the same as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not int or float.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([ -3.5,  0,  4.3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.hardsigmoid(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.  0.5 1. ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">hardsigmoid_</span> <span class="o">=</span> <span class="n">NN_OPS</span><span class="o">.</span><span class="n">HSigmoid</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">hardsigmoid_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="hardtanh"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.hardtanh.html#mindspore.ops.hardtanh">[docs]</a><span class="k">def</span> <span class="nf">hardtanh</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">max_val</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies the hardtanh activation function element-wise. The activation function is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{hardtanh}(input) = \begin{cases}</span>
<span class="sd">            max\_val, &amp; \text{ if } input &gt; max\_val \\</span>
<span class="sd">            min\_val, &amp; \text{ if } input &lt; min\_val \\</span>
<span class="sd">            input, &amp; \text{ otherwise. }</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Linear region range :math:`[min\_val, max\_val]` can be adjusted using `min_val` and `max_val`.</span>

<span class="sd">    Hardtanh Activation Function Graph:</span>

<span class="sd">    .. image:: ../images/Hardtanh.png</span>
<span class="sd">        :align: center</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input Tensor.</span>
<span class="sd">        min_val (Union[int, float], optional): Minimum value of the linear region range. Default: ``-1.0`` .</span>
<span class="sd">        max_val (Union[int, float], optional): Maximum value of the linear region range. Default: ``1.0`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same dtype and shape as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `min_val` is neither float nor int.</span>
<span class="sd">        TypeError: If dtype of `max_val` is neither float nor int.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([-1, -2, 0, 2, 1], mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.hardtanh(x, min_val=-1.0, max_val=1.0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-1. -1.  0.  1.  1.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;input&#39;</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;hardtanh&quot;</span><span class="p">)</span>
    <span class="n">_check_value_type</span><span class="p">(</span><span class="s2">&quot;min_val&quot;</span><span class="p">,</span> <span class="n">min_val</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span> <span class="s2">&quot;hardtanh&quot;</span><span class="p">)</span>
    <span class="n">_check_value_type</span><span class="p">(</span><span class="s2">&quot;max_val&quot;</span><span class="p">,</span> <span class="n">max_val</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span> <span class="s2">&quot;hardtanh&quot;</span><span class="p">)</span>
    <span class="n">input_dtype</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">maximum_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">min_val</span><span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">minimum_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">max_val</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">input</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">input_dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="huber_loss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.huber_loss.html#mindspore.ops.huber_loss">[docs]</a><span class="k">def</span> <span class="nf">huber_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">delta</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the error between the predicted value and the target value,</span>
<span class="sd">    which has the best of both the loss of :func:`mindspore.ops.l1_loss` and the loss of :func:`mindspore.ops.mse_loss`.</span>

<span class="sd">    Assuming that the :math:`x` and :math:`y` are 1-D Tensor, length :math:`N`, the `reduction` parameter</span>
<span class="sd">    is set to ``&#39;none&#39;`` then calculate the loss of :math:`x` and :math:`y` without dimensionality reduction.</span>
<span class="sd">    The formula is as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top</span>

<span class="sd">    with</span>

<span class="sd">    .. math::</span>
<span class="sd">        l_n = \begin{cases}</span>
<span class="sd">            0.5 * (x_n - y_n)^2, &amp; \text{if } |x_n - y_n| &lt; delta; \\</span>
<span class="sd">            delta * (|x_n - y_n| - 0.5 * delta), &amp; \text{otherwise. }</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    where :math:`N` is the batch size.</span>

<span class="sd">    If `reduction` is &quot;mean&quot; or &quot;sum&quot;, then:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">            \operatorname{mean}(L), &amp; \text{if reduction} = \text{&quot;mean&quot;;}\\</span>
<span class="sd">            \operatorname{sum}(L),  &amp; \text{if reduction} = \text{&quot;sum&quot;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Predicted value, Tensor of any dimension.</span>
<span class="sd">        target (Tensor): Target value, has same dtype and shape as the `input` in common cases.</span>
<span class="sd">            However, when the shape of `target` is different from the shape of `input`,</span>
<span class="sd">            and they should be broadcasted to each other.</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the output elements will be summed.</span>

<span class="sd">        delta (Union[int, float]): The threshold to change between two type of loss.</span>
<span class="sd">            The value must be greater than zero. Default: ``1.0`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or Scalar, if `reduction` is ``&#39;none&#39;``, return a Tensor with same shape and dtype as `input`.</span>
<span class="sd">        Otherwise, a scalar value will be returned.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` or `target` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `delta` is neither float nor int.</span>
<span class="sd">        ValueError: If `delta` is less than or equal to 0.</span>
<span class="sd">        ValueError: If `reduction` is not one of ``&#39;none&#39;``, ``&#39;mean&#39;``, ``&#39;sum&#39;``.</span>
<span class="sd">        ValueError: If `input` and `target` have different shapes and cannot be broadcasted to each other.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([1, 2, 10, 2], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; target = Tensor([1, 5, 1, 20], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.huber_loss(x, target, reduction=&quot;mean&quot;, delta=2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        13.5</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;input&#39;</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;huber_loss&quot;</span><span class="p">)</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;target&#39;</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="s2">&quot;huber_loss&quot;</span><span class="p">)</span>
    <span class="n">_check_value_type</span><span class="p">(</span><span class="s2">&quot;delta&quot;</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span> <span class="s2">&quot;huber_loss&quot;</span><span class="p">)</span>
    <span class="n">_check_number_gt_value</span><span class="p">(</span><span class="s2">&quot;delta&quot;</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="s2">&quot;huber_loss&quot;</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">sub_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">abs_</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="n">cond</span> <span class="o">=</span> <span class="n">less_</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span>
    <span class="n">l1</span> <span class="o">=</span> <span class="n">mul_</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">square_</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
    <span class="n">l2</span> <span class="o">=</span> <span class="n">mul_</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="n">sub_</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">delta</span><span class="p">))</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">select_</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">l2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_get_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="s2">&quot;huber_loss&quot;</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_check_adaptive_avg_pool1d_output_size</span><span class="p">(</span><span class="n">output_size</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check the output_size value in adaptive_avg_pool1d op.&quot;&quot;&quot;</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s2">&quot;output_size&quot;</span><span class="p">,</span> <span class="s1">&#39;adaptive_avg_pool1d&#39;</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;output_size&#39;</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="s1">&#39;adaptive_avg_pool1d&#39;</span><span class="p">)</span>


<div class="viewcode-block" id="adaptive_avg_pool1d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.adaptive_avg_pool1d.html#mindspore.ops.adaptive_avg_pool1d">[docs]</a><span class="k">def</span> <span class="nf">adaptive_avg_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a 1D adaptive average pooling over an input Tensor which can be regarded as a composition of 1D input</span>
<span class="sd">    planes.</span>

<span class="sd">    Typically, the input is of shape :math:`(N, C, L_{in})`, adaptive_avg_pool1d outputs regional average</span>
<span class="sd">    in the :math:`L_{in}`-dimension. The output is of shape :math:`(N, C, L_{out})`, where :math:`L_{out}`</span>
<span class="sd">    is defined by `output_size`.</span>

<span class="sd">    Note:</span>
<span class="sd">        :math:`L_{in}` must be divisible by `output_size`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Tensor of shape :math:`(N, C, L_{in})`, with float16 or float32 data type.</span>
<span class="sd">        output_size (int): the target output size :math:`L_{out}`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor of shape :math:`(N, C, L_{out})`, has the same type as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `output_size` is not an int.</span>
<span class="sd">        TypeError: If `input` is neither float16 nor float32.</span>
<span class="sd">        ValueError: If `output_size` is less than 1.</span>
<span class="sd">        ValueError: If length of shape of `input` is not equal to 3.</span>
<span class="sd">        ValueError: If the last dimension of `input` is smaller than `output_size`.</span>
<span class="sd">        ValueError: If the last dimension of `input` is not divisible by `output_size`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.random.randint(0, 10, [1, 3, 6]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.adaptive_avg_pool1d(input, output_size=2)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 3, 2)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">_check</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
        <span class="n">x_in_shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">x_dtype</span> <span class="o">=</span> <span class="n">dtype_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For adaptive_avg_pool1d, the input input must be tensor&quot;</span><span class="p">)</span>

        <span class="n">_check_adaptive_avg_pool1d_output_size</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">x_in_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">output_size</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For adaptive_avg_pool1d input&#39;s last dimension must be greater or equal to &quot;</span> \
                             <span class="sa">f</span><span class="s2">&quot;output size </span><span class="si">{</span><span class="n">output_size</span><span class="si">}</span><span class="s2">, but got </span><span class="si">{</span><span class="n">x_in_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">x_in_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">%</span> <span class="n">output_size</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For adaptive_avg_pool1d input&#39;s last dimension must be divisible by &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;output size </span><span class="si">{</span><span class="n">output_size</span><span class="si">}</span><span class="s2">, but got </span><span class="si">{</span><span class="n">x_in_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">x_dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For adaptive_avg_pool1d, the input dtype must be float16 or float32, &quot;</span> \
                            <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">x_dtype</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="n">_check</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
    <span class="n">x_in_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">squeeze_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Squeeze</span><span class="p">)(</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">width</span> <span class="o">=</span> <span class="n">x_in_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">width</span> <span class="o">//</span> <span class="n">output_size</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">width</span> <span class="o">-</span> <span class="p">(</span><span class="n">output_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">width</span> <span class="o">//</span> <span class="n">output_size</span><span class="p">)</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">)</span>
    <span class="n">avg_pool_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">AvgPool</span><span class="p">)(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">expand_dims_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">avg_pool_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">squeeze_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">input</span></div>

<div class="viewcode-block" id="layer_norm"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.layer_norm.html#mindspore.ops.layer_norm">[docs]</a><span class="k">def</span> <span class="nf">layer_norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">normalized_shape</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies the Layer Normalization to the input tensor.</span>

<span class="sd">    This operator will normalize the input tensor on given axis. LayerNorm is described in the paper</span>
<span class="sd">    `Layer Normalization &lt;https://arxiv.org/abs/1607.06450&gt;`_.</span>

<span class="sd">    .. math::</span>
<span class="sd">        y = \frac{x - mean}{\sqrt{variance + \epsilon}} * \gamma + \beta</span>

<span class="sd">    where :math:`\gamma` is weight, :math:`\beta` is bias, :math:`\epsilon` is eps.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Tensor of shape :math:`(N, \ldots)`. The input of LayerNorm.</span>
<span class="sd">        normalized_shape (Union(int, tuple[int], list[int])): The normalized shape of `input` for LayerNorm.</span>
<span class="sd">          `normalized_shape` equal to `input_shape[begin_norm_axis:]`, where `begin_norm_axis` represents the axis</span>
<span class="sd">          where normalization begins.</span>
<span class="sd">        weight (Tensor, optional): Learnable parameter :math:`\gamma` . Tensor of shape `normalized_shape`.</span>
<span class="sd">          Default: ``None``, has the same data type with `input`. Initialized to ``1`` when `weight` is None.</span>
<span class="sd">        bias (Tensor, optional): Learnable parameter :math:`\beta` . Tensor of shape `normalized_shape`.</span>
<span class="sd">          Default: ``None``, has the same data type with `input`. Initialized to ``0`` when `bias` is None.</span>
<span class="sd">        eps (float, optional): A value added to the denominator for numerical stability(:math:`\epsilon`).</span>
<span class="sd">          Default: ``1e-5`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        - **output** (Tensor) - The normalized input, has the same type and shape as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If `normalized_shape` is not an integer, a list or a tuple.</span>
<span class="sd">        TypeError: If `eps` is not a float.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1, 2, 3], [1, 2, 3]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; normalized_shape = (3,)</span>
<span class="sd">        &gt;&gt;&gt; gamma = Tensor(np.ones(normalized_shape), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta = Tensor(np.zeros(normalized_shape), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; eps = 1e-7</span>
<span class="sd">        &gt;&gt;&gt; output = ops.layer_norm(input_x, normalized_shape, gamma, beta, eps)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-1.2247448 0. 1.2247448]</span>
<span class="sd">         [-1.2247448 0. 1.2247448]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">normalized_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">bias</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">normalized_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">layer_norm_ext_op</span> <span class="o">=</span> <span class="n">LayerNormExt</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">layer_norm_ext_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">normalized_shape</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">eps</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span></div>


<div class="viewcode-block" id="group_norm"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.group_norm.html#mindspore.ops.group_norm">[docs]</a><span class="k">def</span> <span class="nf">group_norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">num_groups</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Group Normalization over a mini-batch of inputs.</span>

<span class="sd">    Group Normalization is widely used in recurrent neural networks. It applies</span>
<span class="sd">    normalization on a mini-batch of inputs for each single training case as described</span>
<span class="sd">    in the paper `Group Normalization &lt;https://arxiv.org/pdf/1803.08494.pdf&gt;`_. Group Normalization</span>
<span class="sd">    divides the channels into groups and computes within each group the mean and variance for normalization,</span>
<span class="sd">    and it performs very stable over a wide range of batch size. :math:`\gamma` and :math:`\beta` are trainable scale</span>
<span class="sd">    and shift.</span>
<span class="sd">    It can be described using the following formula:</span>

<span class="sd">    .. math::</span>
<span class="sd">        y = \frac{x - \mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta</span>

<span class="sd">    where :math:`\gamma` is `weight`, :math:`\beta` is `bias`, :math:`\epsilon` is `eps`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor) : The input feature with shape :math:`(N, C, *)` where :math:`*` means, any number of</span>
<span class="sd">            additional dimensions.</span>
<span class="sd">        num_groups (int): The number of groups to be divided along the channel dimension.</span>
<span class="sd">        weight (Tensor, optional): The shape :math:`(C,)`, Default: ``None``, has the same data type with `input`.</span>
<span class="sd">        bias (Tensor, optional): The shape :math:`(C,)`, Default: ``None``, has the same data type with `input`.</span>
<span class="sd">        eps (float, optional): A value added to the denominator for numerical stability. Default: ``1e-5`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the normalized and scaled offset tensor, has the same shape and data type as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `num_groups` is not an int.</span>
<span class="sd">        TypeError: If `eps` is not a float.</span>
<span class="sd">        ValueError: If `num_groups` is less than 1.</span>
<span class="sd">        ValueError: If `C` (the second parameter of dimensions of `input`) is not divided by `num_groups`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import group_norm</span>
<span class="sd">        &gt;&gt;&gt; x = ms.Tensor(np.ones([1, 2, 4, 4], np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = group_norm(x, 2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[0. 0. 0. 0.]</span>
<span class="sd">           [0. 0. 0. 0.]</span>
<span class="sd">           [0. 0. 0. 0.]</span>
<span class="sd">           [0. 0. 0. 0.]]</span>
<span class="sd">          [[0. 0. 0. 0.]</span>
<span class="sd">           [0. 0. 0. 0.]</span>
<span class="sd">           [0. 0. 0. 0.]</span>
<span class="sd">           [0. 0. 0. 0.]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">group_norm_op</span> <span class="o">=</span> <span class="n">GroupNorm</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">group_norm_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">num_groups</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">eps</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span></div>

<div class="viewcode-block" id="batch_norm"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.batch_norm.html#mindspore.ops.batch_norm">[docs]</a><span class="k">def</span> <span class="nf">batch_norm</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">running_mean</span><span class="p">,</span> <span class="n">running_var</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Batch Normalization for input data and updated parameters.</span>

<span class="sd">    Batch Normalization is widely used in convolutional neural networks. This operation</span>
<span class="sd">    applies Batch Normalization over inputs to avoid internal covariate shift as described</span>
<span class="sd">    in the paper `Batch Normalization: Accelerating Deep Network Training by Reducing Internal</span>
<span class="sd">    Covariate Shift &lt;https://arxiv.org/abs/1502.03167&gt;`_. It rescales and recenters the</span>
<span class="sd">    features using a mini-batch of data and the learned parameters can be described</span>
<span class="sd">    in the following formula,</span>

<span class="sd">    .. math::</span>

<span class="sd">        y = \frac{x - mean}{\sqrt{variance + \epsilon}} * \gamma + \beta</span>

<span class="sd">    where :math:`\gamma` is `weight`, :math:`\beta` is `bias`, :math:`\epsilon` is `eps`, :math:`mean` is the</span>
<span class="sd">    mean of :math:`x`, :math:`variance` is the variance of :math:`x`.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - For Atlas 200/300/500 inference product,</span>
<span class="sd">          the result accuracy fails to reach 1 due to the square root instruction.</span>

<span class="sd">    Note:</span>
<span class="sd">        - If `training` is `False`, `weight`, `bias`, `running_mean` and `running_var` are Tensors.</span>
<span class="sd">        - If `training` is `True`, `weight`, `bias`, `running_mean` and `running_var` are Parameters.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Tensor of shape :math:`(N, C)`, with float16 or float32 data type.</span>
<span class="sd">        running_mean (Union[Tensor, Parameter]): The shape :math:`(C,)`, has the same data type with `weight`.</span>
<span class="sd">        running_var (Union[Tensor, Parameter]): The shape :math:`(C,)`, has the same data type with `weight`.</span>
<span class="sd">        weight (Union[Tensor, Parameter]): The shape :math:`(C,)`, with float16 or float32 data type.</span>
<span class="sd">        bias (Union[Tensor, Parameter]): The shape :math:`(C,)`, has the same data type with `weight`.</span>
<span class="sd">        training (bool, optional): If `training` is `True`, `mean` and `variance` are computed during training.</span>
<span class="sd">            If `training` is `False`, they&#39;re loaded from checkpoint during inference. Default: ``False`` .</span>
<span class="sd">        momentum (float, optional): The hyper parameter to compute moving average for `running_mean` and `running_var`</span>
<span class="sd">            (e.g. :math:`new\_running\_mean = (1 - momentum) * running\_mean + momentum * current\_mean`).</span>
<span class="sd">            Momentum value must be `[0, 1]`. Default: ``0.1`` .</span>
<span class="sd">        eps (float, optional): A small value added for numerical stability. Default: ``1e-5``, value must be `(0, 1]` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        output_x (Tensor) - The same type and shape as the `input_x`. The shape is :math:`(N, C)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `training` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `eps` or `momentum` is not float.</span>
<span class="sd">        TypeError: If `input_x`, `weight`, `bias`, `running_mean` or `running_var` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input_x`, `weight` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor([[1.0, 2.0], [3.0, 4.0]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; running_mean = Tensor([0.5, 1.5], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; running_var = Tensor([0.1, 0.2], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor([2.0, 2.0], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; bias = Tensor([-1.0, -1.0], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.batch_norm(input_x, running_mean, running_var, weight, bias)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 2.1621194  1.2360122]</span>
<span class="sd">         [14.810596  10.180061 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch_norm_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">)(</span><span class="n">is_training</span><span class="o">=</span><span class="n">training</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">batch_norm_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">running_mean</span><span class="p">,</span> <span class="n">running_var</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></div>


<div class="viewcode-block" id="bias_add"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bias_add.html#mindspore.ops.bias_add">[docs]</a><span class="k">def</span> <span class="nf">bias_add</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the sum of the `input_x` and the `bias` Tensor. Before adding, the `bias` Tensor will be broadcasted to be</span>
<span class="sd">    consistent with the shape of the `input_x` Tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The input tensor. The shape can be 2-5 dimensions. Supported dtypes:</span>

<span class="sd">            - Ascend/CPU: all Number type.</span>
<span class="sd">            - GPU: float16, float32, int8.</span>

<span class="sd">        bias (Tensor): The bias tensor, with shape :math:`(C)`. C must be the same as channel dimension C of</span>
<span class="sd">            `input_x`. It has the same type as `input_x`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same shape and data type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` or `bias` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input_x` and `bias` is inconsistent.</span>
<span class="sd">        TypeError: If dimension of `input_x` is not in the range [2, 5].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.arange(6).reshape((2, 3)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; bias = Tensor(np.random.random(3).reshape((3)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bias_add(input_x, bias)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 3)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">bias_add_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">BiasAdd</span><span class="p">)(</span><span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">bias_add_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span></div>


<div class="viewcode-block" id="binary_cross_entropy"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.binary_cross_entropy.html#mindspore.ops.binary_cross_entropy">[docs]</a><span class="k">def</span> <span class="nf">binary_cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the binary cross entropy(Measure the difference information between two probability distributions) between</span>
<span class="sd">    predictive value `logits` and target value `labels`.</span>

<span class="sd">    Set `logits` as :math:`x`, `labels` as :math:`y`, output as :math:`\ell(x, y)`, the</span>
<span class="sd">    weight of nth batch of binary cross entropy is :math:`w_n`.</span>
<span class="sd">    Let,</span>

<span class="sd">    .. math::</span>
<span class="sd">        L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">        l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right]</span>

<span class="sd">    In which, :math:`L` indicates the loss of all `batch_size`, :math:`l` indicates the loss of one `batch_size`,</span>
<span class="sd">    and :math:`n` indicates one `batch_size` in the :math:`1-N` range. Then,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = \begin{cases}</span>
<span class="sd">        L, &amp; \text{if reduction} = \text{&#39;none&#39;;}\\</span>
<span class="sd">        \operatorname{mean}(L), &amp; \text{if reduction} = \text{&#39;mean&#39;;}\\</span>
<span class="sd">        \operatorname{sum}(L),  &amp; \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - The value of `logits` must range from `0` to `l`.</span>

<span class="sd">    Args:</span>
<span class="sd">        logits (Tensor): The predictive value whose data type must be float16 or float32.</span>
<span class="sd">        labels (Tensor): The target value which has the same shape and data type as `logits`.</span>
<span class="sd">        weight (Tensor, optional): A rescaling weight applied to the loss of each batch element.</span>
<span class="sd">            Its shape must be able to broadcast to that of `logits` and `labels`.</span>
<span class="sd">            And it must have the same shape and data type as `logits`. Default: ``None`` . If set to ``None`` ,</span>
<span class="sd">            the loss function</span>
<span class="sd">            will not consider any sample weights, and each sample will be treated as having equal importance</span>
<span class="sd">            when calculating the loss.</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the weighted mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the output elements will be summed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or Scalar. Returns Tensor that has the same dtype and shape as `logits` if `reduction` is &#39;none&#39;.</span>
<span class="sd">        Otherwise, returns a scalar Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `logits`, `labels` or `weight` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `logits`, `labels` or `weight` (if given) is neither float16 nor float32.</span>
<span class="sd">        ValueError: If `reduction` is not one of ``&#39;none&#39;``, ``&#39;mean&#39;`` or ``&#39;sum&#39;``.</span>
<span class="sd">        ValueError: If shape of `labels` is not the same as `logits` or `weight` (if given).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([0.2, 0.7, 0.1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([0., 1., 0.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.array([1, 2, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.binary_cross_entropy(logits, labels, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.38240486</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">binary_cross_entropy_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">BinaryCrossEntropy</span><span class="p">)(</span><span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">binary_cross_entropy_op</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span></div>


<div class="viewcode-block" id="conv3d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.conv3d.html#mindspore.ops.conv3d">[docs]</a><span class="k">def</span> <span class="nf">conv3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a 3D convolution over an input tensor. The input tensor is typically of</span>
<span class="sd">    shape :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})`, where :math:`N` is batch size, :math:`C`</span>
<span class="sd">    is channel number, :math:`D, H, W` are the depth, height and width of the feature graph, respectively.</span>

<span class="sd">    The output is calculated based on formula:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) +</span>
<span class="sd">        \sum_{k = 0}^{C_{in} - 1} \text{ccor}({\text{weight}(C_{\text{out}_j}, k), \text{X}(N_i, k)})</span>

<span class="sd">    where :math:`bias` is the output channel bias, :math:`ccor` is</span>
<span class="sd">    the `cross-correlation &lt;https://en.wikipedia.org/wiki/Cross-correlation&gt;`_</span>
<span class="sd">    , :math:`weight` is the convolution kernel value and :math:`X` represents the input feature map.</span>

<span class="sd">    Here are the indices&#39; meanings:</span>

<span class="sd">    - :math:`i` corresponds to the batch number, the range is :math:`[0, N-1]`,</span>
<span class="sd">      where :math:`N` is the batch size of the input.</span>

<span class="sd">    - :math:`j` corresponds to the output channel, the range is :math:`[0, C_{out}-1]`,</span>
<span class="sd">      where :math:`C_{out}` is the number of</span>
<span class="sd">      output channels, which is also equal to the number of kernels.</span>

<span class="sd">    - :math:`k` corresponds to the input channel, the range is :math:`[0, C_{in}-1]`,</span>
<span class="sd">      where :math:`C_{in}` is the number of</span>
<span class="sd">      input channels, which is also equal to the number of channels in the convolutional kernels.</span>

<span class="sd">    Therefore, in the above formula, :math:`{bias}(C_{\text{out}_j})` represents the bias of the :math:`j`-th</span>
<span class="sd">    output channel, :math:`{weight}(C_{\text{out}_j}, k)` represents the slice of the :math:`j`-th convolutional</span>
<span class="sd">    kernel in the :math:`k`-th channel, and :math:`{X}(N_i, k)` represents the slice of the :math:`k`-th input</span>
<span class="sd">    channel in the :math:`i`-th batch of the input feature map.</span>

<span class="sd">    The shape of the convolutional kernel is given by</span>
<span class="sd">    :math:`(\text{kernel_size[0]}, \text{kernel_size[1]}, \text{kernel_size[2]})`</span>
<span class="sd">    where :math:`\text{kernel_size[0]}` , :math:`\text{kernel_size[1]}` and :math:`\text{kernel_size[2]}` are the depth,</span>
<span class="sd">    height and width of the kernel, respectively.</span>
<span class="sd">    If we consider the input and output channels as well as the `group` parameter, the complete kernel shape</span>
<span class="sd">    will be :math:`(C_{out}, C_{in} / \text{group}, \text{kernel_size[0]},</span>
<span class="sd">    \text{kernel_size[1]}, \text{kernel_size[2]})`,</span>
<span class="sd">    where `group` is the number of groups dividing `x`&#39;s input channel when applying group convolution.</span>

<span class="sd">    For more details about convolution layer, please refer to `Gradient Based Learning Applied to Document Recognition</span>
<span class="sd">    &lt;http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf&gt;`_.</span>

<span class="sd">    Note:</span>
<span class="sd">        1. On Ascend platform, :math:`groups = 1` must be satisfied.</span>
<span class="sd">        2. On Ascend platform, :math:`dilation=1` must be satisfied.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Tensor of shape :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})`.</span>
<span class="sd">        weight (Tensor): Set size of kernel is :math:`(\text{kernel_size[0]}, \text{kernel_size[1]},</span>
<span class="sd">            \text{kernel_size[2]})`, then the shape is :math:`(C_{out}, C_{in}, \text{kernel_size[0]},</span>
<span class="sd">            \text{kernel_size[1]}, \text{kernel_size[1]})`.</span>
<span class="sd">        bias (Tensor, optional): Bias Tensor with shape :math:`(C_{out})`.</span>
<span class="sd">            When bias is None, zeros will be used. Default: ``None`` .</span>
<span class="sd">        stride (Union[int, tuple[int]], optional): The distance of kernel moving,</span>
<span class="sd">            it can be an int number that represents</span>
<span class="sd">            the depth, height and width of movement or a tuple of three int numbers that</span>
<span class="sd">            represent depth, height and width movement respectively. Default: ``1`` .</span>
<span class="sd">        pad_mode (str, optional): Specifies padding mode. The optional values are</span>
<span class="sd">            ``&quot;same&quot;`` , ``&quot;valid&quot;`` and ``&quot;pad&quot;`` . Default: ``&quot;valid&quot;`` .</span>

<span class="sd">            - ``&quot;same&quot;``: Adopts the way of completion. The depth, height and width of the output will be equal to</span>
<span class="sd">              the input `x` divided by stride. The padding will be evenly calculated in head and tail, top and bottom,</span>
<span class="sd">              left and right directions possiblily.</span>
<span class="sd">              Otherwise, the last extra padding will be calculated from the tail, bottom and the right side.</span>
<span class="sd">              If this mode is set, `pad` must be 0.</span>

<span class="sd">            - ``&quot;valid&quot;``: Adopts the way of discarding. The possible largest depth, height and width of output</span>
<span class="sd">              will be returned without padding. Extra pixels will be discarded. If this mode is set, `pad`</span>
<span class="sd">              must be 0.</span>

<span class="sd">            - ``&quot;pad&quot;``: Implicit paddings on both sides of the input in depth, height and width.</span>
<span class="sd">              The number of `pad` will be padded to the input Tensor borders. `pad` must be greater than or equal to 0.</span>

<span class="sd">        padding (Union[int, tuple[int], list[int]], optional): The pad value to be filled. If `pad` is an integer,</span>
<span class="sd">            the paddings of head, tail, top, bottom, left and right are the same, equal to pad.</span>
<span class="sd">            If `pad` is a tuple/list of 3 integers, the padding of head, tail, top, bottom,</span>
<span class="sd">            left and right equal to pad[0], pad[0], pad[1], pad[1], pad[2] and pad[2] correspondingly. Default: ``0`` .</span>
<span class="sd">        dilation (Union[int, tuple[int]], optional): The data type is int or a tuple of 3 integers</span>
<span class="sd">            :math:`(dilation_d, dilation_h, dilation_w)`. Currently, dilation on depth only supports the case of 1</span>
<span class="sd">            on Ascend backend. Specifies the dilation rate to use for dilated convolution. If set :math:`k &gt; 1`,</span>
<span class="sd">            there will be :math:`k - 1` pixels skipped for each sampling location.</span>
<span class="sd">            The value ranges for the depth, height, and width dimensions are [1, D], [1, H], and [1, W],</span>
<span class="sd">            respectively. Default: ``1`` .</span>
<span class="sd">        groups (int, optional):The number of groups into which the filter is divided. Default: ``1`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the value that applied 3D convolution. The shape is :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})`.</span>

<span class="sd">        `pad_mode` is ``&quot;same&quot;``:</span>

<span class="sd">        .. math::</span>
<span class="sd">            \begin{array}{ll} \\</span>
<span class="sd">                D_{out} = \left \lceil{\frac{D_{in}}{\text{stride[0]}}} \right \rceil \\</span>
<span class="sd">                H_{out} = \left \lceil{\frac{H_{in}}{\text{stride[1]}}} \right \rceil \\</span>
<span class="sd">                W_{out} = \left \lceil{\frac{W_{in}}{\text{stride[2]}}} \right \rceil \\</span>
<span class="sd">            \end{array}</span>

<span class="sd">        `pad_mode` is ``&quot;valid&quot;``:</span>

<span class="sd">        .. math::</span>
<span class="sd">            \begin{array}{ll} \\</span>
<span class="sd">                D_{out} = \left \lfloor{\frac{D_{in} - \text{dilation[0]} \times (\text{kernel_size[0]} - 1) }</span>
<span class="sd">                {\text{stride[0]}} + 1} \right \rfloor \\</span>
<span class="sd">                H_{out} = \left \lfloor{\frac{H_{in} - \text{dilation[1]} \times (\text{kernel_size[1]} - 1) }</span>
<span class="sd">                {\text{stride[1]}} + 1} \right \rfloor \\</span>
<span class="sd">                W_{out} = \left \lfloor{\frac{W_{in} - \text{dilation[2]} \times (\text{kernel_size[2]} - 1) }</span>
<span class="sd">                {\text{stride[2]}} + 1} \right \rfloor \\</span>
<span class="sd">            \end{array}</span>

<span class="sd">        `pad_mode` is ``&quot;pad&quot;``:</span>

<span class="sd">        .. math::</span>
<span class="sd">            \begin{array}{ll} \\</span>
<span class="sd">                D_{out} = \left \lfloor{\frac{D_{in} + padding[0] + padding[1] - (\text{dilation[0]} - 1) \times</span>
<span class="sd">                \text{kernel_size[0]} - 1 }{\text{stride[0]}} + 1} \right \rfloor \\</span>
<span class="sd">                H_{out} = \left \lfloor{\frac{H_{in} + padding[2] + padding[3] - (\text{dilation[1]} - 1) \times</span>
<span class="sd">                \text{kernel_size[1]} - 1 }{\text{stride[1]}} + 1} \right \rfloor \\</span>
<span class="sd">                W_{out} = \left \lfloor{\frac{W_{in} + padding[4] + padding[5] - (\text{dilation[2]} - 1) \times</span>
<span class="sd">                \text{kernel_size[2]} - 1 }{\text{stride[2]}} + 1} \right \rfloor \\</span>
<span class="sd">            \end{array}</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `out_channel` or `groups` is not an int.</span>
<span class="sd">        TypeError: If `stride`, `padding` or `dilation` is neither an int nor a tuple.</span>
<span class="sd">        TypeError: If `bias` is not a Tensor.</span>
<span class="sd">        ValueError: If the shape of `bias` is not :math:`(C_{out})`.</span>
<span class="sd">        ValueError: If `stride` or `dilation` is less than 1.</span>
<span class="sd">        ValueError: If `pad_mode` is not one of &#39;same&#39;, &#39;valid&#39; or &#39;pad&#39;.</span>
<span class="sd">        ValueError: If `padding` is a tuple or list whose length is not equal to 3.</span>
<span class="sd">        ValueError: If `pad_mode` is not equal to &#39;pad&#39; and `pad` is greater than 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([16, 3, 10, 32, 32]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([32, 3, 4, 3, 3]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.conv3d(x, weight, pad_mode=&quot;same&quot;, padding=0, stride=1, dilation=1, groups=1)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (16, 32, 10, 32, 32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.conv3d(x, weight, pad_mode=&quot;valid&quot;, padding=0, stride=1, dilation=1, groups=1)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (16, 32, 7, 30, 30)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.conv3d(x, weight, pad_mode=&quot;pad&quot;, padding=(2, 1, 1), stride=1, dilation=1, groups=1)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (16, 32, 11, 32, 32)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">weight_shape</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">out_channel</span> <span class="o">=</span> <span class="n">weight_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">weight_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">5</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
        <span class="n">_check_conv_iterable_lengths</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">iter_name</span><span class="o">=</span><span class="s1">&#39;stride&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dilation</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
        <span class="n">_check_conv_iterable_lengths</span><span class="p">(</span><span class="n">dilation</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">iter_name</span><span class="o">=</span><span class="s1">&#39;dilation&#39;</span><span class="p">)</span>
    <span class="n">input_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">in_channel</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">in_channel</span> <span class="o">%</span> <span class="n">groups</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">out_channel</span> <span class="o">%</span> <span class="n">groups</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The argument &#39;groups&#39; should be divisible by &#39;in_channel&#39; &quot;</span> \
                        <span class="s2">&quot;and &#39;out_channel&#39;&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="n">_manipulate_padding</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">conv</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Conv3D</span><span class="p">)(</span><span class="n">out_channel</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">groups</span><span class="p">,</span> <span class="s2">&quot;NCDHW&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">conv</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;conv3d&#39;, the &#39;bias&#39; must be a Tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">bias</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">conv_result</span> <span class="o">=</span> <span class="n">conv</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">bias_add</span><span class="p">(</span><span class="n">conv_result</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span></div>


<span class="k">def</span> <span class="nf">_check_positive_int</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="n">arg_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prim_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="n">arg_name</span><span class="o">=</span><span class="n">arg_name</span><span class="p">,</span> <span class="n">prim_name</span><span class="o">=</span><span class="n">prim_name</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_pxiel_shuffle_valid</span><span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="n">factor</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">num</span> <span class="o">%</span> <span class="n">factor</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;pixel_shuffle&#39;, the length of third to last dimension is not divisible&quot;</span>
                         <span class="s2">&quot;by `upscale_factor` squared.&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_pixel_shuffle_unshuffle_input_shape</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Internal function, used to check whether the shape of pixel shuffle or unshuffle input meets the requirements.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For </span><span class="si">{</span><span class="n">cls_name</span><span class="si">}</span><span class="s2">, the dimension of `input` should be larger than 2, but got </span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="pixel_shuffle"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.pixel_shuffle.html#mindspore.ops.pixel_shuffle">[docs]</a><span class="k">def</span> <span class="nf">pixel_shuffle</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">upscale_factor</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies the PixelShuffle operation over input `input` which implements sub-pixel convolutions</span>
<span class="sd">    with stride :math:`1/r` . For more details, refer to</span>
<span class="sd">    `Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network</span>
<span class="sd">    &lt;https://arxiv.org/abs/1609.05158&gt;`_ .</span>

<span class="sd">    Typically, the `input` is of shape :math:`(*, C \times r^2, H, W)` , and the output is of shape</span>
<span class="sd">    :math:`(*, C, H \times r, W \times r)`, where `r` is an upscale factor and `*` is zero or more batch dimensions.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Tensor of shape :math:`(*, C \times r^2, H, W)` . The dimension of `input` is larger than 2,</span>
<span class="sd">            and the length of third to last dimension can be divisible by `upscale_factor` squared.</span>
<span class="sd">        upscale_factor (int): factor to shuffle the input Tensor, and is a positive integer.</span>
<span class="sd">            `upscale_factor` is the above-mentioned :math:`r`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        - **output** (Tensor) - Tensor of shape :math:`(*, C, H \times r, W \times r)` .</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If `upscale_factor` is not a positive integer.</span>
<span class="sd">        ValueError: If the length of third to last dimension is not divisible by `upscale_factor` squared.</span>
<span class="sd">        ValueError: If the dimension of `input` is less than 3.</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = np.arange(3 * 2 * 9 * 4 * 4).reshape((3, 2, 9, 4, 4))</span>
<span class="sd">        &gt;&gt;&gt; input_x = mindspore.Tensor(input_x, mindspore.dtype.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.pixel_shuffle(input_x, 3)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (3, 2, 1, 12, 12)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_positive_int</span><span class="p">(</span><span class="n">upscale_factor</span><span class="p">,</span> <span class="s2">&quot;upscale_factor&quot;</span><span class="p">)</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;pixel_shuffle&quot;</span><span class="p">)</span>
    <span class="n">_check_pixel_shuffle_unshuffle_input_shape</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;pixel_shuffle&quot;</span><span class="p">)</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">shape_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">length</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span>
    <span class="n">pre</span> <span class="o">=</span> <span class="n">idx</span><span class="p">[:</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span>
    <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">idx</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">:]</span>
    <span class="n">_check_pxiel_shuffle_valid</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">upscale_factor</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">c</span> <span class="o">//</span> <span class="n">upscale_factor</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="n">input_perm</span> <span class="o">=</span> <span class="p">(</span><span class="n">pre</span> <span class="o">+</span> <span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">upscale_factor</span><span class="p">,</span> <span class="n">upscale_factor</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">))</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">reshape_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">input_perm</span><span class="p">)</span>
    <span class="n">input_perm</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">length</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)]</span>
    <span class="n">input_perm</span> <span class="o">=</span> <span class="n">input_perm</span> <span class="o">+</span> <span class="p">[</span><span class="n">length</span><span class="p">,</span> <span class="n">length</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span> <span class="n">length</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">input_perm</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">input_perm</span><span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">transpose_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">input_perm</span><span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">reshape_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="n">pre</span> <span class="o">+</span> <span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">upscale_factor</span> <span class="o">*</span> <span class="n">h</span><span class="p">,</span> <span class="n">upscale_factor</span> <span class="o">*</span> <span class="n">w</span><span class="p">)))</span>
    <span class="k">return</span> <span class="nb">input</span></div>


<span class="k">def</span> <span class="nf">_check_pxiel_unshuffle_valid</span><span class="p">(</span><span class="n">num1</span><span class="p">,</span> <span class="n">num2</span><span class="p">,</span> <span class="n">factor</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">num1</span> <span class="o">%</span> <span class="n">factor</span> <span class="o">!=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">num2</span> <span class="o">%</span> <span class="n">factor</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;pixel_unshuffle&#39;, the length of second to last 2 dimension should be divisible &quot;</span>
                         <span class="s2">&quot;by downscale_factor.&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="pixel_unshuffle"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.pixel_unshuffle.html#mindspore.ops.pixel_unshuffle">[docs]</a><span class="k">def</span> <span class="nf">pixel_unshuffle</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">downscale_factor</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies the PixelUnshuffle operation over input `input` which is the inverse of PixelShuffle. For more details,</span>
<span class="sd">    refer to `Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural</span>
<span class="sd">    Network &lt;https://arxiv.org/abs/1609.05158&gt;`_ .</span>

<span class="sd">    Typically, the input is of shape :math:`(*, C, H \times r, W \times r)` , and the output is of shape</span>
<span class="sd">    :math:`(*, C \times r^2, H, W)` , where `r` is a downscale factor and `*` is zero or more batch dimensions.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Tensor of shape :math:`(*, C, H \times r, W \times r)` . The dimension of `input` is larger than</span>
<span class="sd">            2, and the length of second to last dimension or last dimension can be divisible by `downscale_factor` .</span>
<span class="sd">        downscale_factor (int): factor to unshuffle the input Tensor, and is a positive integer.</span>
<span class="sd">            `downscale_factor` is the above-mentioned :math:`r`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        - **output** (Tensor) - Tensor of shape :math:`(*, C \times r^2, H, W)` .</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If `downscale_factor` is not a positive integer.</span>
<span class="sd">        ValueError: If the length of second to last dimension or last dimension is not divisible by `downscale_factor` .</span>
<span class="sd">        ValueError: If the dimension of `input` is less than 3.</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = np.arange(8 * 8).reshape((1, 1, 8, 8))</span>
<span class="sd">        &gt;&gt;&gt; input_x = mindspore.Tensor(input_x, mindspore.dtype.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.pixel_unshuffle(input_x, 2)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 4, 4, 4)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_positive_int</span><span class="p">(</span><span class="n">downscale_factor</span><span class="p">,</span> <span class="s2">&quot;downscale_factor&quot;</span><span class="p">)</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;pixel_unshuffle&quot;</span><span class="p">)</span>
    <span class="n">_check_pixel_shuffle_unshuffle_input_shape</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;pixel_unshuffle&quot;</span><span class="p">)</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">shape_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">length</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span>
    <span class="n">pre</span> <span class="o">=</span> <span class="n">idx</span><span class="p">[:</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span>
    <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">idx</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">:]</span>
    <span class="n">_check_pxiel_unshuffle_valid</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">downscale_factor</span><span class="p">)</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">h</span> <span class="o">//</span> <span class="n">downscale_factor</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">//</span> <span class="n">downscale_factor</span>
    <span class="n">input_perm</span> <span class="o">=</span> <span class="p">(</span><span class="n">pre</span> <span class="o">+</span> <span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">downscale_factor</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">downscale_factor</span><span class="p">))</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">reshape_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">input_perm</span><span class="p">)</span>
    <span class="n">input_perm</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">length</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)]</span>
    <span class="n">input_perm</span> <span class="o">=</span> <span class="n">input_perm</span> <span class="o">+</span> <span class="p">[</span><span class="n">length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">length</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">length</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span> <span class="n">length</span><span class="p">]</span>
    <span class="n">input_perm</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">input_perm</span><span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">transpose_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">input_perm</span><span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">reshape_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="n">pre</span> <span class="o">+</span> <span class="p">(</span><span class="n">c</span> <span class="o">*</span> <span class="n">downscale_factor</span> <span class="o">*</span> <span class="n">downscale_factor</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)))</span>
    <span class="k">return</span> <span class="nb">input</span></div>


<div class="viewcode-block" id="glu"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.glu.html#mindspore.ops.glu">[docs]</a><span class="k">def</span> <span class="nf">glu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes GLU (Gated Linear Unit activation function) of input tensors.</span>

<span class="sd">    .. math::</span>
<span class="sd">        {GLU}(a, b)= a \otimes \sigma(b)</span>

<span class="sd">    where :math:`a` is the first half of the input matrices and :math:`b` is the second half.</span>

<span class="sd">    Here :math:`\sigma` is the sigmoid function, and :math:`\otimes` is the Hadamard product.</span>
<span class="sd">    See `Language Modeling with Gated Convluational Networks &lt;https://arxiv.org/abs/1612.08083&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Tensor to be split. Its dtype is Number, and shape is :math:`(\ast_1, N, \ast_2)`</span>
<span class="sd">            where `*` means, any number of additional dimensions.</span>
<span class="sd">        axis (int, optional): the axis to split the input. It must be int. Default: ``-1`` , the last axis of `x`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the same dtype as the `x`, with the shape :math:`(\ast_1, M, \ast_2)` where :math:`M=N/2`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x` is not Number.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor([[0.1,0.2,0.3,0.4],[0.5,0.6,0.7,0.8]])</span>
<span class="sd">        &gt;&gt;&gt; output = ops.glu(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0.05744425 0.11973753]</span>
<span class="sd">         [0.33409387 0.41398472]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">spilt</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Split</span><span class="p">)(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">output_num</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">spilt</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">sigmoid_</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span></div>


<div class="viewcode-block" id="multi_margin_loss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.multi_margin_loss.html#mindspore.ops.multi_margin_loss">[docs]</a><span class="k">def</span> <span class="nf">multi_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hinge loss for optimizing a multi-class classification.</span>

<span class="sd">    Optimizes a multi-class classification hinge</span>
<span class="sd">    loss (margin-based loss) between input and</span>
<span class="sd">    output.</span>

<span class="sd">    For each mini-batch sample, the loss in terms of the 1D input :math:`x` and scalar output :math:`y` is:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss}(x, y) = \frac{\sum_i \max(0, \text{margin} - x[y] + x[i])^p}{\text{x.size}(0)}</span>

<span class="sd">    where :math:`i\in \{0,,x.size(0)-1\}` and :math:`i \ne y`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input , with shape :math:`(N, C)`. Data type only support float32, float16 or float64.</span>
<span class="sd">            It is :math:`x` in the above formula.</span>
<span class="sd">        target (Tensor): Ground truth labels, with shape :math:`(N,)`. Data type only support int64. The</span>
<span class="sd">            value of target should be non-negative, less than C. It is :math:`y` in the above formula.</span>
<span class="sd">        p (int, optional): The norm degree for pairwise distance. Should be 1 or 2. Default: ``1`` .</span>
<span class="sd">        margin (int, optional): A parameter to change pairwise distance. Default: ``1`` .</span>
<span class="sd">        weight (Tensor, optional): The rescaling weight to each class with shape :math:`(C,)`. Data type only</span>
<span class="sd">            support float16, float32 or float64. Default: ``None`` .</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the weighted mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the output elements will be summed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        - **outputs** - Tensor. If `reduction` is ``&#39;none&#39;``, returns a Tensor with the same shape as `target`.</span>
<span class="sd">          Otherwise, it is a scalar.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `p` or `target` is not int.</span>
<span class="sd">        TypeError: If dtype of `margin` is not int.</span>
<span class="sd">        TypeError: If dtype of `reduction` is not str.</span>
<span class="sd">        TypeError: If dtype of `input` is not float16, float or float64.</span>
<span class="sd">        TypeError: If dtype of `weight` and `input` is not the same.</span>
<span class="sd">        ValueError: If `p` is not 1 or 2.</span>
<span class="sd">        ValueError: If `reduction` is not one of {&#39;none&#39;,&#39;sum&#39;,&#39;mean&#39;}.</span>
<span class="sd">        ValueError: If shape[0] of `input` is not equal to shape[0] of `target`.</span>
<span class="sd">        ValueError: If shape[1] of `input` is not equal to shape[0] of `weight`.</span>
<span class="sd">        ValueError: If rank of `weight` is not 1 or rank of `target` is not 1 or `input` is not 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; inputs = Tensor(np.ones(shape=[3, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; target = Tensor(np.array([1, 2, 1]), mindspore.int64)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.array([1, 1, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.multi_margin_loss(inputs, target, weight=weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.6666667</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">margin</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;multi_margin_loss&#39;, the type of &#39;margin&#39; must be int, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">margin</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">margin_</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">margin</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">MultiMarginLoss</span><span class="p">)(</span><span class="n">p</span><span class="p">,</span> <span class="n">margin_</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">outputs</span></div>


<div class="viewcode-block" id="multilabel_margin_loss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.multilabel_margin_loss.html#mindspore.ops.multilabel_margin_loss">[docs]</a><span class="k">def</span> <span class="nf">multilabel_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hinge loss for optimizing a multi-label classification.</span>

<span class="sd">    Creates a criterion that optimizes a multi-label multi-classification</span>
<span class="sd">    hinge loss (margin-based loss) between input :math:`x` (a 2D mini-batch `Tensor`)</span>
<span class="sd">    and output :math:`y` (which is a 2D `Tensor` of target class indices).</span>
<span class="sd">    For each sample in the mini-batch:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss}(x, y) = \sum_{ij}\frac{\max(0, 1 - (x[y[j]] - x[i]))}{\text{x.size}(0)}</span>

<span class="sd">    where :math:`x \in \left\{0, \; \cdots , \; \text{x.size}(0) - 1\right\}`, \</span>
<span class="sd">    :math:`y \in \left\{0, \; \cdots , \; \text{y.size}(0) - 1\right\}`, \</span>
<span class="sd">    :math:`0 \leq y[j] \leq \text{x.size}(0)-1`, \</span>
<span class="sd">    and :math:`i \neq y[j]` for all :math:`i` and :math:`j`.</span>
<span class="sd">    :math:`y` and :math:`x` must have the same size.</span>
<span class="sd">    The criterion only considers a contiguous block of non-negative targets that</span>
<span class="sd">    starts at the front.</span>
<span class="sd">    This allows for different samples to have variable amounts of target classes.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Predict data, :math:`x` in the formula above. Tensor of shape :math:`(C)`</span>
<span class="sd">            or :math:`(N, C)`, where :math:`N` is the batch size and :math:`C` is the number of classes.</span>
<span class="sd">            Data type must be float16 or float32.</span>
<span class="sd">        target (Tensor): Ground truth data, :math:`y` in the formula above, with the same shape as `input`,</span>
<span class="sd">            data type must be int32 and label targets padded by -1.</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the output elements will be summed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        - **outputs** (Union[Tensor, Scalar]) - The loss of MultilabelMarginLoss.</span>
<span class="sd">          If `reduction` is ``&quot;none&quot;``, its shape is :math:`(N)`.</span>
<span class="sd">          Otherwise, a scalar value will be returned.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` or `target` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `target` is not int32.</span>
<span class="sd">        ValueError: If length of shape of `input` is neither 1 nor 2.</span>
<span class="sd">        ValueError: If shape of `input` is not the same as `target`.</span>
<span class="sd">        ValueError: If `reduction` is not one of ``&#39;none&#39;``, ``&#39;mean&#39;``, ``&#39;sum&#39;``.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">       &gt;&gt;&gt; import mindspore</span>
<span class="sd">       &gt;&gt;&gt; import numpy as np</span>
<span class="sd">       &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">       &gt;&gt;&gt; inputs = Tensor(np.array([[0.1, 0.2, 0.4, 0.8], [0.2, 0.3, 0.5, 0.7]]), mindspore.float32)</span>
<span class="sd">       &gt;&gt;&gt; target = Tensor(np.array([[1, 2, 0, 3], [2, 3, -1, 1]]), mindspore.int32)</span>
<span class="sd">       &gt;&gt;&gt; output = ops.multilabel_margin_loss(inputs, target)</span>
<span class="sd">       &gt;&gt;&gt; print(output)</span>
<span class="sd">       0.325</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">MultilabelMarginLoss</span><span class="p">)(</span><span class="n">reduction</span><span class="p">)</span>
    <span class="n">outputs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">outputs</span></div>


<div class="viewcode-block" id="multilabel_soft_margin_loss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.multilabel_soft_margin_loss.html#mindspore.ops.multilabel_soft_margin_loss">[docs]</a><span class="k">def</span> <span class="nf">multilabel_soft_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the MultiLabelSoftMarginLoss.</span>
<span class="sd">    The multi-label soft margin loss is a commonly used loss function in multi-label classification tasks</span>
<span class="sd">    where an input sample can belong to multiple classes.</span>
<span class="sd">    Given an input :math:`input` and binary labels :math:`output` of size :math:`(N,C)`,</span>
<span class="sd">    where :math:`N` denotes the number of samples</span>
<span class="sd">    and :math:`C` denotes the number of classes.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \mathcal{loss\left( input , output \right)} = - \frac{1}{N}\frac{1}{C}\sum_{i = 1}^{N}</span>
<span class="sd">        \sum_{j = 1}^{C}\left(output_{ij}\log\frac{1}{1 + e^{- input_{ij}}} + \left( 1 - output_{ij}</span>
<span class="sd">        \right)\log\frac{e^{-input_{ij}}}{1 + e^{-input_{ij}}} \right)</span>

<span class="sd">    where :math:`input_{ij}` represents the predicted score of sample :math:`i` for class :math:`j`.</span>
<span class="sd">    :math:`output_{ij}` represents the binary label of sample :math:`i` for class :math:`j`, where</span>
<span class="sd">    sample :math:`i` belongs to class :math:`j` if :math:`output_{ij}=1` , and sample :math:`i` does</span>
<span class="sd">    not belong to class :math:`j` if :math:`output_{ij}=0`. For a multi-label classification task, each</span>
<span class="sd">    sample may have multiple labels with a value of 1 in the binary label :math:`output`. `weight` will</span>
<span class="sd">    multiply to the loss of each class if given.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): A tensor of shape :math:`(N, C)` , where N is batch size and C is number of classes.</span>
<span class="sd">        target (Tensor): The label target Tensor which has the same shape as `input`.</span>
<span class="sd">        weight (Union[Tensor, int, float]): The manual rescaling weight given to each class. Default: ``None``.</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the weighted mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the output elements will be summed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the data type is the same as input, if the `reduction` is ``&#39;none&#39;``,</span>
<span class="sd">        its shape is :math:`(N)` , otherwise it is zero.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor([[0.3, 0.6, 0.6], [0.9, 0.4, 0.2]])</span>
<span class="sd">        &gt;&gt;&gt; target = Tensor([[0.0, 0.0, 1.0], [0.0, 0.0, 1.0]])</span>
<span class="sd">        &gt;&gt;&gt; loss = ops.multilabel_soft_margin_loss(input, target, reduction=&#39;mean&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(loss.asnumpy())</span>
<span class="sd">        0.84693956</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cls_name</span> <span class="o">=</span> <span class="s2">&quot;multilabel_soft_margin_loss&quot;</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;input&#39;</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">)</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;target&#39;</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">)</span>

    <span class="n">input_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">if</span> <span class="n">ops</span><span class="o">.</span><span class="n">is_sequence_value_unknown</span><span class="p">(</span><span class="n">input_shape</span><span class="p">):</span>
        <span class="n">input_shape</span> <span class="o">=</span> <span class="n">tensor_shape_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

    <span class="n">pos</span> <span class="o">=</span> <span class="n">log_</span><span class="p">(</span><span class="n">add_</span><span class="p">(</span><span class="n">exp_</span><span class="p">(</span><span class="o">-</span><span class="nb">input</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">neg</span> <span class="o">=</span> <span class="n">log_</span><span class="p">(</span><span class="n">add_</span><span class="p">(</span><span class="n">exp_</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">mul_</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">pos</span><span class="p">)</span> <span class="o">+</span> <span class="n">mul_</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">target</span><span class="p">,</span> <span class="n">neg</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">mul_</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
    <span class="n">class_dim</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">class_dim</span><span class="p">)</span> <span class="o">/</span> <span class="n">input_shape</span><span class="p">[</span><span class="n">class_dim</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">_get_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">)</span></div>


<div class="viewcode-block" id="gelu"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.gelu.html#mindspore.ops.gelu">[docs]</a><span class="k">def</span> <span class="nf">gelu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">approximate</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gaussian Error Linear Units activation function.</span>

<span class="sd">    GeLU is described in the paper `Gaussian Error Linear Units (GELUs) &lt;https://arxiv.org/abs/1606.08415&gt;`_.</span>
<span class="sd">    And also please refer to `BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</span>
<span class="sd">    &lt;https://arxiv.org/abs/1810.04805&gt;`_.</span>

<span class="sd">    When `approximate` argument is `none`, GeLU is defined as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        GELU(x_i) = x_i*P(X &lt; x_i),</span>

<span class="sd">    where :math:`P` is the cumulative distribution function of the standard Gaussian distribution,</span>
<span class="sd">    :math:`x_i` is the input element.</span>

<span class="sd">    When `approximate` argument is `tanh`, GeLU is estimated with:</span>

<span class="sd">    .. math::</span>
<span class="sd">        GELU(x_i) = 0.5 * x_i * (1 + \tanh(\sqrt(2 / \pi) * (x_i + 0.044715 * x_i^3)))</span>

<span class="sd">    For the related GELU graph, refer to `GELU &lt;https://en.wikipedia.org/wiki/Activation_function#/media/File:Activation_gelu.png&gt;`_ .</span>

<span class="sd">    GELU Activation Function Graph:</span>

<span class="sd">    .. image:: ../images/GELU.png</span>
<span class="sd">        :align: center</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input of the activation function GeLU, the data type is float16, float32 or float64.</span>
<span class="sd">        approximate (str): the gelu approximation algorithm to use. Acceptable vaslues are ``&#39;none&#39;`` and ``&#39;tanh&#39;`` .</span>
<span class="sd">            Default: ``&#39;none&#39;`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not bfloat16, float16, float32 or float64.</span>
<span class="sd">        ValueError: If `approximate` value is neither `none` nor `tanh`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([1.0, 2.0, 3.0], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; result = ops.gelu(x)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [0.841192 1.9545976 2.9963627]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">approximate</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;tanh&#39;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For ops.gelu, approximate value should be either &#39;none&#39; or &#39;tanh&#39;.&quot;</span><span class="p">)</span>

    <span class="n">x_dtype</span> <span class="o">=</span> <span class="n">dtype_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x_dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For gelu, the input dtype must be float16, float32 or float64, &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">x_dtype</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">approximate</span> <span class="o">==</span> <span class="s1">&#39;tanh&#39;</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">gelu_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">sqrt_</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">))</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">div_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">erf_</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="o">+</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="nb">input</span> <span class="o">*</span> <span class="n">output</span> <span class="o">*</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">output</span></div>


<div class="viewcode-block" id="channel_shuffle"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.channel_shuffle.html#mindspore.ops.channel_shuffle">[docs]</a><span class="k">def</span> <span class="nf">channel_shuffle</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">groups</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Divide the channels in a tensor of shape :math:`(*, C, H, W)` into :math:`g` groups and</span>
<span class="sd">    rearrange them as :math:`(*, \frac{C}{g}, g, H*W)`, while keeping the original tensor shapes.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Tensor to be divided, it has shape :math:`(*, C, H, W)`,</span>
<span class="sd">          with float16, float32, int8, int16, int32, int64, uint8, uint16, uint32, uint64 data type.</span>
<span class="sd">        groups (int): Number of groups to divide channels in.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A Tensor, has the same type as the `x`, and has the shape :math:`(*, C, H, W)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If data type of `x` is not one of the following:</span>
<span class="sd">                   float16, float32, int8, int16, int32, int64, uint8, uint16, uint32, uint64.</span>
<span class="sd">        TypeError: If dim of `x` is &lt; 4.</span>
<span class="sd">        TypeError: If `groups` is not a positive number.</span>
<span class="sd">        ValueError: If channel number of `x` is not divisible by `groups`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; group = 2</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(1* 4 * 2 * 2).reshape(1, 4, 2, 2).astype(np.int16))</span>
<span class="sd">        &gt;&gt;&gt; y = mindspore.ops.channel_shuffle(x, group)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[[[ 0  1]</span>
<span class="sd">           [ 2  3]]</span>
<span class="sd">           [[ 8  9]</span>
<span class="sd">           [10 11]]</span>
<span class="sd">           [[ 4  5]</span>
<span class="sd">           [ 6  7]]</span>
<span class="sd">           [[12 13]</span>
<span class="sd">           [14 15]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">channel_shuffle_func</span> <span class="o">=</span> <span class="n">ChannelShuffle</span><span class="p">(</span><span class="n">group</span><span class="o">=</span><span class="n">groups</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">channel_shuffle_func</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span></div>


<div class="viewcode-block" id="lp_pool1d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.lp_pool1d.html#mindspore.ops.lp_pool1d">[docs]</a><span class="k">def</span> <span class="nf">lp_pool1d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applying 1D LPPooling operation on an input Tensor can be regarded as forming a 1D input plane.</span>

<span class="sd">    Typically the input is of shape :math:`(N, C, L_{in})` or :math:`(C, L_{in})`, the output is of shape</span>
<span class="sd">    :math:`(N, C, L_{out})` or :math:`(C, L_{out})`.</span>

<span class="sd">    .. math::</span>
<span class="sd">        L_{out} = \left\lfloor\frac{L_{in} - \text{kernel_size}}{\text{stride}} + 1\right\rfloor</span>

<span class="sd">    The operation is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        f(X) = \sqrt[p]{\sum_{x \in X} x^{p}}</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Tensor of shape :math:`(N, C, L_{in})` or :math:`(C, L_{in})`.</span>
<span class="sd">        norm_type (Union[int, float]): Type of normalization, represents p in the formula, can not be 0,</span>

<span class="sd">            - if p = 1, the result obtained is the sum of elements in the pool nucleus(Proportional to average pooling).</span>
<span class="sd">            - if p = :math:`\infty`, the result is the result of maximum pooling.</span>

<span class="sd">        kernel_size (int): The size of kernel window.</span>
<span class="sd">        stride (int): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the width of movement is stride. Default: ``None`` , which indicates the moving step is `kernel_size` .</span>
<span class="sd">        ceil_mode (bool): Whether to use ceil or floor to calculate output shape. Default: ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        - **output** (Tensor) - LPPool1d result, with shape :math:`(N, C, L_{out})` or :math:`(C, L_{out})`,</span>
<span class="sd">          It has the same data type as `x`, where</span>

<span class="sd">          .. math::</span>
<span class="sd">              L_{out} = \left\lfloor\frac{L_{in} - \text{kernel_size}}{\text{stride}} + 1\right\rfloor</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `kernel_size` or `stride` is not an int.</span>
<span class="sd">        TypeError: If `ceil_mode` is not a bool.</span>
<span class="sd">        TypeError: If `norm_type` is neither float nor int.</span>
<span class="sd">        ValueError: If `norm_type` is equal to 0.</span>
<span class="sd">        ValueError: If `kernel_size` or `stride` is less than 1.</span>
<span class="sd">        ValueError: If length of shape of `x` is not equal to 2 or 3.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(2 * 3 * 4).reshape((2, 3, 4)), dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; out = ops.lp_pool1d(x, norm_type=1, kernel_size=3, stride=1, ceil_mode=False)</span>
<span class="sd">        &gt;&gt;&gt; print(out)</span>
<span class="sd">        [[[ 3.  6.]</span>
<span class="sd">          [15. 18.]</span>
<span class="sd">          [27. 30.]]</span>
<span class="sd">         [[39. 42.]</span>
<span class="sd">          [51. 54.]</span>
<span class="sd">          [63. 66.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">norm_type</span><span class="p">,</span> <span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">)):</span>
        <span class="n">norm_type</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">norm_type</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For lp_pool1d, the type of &#39;norm_type&#39; must be float or int, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">norm_type</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For lp_pool1d, the value of &#39;norm_type&#39; can not be 0.&quot;</span><span class="p">)</span>
    <span class="n">sign</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">Sign</span><span class="p">)()</span>
    <span class="n">squeeze</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">Squeeze</span><span class="p">)(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">expand_dims</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">)()</span>
    <span class="n">_is_squeeze</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">_is_squeeze</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">kernel_size</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">avg_pool1d</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">norm_type</span><span class="p">),</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="n">ceil_mode</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_is_squeeze</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">squeeze</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">((</span><span class="n">sign</span><span class="p">(</span><span class="n">out</span><span class="p">)</span> <span class="o">*</span> <span class="n">ops</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">out</span><span class="p">)))</span> <span class="o">*</span> <span class="n">kernel_size</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">norm_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="lp_pool2d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.lp_pool2d.html#mindspore.ops.lp_pool2d">[docs]</a><span class="k">def</span> <span class="nf">lp_pool2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applying 2D LPPooling operation on an input Tensor can be regarded as forming a 2D input plane.</span>

<span class="sd">    Typically the input is of shape :math:`(N, C, H_{in}, W_{in})`, the output is of shape</span>
<span class="sd">    :math:`(N, C, H_{in}, W_{in})`, with the same shape as input, the operation is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        f(X) = \sqrt[p]{\sum_{x \in X} x^{p}}</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Tensor of shape :math:`(N, C, H_{in}, W_{in})`.</span>
<span class="sd">        norm_type (Union[int, float]): Type of normalization, represents p in the formula, can not be 0,</span>

<span class="sd">            - if p = 1, the result obtained is the sum of elements in the pool nucleus(Proportional to average pooling).</span>
<span class="sd">            - if p = :math:`\infty`, the result is the result of maximum pooling.</span>

<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of kernel window.</span>
<span class="sd">            The data type of kernel_size must be int and the value represents the height and width,</span>
<span class="sd">            or a tuple of two int numbers that represent height and width respectively.</span>
<span class="sd">        stride (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the height and width of movement are both strides, or a tuple of two int numbers that</span>
<span class="sd">            represent height and width of movement respectively.</span>
<span class="sd">            Default: ``None`` , which indicates the moving step is `kernel_size` .</span>
<span class="sd">        ceil_mode (bool): Whether to use ceil or floor to calculate output shape. Default: ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        - **output** (Tensor) - LPPool2d result, with shape :math:`(N, C, H_{in}, W_{in})`,</span>
<span class="sd">          It has the same data type as `x`, where</span>

<span class="sd">          .. math::</span>
<span class="sd">              H_{out} = \left\lfloor\frac{H_{in} - \text{kernel_size}[0]}{\text{stride}[0]} + 1\right\rfloor</span>

<span class="sd">          .. math::</span>
<span class="sd">              W_{out} = \left\lfloor\frac{W_{in} - \text{kernel_size}[1]}{\text{stride}[1]} + 1\right\rfloor</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `kernel_size` or `stride` is neither int nor tuple.</span>
<span class="sd">        TypeError: If `ceil_mode` is not a bool.</span>
<span class="sd">        TypeError: If `norm_type` is neither float nor int.</span>
<span class="sd">        ValueError: If `norm_type` is equal to 0.</span>
<span class="sd">        ValueError: If `kernel_size` or `stride` is less than 1.</span>
<span class="sd">        ValueError: If `kernel_size` or `stride` is a tuple whose length is not equal to `2`.</span>
<span class="sd">        ValueError: If length of shape of `x` is not equal to 4.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(2 * 3 * 4 * 5).reshape((2, 3, 4, 5)), dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; out = ops.lp_pool2d(x, norm_type=1, kernel_size=3, stride=1, ceil_mode=False)</span>
<span class="sd">        &gt;&gt;&gt; print(out)</span>
<span class="sd">        [[[[  54.   63.   72.]</span>
<span class="sd">           [  99.  108.  117.]]</span>
<span class="sd">          [[ 234.  243.  252.]</span>
<span class="sd">           [ 279.  288.  297.]]</span>
<span class="sd">          [[ 414.  423.  432.]</span>
<span class="sd">           [ 459.  468.  477.]]]</span>
<span class="sd">         [[[ 594.  603.  612.]</span>
<span class="sd">           [ 639.  648.  657.]]</span>
<span class="sd">          [[ 774.  783.  792.]</span>
<span class="sd">           [ 819.  828.  837.]]</span>
<span class="sd">          [[ 954.  963.  972.]</span>
<span class="sd">           [ 999. 1008. 1017.]]]]</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">norm_type</span><span class="p">,</span> <span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">)):</span>
        <span class="n">norm_type</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">norm_type</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For lp_pool2d, the type of &#39;norm_type&#39; must be float or int, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">norm_type</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For lp_pool2d, the value of &#39;norm_type&#39; can not be 0.&quot;</span><span class="p">)</span>
    <span class="n">sign</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">Sign</span><span class="p">)()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">kernel_size</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">avg_pool2d</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">norm_type</span><span class="p">),</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="n">ceil_mode</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">((</span><span class="n">sign</span><span class="p">(</span><span class="n">out</span><span class="p">)</span> <span class="o">*</span> <span class="n">ops</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">out</span><span class="p">)))</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">norm_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="mse_loss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.mse_loss.html#mindspore.ops.mse_loss">[docs]</a><span class="k">def</span> <span class="nf">mse_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the mean squared error between the predicted value and the label value.</span>

<span class="sd">    For detailed information, please refer to :class:`mindspore.nn.MSELoss`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Tensor of any dimension.</span>
<span class="sd">        target (Tensor): The input label. Tensor of any dimension, same shape as the `input` in common cases.</span>
<span class="sd">            However, it supports that the shape of `input` is different from the shape of `target`</span>
<span class="sd">            and they should be broadcasted to each other.</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the output elements will be summed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, loss of type float, the shape is zero if `reduction` is ``&#39;mean&#39;`` or ``&#39;sum&#39;`` ,</span>
<span class="sd">        while the shape of output is the broadcasted shape if `reduction` is ``&#39;none&#39;`` .</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If `reduction` is not one of ``&#39;none&#39;`` , ``&#39;mean&#39;`` or ``&#39;sum&#39;``.</span>
<span class="sd">        ValueError: If `input` and `target` have different shapes and cannot be broadcasted.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([[1, 1, 1], [1, 2, 2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.mse_loss(logits, labels, reduction=&#39;none&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 1. 4.]</span>
<span class="sd">         [0. 0. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For ops.mse_loss, the `input` must be tensor&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For ops.mse_loss, the `target` must be tensor&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For ops.mse_loss, `reduction` value should be either &#39;mean&#39;, &#39;none&#39; or &#39;sum&#39;.&quot;</span><span class="p">)</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">square_</span><span class="p">(</span><span class="nb">input</span> <span class="o">-</span> <span class="n">target</span><span class="p">)</span>
    <span class="n">float_type</span> <span class="o">=</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">float_type</span><span class="p">:</span>
        <span class="n">input_dtype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">input_dtype</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">cast_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="n">average_flag</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">reduce_flag</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;sum&#39;</span><span class="p">:</span>
        <span class="n">average_flag</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;none&#39;</span><span class="p">:</span>
        <span class="n">reduce_flag</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">if</span> <span class="n">reduce_flag</span> <span class="ow">and</span> <span class="n">average_flag</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">reduce_mean_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">_get_axis</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">reduce_flag</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">average_flag</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">reduce_sum_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">_get_axis</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">cast_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="msort"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.msort.html#mindspore.ops.msort">[docs]</a><span class="k">def</span> <span class="nf">msort</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sorts the elements in Tensor in ascending order of value along its first dimension.</span>

<span class="sd">    ops.msort(t) is equivalent to ops.Sort(axis=0)(t)[0]. See also :class:`mindspore.ops.Sort()`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input to sort, with float16 or float32 data type.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tensor whose values are the sorted values, with the same shape and data type as input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `input` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; input = ms.Tensor(np.array([[8, 2, 1], [5, 9, 3], [4, 6, 7]]), ms.float16)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.msort(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[4. 2. 1.]</span>
<span class="sd">         [5. 6. 3.]</span>
<span class="sd">         [8. 9. 7.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">Sort</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)(</span><span class="nb">input</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span></div>


<div class="viewcode-block" id="triplet_margin_loss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.triplet_margin_loss.html#mindspore.ops.triplet_margin_loss">[docs]</a><span class="k">def</span> <span class="nf">triplet_margin_loss</span><span class="p">(</span><span class="n">anchor</span><span class="p">,</span> <span class="n">positive</span><span class="p">,</span> <span class="n">negative</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">swap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    TripletMarginLoss operation.</span>
<span class="sd">    See :class:`mindspore.nn.TripletMarginLoss` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        anchor (Tensor): A sample randomly selected from the training set. Data type must be BasicType.</span>
<span class="sd">        positive (Tensor): A sample belonging to the same category as `anchor`, with the same type and shape</span>
<span class="sd">            as `anchor`.</span>
<span class="sd">        negative (Tensor): A sample belonging to the different class from `anchor`, with the same type and shape</span>
<span class="sd">            as `anchor`.</span>
<span class="sd">        margin (float, optional): Make a margin between the positive pair and the negative pair. Default: ``1.0`` .</span>
<span class="sd">        p (int, optional): The degree of norm for pairwise distance. Default: ``2`` .</span>
<span class="sd">        eps (float, optional): Add small value to avoid division by zero. Default: ``1e-06``.</span>
<span class="sd">        swap (bool, optional): The distance swap change the negative distance to the distance between positive</span>
<span class="sd">            sample and negative sample. Default: ``False`` .</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the output elements will be summed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor. If `reduction` is ``&quot;none&quot;``, its shape is :math:`(N)`. Otherwise, a scalar value will be returned.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `anchor` or `positive` or `negative` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `anchor`, `positive` and `negative` is not the same.</span>
<span class="sd">        TypeError: If `margin` is not a float.</span>
<span class="sd">        TypeError: If `p` is not an int.</span>
<span class="sd">        TypeError: If `eps` is not a float.</span>
<span class="sd">        TypeError: If `swap` is not a bool.</span>
<span class="sd">        ValueError: If dimensions of input `anchor`, `positive` and `negative` are less than or equal to 1 at the</span>
<span class="sd">            same time.</span>
<span class="sd">        ValueError: If the dimension of input `anchor` or `positive` or `negative` is bigger than or equal to 8.</span>
<span class="sd">        ValueError: If shape of `anchor`, `positive` and `negative` cannot broadcast.</span>
<span class="sd">        ValueError: If `reduction` is not one of ``&#39;none&#39;``, ``&#39;mean&#39;``, ``&#39;sum&#39;``.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; anchor = Tensor(np.array([[0.3, 0.7], [0.5, 0.5]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; positive = Tensor(np.array([[0.4, 0.6], [0.4, 0.6]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; negative = Tensor(np.array([[0.2, 0.9], [0.3, 0.7]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.triplet_margin_loss(anchor, positive, negative)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.8881968</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">margin</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">margin_tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">margin</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">margin_tensor</span> <span class="o">=</span> <span class="n">margin</span>
    <span class="n">triplet_margin_loss_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">TripletMarginLoss</span><span class="p">)(</span><span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">swap</span><span class="o">=</span><span class="n">swap</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">triplet_margin_loss_op</span><span class="p">(</span><span class="n">anchor</span><span class="p">,</span> <span class="n">positive</span><span class="p">,</span> <span class="n">negative</span><span class="p">,</span> <span class="n">margin_tensor</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;inner linear&quot;&quot;&quot;</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">b</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span> <span class="o">+</span> <span class="n">b</span>
    <span class="k">return</span> <span class="n">out</span>


<span class="k">def</span> <span class="nf">_inner_dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;inner dropout&quot;&quot;&quot;</span>
    <span class="n">_dropout</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Dropout</span><span class="p">)(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span>
    <span class="k">if</span> <span class="mf">0.</span> <span class="o">&lt;</span> <span class="n">p</span> <span class="o">&lt;=</span> <span class="mf">1.</span> <span class="ow">and</span> <span class="n">training</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">x</span>


<span class="k">def</span> <span class="nf">_in_projection</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">w_q</span><span class="p">,</span> <span class="n">w_k</span><span class="p">,</span> <span class="n">w_v</span><span class="p">,</span> <span class="n">b_q</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_k</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_v</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;in projection function&quot;&quot;&quot;</span>
    <span class="n">Eq</span><span class="p">,</span> <span class="n">Ek</span><span class="p">,</span> <span class="n">Ev</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">k</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">w_q_shape</span><span class="p">,</span> <span class="n">w_k_shape</span><span class="p">,</span> <span class="n">w_v_shape</span> <span class="o">=</span> <span class="n">w_q</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">w_k</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">w_v</span><span class="o">.</span><span class="n">shape</span>

    <span class="k">if</span> <span class="n">w_q_shape</span> <span class="o">!=</span> <span class="p">(</span><span class="n">Eq</span><span class="p">,</span> <span class="n">Eq</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expecting query weights shape of </span><span class="si">{</span><span class="p">(</span><span class="n">Eq</span><span class="p">,</span><span class="w"> </span><span class="n">Eq</span><span class="p">)</span><span class="si">}</span><span class="s2">, but got </span><span class="si">{</span><span class="n">w_q_shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">w_k_shape</span> <span class="o">!=</span> <span class="p">(</span><span class="n">Eq</span><span class="p">,</span> <span class="n">Ek</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expecting key weights shape of </span><span class="si">{</span><span class="p">(</span><span class="n">Eq</span><span class="p">,</span><span class="w"> </span><span class="n">Ek</span><span class="p">)</span><span class="si">}</span><span class="s2">, but got </span><span class="si">{</span><span class="n">w_k_shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">w_v_shape</span> <span class="o">!=</span> <span class="p">(</span><span class="n">Eq</span><span class="p">,</span> <span class="n">Ev</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expecting value weights shape of </span><span class="si">{</span><span class="p">(</span><span class="n">Eq</span><span class="p">,</span><span class="w"> </span><span class="n">Ev</span><span class="p">)</span><span class="si">}</span><span class="s2">, but got </span><span class="si">{</span><span class="n">w_v_shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">b_q</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_q_shape</span> <span class="o">=</span> <span class="n">b_q</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">if</span> <span class="n">b_q_shape</span> <span class="o">!=</span> <span class="p">(</span><span class="n">Eq</span><span class="p">,):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expecting query bias shape of </span><span class="si">{</span><span class="p">(</span><span class="n">Eq</span><span class="p">,)</span><span class="si">}</span><span class="s2">, but got </span><span class="si">{</span><span class="n">b_q_shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">b_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_k_shape</span> <span class="o">=</span> <span class="n">b_k</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">if</span> <span class="n">b_k_shape</span> <span class="o">!=</span> <span class="p">(</span><span class="n">Eq</span><span class="p">,):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expecting key bias shape of </span><span class="si">{</span><span class="p">(</span><span class="n">Eq</span><span class="p">,)</span><span class="si">}</span><span class="s2">, but got </span><span class="si">{</span><span class="n">b_k_shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">b_v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_v_shape</span> <span class="o">=</span> <span class="n">b_v</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">if</span> <span class="n">b_v_shape</span> <span class="o">!=</span> <span class="p">(</span><span class="n">Eq</span><span class="p">,):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expecting value bias shape of </span><span class="si">{</span><span class="p">(</span><span class="n">Eq</span><span class="p">,)</span><span class="si">}</span><span class="s2">, but got </span><span class="si">{</span><span class="n">b_v_shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">linear</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">w_q</span><span class="p">,</span> <span class="n">b_q</span><span class="p">),</span> <span class="n">linear</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">w_k</span><span class="p">,</span> <span class="n">b_k</span><span class="p">),</span> <span class="n">linear</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">w_v</span><span class="p">,</span> <span class="n">b_v</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_in_projection_packed</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">k_is_v</span><span class="p">,</span> <span class="n">q_is_k</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;in projecktion packed function&quot;&quot;&quot;</span>
    <span class="n">E</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">k_is_v</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">q_is_k</span><span class="p">:</span>
            <span class="c1"># self-attention</span>
            <span class="k">return</span> <span class="n">linear</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">tensor_split</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># encoder-decoder attention</span>
        <span class="n">w_q</span><span class="p">,</span> <span class="n">w_kv</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">split</span><span class="p">([</span><span class="n">E</span><span class="p">,</span> <span class="n">E</span> <span class="o">*</span> <span class="mi">2</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">b</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">b_q</span> <span class="o">=</span> <span class="n">b_kv</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">b_q</span><span class="p">,</span> <span class="n">b_kv</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">split</span><span class="p">([</span><span class="n">E</span><span class="p">,</span> <span class="n">E</span> <span class="o">*</span> <span class="mi">2</span><span class="p">])</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">linear</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">w_q</span><span class="p">,</span> <span class="n">b_q</span><span class="p">),)</span> <span class="o">+</span> <span class="n">linear</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">w_kv</span><span class="p">,</span> <span class="n">b_kv</span><span class="p">)</span><span class="o">.</span><span class="n">tensor_split</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">w_q</span><span class="p">,</span> <span class="n">w_k</span><span class="p">,</span> <span class="n">w_v</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">tensor_split</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">b</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_q</span> <span class="o">=</span> <span class="n">b_k</span> <span class="o">=</span> <span class="n">b_v</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">b_q</span><span class="p">,</span> <span class="n">b_k</span><span class="p">,</span> <span class="n">b_v</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">tensor_split</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">linear</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">w_q</span><span class="p">,</span> <span class="n">b_q</span><span class="p">),</span> <span class="n">linear</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">w_k</span><span class="p">,</span> <span class="n">b_k</span><span class="p">),</span> <span class="n">linear</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">w_v</span><span class="p">,</span> <span class="n">b_v</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_scaled_dot_product_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">,</span> <span class="n">is_causal</span><span class="p">,</span> <span class="n">is_training</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;scaled dot product attention&quot;&quot;&quot;</span>
    <span class="n">embed_size</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">embed_size_tensor</span> <span class="o">=</span> <span class="n">scalar_to_tensor_</span><span class="p">(</span><span class="n">embed_size</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="n">scaling_factor</span> <span class="o">=</span> <span class="n">embed_size_tensor</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span>
    <span class="n">query</span> <span class="o">=</span> <span class="n">query</span> <span class="o">/</span> <span class="n">scaling_factor</span>

    <span class="k">if</span> <span class="n">is_causal</span><span class="p">:</span>
        <span class="n">L</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">S</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">attn_mask</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">L</span><span class="p">,</span> <span class="n">S</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span><span class="o">.</span><span class="n">tril</span><span class="p">()</span>

    <span class="n">attn</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">scaling_factor</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">attn_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">attn</span> <span class="o">+</span> <span class="n">attn_mask</span>
    <span class="n">attn</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">attn</span> <span class="o">=</span> <span class="n">_inner_dropout</span><span class="p">(</span><span class="n">attn</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">,</span> <span class="n">is_training</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">attn</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_qkv_shape</span><span class="p">(</span><span class="n">query_ndim</span><span class="p">,</span> <span class="n">key_ndim</span><span class="p">,</span> <span class="n">value_ndim</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check the expected shape for `query, `key`, `value` and returns whether the input is batched.&quot;&quot;&quot;</span>
    <span class="c1"># Shape check.</span>
    <span class="k">if</span> <span class="n">query_ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="c1"># Batched Inputs</span>
        <span class="n">is_batched</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="n">key_ndim</span> <span class="o">!=</span> <span class="mi">3</span> <span class="ow">or</span> <span class="n">value_ndim</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For batched `query`, the `key` and `value` must be 3D tensor, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got `key` with </span><span class="si">{</span><span class="n">key_ndim</span><span class="si">}</span><span class="s2">D and `value` with </span><span class="si">{</span><span class="n">value_ndim</span><span class="si">}</span><span class="s2">D.&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">query_ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="c1"># Unbatched Inputs</span>
        <span class="n">is_batched</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">key_ndim</span> <span class="o">!=</span> <span class="mi">2</span> <span class="ow">or</span> <span class="n">value_ndim</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For unbatched `query`, the `key` and `value` must be 2D tensor, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got `key` with </span><span class="si">{</span><span class="n">key_ndim</span><span class="si">}</span><span class="s2">D and `value` with </span><span class="si">{</span><span class="n">value_ndim</span><span class="si">}</span><span class="s2">D.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The `query` should be unbatched 2D or batched 3D tensor, &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;but got `query` with </span><span class="si">{</span><span class="n">query_ndim</span><span class="si">}</span><span class="s2">D.&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">is_batched</span>


<span class="k">def</span> <span class="nf">_check_kpm_shape</span><span class="p">(</span><span class="n">query_ndim</span><span class="p">,</span> <span class="n">kmp_ndim</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;check key_padding_mask shape&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">query_ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">kmp_ndim</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For batched `query`, the `key_padding_mask` must be `None` or 2D, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got `key_padding_mask` with </span><span class="si">{</span><span class="n">kmp_ndim</span><span class="si">}</span><span class="s2">D.&quot;</span><span class="p">)</span>

    <span class="k">elif</span> <span class="n">query_ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">kmp_ndim</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For unbatched `query`, the `key_padding_mask` must be `None` or 1D, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got `key_padding_mask` with </span><span class="si">{</span><span class="n">kmp_ndim</span><span class="si">}</span><span class="s2">D.&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_attn_mask_shape</span><span class="p">(</span><span class="n">query_ndim</span><span class="p">,</span> <span class="n">query_shape</span><span class="p">,</span> <span class="n">key_shape</span><span class="p">,</span> <span class="n">attn_mask_ndim</span><span class="p">,</span>
                           <span class="n">attn_mask_shape</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Check the expected shape for `attn_mask`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Shape check.</span>
    <span class="k">if</span> <span class="n">query_ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">attn_mask_ndim</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For batched `query`, the `attn_mask` must be `None`, 2-D or 3-D, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got `attn_mask` with</span><span class="si">{</span><span class="n">attn_mask_ndim</span><span class="si">}</span><span class="s2">D.&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">query_ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">attn_mask_ndim</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For unbatched `query`, the `attn_mask` must be `None`, 2-D or 3-D, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got `attn_mask` with</span><span class="si">{</span><span class="n">attn_mask_ndim</span><span class="si">}</span><span class="s2">D.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attn_mask_ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">expected_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">query_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">key_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">attn_mask_shape</span> <span class="o">!=</span> <span class="n">expected_shape</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The shape of `attn_mask` must to be </span><span class="si">{</span><span class="n">expected_shape</span><span class="si">}</span><span class="s2">, &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">attn_mask_shape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_inner_pad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;inner pad function for bool type.&quot;&quot;&quot;</span>
    <span class="n">x_dtype</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span>
    <span class="k">if</span> <span class="n">x_dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">pad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">value</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">x_dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>


<span class="k">def</span> <span class="nf">multi_head_attention_forward</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">embed_dim_to_check</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">in_proj_weight</span><span class="p">,</span>
                                 <span class="n">in_proj_bias</span><span class="p">,</span> <span class="n">bias_k</span><span class="p">,</span> <span class="n">bias_v</span><span class="p">,</span> <span class="n">add_zero_attn</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">,</span> <span class="n">out_proj_weight</span><span class="p">,</span>
                                 <span class="n">out_proj_bias</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">key_padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                 <span class="n">use_separate_proj_weight</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">q_proj_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">k_proj_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                 <span class="n">v_proj_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">static_k</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">static_v</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">average_attn_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                 <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">k_is_v</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">q_is_k</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;multi head attetion forward function&quot;&quot;&quot;</span>
    <span class="n">is_batched</span> <span class="o">=</span> <span class="n">_check_qkv_shape</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="n">value</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">key_padding_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_check_kpm_shape</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="n">key_padding_mask</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">attn_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_check_attn_mask_shape</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span>
                               <span class="n">attn_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_batched</span><span class="p">:</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">key_padding_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">key_padding_mask</span> <span class="o">=</span> <span class="n">key_padding_mask</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="n">tgt_len</span><span class="p">,</span> <span class="n">bsz</span><span class="p">,</span> <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">src_len</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">if</span> <span class="n">key_padding_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_kpm_dtype</span> <span class="o">=</span> <span class="n">key_padding_mask</span><span class="o">.</span><span class="n">dtype</span>
        <span class="k">if</span> <span class="n">_kpm_dtype</span> <span class="o">!=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">ops</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">(</span><span class="n">key_padding_mask</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The `key_padding_mask` only supports bool and floating dtypes.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">embed_dim</span> <span class="o">!=</span> <span class="n">embed_dim_to_check</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The `embed_dim` should be </span><span class="si">{</span><span class="n">embed_dim_to_check</span><span class="si">}</span><span class="s2">, but got </span><span class="si">{</span><span class="n">embed_dim</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="n">head_dim</span> <span class="o">=</span> <span class="n">embed_dim</span> <span class="o">//</span> <span class="n">num_heads</span>
    <span class="k">if</span> <span class="n">head_dim</span> <span class="o">*</span> <span class="n">num_heads</span> <span class="o">!=</span> <span class="n">embed_dim</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The `embed_dim` </span><span class="si">{</span><span class="n">embed_dim</span><span class="si">}</span><span class="s2"> can not be divisible by `num_heads` </span><span class="si">{</span><span class="n">num_heads</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">use_separate_proj_weight</span><span class="p">:</span>
        <span class="c1"># allow MHA to have different embedding dims when separate projection weights are used</span>
        <span class="k">if</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span> <span class="o">!=</span> <span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The sequence length and batch dims of `key`: </span><span class="si">{</span><span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2"> do not match &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;`value`: </span><span class="si">{</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The shape of `key` </span><span class="si">{</span><span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> does not match `value` </span><span class="si">{</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="c1"># compute in-projection</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">use_separate_proj_weight</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">in_proj_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`use_separate_proj_weight` is ``False`` but `in_proj_weight` got ``None``.&quot;</span><span class="p">)</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">_in_projection_packed</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">in_proj_weight</span><span class="p">,</span> <span class="n">in_proj_bias</span><span class="p">,</span> <span class="n">k_is_v</span><span class="p">,</span> <span class="n">q_is_k</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">q_proj_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`use_separate_proj_weight` is ``True`` but `q_proj_weight` got ``None``.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">k_proj_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`use_separate_proj_weight` is ``True`` but `k_proj_weight` got ``None``.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">v_proj_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`use_separate_proj_weight` is ``True`` but `v_proj_weight` got ``None``.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">in_proj_bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">b_q</span> <span class="o">=</span> <span class="n">b_k</span> <span class="o">=</span> <span class="n">b_v</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">b_q</span><span class="p">,</span> <span class="n">b_k</span><span class="p">,</span> <span class="n">b_v</span> <span class="o">=</span> <span class="n">in_proj_bias</span><span class="o">.</span><span class="n">tensor_split</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">_in_projection</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">q_proj_weight</span><span class="p">,</span> <span class="n">k_proj_weight</span><span class="p">,</span> <span class="n">v_proj_weight</span><span class="p">,</span> <span class="n">b_q</span><span class="p">,</span> <span class="n">b_k</span><span class="p">,</span> <span class="n">b_v</span><span class="p">)</span>

    <span class="c1"># prep attention mask</span>
    <span class="k">if</span> <span class="n">attn_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">attn_mask</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint8</span><span class="p">:</span>
            <span class="n">attn_mask</span> <span class="o">=</span> <span class="n">attn_mask</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">ops</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">(</span><span class="n">attn_mask</span><span class="p">)</span> <span class="ow">and</span> <span class="n">attn_mask</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`attn_mask` only support float, byte, and bool types, &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;but got not </span><span class="si">{</span><span class="n">attn_mask</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="c1"># ensure attn_mask&#39;s ndim is 3</span>
        <span class="k">if</span> <span class="n">attn_mask</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">correct_2d_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">attn_mask</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">correct_2d_size</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The shape of the `attn_mask` should be </span><span class="si">{</span><span class="n">correct_2d_size</span><span class="si">}</span><span class="s2">, &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">attn_mask</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
            <span class="n">attn_mask</span> <span class="o">=</span> <span class="n">attn_mask</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">attn_mask</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">correct_3d_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">bsz</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">attn_mask</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">correct_3d_size</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The shape of the `attn_mask` should be </span><span class="si">{</span><span class="n">correct_3d_size</span><span class="si">}</span><span class="s2">, &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">attn_mask</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The ndim of `attn_mask` only support 2 or 3, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">attn_mask</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">bias_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">bias_v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">static_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The bias_k cannot be added to static_k.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">static_v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The bias_v cannot be added to static_v.&quot;</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">k</span><span class="p">,</span> <span class="n">bias_k</span><span class="o">.</span><span class="n">tile</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">bsz</span><span class="p">,</span> <span class="mi">1</span><span class="p">))])</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">v</span><span class="p">,</span> <span class="n">bias_v</span><span class="o">.</span><span class="n">tile</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">bsz</span><span class="p">,</span> <span class="mi">1</span><span class="p">))])</span>
        <span class="k">if</span> <span class="n">attn_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attn_mask</span> <span class="o">=</span> <span class="n">_inner_pad</span><span class="p">(</span><span class="n">attn_mask</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">key_padding_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">key_padding_mask</span> <span class="o">=</span> <span class="n">_inner_pad</span><span class="p">(</span><span class="n">key_padding_mask</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">bias_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">bias_v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The bias_k and bias_v should be ``None``&quot;</span>
                             <span class="s2">&quot;at the same time.&quot;</span><span class="p">)</span>

    <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">view</span><span class="p">((</span><span class="n">tgt_len</span><span class="p">,</span> <span class="n">bsz</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">))</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">static_k</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">view</span><span class="p">((</span><span class="n">k</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">bsz</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">))</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">static_k</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">bsz</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The shape[0] of `static_k` should be </span><span class="si">{</span><span class="n">bsz</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">num_heads</span><span class="si">}</span><span class="s2">, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">static_k</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">static_k</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">!=</span> <span class="n">head_dim</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The shape[2] of `static_k` should be </span><span class="si">{</span><span class="n">head_dim</span><span class="si">}</span><span class="s2">, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">static_k</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">static_k</span>
    <span class="k">if</span> <span class="n">static_v</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">view</span><span class="p">((</span><span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">bsz</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">))</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">static_v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">bsz</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The shape[0] of `static_v` should be </span><span class="si">{</span><span class="n">bsz</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">num_heads</span><span class="si">}</span><span class="s2">, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">static_v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">static_v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">!=</span> <span class="n">head_dim</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The shape[2] of `static_v` should be </span><span class="si">{</span><span class="n">head_dim</span><span class="si">}</span><span class="s2">, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">static_v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">static_v</span>

    <span class="k">if</span> <span class="n">add_zero_attn</span><span class="p">:</span>
        <span class="n">zero_attn_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">bsz</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">k</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">zero_attn_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">k</span><span class="o">.</span><span class="n">dtype</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">v</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">zero_attn_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">v</span><span class="o">.</span><span class="n">dtype</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attn_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attn_mask</span> <span class="o">=</span> <span class="n">_inner_pad</span><span class="p">(</span><span class="n">attn_mask</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">key_padding_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">key_padding_mask</span> <span class="o">=</span> <span class="n">_inner_pad</span><span class="p">(</span><span class="n">key_padding_mask</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="n">src_len</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">key_padding_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">key_padding_mask</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">src_len</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The shape of `key_padding_mask` should be </span><span class="si">{</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span><span class="w"> </span><span class="n">src_len</span><span class="p">)</span><span class="si">}</span><span class="s2">, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">key_padding_mask</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="n">key_padding_mask</span> <span class="o">=</span> <span class="n">key_padding_mask</span><span class="o">.</span><span class="n">view</span><span class="p">((</span><span class="n">bsz</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">src_len</span><span class="p">))</span><span class="o">.</span> \
            <span class="n">tile</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">bsz</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">src_len</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attn_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attn_mask</span> <span class="o">=</span> <span class="n">key_padding_mask</span>
        <span class="k">elif</span> <span class="n">attn_mask</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">:</span>
            <span class="n">attn_mask</span> <span class="o">=</span> <span class="n">attn_mask</span><span class="o">.</span><span class="n">logical_or</span><span class="p">(</span><span class="n">key_padding_mask</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">attn_mask</span> <span class="o">=</span> <span class="n">attn_mask</span> <span class="o">+</span> <span class="n">key_padding_mask</span>

    <span class="k">if</span> <span class="n">attn_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">attn_mask</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">:</span>
        <span class="n">new_attn_mask</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">attn_mask</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">q</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">attn_mask</span> <span class="o">=</span> <span class="n">new_attn_mask</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">attn_mask</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">),</span> <span class="n">new_attn_mask</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">attn_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">attn_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">attn_mask</span> <span class="o">=</span> <span class="n">attn_mask</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">attn_mask</span> <span class="o">=</span> <span class="n">attn_mask</span><span class="o">.</span><span class="n">view</span><span class="p">((</span><span class="n">bsz</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">src_len</span><span class="p">))</span>

    <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">view</span><span class="p">((</span><span class="n">bsz</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">))</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">view</span><span class="p">((</span><span class="n">bsz</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">src_len</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">))</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">view</span><span class="p">((</span><span class="n">bsz</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">src_len</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">))</span>

    <span class="n">attn_output</span><span class="p">,</span> <span class="n">attn_output_weights</span> <span class="o">=</span> <span class="n">_scaled_dot_product_attention</span><span class="p">(</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">,</span> <span class="n">is_causal</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">((</span><span class="n">bsz</span> <span class="o">*</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">))</span>

    <span class="n">attn_output</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">attn_output</span><span class="p">,</span> <span class="n">out_proj_weight</span><span class="p">,</span> <span class="n">out_proj_bias</span><span class="p">)</span>
    <span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="o">.</span><span class="n">view</span><span class="p">((</span><span class="n">tgt_len</span><span class="p">,</span> <span class="n">bsz</span><span class="p">,</span> <span class="n">attn_output</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

    <span class="n">attn_output_weights</span> <span class="o">=</span> <span class="n">attn_output_weights</span><span class="o">.</span><span class="n">view</span><span class="p">((</span><span class="n">bsz</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">average_attn_weights</span><span class="p">:</span>
        <span class="n">attn_output_weights</span> <span class="o">=</span> <span class="n">attn_output_weights</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_heads</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_batched</span><span class="p">:</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attn_output_weights</span> <span class="o">=</span> <span class="n">attn_output_weights</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">attn_output</span><span class="p">,</span> <span class="n">attn_output_weights</span>


<div class="viewcode-block" id="max_pool2d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.max_pool2d.html#mindspore.ops.max_pool2d">[docs]</a><span class="k">def</span> <span class="nf">max_pool2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs a 2D max pooling on the input Tensor.</span>

<span class="sd">    Typically, the input is a Tensor with shape :math:`(N_{in}, C_{in}, H_{in}, W_{in})`, outputs</span>
<span class="sd">    regional maximum in the :math:`(H_{in}, W_{in})`-dimension. Given `kernel_size`</span>
<span class="sd">    :math:`ks = (h_{ker}, w_{ker})` and `stride` :math:`s = (s_0, s_1)`, the operation is as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(N_i, C_j, h, w) =</span>
<span class="sd">        \max_{m=0, \ldots, h_{ker}-1} \max_{n=0, \ldots, w_{ker}-1}</span>
<span class="sd">        \text{input}(N_i, C_j, s_0 \times h + m, s_1 \times w + n)</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Tensor of shape :math:`(N_{in}, C_{in}, H_{in}, W_{in})` with data type of int8,</span>
<span class="sd">            int16, int32, int64, uint8, uint16, uint32, uint64, float16, float32 or float64 in CPU or GPU</span>
<span class="sd">            while that of uint16 in Ascend.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of kernel used to take the maximum value and arg</span>
<span class="sd">            value, is an int number that represents height and width of the kernel, or a tuple of</span>
<span class="sd">            two int numbers that represent height and width respectively.</span>
<span class="sd">        stride (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the height and width of movement are both stride, or a tuple of two int numbers that</span>
<span class="sd">            represent height and width of movement respectively.</span>
<span class="sd">            Default: ``None`` , which indicates the moving step is `kernel_size` .</span>
<span class="sd">        padding (Union[int, tuple[int]]): An int number that represents the height and width of movement are both</span>
<span class="sd">            strides, or a tuple of two int numbers that represent height and width of movement respectively.</span>
<span class="sd">            Default: ``0`` .</span>
<span class="sd">        dilation (Union[int, tuple[int]]): Control the stride of elements in the kernel. Default: ``1`` .</span>
<span class="sd">        return_indices (bool): Whether to output the indices of max value. Default: ``False`` .</span>
<span class="sd">        ceil_mode (bool): Whether to use ceil instead of floor to calculate output shape. Default: ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        If `return_indices` is ``False`` , return a Tensor `output`, else return a tuple (`output`, `argmax`).</span>

<span class="sd">        - **output** (Tensor) - Maxpooling result, with shape :math:`(N_{out}, C_{out}, H_{out}, W_{out})`.</span>
<span class="sd">          It has the same data type as `x`.</span>

<span class="sd">        .. math::</span>
<span class="sd">            H_{out} = \left\lfloor\frac{H_{in} + 2 * \text{padding[0]} - \text{dilation[0]}</span>
<span class="sd">                \times (\text{kernel_size[0]} - 1) - 1}{\text{stride[0]}} + 1\right\rfloor</span>

<span class="sd">        .. math::</span>
<span class="sd">            W_{out} = \left\lfloor\frac{W_{in} + 2 * \text{padding[1]} - \text{dilation[1]}</span>
<span class="sd">                \times (\text{kernel_size[1]} - 1) - 1}{\text{stride[1]}} + 1\right\rfloor</span>

<span class="sd">        - **argmax** (Tensor) - Index corresponding to the maximum value. In CPU and GPU, data type is int64</span>
<span class="sd">          while that is uint16 in Ascend. It will be return only when `return_indices` is True.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        ValueError: If length of shape of `x` is not equal to 4.</span>
<span class="sd">        TypeError: If `kernel_size` , `stride` , `padding` or `dilation` is not int or tuple.</span>
<span class="sd">        ValueError: If `kernel_size`, `stride` or `dilation` is less than 1.</span>
<span class="sd">        ValueError: If `padding` is less than 0.</span>
<span class="sd">        ValueError: If `padding` is more than half of `kernel_size`.</span>
<span class="sd">        TypeError: If `ceil_mode` is not bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(20 * 16 * 50 * 32).reshape((20, 16, 50, 32)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output_tensor, argmax = ops.max_pool2d(x, kernel_size=(3, 2), stride=(2, 1), return_indices=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output_tensor.shape)</span>
<span class="sd">        (20, 16, 24, 31)</span>
<span class="sd">        &gt;&gt;&gt; print(argmax.shape)</span>
<span class="sd">        (20, 16, 24, 31)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">strides</span> <span class="o">=</span> <span class="n">stride</span> <span class="k">if</span> <span class="p">(</span><span class="n">stride</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="k">else</span> <span class="n">kernel_size</span>
    <span class="n">max_pool_with_argmax_v2_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">MaxPoolWithArgmaxV2</span><span class="p">)(</span>
        <span class="n">kernel_size</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">)</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">max_pool_with_argmax_v2_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">return_indices</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">indices</span>
    <span class="k">return</span> <span class="n">out</span></div>


<span class="k">def</span> <span class="nf">prompt_flash_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">,</span> <span class="n">actual_seq_lengths</span><span class="p">,</span> <span class="n">actual_seq_lengths_kv</span><span class="p">,</span> <span class="n">pse_shift</span><span class="p">,</span>
                           <span class="n">deq_scale1</span><span class="p">,</span> <span class="n">quant_scale1</span><span class="p">,</span> <span class="n">deq_scale2</span><span class="p">,</span> <span class="n">quant_scale2</span><span class="p">,</span> <span class="n">quant_offset2</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span>
                           <span class="n">scale_value</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">pre_tokens</span><span class="o">=</span><span class="mi">2147483547</span><span class="p">,</span> <span class="n">next_tokens</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">input_layout</span><span class="o">=</span><span class="s1">&#39;BSH&#39;</span><span class="p">,</span>
                           <span class="n">num_key_value_heads</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sparse_mode</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">inner_precise</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The interface for fully inference.</span>
<span class="sd">    B -- Batch size</span>
<span class="sd">    S -- Sequence length</span>
<span class="sd">    H -- Hidden size</span>

<span class="sd">    Note:</span>
<span class="sd">    experiment ops</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        query (Tensor) - The query tensor with data type of float16 or float32.</span>
<span class="sd">          Input tensor of shape :math:`(B, S, H)` / `(B, N, S, D)`.</span>
<span class="sd">        key (Tensor) - The key tensor with data type of float16 or float32.</span>
<span class="sd">          Input tensor of shape :math:`(B, S, H)` / `(B, N, S, D)`.</span>
<span class="sd">        value (Tensor) - The value tensor with data type of float16 or float32.</span>
<span class="sd">          Input tensor of shape :math:`(B, S, H)` / `(B, N, S, D)`.</span>
<span class="sd">        attn_mask (Tensor) - The attention mask tensor with data type of float16 or float32.</span>
<span class="sd">          For each element, 0 indicates retention and 1 indicates discard. Input tensor of shape :math:`(B, 1, S, S)`.</span>
<span class="sd">        actual_seq_lengths (Tensor): Describe actual sequence length of each input with data type of int64.</span>
<span class="sd">        actual_seq_lengths_kv (Tensor): Describe actual sequence length of each input with data type of int64.</span>
<span class="sd">        pse_shift (Tensor) - The position encoding tensor with data type of float16 or float32.</span>
<span class="sd">        dep_scale1 (Tensor)</span>
<span class="sd">        quant_scale1 (Tensor)</span>
<span class="sd">        deq_scale2 (Tensor)</span>
<span class="sd">        quant_scale2 (Tensor)</span>
<span class="sd">        quant_offset2 (Tensor)</span>
<span class="sd">        num_heads (int): The number of heads.</span>
<span class="sd">        scale_value (float): The scale value indicating the scale coefficient, which is used as the scalar of</span>
<span class="sd">          Muls in the calculation. Default: 1.0.</span>
<span class="sd">        pre_tokens (int): Previous tokens. Default: 2147483547.</span>
<span class="sd">        next_tokens (int): next tokens.  Default: 0.</span>
<span class="sd">          indicate the upper triangle, Indicate the number of data blocks involved in the calculation. The value 0</span>
<span class="sd">          indicates that the data blocks in the upper triangle are not involved in the calculation</span>
<span class="sd">        input_layout (str): the data layout of the input qkv, support `(BSH)` and `(BNSD)`, Default `BSH`.</span>
<span class="sd">        num_key_value_heads (int): head numbers of key/value which are used in GQA algorithm.</span>
<span class="sd">          The value o indicates if the key and value have the same head nums, use numHeads.  Default: 0.</span>
<span class="sd">        sparse_mode (int): Default: 0</span>
<span class="sd">        inner_precise (int): 0, float16 high precision. 1, high performance. default 1</span>


<span class="sd">    Outputs:</span>
<span class="sd">        attention_out (Tensor) - Input tensor of shape :math:`(B, S, H)` / `(B, N, S, D)`.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops.function.nn_func import prompt_flash_attention</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; B = 1</span>
<span class="sd">        &gt;&gt;&gt; N = 16</span>
<span class="sd">        &gt;&gt;&gt; S = 256</span>
<span class="sd">        &gt;&gt;&gt; D = 16</span>
<span class="sd">        &gt;&gt;&gt; query = Tensor(np.ones((B, N, S, D), dtype=np.float16))</span>
<span class="sd">        &gt;&gt;&gt; key = Tensor(np.ones((B, N, S, D), dtype=np.float16))</span>
<span class="sd">        &gt;&gt;&gt; value = Tensor(np.ones((B, N, S, D), dtype=np.float16))</span>
<span class="sd">        &gt;&gt;&gt; out = ops.prompt_flash_attention(query, key, value, None, None, None, None, None, None, None, None,</span>
<span class="sd">                                             None, N, input_layout=&#39;BNSD&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(out.shape)</span>
<span class="sd">        (1, 16, 256, 16)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">pfa</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">PromptFlashAttention</span><span class="p">)(</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">scale_value</span><span class="p">,</span> <span class="n">pre_tokens</span><span class="p">,</span> <span class="n">next_tokens</span><span class="p">,</span> <span class="n">input_layout</span><span class="p">,</span>
                                                       <span class="n">num_key_value_heads</span><span class="p">,</span> <span class="n">sparse_mode</span><span class="p">,</span> <span class="n">inner_precise</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pfa</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">,</span> <span class="n">actual_seq_lengths</span><span class="p">,</span> <span class="n">actual_seq_lengths_kv</span><span class="p">,</span> <span class="n">pse_shift</span><span class="p">,</span> <span class="n">deq_scale1</span><span class="p">,</span>
               <span class="n">quant_scale1</span><span class="p">,</span> <span class="n">deq_scale2</span><span class="p">,</span> <span class="n">quant_scale2</span><span class="p">,</span> <span class="n">quant_offset2</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">incre_flash_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">,</span> <span class="n">actual_seq_lengths</span><span class="p">,</span> <span class="n">pse_shift</span><span class="p">,</span> <span class="n">dequant_scale1</span><span class="p">,</span> <span class="n">quant_scale1</span><span class="p">,</span>
                          <span class="n">dequant_scale2</span><span class="p">,</span> <span class="n">quant_scale2</span><span class="p">,</span> <span class="n">quant_offset2</span><span class="p">,</span> <span class="n">antiquant_scale</span><span class="p">,</span> <span class="n">antiquant_offset</span><span class="p">,</span> <span class="n">block_table</span><span class="p">,</span>
                          <span class="n">num_heads</span><span class="p">,</span> <span class="n">input_layout</span><span class="o">=</span><span class="s2">&quot;BSH&quot;</span><span class="p">,</span> <span class="n">scale_value</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">num_key_value_heads</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                          <span class="n">inner_precise</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The interface for fully inference.</span>

<span class="sd">    B -- Batch size</span>

<span class="sd">    S -- Sequence length</span>

<span class="sd">    H -- Hidden size</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>
<span class="sd">        If there is no input parameter and no default value, None needs to be passed.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **query** (Tensor) - The query tensor with data type of float16 or bfloat16.</span>
<span class="sd">          Input tensor of shape :math:`(B, 1, H)` / :math:`(B, N, 1, D)`.</span>
<span class="sd">        - **key** (TensorList) - The key tensor with data type of float16 or bfloat16.</span>
<span class="sd">          Input tensor of shape :math:`(B, S, H)` / :math:`(B, N, S, D)`.</span>
<span class="sd">        - **value** (TensorList) - The value tensor with data type of float16 or bfloat16.</span>
<span class="sd">          Input tensor of shape :math:`(B, S, H)` / :math:`(B, N, S, D)`.</span>
<span class="sd">        - **attn_mask** (Tensor) - The attention mask tensor with data type of float16 or bool.</span>
<span class="sd">          Input tensor of shape :math:`(B, S)` / :math:`(B, 1, S)` / :math:`(B, 1, 1, S)`.</span>
<span class="sd">        - **actual_seq_lengths** (Tensor) - Describe actual sequence length of each input with data type of int.</span>
<span class="sd">        - **pse_shift** (Tensor) - The position encoding tensor with data type of float16 or float32.</span>
<span class="sd">        - **dequant_scale1** (Tensor) - Quantitative parametor, the tensor with data type of uint64.</span>
<span class="sd">        - **quant_scale1** (Tensor) - Quantitative parametor, the tensor with data type of float.</span>
<span class="sd">        - **dequant_scale2** (Tensor) - Quantitative parametor, the tensor with data type of uint64.</span>
<span class="sd">        - **quant_scale2** (Tensor) - Quantitative parametor, the tensor with data type of float.</span>
<span class="sd">        - **quant_offset2** (Tensor) - Quantitative parametor, the tensor with data type of float.</span>
<span class="sd">        - **antiquant_scale** (Tensor) - Quantitative parametor, the tensor with data type of float.</span>
<span class="sd">        - **antiquant_offset** (Tensor) - Quantitative parametor, the tensor with data type of float.</span>
<span class="sd">        - **block_table** (Tensor) - The tensor with data type of float.</span>
<span class="sd">        - **num_heads**  (int) - The number of heads.</span>
<span class="sd">        - **input_layout** (str) - the data layout of the input qkv, support `(BSH)` and `(BNSD)`. Default `BSH`.</span>
<span class="sd">        - **scale_value** (double) - The scale value indicating the scale coefficient, which is used as the scalar of</span>
<span class="sd">          Muls in the calculation. Default: 1.0.</span>
<span class="sd">        - **num_key_value_heads** (int) - head numbers of key/value which are used in GQA algorithm.</span>
<span class="sd">          The value o indicates if the key and value have the same head nums, use numHeads.  Default: 0.</span>
<span class="sd">        - **block_size** (int) - Default: 0.</span>
<span class="sd">        - **inner_precise** (int) - Default: 1.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **attention_out** (Tensor) - Input tensor of shape :math:`(B, 1, H)` / :math:`(B, N, 1, D)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_ifa</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">IncreFlashAttention</span><span class="p">)(</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">input_layout</span><span class="p">,</span> <span class="n">scale_value</span><span class="p">,</span> <span class="n">num_key_value_heads</span><span class="p">,</span>
                                                       <span class="n">block_size</span><span class="p">,</span> <span class="n">inner_precise</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_ifa</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">,</span> <span class="n">actual_seq_lengths</span><span class="p">,</span> <span class="n">pse_shift</span><span class="p">,</span> <span class="n">dequant_scale1</span><span class="p">,</span> <span class="n">quant_scale1</span><span class="p">,</span>
                <span class="n">dequant_scale2</span><span class="p">,</span> <span class="n">quant_scale2</span><span class="p">,</span> <span class="n">quant_offset2</span><span class="p">,</span> <span class="n">antiquant_scale</span><span class="p">,</span> <span class="n">antiquant_offset</span><span class="p">,</span> <span class="n">block_table</span><span class="p">)</span>


<div class="viewcode-block" id="embedding"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.embedding.html#mindspore.ops.embedding">[docs]</a><span class="k">def</span> <span class="nf">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">norm_type</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">scale_grad_by_freq</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Retrieve the word embeddings in `weight` using indices specified in `input`.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        On Ascend, the behavior is unpredictable when the value of input is invalid.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The indices used to lookup in the `weight`. The data type must be mindspore.int32 or</span>
<span class="sd">            mindspore.int64, and the value should be in range `[0, weight.shape[0])`.</span>
<span class="sd">        weight (Parameter): The matrix where to lookup from. The shape must be 2D.</span>
<span class="sd">        padding_idx (int, optional): If the value is not None, the corresponding row of `weight` will not be updated</span>
<span class="sd">            in training. The value should be in range `[-weight.shape[0], weight.shape[0])` if it&#39;s not ``None``.</span>
<span class="sd">            Default ``None``.</span>
<span class="sd">        max_norm (float, optional): If not None, firstly get the p-norm result of the `weight` specified by `input`</span>
<span class="sd">            where p is specified by `norm_type`; if the result is larger then `max_norm`, update the `weight`</span>
<span class="sd">            with :math:`\frac{max\_norm}{result+1e^{-7}}` in-place. Default ``None``.</span>
<span class="sd">        norm_type (float, optional): Indicates the value of p in p-norm. Default ``2.0``.</span>
<span class="sd">        scale_grad_by_freq (bool, optional): If ``True`` the gradients will be scaled by the inverse of frequency of</span>
<span class="sd">            the index in `input`. Default ``False``.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same data type as `weight`, the shape is :math:`(*input.shape, weight.shape[1])`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If `padding_idx` is out of valid range.</span>
<span class="sd">        ValueError: If the shape of `weight` is invalid.</span>
<span class="sd">        TypeError: `weight` is not a :class:`mindspore.Parameter`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, Parameter, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor([[1, 0, 1, 1], [0, 0, 1, 0]])</span>
<span class="sd">        &gt;&gt;&gt; weight = Parameter(np.random.randn(3, 3).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.embedding(input, weight, max_norm=0.4)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[ 5.49015924e-02,  3.47811311e-01, -1.89771220e-01],</span>
<span class="sd">          [ 2.09307984e-01, -2.24846993e-02,  3.40124398e-01],</span>
<span class="sd">          [ 5.49015924e-02,  3.47811311e-01, -1.89771220e-01],</span>
<span class="sd">          [ 5.49015924e-02,  3.47811311e-01, -1.89771220e-01]],</span>
<span class="sd">         [[ 2.09307984e-01, -2.24846993e-02,  3.40124398e-01],</span>
<span class="sd">          [ 2.09307984e-01, -2.24846993e-02,  3.40124398e-01],</span>
<span class="sd">          [ 5.49015924e-02,  3.47811311e-01, -1.89771220e-01],</span>
<span class="sd">          [ 2.09307984e-01, -2.24846993e-02,  3.40124398e-01]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For Embedding, the weight must be a mindspore.Parameter, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">embedding_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">padding_idx</span><span class="p">,</span> <span class="n">max_norm</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">,</span> <span class="n">scale_grad_by_freq</span><span class="p">)</span></div>


<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;adaptive_avg_pool1d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;adaptive_avg_pool2d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;adaptive_avg_pool3d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;adaptive_max_pool1d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;adaptive_max_pool2d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;adaptive_max_pool3d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;avg_pool1d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;avg_pool2d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;avg_pool3d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;batch_norm&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bias_add&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bidense&#39;</span><span class="p">,</span>
    <span class="s1">&#39;binary_cross_entropy&#39;</span><span class="p">,</span>
    <span class="s1">&#39;binary_cross_entropy_with_logits&#39;</span><span class="p">,</span>
    <span class="s1">&#39;cosine_embedding_loss&#39;</span><span class="p">,</span>
    <span class="s1">&#39;max_pool2d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;max_pool3d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;kl_div&#39;</span><span class="p">,</span>
    <span class="s1">&#39;celu&#39;</span><span class="p">,</span>
    <span class="s1">&#39;dense&#39;</span><span class="p">,</span>
    <span class="s1">&#39;deformable_conv2d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;dropout1d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;dropout2d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;dropout3d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;embedding&#39;</span><span class="p">,</span>
    <span class="s1">&#39;fast_gelu&#39;</span><span class="p">,</span>
    <span class="s1">&#39;fractional_max_pool2d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;fractional_max_pool3d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;pixel_shuffle&#39;</span><span class="p">,</span>
    <span class="s1">&#39;pixel_unshuffle&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hardshrink&#39;</span><span class="p">,</span>
    <span class="s1">&#39;is_floating_point&#39;</span><span class="p">,</span>
    <span class="s1">&#39;flip&#39;</span><span class="p">,</span>
    <span class="s1">&#39;fliplr&#39;</span><span class="p">,</span>
    <span class="s1">&#39;flipud&#39;</span><span class="p">,</span>
    <span class="s1">&#39;intopk&#39;</span><span class="p">,</span>
    <span class="s1">&#39;interpolate&#39;</span><span class="p">,</span>
    <span class="s1">&#39;upsample&#39;</span><span class="p">,</span>
    <span class="s1">&#39;layer_norm&#39;</span><span class="p">,</span>
    <span class="s1">&#39;log_softmax&#39;</span><span class="p">,</span>
    <span class="s1">&#39;mish&#39;</span><span class="p">,</span>
    <span class="s1">&#39;lrn&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hardswish&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hardtanh&#39;</span><span class="p">,</span>
    <span class="s1">&#39;huber_loss&#39;</span><span class="p">,</span>
    <span class="s1">&#39;softsign&#39;</span><span class="p">,</span>
    <span class="s1">&#39;softshrink&#39;</span><span class="p">,</span>
    <span class="s1">&#39;soft_shrink&#39;</span><span class="p">,</span>
    <span class="s1">&#39;softplus&#39;</span><span class="p">,</span>
    <span class="s1">&#39;selu&#39;</span><span class="p">,</span>
    <span class="s1">&#39;silu&#39;</span><span class="p">,</span>
    <span class="s1">&#39;soft_margin_loss&#39;</span><span class="p">,</span>
    <span class="s1">&#39;softmax&#39;</span><span class="p">,</span>
    <span class="s1">&#39;softmin&#39;</span><span class="p">,</span>
    <span class="s1">&#39;pdist&#39;</span><span class="p">,</span>
    <span class="s1">&#39;pad&#39;</span><span class="p">,</span>
    <span class="s1">&#39;pad_ext&#39;</span><span class="p">,</span>
    <span class="s1">&#39;prelu&#39;</span><span class="p">,</span>
    <span class="s1">&#39;mirror_pad&#39;</span><span class="p">,</span>
    <span class="s1">&#39;cross_entropy&#39;</span><span class="p">,</span>
    <span class="s1">&#39;grid_sample&#39;</span><span class="p">,</span>
    <span class="s1">&#39;smooth_l1_loss&#39;</span><span class="p">,</span>
    <span class="s1">&#39;l1_loss&#39;</span><span class="p">,</span>
    <span class="s1">&#39;threshold&#39;</span><span class="p">,</span>
    <span class="s1">&#39;leaky_relu&#39;</span><span class="p">,</span>
    <span class="s1">&#39;nll_loss&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ctc_loss&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ctc_greedy_decoder&#39;</span><span class="p">,</span>
    <span class="s1">&#39;dropout&#39;</span><span class="p">,</span>
    <span class="s1">&#39;conv3d_transpose&#39;</span><span class="p">,</span>
    <span class="s1">&#39;conv1d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;conv2d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;conv_transpose2d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;sigmoid&#39;</span><span class="p">,</span>
    <span class="s1">&#39;logsigmoid&#39;</span><span class="p">,</span>
    <span class="s1">&#39;relu&#39;</span><span class="p">,</span>
    <span class="s1">&#39;relu6&#39;</span><span class="p">,</span>
    <span class="s1">&#39;rrelu&#39;</span><span class="p">,</span>
    <span class="s1">&#39;conv3d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;glu&#39;</span><span class="p">,</span>
    <span class="s1">&#39;margin_ranking_loss&#39;</span><span class="p">,</span>
    <span class="s1">&#39;multi_margin_loss&#39;</span><span class="p">,</span>
    <span class="s1">&#39;multilabel_margin_loss&#39;</span><span class="p">,</span>
    <span class="s1">&#39;multilabel_soft_margin_loss&#39;</span><span class="p">,</span>
    <span class="s1">&#39;elu&#39;</span><span class="p">,</span>
    <span class="s1">&#39;gelu&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hinge_embedding_loss&#39;</span><span class="p">,</span>
    <span class="s1">&#39;gaussian_nll_loss&#39;</span><span class="p">,</span>
    <span class="s1">&#39;lp_pool1d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;lp_pool2d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;max_unpool1d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;max_unpool2d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;max_unpool3d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;mse_loss&#39;</span><span class="p">,</span>
    <span class="s1">&#39;msort&#39;</span><span class="p">,</span>
    <span class="s1">&#39;triplet_margin_loss&#39;</span><span class="p">,</span>
    <span class="s1">&#39;channel_shuffle&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hardsigmoid&#39;</span><span class="p">,</span>
    <span class="s1">&#39;group_norm&#39;</span>
<span class="p">]</span>
<span class="n">__all__</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>