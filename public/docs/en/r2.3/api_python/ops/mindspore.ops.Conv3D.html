<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore.ops.Conv3D &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script><script src="../../_static/jquery.js"></script>
        <script src="../../_static/js/theme.js"></script><script src="../../_static/underscore.js"></script><script src="../../_static/doctools.js"></script><script src="../../_static/js/mermaid-9.3.0.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script><script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script><script async="async" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/mathjax/MathJax-3.2.2/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="mindspore.ops.Conv3DTranspose" href="mindspore.ops.Conv3DTranspose.html" />
    <link rel="prev" title="mindspore.ops.Conv2DTranspose" href="mindspore.ops.Conv2DTranspose.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../design/overview.html">MindSpore Design Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/programming_paradigm.html">Functional and Object-Oriented Fusion Programming Paradigm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/dynamic_graph_and_static_graph.html">Combination of Dynamic and Static Graphs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/distributed_training_design.html">Distributed Parallel Native</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/data_engine.html">High Performance Data Processing Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/all_scenarios.html">Full-scenarios Unified Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/graph_fusion_engine.html">Graph-Kernel Fusion Acceleration Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/pluggable_device.html">Third-Party Hardware Interconnection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../note/official_models.html">Official Models</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../mindspore.ops.primitive.html">mindspore.ops.primitive</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.primitive.html#operator-primitives">Operator Primitives</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../mindspore.ops.primitive.html#neural-network-layer-operators">Neural Network Layer Operators</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="../mindspore.ops.primitive.html#neural-network">Neural Network</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.AvgPool3D.html">mindspore.ops.AvgPool3D</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.Conv2D.html">mindspore.ops.Conv2D</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.Conv2DTranspose.html">mindspore.ops.Conv2DTranspose</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">mindspore.ops.Conv3D</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.Conv3DTranspose.html">mindspore.ops.Conv3DTranspose</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.CTCGreedyDecoder.html">mindspore.ops.CTCGreedyDecoder</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.Dense.html">mindspore.ops.Dense</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.Dropout.html">mindspore.ops.Dropout</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.Dropout2D.html">mindspore.ops.Dropout2D</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.Dropout3D.html">mindspore.ops.Dropout3D</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.DynamicGRUV2.html">mindspore.ops.DynamicGRUV2</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.DynamicRNN.html">mindspore.ops.DynamicRNN</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.Flatten.html">mindspore.ops.Flatten</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.FractionalMaxPool3DWithFixedKsize.html">mindspore.ops.FractionalMaxPool3DWithFixedKsize</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.LRN.html">mindspore.ops.LRN</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.LSTM.html">mindspore.ops.LSTM</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.MaxPool.html">mindspore.ops.MaxPool</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.MaxPool3D.html">mindspore.ops.MaxPool3D</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.MaxPool3DWithArgmax.html">mindspore.ops.MaxPool3DWithArgmax</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.MaxPoolWithArgmaxV2.html">mindspore.ops.MaxPoolWithArgmaxV2</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.MaxUnpool2D.html">mindspore.ops.MaxUnpool2D</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.MaxUnpool3D.html">mindspore.ops.MaxUnpool3D</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.MirrorPad.html">mindspore.ops.MirrorPad</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.Pad.html">mindspore.ops.Pad</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.EmbeddingLookup.html">mindspore.ops.EmbeddingLookup</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.Padding.html">mindspore.ops.Padding</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ResizeBilinear.html">mindspore.ops.ResizeBilinear</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../mindspore.ops.primitive.html#loss-function">Loss Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mindspore.ops.primitive.html#activation-function">Activation Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mindspore.ops.primitive.html#optimizer">Optimizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mindspore.ops.primitive.html#distance-function">Distance Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mindspore.ops.primitive.html#sampling-operator">Sampling Operator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mindspore.ops.primitive.html#image-processing">Image Processing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mindspore.ops.primitive.html#text-processing">Text Processing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.primitive.html#mathematical-operators">Mathematical Operators</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.primitive.html#tensor-operation-operator">Tensor Operation Operator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.primitive.html#parameter-operation-operator">Parameter Operation Operator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.primitive.html#data-operation-operator">Data Operation Operator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.primitive.html#communication-operator">Communication Operator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.primitive.html#debugging-operator">Debugging Operator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.primitive.html#sparse-operator">Sparse Operator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.primitive.html#frame-operators">Frame Operators</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.primitive.html#operator-information-registration">Operator Information Registration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.primitive.html#customizing-operator">Customizing Operator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.primitive.html#spectral-operator">Spectral Operator</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.experimental.html">mindspore.experimental</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Mapping</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../note/api_mapping/pytorch_api_mapping.html">PyTorch and MindSpore API Mapping Table</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Migration Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/overview.html">Overview of Migration Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/enveriment_preparation.html">Environment Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/analysis_and_preparation.html">Model Analysis and Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/model_development/model_development.html">Network Constructing Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/debug_and_tune.html">Debugging and Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/sample_code.html">Network Migration Debugging Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/faq.html">FAQs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Syntax Support</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../note/static_graph_syntax_support.html">Static Graph Syntax Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/static_graph_syntax/operators.html">Static Graph Syntax - Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/static_graph_syntax/statements.html">Static Graph Syntax - Python Statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/static_graph_syntax/python_builtin_functions.html">Static Graph Syntax - Python Built-in Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/index_support.html">Tensor Index Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Environment Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../note/env_var_list.html">Environment Variables</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../faq/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/data_processing.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/implement_problem.html">Implement Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/network_compilation.html">Network Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/operators_compile.html">Operators Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/usage_migrate_3rd.html">Migration from a Third-party Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/precision_tuning.html">Precision Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/distributed_parallel.html">Distributed Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/feature_advice.html">Feature Advice</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../mindspore.ops.primitive.html">mindspore.ops.primitive</a> &raquo;</li>
      <li>mindspore.ops.Conv3D</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/api_python/ops/mindspore.ops.Conv3D.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="mindspore-ops-conv3d">
<h1>mindspore.ops.Conv3D<a class="headerlink" href="#mindspore-ops-conv3d" title="Permalink to this headline"></a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Conv3D">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Conv3D</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">out_channel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'valid'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'NCDHW'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/mindspore/ops/operations/nn_ops.html#Conv3D"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Conv3D" title="Permalink to this definition"></a></dt>
<dd><p>3D convolution layer.</p>
<p>Applies a 3D convolution over an input tensor which is typically of shape
<span class="math notranslate nohighlight">\((N, C_{in}, D_{in}, H_{in}, W_{in})\)</span>,
where <span class="math notranslate nohighlight">\(N\)</span> is batch size, <span class="math notranslate nohighlight">\(C\)</span> is channel number, <span class="math notranslate nohighlight">\(D\)</span> is feature depth,
<span class="math notranslate nohighlight">\(H\)</span> is feature height, <span class="math notranslate nohighlight">\(W\)</span> is feature width.</p>
<p>The output is calculated based on formula:</p>
<div class="math notranslate nohighlight">
\[\text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) +
\sum_{k = 0}^{C_{in} - 1} \text{ccor}({\text{weight}(C_{\text{out}_j}, k), \text{X}(N_i, k)})\]</div>
<p>where <span class="math notranslate nohighlight">\(bias\)</span> is the output channel bias, <span class="math notranslate nohighlight">\(ccor\)</span> is
the <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>,
, <span class="math notranslate nohighlight">\(weight\)</span> is the convolution kernel value and <span class="math notranslate nohighlight">\(X\)</span> represents the input feature map.</p>
<p>Here are the indices’ meanings:
- <span class="math notranslate nohighlight">\(i\)</span> corresponds to the batch number, ranging from 0 to N-1, where N is the batch size of the input.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(j\)</span> corresponds to the output channel, ranging from 0 to C_{out}-1, where C_{out} is the number of
output channels, which is also equal to the number of kernels.</p></li>
<li><p><span class="math notranslate nohighlight">\(k\)</span> corresponds to the input channel, ranging from 0 to C_{in}-1, where C_{in} is the number of
input channels, which is also equal to the number of channels in the convolutional kernels.</p></li>
</ul>
<p>Therefore, in the above formula, <span class="math notranslate nohighlight">\({bias}(C_{out_j})\)</span> represents the bias of the <span class="math notranslate nohighlight">\(j\)</span>-th
output channel, <span class="math notranslate nohighlight">\({weight}(C_{out_j}, k)\)</span> represents the slice of the <span class="math notranslate nohighlight">\(j\)</span>-th convolutional
kernel in the <span class="math notranslate nohighlight">\(k\)</span>-th channel, and <span class="math notranslate nohighlight">\({X}(N_i, k)\)</span> represents the slice of the <span class="math notranslate nohighlight">\(k\)</span>-th input
channel in the <span class="math notranslate nohighlight">\(i\)</span>-th batch of the input feature map.</p>
<p>The shape of the convolutional kernel is given by
<span class="math notranslate nohighlight">\((\text{kernel_size[0]}, \text{kernel_size[1]}, \text{kernel_size[2]})\)</span>
where <span class="math notranslate nohighlight">\(kernel\_size[0]\)</span> , <span class="math notranslate nohighlight">\(kernel\_size[1]\)</span> and <span class="math notranslate nohighlight">\(kernel\_size[2]\)</span> are the depth,
height and width of the kernel, respectively.
If we consider the input and output channels as well as the <cite>group</cite> parameter, the complete kernel shape
will be <span class="math notranslate nohighlight">\((C_{out}, C_{in} / \text{group}, \text{kernel_size[0]},
\text{kernel_size[1]}, \text{kernel_size[2]})\)</span>,
where <cite>group</cite> is the number of groups dividing <cite>x</cite>’s input channel when applying group convolution.</p>
<p>For more details about convolution layer, please refer to <a class="reference external" href="http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf">Gradient Based Learning Applied to Document Recognition</a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ol class="arabic simple">
<li><p>On Ascend platform, <cite>groups = 1</cite> must be satisfied.</p></li>
<li><p>On Ascend <cite>dilation</cite> on depth only supports the case of 1.</p></li>
</ol>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>out_channel</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Specifies output channel <span class="math notranslate nohighlight">\(C_{out}\)</span>.</p></li>
<li><p><strong>kernel_size</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>]</em>) – Specifies the depth, height and width of the 3D convolution kernel.
It can be a single int or a tuple of 3 integers. A single int means the value is for depth, height
and the width. A tuple of 3 ints means the first value is for depth and
the rest is for the height and width.</p></li>
<li><p><strong>mode</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – Modes for different convolutions. It is currently not used. Default: <code class="docutils literal notranslate"><span class="pre">1</span></code> .</p></li>
<li><p><strong>stride</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>]</em><em>, </em><em>optional</em>) – The distance of kernel moving, it can be an int number
that represents the depth, height and width of movement or a tuple of three int numbers that
represent depth, height and width movement respectively. Default: <code class="docutils literal notranslate"><span class="pre">1</span></code> .</p></li>
<li><p><strong>pad_mode</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><em>optional</em>) – <p>Specifies the padding mode with a padding value of 0. It can be set to:
<code class="docutils literal notranslate"><span class="pre">&quot;same&quot;</span></code> , <code class="docutils literal notranslate"><span class="pre">&quot;valid&quot;</span></code> or <code class="docutils literal notranslate"><span class="pre">&quot;pad&quot;</span></code> . Default: <code class="docutils literal notranslate"><span class="pre">&quot;valid&quot;</span></code> .</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;same&quot;</span></code>: Pad the input around its depth/height/width dimension so that the shape of input and output
are the same when <cite>stride</cite> is set to <code class="docutils literal notranslate"><span class="pre">1</span></code>.
The amount of padding to is calculated by the operator internally.  If the amount is even,
it isuniformly distributed around the input, if it is odd, the excess amount goes
to the front/right/bottom side.
If this mode is set, <cite>pad</cite> must be 0.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;valid&quot;</span></code>: No padding is applied to the input, and the output returns the maximum
possible depth, height and width. Extra pixels that could not complete a full stride will
be discarded. If this mode is set, <cite>pad</cite> must be 0.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;pad&quot;</span></code>: Pad the input with a specified amount. In this mode, the amount of padding
in the depth, height and width dimension is determined by the <cite>pad</cite> parameter.
If this mode is set, <cite>pad</cite> must be greater than or equal to 0.</p></li>
</ul>
</p></li>
<li><p><strong>pad</strong> (<em>Union</em><em>(</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>)</em><em>, </em><em>optional</em>) – Specifies the amount of padding to apply on input
when <cite>pad_mode</cite> is set to <code class="docutils literal notranslate"><span class="pre">&quot;pad&quot;</span></code>. It can be a single int or a tuple of 6 ints.
If <cite>pad</cite> is one integer, the paddings of head, tail, top, bottom,
left and right are the same, equal to <cite>pad</cite>. If <cite>pad</cite> is a tuple with 6 integers, the
paddings of head, tail, top, bottom, left and right is equal to pad[0],
pad[1], pad[2], pad[3], pad[4] and pad[5] accordingly. Default: <code class="docutils literal notranslate"><span class="pre">0</span></code> .</p></li>
<li><p><strong>dilation</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>]</em><em>, </em><em>optional</em>) – Specifies the dilation rate to use for dilated convolution.
It can be a single int or a tuple of 3 integers. A single int means the dilation size is the same
in the depth, height and width directions. A tuple of 3 ints represents the dilation size in
the depth, height and width directions, respectively.
Assuming <span class="math notranslate nohighlight">\(dilation=(d0, d1, d2)\)</span>, the convolutional kernel samples the input with a
spacing of <span class="math notranslate nohighlight">\(d0-1\)</span> elements in the depth direction,
<span class="math notranslate nohighlight">\(d1-1\)</span> elements in the height direction, <span class="math notranslate nohighlight">\(d2-1\)</span> elements in the
width direction respectively. The values in the depth, height and width dimensions are in the
ranges [1, D], [1, H] and [1, W], respectively.
Default: <code class="docutils literal notranslate"><span class="pre">1</span></code> .</p></li>
<li><p><strong>group</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – The number of groups into which the filter is divided. <cite>in_channels</cite>
and <cite>out_channels</cite> must be divisible by <cite>group</cite>. Default: <code class="docutils literal notranslate"><span class="pre">1</span></code> .</p></li>
<li><p><strong>data_format</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><em>optional</em>) – The optional value for data format. Currently only support <code class="docutils literal notranslate"><span class="pre">&quot;NCDHW&quot;</span></code> .</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((N, C_{in}, D_{in}, H_{in}, W_{in})\)</span>.
Currently input data type only support float16 and float32.</p></li>
<li><p><strong>weight</strong> (Tensor) - Set size of kernel is <span class="math notranslate nohighlight">\((k_d, K_h, K_w)\)</span>, then the shape is
<span class="math notranslate nohighlight">\((C_{out}, C_{in}/groups, k_d, K_h, K_w)\)</span>.
Currently weight data type only support float16 and float32.</p></li>
<li><p><strong>bias</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((C_{out})\)</span>. When bias is None, zeros will be used.
Default: <code class="docutils literal notranslate"><span class="pre">None</span></code> .</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the value that applied 3D convolution. The shape is <span class="math notranslate nohighlight">\((N, C_{out}, D_{out}, H_{out}, W_{out})\)</span>.</p>
<p><cite>pad_mode</cite> is <code class="docutils literal notranslate"><span class="pre">&quot;same&quot;</span></code>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ll} \\
    D_{out} = \left \lceil{\frac{D_{in}}{\text{stride[0]}}} \right \rceil \\
    H_{out} = \left \lceil{\frac{H_{in}}{\text{stride[1]}}} \right \rceil \\
    W_{out} = \left \lceil{\frac{W_{in}}{\text{stride[2]}}} \right \rceil \\
\end{array}\end{split}\]</div>
<p><cite>pad_mode</cite> is <code class="docutils literal notranslate"><span class="pre">&quot;valid&quot;</span></code>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ll} \\
    D_{out} = \left \lfloor{\frac{D_{in} - \text{dilation[0]} \times (\text{kernel_size[0]} - 1) }
    {\text{stride[0]}} + 1} \right \rfloor \\
    H_{out} = \left \lfloor{\frac{H_{in} - \text{dilation[1]} \times (\text{kernel_size[1]} - 1) }
    {\text{stride[1]}} + 1} \right \rfloor \\
    W_{out} = \left \lfloor{\frac{W_{in} - \text{dilation[2]} \times (\text{kernel_size[2]} - 1) }
    {\text{stride[2]}} + 1} \right \rfloor \\
\end{array}\end{split}\]</div>
<p><cite>pad_mode</cite> is <code class="docutils literal notranslate"><span class="pre">&quot;pad&quot;</span></code>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ll} \\
    D_{out} = \left \lfloor{\frac{D_{in} + pad[0] + pad[1] - (\text{dilation[0]} - 1) \times
    \text{kernel_size[0]} - 1 }{\text{stride[0]}} + 1} \right \rfloor \\
    H_{out} = \left \lfloor{\frac{H_{in} + pad[2] + pad[3] - (\text{dilation[1]} - 1) \times
    \text{kernel_size[1]} - 1 }{\text{stride[1]}} + 1} \right \rfloor \\
    W_{out} = \left \lfloor{\frac{W_{in} + pad[4] + pad[5] - (\text{dilation[2]} - 1) \times
    \text{kernel_size[2]} - 1 }{\text{stride[2]}} + 1} \right \rfloor \\
\end{array}\end{split}\]</div>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>out_channel</cite> or <cite>group</cite> is not an int.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>kernel_size</cite>, <cite>stride</cite>, <cite>pad</cite> or <cite>dilation</cite> is neither an int nor a tuple.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If <cite>out_channel</cite>, <cite>kernel_size</cite>, <cite>stride</cite> or <cite>dilation</cite> is less than 1.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If <cite>pad</cite> is less than 0.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If <cite>pad_mode</cite> is not one of ‘same’, ‘valid’ or ‘pad’.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If <cite>pad</cite> is a tuple whose length is not equal to 6.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If <cite>pad_mode</cite> is not equal to ‘pad’ and <cite>pad</cite> is not equal to (0, 0, 0, 0, 0, 0).</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If <cite>data_format</cite> is not ‘NCDHW’.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">ops</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># case 1: specify kernel_size with tuple, all parameters use default values.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">conv3d</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Conv3D</span><span class="p">(</span><span class="n">out_channel</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">conv3d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(16, 32, 7, 30, 30)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># case 2: specify kernel_size with int, all parameters use default values.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">40</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">conv3d</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Conv3D</span><span class="p">(</span><span class="n">out_channel</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">conv3d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(10, 40, 30, 30, 30)</span>
<span class="go"> &gt;&gt;&gt; # case 3: stride=(1, 2, 3), other parameters being default.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">40</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">conv3d</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Conv3D</span><span class="p">(</span><span class="n">out_channel</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">conv3d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(10, 40, 30, 15, 10)</span>
<span class="go"> &gt;&gt;&gt; # case 4: pad_mode=&quot;pad&quot;, other parameters being default.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">40</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">conv3d</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Conv3D</span><span class="p">(</span><span class="n">out_channel</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;pad&quot;</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">conv3d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(10, 40, 34, 34, 34)</span>
<span class="go"> &gt;&gt;&gt; # case 5: dilation=(1, 1, 1), other parameters being default.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">40</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">conv3d</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Conv3D</span><span class="p">(</span><span class="n">out_channel</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">conv3d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(10, 40, 30, 30, 30)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># case 6: group=1, other parameters being default.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">40</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">conv3d</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Conv3D</span><span class="p">(</span><span class="n">out_channel</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">conv3d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(10, 40, 30, 30, 30)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># case 7: All parameters are specified.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">40</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">conv3d</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Conv3D</span><span class="p">(</span><span class="n">out_channel</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;pad&quot;</span><span class="p">,</span>
<span class="gp">... </span>                    <span class="n">pad</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">conv3d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(10, 40, 34, 17, 12)</span>
</pre></div>
</div>
</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="mindspore.ops.Conv2DTranspose.html" class="btn btn-neutral float-left" title="mindspore.ops.Conv2DTranspose" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="mindspore.ops.Conv3DTranspose.html" class="btn btn-neutral float-right" title="mindspore.ops.Conv3DTranspose" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>