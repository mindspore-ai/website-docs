<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Functional and Object-Oriented Fusion Programming Paradigm &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script src="../_static/js/mermaid-9.3.0.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script><script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script><script async="async" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/mathjax/MathJax-3.2.2/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Combination of Dynamic and Static Graphs" href="dynamic_graph_and_static_graph.html" />
    <link rel="prev" title="MindSpore Design Overview" href="overview.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Design</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">MindSpore Design Overview</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Functional and Object-Oriented Fusion Programming Paradigm</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#object-oriented-programming">Object-oriented Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="#functional-programming">Functional Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="#functional-differential-programming">Functional Differential Programming</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#automatic-differentiation">Automatic Differentiation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#forward-mode-ad">Forward Mode AD</a></li>
<li class="toctree-l4"><a class="reference internal" href="#reverse-mode-ad">Reverse Mode AD</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#grad"><code class="docutils literal notranslate"><span class="pre">grad</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#grad-design"><code class="docutils literal notranslate"><span class="pre">grad</span></code> Design</a></li>
<li class="toctree-l4"><a class="reference internal" href="#grad-implementation"><code class="docutils literal notranslate"><span class="pre">grad</span></code> Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#grad-example"><code class="docutils literal notranslate"><span class="pre">grad</span></code> Example</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#forward-automatic-differentiation-implementation">Forward Automatic Differentiation Implementation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#functional-+-object-oriented-fusion-programming">Functional + Object-Oriented Fusion Programming</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="dynamic_graph_and_static_graph.html">Combination of Dynamic and Static Graphs</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_training_design.html">Distributed Parallel Native</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_engine.html">High Performance Data Processing Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="all_scenarios.html">Full-scenarios Unified Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="graph_fusion_engine.html">Graph-Kernel Fusion Acceleration Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="pluggable_device.html">Third-Party Hardware Interconnection</a></li>
<li class="toctree-l1"><a class="reference internal" href="glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/official_models.html">Official Models</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.experimental.html">mindspore.experimental</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Mapping</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/pytorch_api_mapping.html">PyTorch and MindSpore API Mapping Table</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Migration Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/overview.html">Overview of Migration Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/enveriment_preparation.html">Environment Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/analysis_and_preparation.html">Model Analysis and Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/model_development/model_development.html">Network Constructing Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/debug_and_tune.html">Debugging and Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/sample_code.html">Network Migration Debugging Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/faq.html">FAQs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Syntax Support</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax_support.html">Static Graph Syntax Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax/operators.html">Static Graph Syntax - Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax/statements.html">Static Graph Syntax - Python Statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax/python_builtin_functions.html">Static Graph Syntax - Python Built-in Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/index_support.html">Tensor Index Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Environment Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/env_var_list.html">Environment Variables</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/data_processing.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/implement_problem.html">Implement Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/network_compilation.html">Network Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/operators_compile.html">Operators Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/usage_migrate_3rd.html">Migration from a Third-party Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/precision_tuning.html">Precision Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/distributed_parallel.html">Distributed Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/feature_advice.html">Feature Advice</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Functional and Object-Oriented Fusion Programming Paradigm</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/design/programming_paradigm.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section class="tex2jax_ignore mathjax_ignore" id="functional-and-object-oriented-fusion-programming-paradigm">
<h1>Functional and Object-Oriented Fusion Programming Paradigm<a class="headerlink" href="#functional-and-object-oriented-fusion-programming-paradigm" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.3/docs/mindspore/source_en/design/programming_paradigm.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/resource/_static/logo_source_en.svg" /></a></p>
<p>Programming paradigm refers to the programming style or programming approach of a programming language. Typically, AI frameworks rely on the programming paradigm of the programming language used by the front-end programming interface for the construction and training of neural networks. MindSpore, as an AI+scientific computing convergence computing framework, provides object-oriented programming and functional programming support for AI and scientific computing scenarios, respectively. At the same time, in order to enhance the flexibility and ease of use of the framework, a functional + object-oriented fusion programming paradigm is proposed, which effectively reflects the advantages of functional automatic differentiation mechanism.</p>
<p>The following describes each of the three types of programming paradigms supported by MindSpore and their simple examples.</p>
<section id="object-oriented-programming">
<h2>Object-oriented Programming<a class="headerlink" href="#object-oriented-programming" title="Permalink to this headline"></a></h2>
<p>Object-oriented programming (OOP) is a programming method that decomposes programs into modules (classes) that encapsulate data and related operations, with objects being instances of classes. Object-oriented programming uses objects as the basic unit of a program, encapsulating the program and data in order to improve the reusability, flexibility and extensibility of the software, and the program in the object can access and often modify the data associated with the object.</p>
<p>In a general programming scenario, code and data are the two core components. Object-oriented programming is to design data structures for specific objects to define classes (Class). The class usually consists of the following two parts, corresponding to code and data, respectively:</p>
<ul class="simple">
<li><p>Methods</p></li>
<li><p>Attributes</p></li>
</ul>
<p>For different objects obtained after the instantiation of the same Class, the methods and attributes are the same, but the difference is the values of the attributes. The different attribute values determine the internal state of the object, so OOP can be good for state management.</p>
<p>The following is an example of a simple class constructed in Python:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Sample</span><span class="p">:</span> <span class="c1">#class declaration</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span> <span class="c1"># class constructor (code)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span> <span class="c1"># attribute (data)</span>

    <span class="k">def</span> <span class="nf">set_name</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span> <span class="c1"># method declaration (code)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span> <span class="c1"># method implementation (code)</span>
</pre></div>
</div>
<p>For constructing a neural network, the primary component is the network layer (Layer), and a neural network layer contains the following components:</p>
<ul class="simple">
<li><p>Tensor Operation</p></li>
<li><p>Weights</p></li>
</ul>
<p>These two correspond exactly to the Methods and Attributes of the class, and the weights themselves are the internal states of the neural network layer, so using the class to construct Layers naturally fits its definition. In addition, we wish to use the neural network layer for stacking and construct deep neural networks when programming, and new Layer classes can be easily constructed by combining Layer objects using OOP programming. In addition, we wish to use neural network layers for stacking and constructing deep neural networks when programming, and new Layer classes can be easily constructed by combining Layer objects by using OOP programming.</p>
<p>The following is an example of a neural network class constructed by using MindSpore:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">Parameter</span>
<span class="kn">from</span> <span class="nn">mindspore.common.initializer</span> <span class="kn">import</span> <span class="n">initializer</span>

<span class="k">class</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">has_bias</span><span class="p">):</span> <span class="c1"># class constructor (code)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="s1">&#39;normal&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">out_features</span><span class="p">,</span> <span class="n">in_features</span><span class="p">],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="s1">&#39;weight&#39;</span><span class="p">)</span> <span class="c1"># layer weight (data)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">out_features</span><span class="p">],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="s1">&#39;bias&#39;</span><span class="p">)</span> <span class="c1"># layer weight (data)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span> <span class="c1"># method declaration (code)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="c1"># tensor transformation (code)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="c1"># tensor transformation (code)</span>
        <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
<p>In addition to the construction of the neural network layer by using the object-oriented programming paradigm, MindSpore supports pure object-oriented programming to construct the neural network training logic, where the forward computation, back propagation, gradient optimization and other operations of the neural network are constructed by using classes. The following is an example of pure object-oriented programming.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">value_and_grad</span>

<span class="k">class</span> <span class="nc">TrainOneStepCell</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">network</span> <span class="o">=</span> <span class="n">network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad_fn</span> <span class="o">=</span> <span class="n">value_and_grad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">parameters</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
        <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>

<span class="n">network</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">()</span>
<span class="n">network_with_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">WithLossCell</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="mf">0.001</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">TrainOneStepCell</span><span class="p">(</span><span class="n">network_with_loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
</pre></div>
</div>
<p>At this point, both the neural network and its training process are managed by using classes that inherit from <code class="docutils literal notranslate"><span class="pre">nn.Cell</span></code>, which can be easily compiled and accelerated as a computational graph.</p>
</section>
<section id="functional-programming">
<h2>Functional Programming<a class="headerlink" href="#functional-programming" title="Permalink to this headline"></a></h2>
<p>Functional programming is a programming paradigm that treats computer operations as functions and avoids the use of program state and mutable objects.</p>
<p>In the functional programming, functions are treated as first-class citizens, which means they can be bound to names (including local identifiers), passed as arguments, and returned from other functions, just like any other data type. This allows programs to be written in a declarative and composable style, where small functions are combined in a modular fashion. Functional programming is sometimes seen as synonymous with pure functional programming, a subset of functional programming that treats all functions as deterministic mathematical functions or pure functions. When a pure function is called with some given parameters, it will always return the same result and is not affected by any mutable state or other side effects.</p>
<p>The functional programming has two core features that make it well suited to the needs of scientific computing:</p>
<ol class="arabic simple">
<li><p>The programming function semantics are exactly equivalent to the mathematical function semantics.</p></li>
<li><p>Determinism, if the same input is given, the same output is returned. No side effects.</p></li>
</ol>
<p>Due to this feature of determinism, by limiting side effects, programs can have fewer errors, are easier to debug and test, and are more suitable for formal verification.</p>
<p>MindSpore provides pure functional programming support. With the numerical computation interfaces provided by <code class="docutils literal notranslate"><span class="pre">mindspore.numpy</span></code> and <code class="docutils literal notranslate"><span class="pre">mindspore.scipy</span></code>, you can easily program scientific computations. The following is an example of using functional programming:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.numpy</span> <span class="k">as</span> <span class="nn">mnp</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">grad</span>

<span class="n">grad_tanh</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">mnp</span><span class="o">.</span><span class="n">tanh</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">grad_tanh</span><span class="p">(</span><span class="mf">2.0</span><span class="p">))</span>
<span class="c1"># 0.070650816</span>

<span class="nb">print</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">mnp</span><span class="o">.</span><span class="n">tanh</span><span class="p">))(</span><span class="mf">2.0</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">mnp</span><span class="o">.</span><span class="n">tanh</span><span class="p">)))(</span><span class="mf">2.0</span><span class="p">))</span>
<span class="c1"># -0.13621868</span>
<span class="c1"># 0.25265405</span>
</pre></div>
</div>
<p>In line with the needs of the functional programming paradigm, MindSpore provides a variety of functional transformation interfaces, including automatic differentiation, automatic vectorization, automatic parallelism, just-in-time compilation, data sinking and other functional modules, which are briefly described below:</p>
<ul class="simple">
<li><p>Automatic differentiation: <code class="docutils literal notranslate"><span class="pre">grad</span></code>, <code class="docutils literal notranslate"><span class="pre">value_and_grad</span></code>, providing differential function transformation.</p></li>
<li><p>Automatic vectorization: A higher-order function for mapping a function fn along the parameter axis.</p></li>
<li><p>Automatic parallelism: <code class="docutils literal notranslate"><span class="pre">shard</span></code>, a functional operator slice, specifying the distribution strategy of the function input/output Tensor.</p></li>
<li><p>Just-in-time compilation: <code class="docutils literal notranslate"><span class="pre">jit</span></code>, which compiles a Python function into a callable MindSpore graph.</p></li>
<li><p>Data sinking: <code class="docutils literal notranslate"><span class="pre">data_sink</span></code>, transform the input function to obtain a function that can use the data sink pattern.</p></li>
</ul>
<p>Based on the above function transformation interfaces, function transformations can be used quickly and efficiently to implement complex functions when using the functional programming paradigm.</p>
</section>
<section id="functional-differential-programming">
<h2>Functional Differential Programming<a class="headerlink" href="#functional-differential-programming" title="Permalink to this headline"></a></h2>
<section id="automatic-differentiation">
<h3>Automatic Differentiation<a class="headerlink" href="#automatic-differentiation" title="Permalink to this headline"></a></h3>
<p>Modern AI algorithm, such as deep learning, uses a large amount of data to learn and fit an optimized model with parameters. This training process often uses loss back-propagation to update parameters. <strong>Automatic differentiation (AD)</strong> is one of the key techniques.</p>
<p>Automatic differentiation is a derivation method between neumerical differentiation and symbolic differentiation. The key concept of AD is to divide the calculation of the computer program into a finite set with basic operations. The derivations of all the basic operations are known. After calculating the derivation of all the basic operations, AD uses chain rule to combine them and gets the final gradient.</p>
<p>The formula of chain rule is:</p>
<div class="math notranslate nohighlight">
\[
(f\circ g)^{'}(x)=f^{'}(g(x))g^{'}(x) \tag{1}
\]</div>
<p>Based on how to connect the gradient of basic components, AD can be divided into <strong>forward mode AD</strong> and <strong>reverse mode AD</strong>.</p>
<ul class="simple">
<li><p>Forward Automatic Differentiation (also known as tangent linear mode AD) or forward cumulative gradient (forward mode).</p></li>
<li><p>Reverse Automatic Differentiation (also known as adjoint mode AD) or reverse cumulative gradient (reverse mode).</p></li>
</ul>
<p>Let’s take formula (2) as an example to introduce the specific calculation method of forward and reverse differentiation:</p>
<div class="math notranslate nohighlight">
\[
y=f(x_{1},x_{2})=ln(x_{1})+x_{1}x_{2}-sin(x_{2}) \tag{2}
\]</div>
<p>When we use the forward automatic differentiation formula (2) at <span class="math notranslate nohighlight">\(x_{1}=2, x_{2}=5\)</span>, <span class="math notranslate nohighlight">\(frac{partial y}{partial x_{1}}\)</span>, the direction of derivation of forward automatic differentiation is consistent with the evaluation direction of the original function, and the original function result and the differential result can be obtained at the same time.</p>
<p><img alt="image" src="../_images/forward_ad.png" /></p>
<p>When using reverse automatic differentiation, the direction of differentiation of the reverse automatic differentiation is opposite to the evaluation direction of the original function, and the differential result depends on the running result of the original function.</p>
<p><img alt="image" src="../_images/backward_ad.png" /></p>
<p>MindSpore first developed automatic differentiation based on the reverse pattern, and implemented forward differentiation on the basis of this method.</p>
<p>In order to explain the differences between forward mode AD and reverse mode AD in further, we generalize the derived function to F, which has an N input and an M output:</p>
<div class="math notranslate nohighlight">
\[
(Y_{1},Y_{2},...,Y_{M})=F(X_{1},X_{2},...,X_{N}) \tag{3}
\]</div>
<p>The gradient of function <span class="math notranslate nohighlight">\(F()\)</span> is a Jacobian matrix.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\left[
 \begin{matrix}
   \frac{\partial Y_{1}}{\partial X_{1}}&amp; ... &amp; \frac{\partial Y_{1}}{\partial X_{N}} \\
   ... &amp; ... &amp; ... \\
   \frac{\partial Y_{M}}{\partial X_{1}} &amp; ... &amp; \frac{\partial Y_{M}}{\partial X_{N}}
  \end{matrix}
  \right]
\end{split}\tag{4}
\]</div>
<section id="forward-mode-ad">
<h4>Forward Mode AD<a class="headerlink" href="#forward-mode-ad" title="Permalink to this headline"></a></h4>
<p>In forward mode AD, the calculation of gradient starts from inputs. So, for each calculation, we can get the gradient of outputs with respect to one input, which is one column of the Jacobian matrix.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\left[
 \begin{matrix}
   \frac{\partial Y_{1}}{\partial X_{1}}\\
   ...  \\
   \frac{\partial Y_{M}}{\partial X_{1}}
  \end{matrix}
  \right]
\end{split}\tag{5}
\]</div>
<p>In order to get this value, AD divies the program into a series of basic operations. The gradient rules of these basic operations is known. The basic operation can also be represented as a function <span class="math notranslate nohighlight">\(f\)</span> with <span class="math notranslate nohighlight">\(n\)</span> inputs and <span class="math notranslate nohighlight">\(m\)</span> outputs:</p>
<div class="math notranslate nohighlight">
\[
(y_{1},y_{2},...,y_{m})=f(x_{1},x_{2},...,x_{n}) \tag{6}
\]</div>
<p>Since we have defined the gradient rule of <span class="math notranslate nohighlight">\(f\)</span>, we know the jacobian matrix of <span class="math notranslate nohighlight">\(f\)</span>. So we can calculate the Jacobian-vector-product (Jvp) and use the chain rule to get the gradient outoput.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\left[
 \begin{matrix}
   \frac{\partial y_{1}}{\partial X_{i}}\\
   ...  \\
   \frac{\partial y_{m}}{\partial X_{i}}
  \end{matrix}
  \right]=\left[
 \begin{matrix}
   \frac{\partial y_{1}}{\partial x_{1}}&amp; ... &amp; \frac{\partial y_{1}}{\partial x_{n}} \\
   ... &amp; ... &amp; ... \\
   \frac{\partial y_{m}}{\partial x_{1}} &amp; ... &amp; \frac{\partial y_{M}}{\partial x_{n}}
  \end{matrix}
  \right]\left[
 \begin{matrix}
   \frac{\partial x_{1}}{\partial X_{i}}\\
   ...  \\
   \frac{\partial x_{n}}{\partial X_{i}}
  \end{matrix}
  \right]
\end{split}\tag{7}
\]</div>
</section>
<section id="reverse-mode-ad">
<h4>Reverse Mode AD<a class="headerlink" href="#reverse-mode-ad" title="Permalink to this headline"></a></h4>
<p>In reverse mode AD, the calculation of gradient starts from outputs. So, for each calculation, we can get the gradient of one output with respect to inputs, which is one row of the Jacobian matrix.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\left[
 \begin{matrix}
   \frac{\partial Y_{1}}{\partial X_{1}}&amp; ... &amp; \frac{\partial Y_{1}}{\partial X_{N}} \\
  \end{matrix}
  \right]
\end{split}\tag{8}
\]</div>
<p>In order to get this value, AD divies the program into a series of basic operations. The gradient rules of these basic operations is known. The basic operation can also be represented as a function <span class="math notranslate nohighlight">\(f\)</span> with n inputs and m outputs:</p>
<div class="math notranslate nohighlight">
\[
(y_{1},y_{2},...,y_{m})=f(x_{1},x_{2},...,x_{n}) \tag{9}
\]</div>
<p>Since we have defined the gradient rule of <span class="math notranslate nohighlight">\(f\)</span>, we know the jacobian matrix of <span class="math notranslate nohighlight">\(f\)</span>. So we can calculate the Vector-Jacobian-product (Vjp) and use the chain rule to get the gradient outoput.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\left[
 \begin{matrix}
   \frac{\partial Y_{j}}{\partial x_{1}}&amp; ... &amp; \frac{\partial Y_{j}}{\partial x_{N}} \\
  \end{matrix}
  \right]=\left[
 \begin{matrix}
   \frac{\partial Y_{j}}{\partial y_{1}}&amp; ... &amp; \frac{\partial Y_{j}}{\partial y_{m}} \\
  \end{matrix}
  \right]\left[
 \begin{matrix}
   \frac{\partial y_{1}}{\partial x_{1}}&amp; ... &amp; \frac{\partial y_{1}}{\partial x_{n}} \\
   ... &amp; ... &amp; ... \\
   \frac{\partial y_{m}}{\partial x_{1}} &amp; ... &amp; \frac{\partial y_{m}}{\partial x_{n}}
  \end{matrix}
  \right]
\end{split}\tag{10}
\]</div>
</section>
</section>
<section id="grad">
<h3><code class="docutils literal notranslate"><span class="pre">grad</span></code><a class="headerlink" href="#grad" title="Permalink to this headline"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">grad</span></code> uses reverse mode AD, which calcultes gradients from network outputs.</p>
<section id="grad-design">
<h4><code class="docutils literal notranslate"><span class="pre">grad</span></code> Design<a class="headerlink" href="#grad-design" title="Permalink to this headline"></a></h4>
<p>Consuming that the origin function of defining model is as follows:</p>
<div class="math notranslate nohighlight">
\[
f(g(x, y, z)) \tag{11}
\]</div>
<p>Then the gradient of <span class="math notranslate nohighlight">\(f()\)</span> to <span class="math notranslate nohighlight">\(x\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
\frac{df}{dx}=\frac{df}{dg}\frac{dg}{dx}\frac{dx}{dx}+\frac{df}{dg}\frac{dg}{dy}\frac{dy}{dx}+\frac{df}{dg}\frac{dg}{dz}\frac{dz}{dx}\tag{12}
\]</div>
<p>The formula of <span class="math notranslate nohighlight">\(\frac{df}{dy}\)</span> and <span class="math notranslate nohighlight">\(\frac{df}{dz}\)</span> is similar to <span class="math notranslate nohighlight">\(\frac{df}{dx}\)</span>.</p>
<p>Based on chain rule, we define gradient function <code class="docutils literal notranslate"><span class="pre">bprop:</span> <span class="pre">dout-&gt;(df,</span> <span class="pre">dinputs)</span></code> for every functions (including operators and graph). Here, <code class="docutils literal notranslate"><span class="pre">df</span></code> means gradients with respect to free variables (variables defined outside the function) and <code class="docutils literal notranslate"><span class="pre">dinputs</span></code> is gradients to function inputs. Then we use total derivative rule to accumulate <code class="docutils literal notranslate"><span class="pre">(df,</span> <span class="pre">dinputs)</span></code> to correspond variables.</p>
<p>MindIR has developed the formulas for branching, loops and closures. So if we define the gradient rules correctly, we can get the correct gradient.</p>
<p>Define operator K, backward mode AD can be represented as:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>v = (func, inputs)
F(v): {
    (result, bprop) = K(func)(K(inputs))
    df, dinputs = bprop(dout)
    v.df += df
    v.dinputs += dinputs
}
</pre></div>
</div>
</section>
<section id="grad-implementation">
<h4><code class="docutils literal notranslate"><span class="pre">grad</span></code> Implementation<a class="headerlink" href="#grad-implementation" title="Permalink to this headline"></a></h4>
<p>In <code class="docutils literal notranslate"><span class="pre">grad</span></code> process, the function that needs to calculate gradient will be taken out and used as the input of automatic differentiation module.</p>
<p>AD module will map input function to gradient <code class="docutils literal notranslate"><span class="pre">fprop</span></code>.</p>
<p>The output gradient has form <code class="docutils literal notranslate"><span class="pre">fprop</span> <span class="pre">=</span> <span class="pre">(forward_result,</span> <span class="pre">bprop)</span></code>. <code class="docutils literal notranslate"><span class="pre">forward_result</span></code> is the output node of the origin function. <code class="docutils literal notranslate"><span class="pre">bprop</span></code> is the gradient function which relies on the closure object of <code class="docutils literal notranslate"><span class="pre">fprop</span></code>. <code class="docutils literal notranslate"><span class="pre">bprop</span></code> has only one input <code class="docutils literal notranslate"><span class="pre">dout</span></code>. <code class="docutils literal notranslate"><span class="pre">inputs</span></code> and <code class="docutils literal notranslate"><span class="pre">outputs</span></code> are the called inputs and outputs of <code class="docutils literal notranslate"><span class="pre">fprop</span></code>.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="n">MapObject</span><span class="p">();</span><span class="w"> </span><span class="c1">// Map ValueNode/Parameter/FuncGraph/Primitive object</span>
<span class="w">  </span><span class="n">MapMorphism</span><span class="p">();</span><span class="w"> </span><span class="c1">// Map CNode morphism</span>
<span class="w">  </span><span class="n">res</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">k_graph</span><span class="p">();</span><span class="w"> </span><span class="c1">// res is fprop object of gradient function</span>
</pre></div>
</div>
<p>When generating gradient function object, we need to do a series of mapping from origin function to gradient function. These mapping will generate gradient function nodes and we will connect these nodes according to reverse mode AD rules.</p>
<p>For each subgraph of origin function, we will create an <code class="docutils literal notranslate"><span class="pre">DFunctor</span></code> object, for mapping the original function object to a gradient function object. <code class="docutils literal notranslate"><span class="pre">Dfunctor</span></code> will run <code class="docutils literal notranslate"><span class="pre">MapObject</span></code> and <code class="docutils literal notranslate"><span class="pre">MapMorphism</span></code> to do the mapping.</p>
<p><code class="docutils literal notranslate"><span class="pre">MapObject</span></code> implements the mapping of the original function node to the gradient function node, including the mapping of free variables, parameter nodes, and ValueNode.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">MapFvObject</span><span class="p">();</span><span class="w"> </span><span class="c1">// map free variables</span>
<span class="n">MapParamObject</span><span class="p">();</span><span class="w"> </span><span class="c1">// map parameters</span>
<span class="n">MapValueObject</span><span class="p">();</span><span class="w"> </span><span class="c1">// map ValueNodes</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">MapFvObject</span></code> maps free variables.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MapParamObject</span></code> maps parameter nodes.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MapValueObject</span></code> mainly maps <code class="docutils literal notranslate"><span class="pre">Primitive</span></code> and <code class="docutils literal notranslate"><span class="pre">FuncGraph</span></code> objects.</p></li>
</ul>
<p>For <code class="docutils literal notranslate"><span class="pre">FuncGraph</span></code>, we need to create another <code class="docutils literal notranslate"><span class="pre">DFunctor</span></code> object and perform the mapping, which is a recursion process. <code class="docutils literal notranslate"><span class="pre">Primitive</span></code> defines the type of the operator. We need to define gradient function for every <code class="docutils literal notranslate"><span class="pre">Primitive</span></code>.</p>
<p>MindSpore defines these gradient functions in Python, taking <code class="docutils literal notranslate"><span class="pre">sin</span></code> operator for example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">mindspore.ops._grad.grad_base</span> <span class="kn">import</span> <span class="n">bprop_getters</span>

<span class="nd">@bprop_getters</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">Sin</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">get_bprop_sin</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Grad definition for `Sin` operation.&quot;&quot;&quot;</span>
    <span class="n">cos</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Cos</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">bprop</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="n">dout</span> <span class="o">*</span> <span class="n">cos</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">dx</span><span class="p">,)</span>

    <span class="k">return</span> <span class="n">bprop</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">x</span></code> is the input to the original function object <code class="docutils literal notranslate"><span class="pre">sin</span></code>. <code class="docutils literal notranslate"><span class="pre">out</span></code> is the output of the original function object <code class="docutils literal notranslate"><span class="pre">sin</span></code>, and <code class="docutils literal notranslate"><span class="pre">dout</span></code> is the gradient input of the current accumulation.</p>
<p>When <code class="docutils literal notranslate"><span class="pre">MapObject</span></code> completes the mapping of the above nodes, <code class="docutils literal notranslate"><span class="pre">MapMorphism</span></code> recursively implements the state injection of <code class="docutils literal notranslate"><span class="pre">CNode</span></code> from the output node of the original function, establishes a backpropagation link between nodes, and realizes gradient accumulation.</p>
</section>
<section id="grad-example">
<h4><code class="docutils literal notranslate"><span class="pre">grad</span></code> Example<a class="headerlink" href="#grad-example" title="Permalink to this headline"></a></h4>
<p>Let’s build a simple network to represent the formula:</p>
<div class="math notranslate nohighlight">
\[
f(x) = cos(sin(x)) \tag{13}
\]</div>
<p>And derive the input <code class="docutils literal notranslate"><span class="pre">x</span></code> of formula (13):</p>
<div class="math notranslate nohighlight">
\[
f'(x) = -sin(sin(x)) * cos(x) \tag{14}
\]</div>
<p>The structure of the network in formula (13) in MindSpore is implemented as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sin</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Sin</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cos</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Cos</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
<p>The structure of a forward network is:</p>
<p><img alt="auto-gradient-foward" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/docs/mindspore/source_zh_cn/design/images/auto_gradient_foward.png" /></p>
<p>After the network is reversely differential, the resulting differential network structure is:</p>
<p><img alt="auto-gradient-forward2" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/docs/mindspore/source_zh_cn/design/images/auto_gradient_forward2.png" /></p>
</section>
</section>
<section id="forward-automatic-differentiation-implementation">
<h3>Forward Automatic Differentiation Implementation<a class="headerlink" href="#forward-automatic-differentiation-implementation" title="Permalink to this headline"></a></h3>
<p>Besides <code class="docutils literal notranslate"><span class="pre">grad</span></code>, Mindspore has developed forward mode automatic differentiation method <code class="docutils literal notranslate"><span class="pre">jvp</span></code> (Jacobian-Vector-Product).</p>
<p>Compared to reverse mode AD, forward mode AD is more suitable for networks whose input dimension is smaller than output dimension. Mindspore forward mode AD is developed based on reversed mode Grad function.</p>
<p><img alt="auto-gradient-jvp" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/docs/mindspore/source_zh_cn/design/images/auto_gradient_jvp.png" /></p>
<p>The network in black is the origin function. After the first derivative based on one input <span class="math notranslate nohighlight">\(x\)</span>, we get the network in blue. The second is the blue plot for the <span class="math notranslate nohighlight">\(v\)</span> derivative, resulting in a yellow plot.</p>
<p>This yellow network is the forward mode AD gradient network of black network. Since blue network is a linear network for vector <span class="math notranslate nohighlight">\(v\)</span>, there will be no connection between blue network and yellow network. So, all the nodes in blue are dangling nodes. We can use only blue and yellow nodes to calculate the gradient.</p>
<section id="references">
<h4>References<a class="headerlink" href="#references" title="Permalink to this headline"></a></h4>
<p>[1] Baydin, A.G. et al., 2018. <a class="reference external" href="https://arxiv.org/abs/1502.05767">Automatic differentiation in machine learning: A survey</a>. arXiv.org. [Accessed September 1, 2021].</p>
</section>
</section>
</section>
<section id="functional-+-object-oriented-fusion-programming">
<h2>Functional + Object-Oriented Fusion Programming<a class="headerlink" href="#functional-+-object-oriented-fusion-programming" title="Permalink to this headline"></a></h2>
<p>Taking into account the flexibility and ease of use of the neural network model construction and training process, combined with MindSpore’s own functional automatic differentiation mechanism, MindSpore has designed a functional + object-oriented fusion programming paradigm for AI model training, which can combine the advantages of object-oriented programming and functional programming. The same set of automatic differentiation mechanism is also used to achieve the compatibility of deep learning back propagation and scientific computing automatic differentiation, supporting the compatibility of AI and scientific computing modeling from the bottom. The following is a typical process for the functional + object-oriented fusion programming:</p>
<ol class="arabic simple">
<li><p>Constructing neural networks with classes.</p></li>
<li><p>Instantiating neural network objects.</p></li>
<li><p>Constructing the forward function, and connecting the neural network and the loss function.</p></li>
<li><p>Using function transformations to obtain gradient calculation (back propagation) functions.</p></li>
<li><p>Constructing training process functions.</p></li>
<li><p>Calling functions for training.</p></li>
</ol>
<p>The following is a simple example of functional + object-oriented fusion programming:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Class definition</span>
<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="o">......</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="o">......</span>

<span class="c1"># Object instantiation</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span> <span class="c1"># network</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span> <span class="c1"># loss function</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">lr</span><span class="p">)</span> <span class="c1"># optimizer</span>

<span class="c1"># define forward function</span>
<span class="k">def</span> <span class="nf">forword_fn</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">logits</span>

<span class="c1"># get grad function</span>
<span class="n">grad_fn</span> <span class="o">=</span> <span class="n">value_and_grad</span><span class="p">(</span><span class="n">forward_fn</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">optim</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># define train step function</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
    <span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">logits</span><span class="p">),</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">grad_fn</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span> <span class="c1"># get values and gradients</span>
    <span class="n">optimizer</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span> <span class="c1"># update gradient</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">logits</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">():</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
</pre></div>
</div>
<p>As in the above example, object-oriented programming is used in the construction of the neural network, and the neural network layers are constructed in a manner consistent with the conventions of AI programming. When performing forward computation and backward propagation, MindSpore uses functional programming to construct the forward computation as a function, then obtain <code class="docutils literal notranslate"><span class="pre">grad_fn</span></code> by function transformation, and finally obtain the gradient corresponding to the weights by executing <code class="docutils literal notranslate"><span class="pre">grad_fn</span></code>.</p>
<p>The functional + object-oriented fusion programming ensures the ease of use of neural network construction and improves the flexibility of training processes such as forward computation and backward propagation, which is the default programming paradigm recommended by MindSpore.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="overview.html" class="btn btn-neutral float-left" title="MindSpore Design Overview" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="dynamic_graph_and_static_graph.html" class="btn btn-neutral float-right" title="Combination of Dynamic and Static Graphs" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>