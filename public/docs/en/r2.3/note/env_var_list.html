<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Environment Variables &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script src="../_static/js/mermaid-9.3.0.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Installation" href="../faq/installation.html" />
    <link rel="prev" title="Tensor Index Support" href="index_support.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../design/overview.html">MindSpore Design Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/programming_paradigm.html">Functional and Object-Oriented Fusion Programming Paradigm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/dynamic_graph_and_static_graph.html">Combination of Dynamic and Static Graphs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/distributed_training_design.html">Distributed Parallel Native</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/data_engine.html">High Performance Data Processing Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/all_scenarios.html">Full-scenarios Unified Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/graph_fusion_engine.html">Graph-Kernel Fusion Acceleration Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/pluggable_device.html">Third-Party Hardware Interconnection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="official_models.html">Official Models</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.hal.html">mindspore.hal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.multiprocessing.html">mindspore.multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.experimental.html">mindspore.experimental</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Mapping</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api_mapping/pytorch_api_mapping.html">PyTorch and MindSpore API Mapping Table</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Migration Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/overview.html">Overview of Migration Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/enveriment_preparation.html">Environment Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/analysis_and_preparation.html">Model Analysis and Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/model_development/model_development.html">Network Constructing Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/debug_and_tune.html">Debugging and Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/sample_code.html">Network Migration Debugging Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/faq.html">FAQs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Syntax Support</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="static_graph_syntax_support.html">Static Graph Syntax Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="static_graph_syntax/operators.html">Static Graph Syntax - Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="static_graph_syntax/statements.html">Static Graph Syntax - Python Statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="static_graph_syntax/python_builtin_functions.html">Static Graph Syntax - Python Built-in Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="index_support.html">Tensor Index Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Environment Variables</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Environment Variables</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#data-processing">Data Processing</a></li>
<li class="toctree-l2"><a class="reference internal" href="#graph-compilation">Graph Compilation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#dump-debugging">Dump Debugging</a></li>
<li class="toctree-l2"><a class="reference internal" href="#distributed-parallel">Distributed Parallel</a></li>
<li class="toctree-l2"><a class="reference internal" href="#operators-compile">Operators Compile</a></li>
<li class="toctree-l2"><a class="reference internal" href="#log">Log</a></li>
<li class="toctree-l2"><a class="reference internal" href="#third-party-library">Third-party Library</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cann">CANN</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/data_processing.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/implement_problem.html">Implement Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/network_compilation.html">Network Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/operators_compile.html">Operators Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/usage_migrate_3rd.html">Migration from a Third-party Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/precision_tuning.html">Precision Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/distributed_parallel.html">Distributed Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/feature_advice.html">Feature Advice</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Environment Variables</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/note/env_var_list.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="environment-variables">
<h1>Environment Variables<a class="headerlink" href="#environment-variables" title="Permalink to this headline"></a></h1>
<a class="reference external image-reference" href="https://gitee.com/mindspore/docs/blob/r2.3/docs/mindspore/source_en/note/env_var_list.rst"><img alt="View Source on Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/resource/_static/logo_source_en.svg" /></a>
<p>MindSpore environment variables are as follows:</p>
<section id="data-processing">
<h2>Data Processing<a class="headerlink" href="#data-processing" title="Permalink to this headline"></a></h2>
<table class="colwidths-given docutils align-default">
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 10%" />
<col style="width: 30%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Environment Variable</p></th>
<th class="head"><p>Function</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Value Range</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>DATASET_ENABLE_NUMA</p></td>
<td><p>Determines whether to enable numa feature for dataset module. Most of time this configuration can improve performance on distribute scenario.</p></td>
<td><p>String</p></td>
<td><p>True: Enables the numa feature for dataset module.</p></td>
<td><p>This variable is used together with libnuma.so.</p></td>
</tr>
<tr class="row-odd"><td><p>MS_CACHE_HOST</p></td>
<td><p>Specifies the IP address of the host where the cache server is located when the cache function is enabled.</p></td>
<td><p>String</p></td>
<td><p>IP address of the host where the cache server is located.</p></td>
<td><p>This variable is used together with MS_CACHE_PORT.</p></td>
</tr>
<tr class="row-even"><td><p>MS_CACHE_PORT</p></td>
<td><p>Specifies the port number of the host where the cache server is located when the cache function is enabled.</p></td>
<td><p>String</p></td>
<td><p>Port number of the host where the cache server is located.</p></td>
<td><p>This variable is used together with MS_CACHE_HOST.</p></td>
</tr>
<tr class="row-odd"><td><p>MS_DATASET_SINK_QUEUE</p></td>
<td><p>Specifies the size of data queue in sink mode.</p></td>
<td><p>Integer</p></td>
<td><p>1~128: Valid range of queue size.</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>MS_ENABLE_NUMA</p></td>
<td><p>Whether to enable numa feature in global context to improve end-to-end performance.</p></td>
<td><p>String</p></td>
<td><p>True: Enables the numa feature in global context.</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>OPTIMIZE</p></td>
<td><p>Determines whether to optimize the pipeline tree for dataset during data processing. This variable can improve the data processing efficiency in the data processing operator fusion scenario.</p></td>
<td><p>String</p></td>
<td><p>true: enables pipeline tree optimization.</p>
<p>false: disables pipeline tree optimization.</p>
</td>
<td></td>
</tr>
</tbody>
</table>
<p>For more information, see <a class="reference external" href="https://mindspore.cn/tutorials/experts/en/r2.3/dataset/cache.html">Single-Node Data Cache</a> and <a class="reference external" href="https://mindspore.cn/tutorials/experts/en/r2.3/dataset/optimize.html">Optimizing the Data Processing</a>.</p>
</section>
<section id="graph-compilation">
<h2>Graph Compilation<a class="headerlink" href="#graph-compilation" title="Permalink to this headline"></a></h2>
<table class="colwidths-given docutils align-default">
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 10%" />
<col style="width: 30%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Environment Variable</p></th>
<th class="head"><p>Function</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Value Range</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>MS_DEV_JIT_SYNTAX_LEVEL</p></td>
<td><p>Specify the syntax support level of static graph mode.</p></td>
<td><p>Integer</p></td>
<td><p>0: Specify the syntax support level of static graph mode as STRICT level. Only basic syntaxes is supported, and execution performance is optimal. Can be used for MindIR load and export.</p>
<p>2: Specify the syntax support level of static graph mode as LAX level. More complex syntaxes are supported, compatible with all Python syntax as much as possible. Cannot be used for MindIR load and export due to some syntax that may not be able to be exported.</p>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>MS_JIT_MODULES</p></td>
<td><p>Specify which modules in static graph mode require JIT static compilation, and their functions and methods will be compiled into static calculation graphs.</p></td>
<td><p>String</p></td>
<td><p>The module name, corresponding to the name of the imported top-level module. If there are more than one, separate them with commas. For example, <cite>export MS_JIT_MODULES=mindflow,mindyolo</cite>.</p></td>
<td><p>By default, modules other than third-party libraries will be perform JIT static compilation, and MindSpore suites such as <cite>mindflow</cite> and <cite>mindyolo</cite> will not be treated as third-party libraries. See <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.3/note/static_graph_syntax_support.html#calling-the-third-party-libraries">Calling the Third-party Libraries</a> for more details. If there is a module similar to MindSpore suites, which contains <cite>nn.Cell</cite>, <cite>&#64;ms.jit</cite> decorated functions or functions to be compiled into static calculation graphs, you can configure the environment variable, so that the module will be perform JIT static compilation instead of being treated as third-party library.</p></td>
</tr>
<tr class="row-even"><td><p>MS_JIT_IGNORE_MODULES</p></td>
<td><p>Specify which modules are treated as third-party libraries in static graph mode without JIT static compilation. Their functions and methods will be interpreted and executed.</p></td>
<td><p>String</p></td>
<td><p>The module name, corresponding to the name of the imported top-level module. If there are more than one, separate them with commas. For example, <cite>export MS_JIT_IGNORE_MODULES=numpy,scipy</cite>.</p></td>
<td><p>Static graph mode can automatically recognize third-party libraries, and generally there is no need to set this environment variable for recognizable third-party libraries such as NumPy and Scipy. If <cite>MS_JIT_IGNORE_MODULES</cite> and <cite>MS_JIT_MODULES</cite> specify the same module name at the same time, the former takes effect and the latter does not.</p></td>
</tr>
<tr class="row-odd"><td><p>MS_DEV_FALLBACK_DUMP_NODE</p></td>
<td><p>Print syntax expressions supported by <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.3/design/dynamic_graph_and_static_graph.html#static-graph-syntax-enhancement">Static Graph Syntax Enhancement</a> in the code.</p></td>
<td><p>Integer</p></td>
<td><p>1: Enable printing.</p>
<p>No setting or other value: Disable printing.</p>
</td>
<td></td>
</tr>
<tr class="row-even"><td><p>MS_JIT</p></td>
<td><p>Specify whether to use just-in-time compilation.</p></td>
<td><p>Integer</p></td>
<td><p>0: Do not use just-in-time compilation, and the network script is executed directly in dynamic graph (PyNative) mode.</p>
<p>No setting or other value: Determine whether to execute static graph (Graph) mode or dynamic graph (PyNative) mode according to the network script.</p>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>MS_DEV_FORCE_USE_COMPILE_CACHE</p></td>
<td><p>Specify whether to use the compilation cache directly without checking whether the network script has been modified.</p></td>
<td><p>Integer</p></td>
<td><p>1: Do not check whether the network script has been modified, directly use the compilation cache. It is recommended to only use it during debugging. For example, the network script only adds print statements for printing and debugging.</p>
<p>No setting or other value: Detect changes in network scripts, and only use the compilation cache when the network scripts have not been modified.</p>
</td>
<td></td>
</tr>
<tr class="row-even"><td><p>MS_DEV_SIDE_EFFECT_LOAD_ELIM</p></td>
<td><p>Optimize redundant memory copy operations.</p></td>
<td><p>Integer</p></td>
<td><p>0: Do not do video memory optimization, occupy the most video memory.</p>
<p>1: Conservatively do some memory optimization.</p>
<p>2: Under the premise of losing a certain amount of compilation performance, optimize the video memory as much as possible.</p>
<p>3: The accuracy of the network is not guaranteed, and the memory consumption is minimal.</p>
<p>Default: 1</p>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>MS_DEV_SAVE_GRAPHS</p></td>
<td><p>Specify whether to save IR files.</p></td>
<td><p>Integer</p></td>
<td><p>0: Disable saving IR files.</p>
<p>1: Some intermediate files will be generated during graph compilation.</p>
<p>2: Based on level1, generate more IR files related to backend process.</p>
<p>3: Based on level2, generate visualization computing graphs and detailed frontend IR graphs.</p>
</td>
<td></td>
</tr>
<tr class="row-even"><td><p>MS_DEV_SAVE_GRAPHS_PATH</p></td>
<td><p>Specify path to save IR files.</p></td>
<td><p>String</p></td>
<td><p>Path to save IR files.</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>MS_DEV_DUMP_IR_FORMAT</p></td>
<td><p>Configure what information is displayed in IR graphs.</p></td>
<td><p>Integer</p></td>
<td><p>0: Except for the return node, only the operator and operand of the node are displayed, and the detailed information of subgraph is simplified.</p>
<p>1: Display all information except debug info and scope.</p>
<p>2 or not set: Display all information.</p>
</td>
<td></td>
</tr>
<tr class="row-even"><td><p>MS_DEV_DUMP_IR_INTERVAL</p></td>
<td><p>Set to save an IR file every few IR files to reduce the number of IR files.</p></td>
<td><p>Integer</p></td>
<td><p>1 or not set: Save all IR files.</p>
<p>Other values: Save IR files at specified intervals.</p>
</td>
<td><p>When this environment variable is enabled together with MS_DEV_DUMP_IR_PASSES, the rules of MS_DEV_DUMP_IR_PASSES take priority, and this environment variable will not take effect.</p></td>
</tr>
<tr class="row-odd"><td><p>MS_DEV_DUMP_IR_PASSES</p></td>
<td><p>Specify which IR files to save based on the file name.</p></td>
<td><p>String</p></td>
<td><p>Pass’s name of part of its name. If there are multiple, use commas to separate them. For example, <cite>export MS_DEV_DUMP_IR_PASSES=recompute,renormalize</cite>.</p></td>
<td><p>When setting this environment variable, regardless of the value of MS_DEV_SAVE_GRAPHS, detailed frontend IR files will be filtered and printed.</p></td>
</tr>
<tr class="row-even"><td><p>GRAPH_OP_RUN</p></td>
<td><p>When running the pipeline large network model in task sinking mode in graph mode, it may not be able to start as expected due to the limitation of stream resources.
This environment variable can specify the execution mode of the graph mode.
Set this variable to 0, indicating that model will be executed in non-task sinking mode which is the default execution mode.
Set this variable to 1, indicating a non-task sinking mode, which has no flow restrictions, but has degraded performance.</p></td>
<td><p>Integer</p></td>
<td><p>0: task sinking mode.</p>
<p>1: non-task sinking mode.</p>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>MS_KERNEL_LAUNCH_SKIP</p></td>
<td><p>Specifies the kernel or subgraph to skip during execution.</p></td>
<td><p>String</p></td>
<td><p>ALL or all: skip the execution of all kernels and subgraphs</p>
<p>kernel name (such as ReLU) : skip the execution of all ReLU kernels</p>
<p>subgraph name (such as kernel_graph_1) : skip the execution of subgraph kernel_graph_1, used for subgraph sink mode</p>
</td>
<td></td>
</tr>
<tr class="row-even"><td><p>MS_PYNATIVE_GE</p></td>
<td><p>Whether GE is executed in PyNative mode.</p></td>
<td><p>Integer</p></td>
<td><p>0: GE is not executed.</p>
<p>1: GE is executed.</p>
<p>Default: 0</p>
</td>
<td><p>Experimental environment variable.</p></td>
</tr>
<tr class="row-odd"><td><p>GC_COLLECT_IN_CELL</p></td>
<td><p>Whether to perform garbage collection on unused Cell objects</p></td>
<td><p>Integer</p></td>
<td><p>1: Perform garbage collection on unused Cell objects</p>
<p>No setting or other value: not calling the garbage collection</p>
</td>
<td></td>
</tr>
<tr class="row-even"><td><p>MS_DEV_USE_PY_BPROP</p></td>
<td><p>The op which set by environment will use python bprop instead of cpp expander bprop</p></td>
<td><p>String</p></td>
<td><p>Op name, can set more than one name, split by ‘,’</p></td>
<td><p>Experimental environment variable. It will run fail when python bprop does not exist</p></td>
</tr>
<tr class="row-odd"><td><p>MS_DEV_DISABLE_BPROP_CACHE</p></td>
<td><p>Disable to use bprop’s graph cache</p></td>
<td><p>String</p></td>
<td><p>‘on’, indicating that disable to use bprop’s graph cache</p></td>
<td><p>Experimental environment variable. When set env on, it will slow down building bprop’s graph</p></td>
</tr>
<tr class="row-even"><td><p>MS_DEV_DISABLE_TRACE</p></td>
<td><p>Disable trace function</p></td>
<td><p>String</p></td>
<td><p>‘on’, indicating that disable trace function</p></td>
<td><p>Experimental environment variable.</p></td>
</tr>
<tr class="row-odd"><td><p>MS_ENABLE_IO_REUSE</p></td>
<td><p>Turn on the graph input/output memory multiplexing flag</p></td>
<td><p>Integer</p></td>
<td><p>1: Enable this function.</p>
<p>0: not enabled.</p>
<p>Default value: 0</p>
</td>
<td><p>Ascend AI processor environment GE process use only.</p></td>
</tr>
<tr class="row-even"><td><p>MS_DISABLE_REF_MODE</p></td>
<td><p>Forcibly setting to turn off ref mode</p></td>
<td><p>Integer</p></td>
<td><p>0: Does not turn off ref mode.</p>
<p>1: Forcibly turn off ref mode.</p>
<p>Default value: 0.</p>
</td>
<td><p>This environment variable will be removed subsequently and is not recommended.</p>
<p>Ascend AI processor environment GE process use only.</p>
</td>
</tr>
</tbody>
</table>
</section>
<section id="dump-debugging">
<h2>Dump Debugging<a class="headerlink" href="#dump-debugging" title="Permalink to this headline"></a></h2>
<table class="colwidths-given docutils align-default">
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 10%" />
<col style="width: 30%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Environment Variable</p></th>
<th class="head"><p>Function</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Value Range</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>MINDSPORE_DUMP_CONFIG</p></td>
<td><p>Specify the path of the configuration file that the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.3/debug/dump.html#synchronous-dump">cloud-side Dump</a>
or the <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/use/benchmark_tool.html#dump">device-side Dump</a> depends on.</p></td>
<td><p>String</p></td>
<td><p>File path, which can be a relative path or an absolute path.</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>MS_DIAGNOSTIC_DATA_PATH</p></td>
<td><p>When the <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.3/debug/dump.html#synchronous-dump">cloud-side Dump</a> is enabled,
if the <cite>path</cite> field is not set or set to an empty string in the Dump configuration file, then <cite>$MS_DIAGNOSTIC_DATA_PATH</cite> <cite>/debug_dump is regarded as path.
If the `path</cite> field in configuration file is not empty, it is still used as the path to save Dump data.</p></td>
<td><p>String</p></td>
<td><p>File path, only absolute path is supported.</p></td>
<td><p>This variable is used together with MINDSPORE_DUMP_CONFIG.</p></td>
</tr>
<tr class="row-even"><td><p>MS_DEV_DUMP_BPROP</p></td>
<td><p>Dump bprop ir file in current path</p></td>
<td><p>String</p></td>
<td><p>‘on’, indicating that dump bprop ir file in current path</p></td>
<td><p>Experimental environment variable.</p></td>
</tr>
<tr class="row-odd"><td><p>MS_DEV_DUMP_PACK</p></td>
<td><p>Dump trace ir file in current path</p></td>
<td><p>String</p></td>
<td><p>‘on’, indicating that dump trace ir file in current path</p></td>
<td><p>Experimental environment variable.</p></td>
</tr>
<tr class="row-even"><td><p>ENABLE_MS_DEBUGGER</p></td>
<td><p>Determines whether to enable Debugger during training.</p></td>
<td><p>Boolean</p></td>
<td><p>1: enables Debugger.</p>
<p>0: disables Debugger.</p>
</td>
<td><p>This variable is used together with MS_DEBUGGER_HOST and MS_DEBUGGER_PORT.</p></td>
</tr>
<tr class="row-odd"><td><p>MS_DEBUGGER_HOST</p></td>
<td><p>Specifies the IP of the MindSpore Insight Debugger Server.</p></td>
<td><p>String</p></td>
<td><p>IP address of the host where the MindSpore Insight Debugger Server is located.</p></td>
<td><p>This variable is used together with ENABLE_MS_DEBUGGER=1 and MS_DEBUGGER_PORT.</p></td>
</tr>
<tr class="row-even"><td><p>MS_DEBUGGER_PARTIAL_MEM</p></td>
<td><p>Determines whether to enable partial memory overcommitment. (Memory overcommitment is disabled only for nodes selected on Debugger.)</p></td>
<td><p>Boolean</p></td>
<td><p>1: enables memory overcommitment for nodes selected on Debugger.</p>
<p>0: disables memory overcommitment for nodes selected on Debugger.</p>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>MS_DEBUGGER_PORT</p></td>
<td><p>Specifies the port for connecting to the MindSpore Insight Debugger Server.</p></td>
<td><p>Integer</p></td>
<td><p>Port number ranges from 1 to 65536.</p></td>
<td><p>This variable is used together with ENABLE_MS_DEBUGGER=1 and MS_DEBUGGER_HOST.</p></td>
</tr>
<tr class="row-even"><td><p>MS_OM_PATH</p></td>
<td><p>Specifies the save path for the file <cite>analyze_fail.ir/*.npy</cite> which is dumped if task exception or a compiling graph error occurred.
The file will be saved to the path of <cite>the_specified_directory</cite> <cite>/rank_${rank_id}/om/</cite>.</p></td>
<td><p>String</p></td>
<td><p>File path, which can be a relative path or an absolute path.</p></td>
<td></td>
</tr>
</tbody>
</table>
<p>For more information, see <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.3/debug/dump.html">Using Dump in the Graph Mode</a> and <a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/master/debugger.html">Debugger</a>.</p>
</section>
<section id="distributed-parallel">
<h2>Distributed Parallel<a class="headerlink" href="#distributed-parallel" title="Permalink to this headline"></a></h2>
<table class="colwidths-given docutils align-default">
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 10%" />
<col style="width: 30%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Environment Variable</p></th>
<th class="head"><p>Function</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Value Range</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>RANK_ID</p></td>
<td><p>Specifies the logical ID of the Ascend AI Processor called during deep learning.</p></td>
<td><p>Integer</p></td>
<td><p>The value ranges from 0 to 7. When multiple servers are running concurrently, <cite>DEVICE_ID`s in different servers may be the same.
RANK_ID can be used to avoid this problem. `RANK_ID = SERVER_ID * DEVICE_NUM + DEVICE_ID</cite>, and DEVICE_ID indicates the sequence number of the Ascend AI processor of the current host.</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>RANK_SIZE</p></td>
<td><p>Specifies the number of Ascend AI Processors to be called during deep learning.</p>
<p>Note: When the Ascend AI Processor is used, specified by user when a distributed case is executed.</p>
</td>
<td><p>Integer</p></td>
<td><p>The number of Ascend AI Processors to be called ranges from 1 to 8.</p></td>
<td><p>This variable is used together with RANK_TABLE_FILE</p></td>
</tr>
<tr class="row-even"><td><p>RANK_TABLE_FILE or MINDSPORE_HCCL_CONFIG_PATH</p></td>
<td><p>Specifies the file to which a path points, including <cite>device_ip</cite> corresponding to multiple Ascend AI Processor <cite>device_id</cite>.</p>
<p>Note: When the Ascend AI Processor is used, specified by user when a distributed case is executed.</p>
</td>
<td><p>String</p></td>
<td><p>File path, which can be a relative path or an absolute path.</p></td>
<td><p>This variable is used together with RANK_SIZE.</p></td>
</tr>
<tr class="row-odd"><td><p>MS_COMM_COMPILER_OPT</p></td>
<td><p>Specifies the maximum number of communication operators that can be replaced by corresponding communication subgraph during Ascend backend compilation in graph mode.</p>
<p>Note: When the Ascend AI Processor is used, specified by user when a distributed case is executed.</p>
</td>
<td><p>Integer</p></td>
<td><p>-1 or an positive integer: communication subgraph extraction and reuse is enabled. -1 means that default value will be used. A positive integer means that the user specified value will be used.</p>
<p>Do not set or set other values:: communication subgraph extraction and reuse is turned off.</p>
</td>
<td></td>
</tr>
<tr class="row-even"><td><p>DEVICE_ID</p></td>
<td><p>The ID of the Ascend AI processor, which is the Device’s serial number on the AI server.</p></td>
<td><p>Integer</p></td>
<td><p>The ID of the Rise AI processor, value range: [0, number of actual Devices-1].</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>MS_ROLE</p></td>
<td><p>Specifies the role of this process.</p></td>
<td><p>String</p></td>
<td><p>MS_SCHED: represents the Scheduler process, a training task starts only one Scheduler, which is responsible for networking, disaster recovery, etc., and does not execute the training code.</p>
<p>MS_WORKER: represents the Worker process, which generally sets up the distributed training process for this role.</p>
<p>MS_PSERVER: represents the Parameter Server process, and this role is only valid in Parameter Server mode. Please refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.3/parallel/parameter_server_training.html">Parameter Server mode</a> .</p>
</td>
<td><p>The Worker and Parameter Server processes register with the Scheduler process to complete the networking.</p></td>
</tr>
<tr class="row-even"><td><p>MS_SCHED_HOST</p></td>
<td><p>Specifies the IP address of the Scheduler.</p></td>
<td><p>String</p></td>
<td><p>Legal IP address.</p></td>
<td><p>The current version does not support IPv6 addresses.</p></td>
</tr>
<tr class="row-odd"><td><p>MS_SCHED_PORT</p></td>
<td><p>Specifies the Scheduler binding port number.</p></td>
<td><p>Integer</p></td>
<td><p>Port number in the range of 1024 to 65535.</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>MS_NODE_ID</p></td>
<td><p>Specifies the ID of this process, unique within the cluster.</p></td>
<td><p>String</p></td>
<td><p>Represents the unique ID of this process, which is automatically generated by MindSpore by default.</p></td>
<td><p>MS_NODE_ID needs to be set in the following cases. Normally it does not need to be set and is automatically generated by MindSpore:</p>
<p>Enable Disaster Recovery Scenario: Disaster recovery requires obtaining the current process ID and thus re-registering with the Scheduler.</p>
<p>Enable GLOG log redirection scenario: In order to ensure that the logs of each training process are saved independently, it is necessary to set the process ID, which is used as the log saving path suffix.</p>
<p>Specify process rank id scenario: users can specify the rank id of this process by setting MS_NODE_ID to some integer.</p>
</td>
</tr>
<tr class="row-odd"><td><p>MS_WORKER_NUM</p></td>
<td><p>Specifies the number of processes with the role MS_WORKER.</p></td>
<td><p>Integer</p></td>
<td><p>Integers greater than 0.</p></td>
<td><p>The number of Worker processes started by the user should be equal to the value of this environment variable. If it is less than this value, the networking fails; if it is greater than this value, the Scheduler process will complete the networking according to the order of Worker registration, and the redundant Worker processes will fail to start.</p></td>
</tr>
<tr class="row-even"><td><p>MS_SERVER_NUM</p></td>
<td><p>Specifies the number of processes with the role MS_PSERVER.</p></td>
<td><p>Integer</p></td>
<td><p>Integers greater than 0.</p></td>
<td><p>The setting is only required in Parameter Server training mode.</p></td>
</tr>
<tr class="row-odd"><td><p>MS_ENABLE_RECOVERY</p></td>
<td><p>Turn on disaster tolerance.</p></td>
<td><p>Integer</p></td>
<td><p>1 for on, 0 for off. The default is 0.</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>MS_RECOVERY_PATH</p></td>
<td><p>Persistent path folder.</p></td>
<td><p>String</p></td>
<td><p>Legal user directory.</p></td>
<td><p>The Worker and Scheduler processes perform the necessary persistence during execution, such as node information for restoring the grouping and training the intermediate state of the service, and are saved via files.</p></td>
</tr>
<tr class="row-odd"><td><p>MS_HCCL_CM_INIT</p></td>
<td><p>Whether to use the CM method to initialize the HCCL.</p></td>
<td><p>Integer</p></td>
<td><p>1 for using the method, 0 for not using. The default is 0.</p></td>
<td><p>This environment variable is only recommended to be turned on for Ascend hardware platforms with a large number of communication domains. Turning on this environment variable reduces the memory footprint of the HCCL collection communication libraries, and the training tasks are executed in the same way as the rank table startup.</p></td>
</tr>
<tr class="row-even"><td><p>GROUP_INFO_FILE</p></td>
<td><p>Specify communication group information storage path</p></td>
<td><p>String</p></td>
<td><p>Communication group information file path, supporting relative path and absolute path.</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>MS_SIMULATION_LEVEL</p></td>
<td><p>Specify simulation compilation level.</p></td>
<td><p>Integer</p></td>
<td><p>0: Only the hardware independent compilation is processed; 1: the hardware related compilation is also processed.The default simulation compilation is turned off.</p></td>
<td><p>This environment variable is mainly used to simulate the compilation of a specific rank card in distributed training, and requires the combination of RANK_SIZE and RANK_ID.</p></td>
</tr>
</tbody>
</table>
<p>See <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.3/parallel/dynamic_cluster.html">Dynamic Cluster</a> for more details about Dynamic Cluster.</p>
</section>
<section id="operators-compile">
<h2>Operators Compile<a class="headerlink" href="#operators-compile" title="Permalink to this headline"></a></h2>
<table class="colwidths-given docutils align-default">
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 10%" />
<col style="width: 30%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Environment Variable</p></th>
<th class="head"><p>Function</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Value Range</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>MS_BUILD_PROCESS_NUM</p></td>
<td><p>Specifies the number of parallel operator build processes during Ascend backend compilation.</p></td>
<td><p>Integer</p></td>
<td><p>The number of parallel operator build processes ranges from 1 to 24.</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>MS_COMPILER_CACHE_ENABLE</p></td>
<td><p>Specifies whether to save or load the compile cache.
The function is the same as the <a class="reference external" href="https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore/mindspore.set_context.html#mindspore.set_context">enable_compile_cache</a> in MindSpore context.</p>
<p>Note: This environment variable has lower precedence than the context <cite>enable_compile_cache</cite>.</p>
</td>
<td><p>Integer</p></td>
<td><p>0: Disable the compile cache</p>
<p>1: Enable the compile cache</p>
</td>
<td><p>If it is used together with <cite>MS_COMPILER_CACHE_PATH</cite>, the directory for storing the cache files is <cite>${MS_COMPILER_CACHE_PATH}</cite> <cite>/rank_${RANK_ID}</cite> <cite>/graph_cache/</cite>.
<cite>RANK_ID</cite> is the unique ID for multi-cards training, the single card scenario defaults to <cite>RANK_ID=0</cite>.</p></td>
</tr>
<tr class="row-even"><td><p>MS_COMPILER_CACHE_PATH</p></td>
<td><p>MindSpore compile cache directory and save the graph or operator cache files like <cite>graph_cache</cite>, <cite>kernel_meta</cite>, <cite>somas_meta</cite>.</p></td>
<td><p>String</p></td>
<td><p>File path, which can be a relative path or an absolute path.</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>MS_COMPILER_OP_LEVEL</p></td>
<td><p>Enable debug function and generate the TBE instruction mapping file during Ascend backend compilation.</p>
<p>Note: Only Ascend backend.</p>
</td>
<td><p>Integer</p></td>
<td><p>The value of compiler op level should be one of [0, 1, 2, 3, 4].</p>
<p>0: Turn off op debug and delete op compile cache files</p>
<p>1: Turn on debug, generate the <cite>*.cce</cite> and <cite>*_loc.json</cite></p>
<p>2: Turn on debug, generate the <cite>*.cce</cite> and <cite>*_loc.json</cite> files and turn off the compile optimization switch (The CCEC compiler option is set to <cite>-O0-g</cite>) at the same time</p>
<p>3: Turn off op debug (default)</p>
<p>4: Turn off op debug, generate the <cite>*.cce</cite> and <cite>*_loc.json</cite> files, generate UB fusion calculation description files (<cite>{$kernel_name}_compute.json</cite>) for fusion ops</p>
</td>
<td><p>When an AICore Error occurs, if you need to save the cce file of ops, you can set the <cite>MS_COMPILER_OP_LEVEL</cite> to 1 or 2</p></td>
</tr>
<tr class="row-even"><td><p>MS_DEV_DISABLE_PREBUILD</p></td>
<td><p>Turn off operator prebuild processes during Ascend backend compilation. The prebuild processing may fix the attr <cite>fusion_type</cite> of the operate, and then affect the operator fusion.
If the performance of fusion operator can not meet the expectations, try to turn on this environment variable to verify if there is the performance problem of fusion operator.</p></td>
<td><p>Boolean</p></td>
<td><p>true: turn off prebuild</p>
<p>false: enable prebuild</p>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>MINDSPORE_OP_INFO_PATH</p></td>
<td><p>Specify the path to the operator library load file</p></td>
<td><p>string</p></td>
<td><p>Absolute path of the file</p>
<p>Default: No setting.</p>
</td>
<td><p>Inference only</p></td>
</tr>
<tr class="row-even"><td><p>MS_ASCEND_CHECK_OVERFLOW_MODE</p></td>
<td><p>Setting the output mode of floating-point calculation results</p></td>
<td><p>String</p></td>
<td><p>SATURATION_MODE: Saturation mode.</p>
<p>INFNAN_MODE: INF/NAN mode.</p>
<p>Default value: INFNAN_MODE.</p>
</td>
<td><p>Saturation mode: Saturates to floating-point extremes (+-MAX) when computation overflows.</p>
<p>INF/NAN mode: Follows the IEEE 754 standard and outputs INF/NAN calculations as defined.</p>
<p>Atlas A2 training series use only.</p>
</td>
</tr>
</tbody>
</table>
<p>For more information, see <a class="reference external" href="https://mindspore.cn/docs/en/r2.3/faq/operators_compile.html">FAQ</a>.</p>
</section>
<section id="log">
<h2>Log<a class="headerlink" href="#log" title="Permalink to this headline"></a></h2>
<table class="colwidths-given docutils align-default">
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 10%" />
<col style="width: 30%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Environment Variable</p></th>
<th class="head"><p>Function</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Value Range</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>GLOG_log_dir</p></td>
<td><p>Specifies the log level.</p></td>
<td><p>String</p></td>
<td><p>File path, which can be a relative path or an absolute path.</p></td>
<td><p>This variable is used together with GLOG_logtostderr</p>
<p>If the value of <cite>GLOG_logtostderr</cite> is 0, this variable must be set</p>
<p>If <cite>GLOG_log_dir</cite> is specified and the value of <cite>GLOG_logtostderr</cite> is 1, the logs are output to the screen and not to the file</p>
<p>The log saving path is: <cite>specified path/rank_${rank_id}/logs/</cite>. Under non-distributed training scenario, <cite>rank_id</cite> is 0, while under distributed training scenario, <cite>rank_id</cite> is the ID of the current device in the cluster</p>
<p>C++ and Python logs are output to different files. The C++ logs follow the <cite>GLOG</cite> log file naming rules. In this case <cite>mindspore.machine name. user name.log.log level.timestamp.Process ID</cite>, the Python log file name is <cite>mindspore.log.process ID</cite>.</p>
<p><cite>GLOG_log_dir</cite> can only contain upper and lower case letters, numbers, “-”, “_”, “/” characters, etc.</p>
</td>
</tr>
<tr class="row-odd"><td><p>GLOG_max_log_size</p></td>
<td><p>Control the size of the MindSpore C++ module log file. You can change the default maximum value of the log file with this environment variable</p></td>
<td><p>Integer</p></td>
<td><p>Positive integer. Default value: 50MB</p></td>
<td><p>If the current written log file exceeds the maximum value, the new output log content is written to a new log file</p></td>
</tr>
<tr class="row-even"><td><p>GLOG_logtostderr</p></td>
<td><p>Specifies the log output mode.</p></td>
<td><p>Integer</p></td>
<td><p>1: logs are output to the screen</p>
<p>0: logs are output to a file</p>
<p>Default: 1</p>
</td>
<td><p>This variable is used together with GLOG_log_dir</p></td>
</tr>
<tr class="row-odd"><td><p>GLOG_stderrthreshold</p></td>
<td><p>The log module will print logs to the screen when these logs are output to a file. This environment variable is used to control the log level printed to the screen in this scenario.</p></td>
<td><p>Integer</p></td>
<td><p>0-DEBUG</p>
<p>1-INFO</p>
<p>2-WARNING</p>
<p>3-ERROR</p>
<p>4-CRITICAL</p>
<p>Default: 2</p>
</td>
<td></td>
</tr>
<tr class="row-even"><td><p>GLOG_v</p></td>
<td><p>Specifies the log level.</p></td>
<td><p>Integer</p></td>
<td><p>0-DEBUG</p>
<p>1-INFO</p>
<p>2-WARNING</p>
<p>3-ERROR, indicating that the program execution error, output error log, and the program may not terminate</p>
<p>4-CRITICAL, indicating that the execution of the program is abnormal, and the program may not terminate</p>
<p>Default: 2.</p>
</td>
<td><p>After a log level is specified, output log messages greater than or equal to that level</p></td>
</tr>
<tr class="row-odd"><td><p>logger_backupCount</p></td>
<td><p>Controls the number of mindspore Python module log files.</p></td>
<td><p>Integer</p></td>
<td><p>Default: 30</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>logger_maxBytes</p></td>
<td><p>Controls the size of the mindspore Python module log file.</p></td>
<td><p>Integer</p></td>
<td><p>Default: 52428800 bytes</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>MS_SUBMODULE_LOG_v</p></td>
<td><p>Specifies log levels of C++ sub modules of MindSpore.</p></td>
<td><p>Dict {String:Integer…}</p></td>
<td><p>0-DEBUG</p>
<p>1-INFO</p>
<p>2-WARNING</p>
<p>3-ERROR</p>
</td>
<td><p>The assignment way is:<cite>MS_SUBMODULE_LOG_v=”{SubModule1:LogLevel1,SubModule2:LogLevel2,…}”</cite></p>
<p>The log level of the specified sub-module will override the setting of <cite>GLOG_v</cite> in this module, where the log level of the sub-module <cite>LogLevel</cite> has the same meaning as that of <cite>GLOG_v</cite>. For a detailed list of MindSpore sub-modules, see <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.3/mindspore/core/utils/log_adapter.cc">sub-module_names</a>.</p>
<p>For example, you can set the log level of <cite>PARSER</cite> and <cite>ANALYZER</cite> modules to WARNING and the log level of other modules to INFO by <cite>GLOG_v=1 MS_SUBMODULE_LOG_v=”{PARSER:2,ANALYZER:2}”</cite>.</p>
</td>
</tr>
<tr class="row-even"><td><p>GLOG_logfile_mode</p></td>
<td><p>The GLOG environment variable used to control the permissions of the GLOG log files in MindSpore</p></td>
<td><p>octal number</p></td>
<td><p>Refer to the numerical representation of the Linux file permission setting, default value: 0640 (value taken)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>MS_RDR_ENABLE</p></td>
<td><p>Determines whether to enable running data recorder (RDR).
If a running exception occurs in MindSpore, the pre-recorded data in MindSpore is automatically exported to assist in locating the cause of the running exception.</p></td>
<td><p>Integer</p></td>
<td><p>1：enables RDR</p>
<p>0：disables RDR</p>
</td>
<td><p>This variable is used together with <cite>MS_RDR_MODE</cite> and <cite>MS_RDR_PATH</cite>.</p></td>
</tr>
<tr class="row-even"><td><p>MS_RDR_MODE</p></td>
<td><p>Determines the exporting mode of running data recorder (RDR).</p></td>
<td><p>Integer</p></td>
<td><p>1：export data when training process terminates in exceptional scenario</p>
<p>2：export data when training process terminates in both exceptional scenario and normal scenario.</p>
<p>Default: 1.</p>
</td>
<td><p>This variable is used together with <cite>MS_RDR_ENABLE=1</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p>MS_RDR_PATH</p></td>
<td><p>Specifies the system path for storing the data recorded by running data recorder (RDR).</p></td>
<td><p>String</p></td>
<td><p>Directory path, which should be an absolute path.</p></td>
<td><p>This variable is used together with <cite>MS_RDR_ENABLE=1</cite>. The final directory for recording data is <cite>${MS_RDR_PATH}</cite> <cite>/rank_${RANK_ID}/rdr/</cite>.
<cite>RANK_ID</cite> is the unique ID for multi-cards training, the single card scenario defaults to <cite>RANK_ID=0</cite>.</p></td>
</tr>
<tr class="row-even"><td><p>MS_EXCEPTION_DISPLAY_LEVEL</p></td>
<td><p>Control the display level of exception information</p></td>
<td><p>Integer</p></td>
<td><p>0: display exception information related to model developers and framework developers</p>
<p>1: display exception information related to model developers</p>
<p>Default: 0</p>
</td>
<td></td>
</tr>
</tbody>
</table>
<p>Note: glog does not support log file wrapping. If you need to control the log file occupation of disk space, you can use the log file management tool provided by the operating system, for example: logrotate for Linux. Please set the log environment variables before <cite>import mindspore</cite> .</p>
<p>For more detailed information about RDR, refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.3/debug/rdr.html#running-data-recorder">Running Data Recorder</a> .</p>
</section>
<section id="third-party-library">
<h2>Third-party Library<a class="headerlink" href="#third-party-library" title="Permalink to this headline"></a></h2>
<table class="colwidths-given docutils align-default">
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 10%" />
<col style="width: 30%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Environment Variable</p></th>
<th class="head"><p>Function</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Value Range</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>OPTION_PROTO_LIB_PATH</p></td>
<td><p>Specifies the RPOTO dependent library path.</p></td>
<td><p>String</p></td>
<td><p>File path, which can be a relative path or an absolute path.</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION</p></td>
<td><p>Choose which language to use for the Protocol Buffers back-end implementation</p></td>
<td><p>String</p></td>
<td><p>“cpp”: implementation using c++ backend</p>
<p>“python”: implementation using python back-end</p>
<p>No setting or other value: implementation using python backend</p>
</td>
<td></td>
</tr>
<tr class="row-even"><td><p>ASCEND_OPP_PATH</p></td>
<td><p>OPP package installation path</p></td>
<td><p>String</p></td>
<td><p>Absolute path for OPP package installation</p></td>
<td><p>Required for Ascend AI processor environments only; the environment generally provided to the user is already configured and need not be concerned.</p></td>
</tr>
<tr class="row-odd"><td><p>ASCEND_AICPU_PATH</p></td>
<td><p>AICPU package installation path</p></td>
<td><p>String</p></td>
<td><p>Absolute path of the AICPU package installation</p></td>
<td><p>Required for Ascend AI processor environments only; the environment generally provided to the user is already configured and need not be concerned.</p></td>
</tr>
<tr class="row-even"><td><p>ASCEND_CUSTOM_OPP_PATH</p></td>
<td><p>the installation path of the custom operator package</p></td>
<td><p>String</p></td>
<td><p>the absolute path of custom operator package installation</p></td>
<td><p>Required for Ascend AI processor environments only; the environment generally provided to the user is already configured and need not be concerned.</p></td>
</tr>
<tr class="row-odd"><td><p>ASCEND_TOOLKIT_PATH</p></td>
<td><p>TOOLKIT package installation path</p></td>
<td><p>String</p></td>
<td><p>the absolute path of custom operator package installation</p></td>
<td><p>Required for Ascend AI processor environments only; the environment generally provided to the user is already configured and need not be concerned.</p></td>
</tr>
<tr class="row-even"><td><p>CUDA_HOME</p></td>
<td><p>CUDA installation path</p></td>
<td><p>String</p></td>
<td><p>Absolute path for CUDA package installation</p></td>
<td><p>Required for GPU environment only, generally no need to set. If multiple versions of CUDA are installed in the GPU environment, it is recommended to configure this environment variable in order to avoid confusion.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="cann">
<h2>CANN<a class="headerlink" href="#cann" title="Permalink to this headline"></a></h2>
<p>For more information about CANN’s environment variables, see <a class="reference external" href="https://www.hiascend.com/document/detail/zh/canncommercial/70RC1/reference/envvar/envref_07_0001.html">Ascend community</a> . Please set the environment variables for CANN before <cite>import mindspore</cite> .</p>
<table class="colwidths-given docutils align-default">
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 10%" />
<col style="width: 30%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Environment Variable</p></th>
<th class="head"><p>Function</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Value Range</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>MS_FORMAT_MODE</p></td>
<td><p>Set the default preferred format for Ascend GE processes, with the entire network set to ND format</p></td>
<td><p>Integer</p></td>
<td><p>1: The operator prioritizes the ND format.</p>
<p>0: The operator prioritizes private formats.</p>
<p>Default value: 1</p>
</td>
<td><p>This environment variable affects the choice of format for the operator, which has an impact on network execution performance and memory usage, and can be tested by setting this option to get a better choice of operator format in terms of performance and memory.</p>
<p>Ascend AI processor environment GE processes only.</p>
</td>
</tr>
</tbody>
</table>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index_support.html" class="btn btn-neutral float-left" title="Tensor Index Support" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../faq/installation.html" class="btn btn-neutral float-right" title="Installation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>