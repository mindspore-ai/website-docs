<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Implement Problem &mdash; MindSpore master documentation</title><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Operators Compile" href="operators_compile.html" />
    <link rel="prev" title="Data Processing" href="data_processing.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_processing.html">Data Processing</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Implement Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="operators_compile.html">Operators Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="usage_migrate_3rd.html">Migration from a Third-party Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="precision_tuning.html">Precision Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_configure.html">Distributed Configure</a></li>
<li class="toctree-l1"><a class="reference internal" href="inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="feature_advice.html">Feature Advice</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Implement Problem</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/implement_problem.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="implement-problem">
<h1>Implement Problem<a class="headerlink" href="#implement-problem" title="Permalink to this headline"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Linux</span></code> <code class="docutils literal notranslate"><span class="pre">Windows</span></code> <code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code> <code class="docutils literal notranslate"><span class="pre">Environment</span> <span class="pre">Preparation</span></code> <code class="docutils literal notranslate"><span class="pre">Basic</span></code> <code class="docutils literal notranslate"><span class="pre">Intermediate</span></code></p>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.3/docs/mindspore/faq/source_en/implement_problem.md" target="_blank"><img src="https://gitee.com/mindspore/docs/raw/r1.3/resource/_static/logo_source.png"></a></p>
<p><font size=3><strong>Q: What is the function of the <code class="docutils literal notranslate"><span class="pre">.meta</span></code> file generated after the model is saved using MindSpore? Can the <code class="docutils literal notranslate"><span class="pre">.meta</span></code> file be used to import the graph structure?</strong></font></p>
<p>A: The <code class="docutils literal notranslate"><span class="pre">.meta</span></code> file is a built graph structure. However, this structure cannot be directly imported currently. If you do not know the graph structure, you still need to use the MindIR file to import the network.</p>
<br/>
<p><font size=3><strong>Q: Can the <code class="docutils literal notranslate"><span class="pre">yolov4-tiny-3l.weights</span></code> model file be directly converted into a MindSpore model?</strong></font></p>
<p>A: No. You need to convert the parameters trained by other frameworks into the MindSpore format, and then convert the model file into a MindSpore model.</p>
<br/>
<p><font size=3><strong>Q: Why an error is reported when MindSpore is used to set <code class="docutils literal notranslate"><span class="pre">model.train</span></code>?</strong></font></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">LossMonitor</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">LossMonitor</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>A: If the offloading mode has been set, it cannot be set to non-offloading mode. This is a restriction on the running mechanism.</p>
<br/>
<p><font size=3><strong>Q: What should I pay attention to when using MindSpore to train a model in the <code class="docutils literal notranslate"><span class="pre">eval</span></code> phase? Can the network and parameters be loaded directly? Does the optimizer need to be used in the model?</strong></font></p>
<p>A: It mainly depends on what is required in the <code class="docutils literal notranslate"><span class="pre">eval</span></code> phase. For example, the output of the <code class="docutils literal notranslate"><span class="pre">eval</span></code> network of the image classification task is the probability value of each class, and the <code class="docutils literal notranslate"><span class="pre">acc</span></code> is computed with the corresponding label.
In most cases, the training network and parameters can be directly reused. Note that the inference mode needs to be set.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">net</span><span class="o">.</span><span class="n">set_train</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>The optimizer is not required in the <code class="docutils literal notranslate"><span class="pre">eval</span></code> phase. However, if the <code class="docutils literal notranslate"><span class="pre">model.eval</span></code> API of MindSpore needs to be used, the <code class="docutils literal notranslate"><span class="pre">loss</span> <span class="pre">function</span></code> needs to be configured. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a model.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;top_1_accuracy&#39;</span><span class="p">,</span> <span class="s1">&#39;top_5_accuracy&#39;</span><span class="p">})</span>
<span class="c1"># Evaluate the model.</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<br/>
<p><font size=3><strong>Q: How do I use <code class="docutils literal notranslate"><span class="pre">param_group</span></code> in SGD to reduce the learning rate?</strong></font></p>
<p>A: To change the value according to <code class="docutils literal notranslate"><span class="pre">epoch</span></code>, use <a class="reference external" href="https://mindspore.cn/docs/api/en/r1.3/api_python/mindspore.nn.html#dynamic-lr">Dynamic LR</a> and set <code class="docutils literal notranslate"><span class="pre">step_per_epoch</span></code> to <code class="docutils literal notranslate"><span class="pre">step_size</span></code>. To change the value according to <code class="docutils literal notranslate"><span class="pre">step</span></code>, set <code class="docutils literal notranslate"><span class="pre">step_per_epoch</span></code> to 1. You can also use <a class="reference external" href="https://mindspore.cn/docs/api/en/r1.3/api_python/mindspore.nn.html#dynamic-learning-rate">LearningRateSchedule</a>.</p>
<br/>
<p><font size=3><strong>Q: How do I modify parameters (such as the dropout value) on MindSpore?</strong></font></p>
<p>A: When building a network, use <code class="docutils literal notranslate"><span class="pre">if</span> <span class="pre">self.training:</span> <span class="pre">x</span> <span class="pre">=</span> <span class="pre">dropput(x)</span></code>. When reasoning, set <code class="docutils literal notranslate"><span class="pre">network.set_train(mode_false)</span></code> before execution to disable the dropout function. During training, set <code class="docutils literal notranslate"><span class="pre">network.set_train(mode_false)</span></code> to True to enable the dropout function.</p>
<br/>
<p><font size=3><strong>Q: How do I view the number of model parameters?</strong></font></p>
<p>A: You can load the checkpoint to count the parameter number. Variables in the momentum and optimizer may be counted, so you need to filter them out.
You can refer to the following APIs to collect the number of network parameters:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">count_params</span><span class="p">(</span><span class="n">net</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Count number of parameters in the network</span>
<span class="sd">    Args:</span>
<span class="sd">        net (mindspore.nn.Cell): Mindspore network instance</span>
<span class="sd">    Returns:</span>
<span class="sd">        total_params (int): Total number of trainable params</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">total_params</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">():</span>
        <span class="n">total_params</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">total_params</span>
</pre></div>
</div>
<p><a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.3/model_zoo/research/cv/tinynet/src/utils.py">Script Link</a>.</p>
<br/>
<p><font size=3><strong>Q: How do I monitor the loss during training and save the training parameters when the <code class="docutils literal notranslate"><span class="pre">loss</span></code> is the lowest?</strong></font></p>
<p>A: You can customize a <code class="docutils literal notranslate"><span class="pre">callback</span></code>.For details, see the writing method of <code class="docutils literal notranslate"><span class="pre">ModelCheckpoint</span></code>. In addition, the logic for determining loss is added.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">EarlyStop</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">def</span> <span class="nf">step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">run_context</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span>  <span class="o">****</span><span class="p">(</span><span class="n">get</span> <span class="n">current</span> <span class="n">loss</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">==</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">loss</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span>
            <span class="c1"># do save ckpt</span>
</pre></div>
</div>
<br/>
<p><font size=3><strong>Q: How do I obtain the expected <code class="docutils literal notranslate"><span class="pre">feature</span> <span class="pre">map</span></code> when <code class="docutils literal notranslate"><span class="pre">nn.Conv2d</span></code> is used?</strong></font></p>
<p>A: For details about how to derive the <code class="docutils literal notranslate"><span class="pre">Conv2d</span> <span class="pre">shape</span></code>, click <a class="reference external" href="https://www.mindspore.cn/docs/api/en/r1.3/api_python/nn/mindspore.nn.Conv2d.html#mindspore.nn.Conv2d">here</a> Change <code class="docutils literal notranslate"><span class="pre">pad_mode</span></code> of <code class="docutils literal notranslate"><span class="pre">Conv2d</span></code> to <code class="docutils literal notranslate"><span class="pre">same</span></code>. Alternatively, you can calculate the <code class="docutils literal notranslate"><span class="pre">pad</span></code> based on the Conv2d shape derivation formula to keep the <code class="docutils literal notranslate"><span class="pre">shape</span></code> unchanged. Generally, the pad is <code class="docutils literal notranslate"><span class="pre">(kernel_size-1)//2</span></code>.</p>
<br/>
<p><font size=3><strong>Q: Can MindSpore be used to customize a loss function that can return multiple values?</strong></font></p>
<p>A: After customizing the <code class="docutils literal notranslate"><span class="pre">loss</span> <span class="pre">function</span></code>, you need to customize <code class="docutils literal notranslate"><span class="pre">TrainOneStepCell</span></code>. The number of <code class="docutils literal notranslate"><span class="pre">sens</span></code> for implementing gradient calculation is the same as the number of <code class="docutils literal notranslate"><span class="pre">network</span></code> outputs. For details, see the following:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">MyLoss</span><span class="p">()</span>
<span class="n">loss_with_net</span> <span class="o">=</span> <span class="n">MyWithLossCell</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">)</span>
<span class="n">train_net</span> <span class="o">=</span> <span class="n">MyTrainOneStepCell</span><span class="p">(</span><span class="n">loss_with_net</span><span class="p">,</span> <span class="n">optim</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="o">=</span><span class="n">train_net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
<br/>
<p><font size=3><strong>Q: How does MindSpore implement the early stopping function?</strong></font></p>
<p>A: You can customize the <code class="docutils literal notranslate"><span class="pre">callback</span></code> method to implement the early stopping function.
Example: When the loss value decreases to a certain value, the training stops.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">EarlyStop</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">control_loss</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EarlyStep</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_control_loss</span> <span class="o">=</span> <span class="n">control_loss</span>

    <span class="k">def</span> <span class="nf">step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">run_context</span><span class="p">):</span>
        <span class="n">cb_params</span> <span class="o">=</span> <span class="n">run_context</span><span class="o">.</span><span class="n">original_args</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">cb_params</span><span class="o">.</span><span class="n">net_outputs</span>
        <span class="k">if</span> <span class="n">loss</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_control_loss</span><span class="p">:</span>
            <span class="c1"># Stop training.</span>
            <span class="n">run_context</span><span class="o">.</span><span class="n">_stop_requested</span> <span class="o">=</span> <span class="kc">True</span>

<span class="n">stop_cb</span> <span class="o">=</span> <span class="n">EarlyStop</span><span class="p">(</span><span class="n">control_loss</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">epoch_size</span><span class="p">,</span> <span class="n">ds_train</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">stop_cb</span><span class="p">])</span>
</pre></div>
</div>
<br/>
<p><font size=3><strong>Q: After a model is trained, how do I save the model output in text or <code class="docutils literal notranslate"><span class="pre">npy</span></code> format?</strong></font></p>
<p>A: The network output is <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>. You need to use the <code class="docutils literal notranslate"><span class="pre">asnumpy()</span></code> method to convert the <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> to <code class="docutils literal notranslate"><span class="pre">NumPy</span></code> and then save the data. For details, see the following:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">out</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;output.npy&quot;</span><span class="p">,</span> <span class="n">out</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
</pre></div>
</div>
<br/>
<p><font size=3><strong>Q: What can I do if an error “Create python object `&lt;class ‘mindspore.common.tensor.Tensor’&gt;` failed, only support create Cell or Primitive object.” is reported?</strong></font></p>
<p>A: Currently in graph mode, the <code class="docutils literal notranslate"><span class="pre">construct</span></code> function (or the function decorated by the <code class="docutils literal notranslate"><span class="pre">&#64;ms_function</span></code> decorator) only supports the construction of <code class="docutils literal notranslate"><span class="pre">Cell</span></code> and <code class="docutils literal notranslate"><span class="pre">Primitive</span> <span class="pre">object</span></code>. The construction of <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> is not supported, that is, the syntax <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">Tensor(args...)</span></code> is not supported.</p>
<p>If it is a constant tensor, please define it in the function <code class="docutils literal notranslate"><span class="pre">__init__</span></code>. If not, you can use the <code class="docutils literal notranslate"><span class="pre">&#64;constexpr</span></code> decorator to modify the function and generate the <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> in the function.</p>
<p>Please see the usage of <code class="docutils literal notranslate"><span class="pre">&#64;constexpr</span></code> in <a class="reference external" href="https://www.mindspore.cn/docs/api/en/r1.3/api_python/ops/mindspore.ops.constexpr.html">https://www.mindspore.cn/docs/api/en/r1.3/api_python/ops/mindspore.ops.constexpr.html</a>.</p>
<p>The constant <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> used on the network can be used as a network attribute and defined in <code class="docutils literal notranslate"><span class="pre">init</span></code>, that is, <code class="docutils literal notranslate"><span class="pre">self.x</span> <span class="pre">=</span> <span class="pre">Tensor(args...)</span></code>. Then the constant can be used in the <code class="docutils literal notranslate"><span class="pre">construct</span></code> function (or the function decorated by the <code class="docutils literal notranslate"><span class="pre">&#64;ms_function</span></code> decorator).</p>
<p>In the following example, <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> of <code class="docutils literal notranslate"><span class="pre">shape</span> <span class="pre">=</span> <span class="pre">(3,</span> <span class="pre">4),</span> <span class="pre">dtype</span> <span class="pre">=</span> <span class="pre">int64</span></code> is generated by <code class="docutils literal notranslate"><span class="pre">&#64;constexpr</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">generate_tensor</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)))</span>
</pre></div>
</div>
<br/>
<p><font size=3><strong>Q: What can I do if an error “‘self.xx’ should be defined in the class ‘<strong>init</strong>’ function.” is reported?</strong></font></p>
<p>A: If you want to assign for a class member such as <code class="docutils literal notranslate"><span class="pre">self.xx</span></code> in the function <code class="docutils literal notranslate"><span class="pre">construct</span></code>, <code class="docutils literal notranslate"><span class="pre">self.xx</span></code> must have been defined to a <a class="reference external" href="https://www.mindspore.cn/docs/api/en/r1.3/api_python/mindspore.html#mindspore.Parameter"><code class="docutils literal notranslate"><span class="pre">Parameter</span></code></a> type firstly while the other types are not supported. But the local variable <code class="docutils literal notranslate"><span class="pre">xx</span></code> is not under the regulation.</p>
<br/>
<p><font size=3><strong>Q: What can I do if an error “This comparator ‘AnyValue’ is not supported. For statement ‘is’, only support compare with ‘None’, ‘False’ or ‘True’” is reported?</strong></font></p>
<p>A: For the syntax <code class="docutils literal notranslate"><span class="pre">is</span></code> or <code class="docutils literal notranslate"><span class="pre">is</span> <span class="pre">not</span></code>, currently <code class="docutils literal notranslate"><span class="pre">MindSpore</span></code> only supports comparisons with <code class="docutils literal notranslate"><span class="pre">True</span></code>, <code class="docutils literal notranslate"><span class="pre">False</span></code> and <code class="docutils literal notranslate"><span class="pre">None</span></code>. Other types, such as strings, are not supported.</p>
<br/>
<p><font size=3><strong>Q: What can I do if an error “MindSpore does not support comparison with operators more than one now, ops size =2” is reported?</strong></font></p>
<p>A: For comparison statements, <code class="docutils literal notranslate"><span class="pre">MindSpore</span></code> supports at most one operator. Please modify your code. For example, you can use <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">&lt;</span> <span class="pre">x</span> <span class="pre">and</span> <span class="pre">x</span> <span class="pre">&lt;</span> <span class="pre">3</span></code> to take the place of <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">&lt;</span> <span class="pre">x</span> <span class="pre">&lt;</span> <span class="pre">3</span></code>.</p>
<br/>
<p><font size=3><strong>Q: What can I do if an error “TypeError: The function construct need 1 positional argument and 0 default argument, but provided 2” is reported?</strong></font></p>
<p>A: When you call the instance of a network, the function <code class="docutils literal notranslate"><span class="pre">construct</span></code> will be executed. And the program will check the number of parameters required by the function <code class="docutils literal notranslate"><span class="pre">construct</span></code> and the number of parameters actually given. If they are not equal, the above exception will be thrown.
Please check your code to make sure they are equal.</p>
<br/>
<p><font size=3><strong>Q: What can I do if an error “Type Join Failed” or “Shape Join Failed” is reported?</strong></font></p>
<p>A: In the inference stage of front-end compilation, the abstract types of nodes, including <code class="docutils literal notranslate"><span class="pre">type</span></code> and <code class="docutils literal notranslate"><span class="pre">shape</span></code>, will be inferred. Common abstract types include <code class="docutils literal notranslate"><span class="pre">AbstractScalar</span></code>, <code class="docutils literal notranslate"><span class="pre">AbstractTensor</span></code>, <code class="docutils literal notranslate"><span class="pre">AbstractFunction</span></code>, <code class="docutils literal notranslate"><span class="pre">AbstractTuple</span></code>, <code class="docutils literal notranslate"><span class="pre">AbstractList</span></code>, etc. In some scenarios, such as multi-branch scenarios, the abstract types of the return values of different branches will be joined to infer the abstract type of the returned result. If these abstract types do not match, or <code class="docutils literal notranslate"><span class="pre">type</span></code>/<code class="docutils literal notranslate"><span class="pre">shape</span></code> are inconsistent, the above exception will be thrown.</p>
<p>When an error similar to “Type Join Failed: dtype1 = Float32, dtype2 = Float16” appears, it means that the data types are inconsistent, resulting in an exception when joining abstract. According to the provided data types and code line, the error can be quickly located. In addition, the specific abstract information and node information are provided in the error message. You can view the MindIR information through the <code class="docutils literal notranslate"><span class="pre">analyze_fail.dat</span></code> file to locate and solve the problem. For specific introduction of MindIR, please refer to <a class="reference external" href="https://www.mindspore.cn/docs/note/en/r1.3/design/mindir.html">MindSpore IR (MindIR)</a>. The code sample is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">context</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cast</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">a</span> <span class="o">&gt;</span> <span class="n">b</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">ms</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>

<span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">input_a</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">input_b</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">out_me</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_a</span><span class="p">,</span> <span class="n">input_b</span><span class="p">)</span>
</pre></div>
</div>
<p>The result is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>TypeError: The return values of different branches do not match. Type Join Failed: dtype1 = Float32, dtype2 = Float16. The abstract type of the return value of the current branch is AbstractTensor(shape: (2, 3, 4, 5), element: AbstractScalar(Type: Float16, Value: AnyValue, Shape: NoShape), value_ptr: 0x32ed00e0, value: AnyValue), and that of the previous branch is AbstractTensor(shape: (2, 3, 4, 5), element: AbstractScalar(Type: Float32, Value: AnyValue, Shape: NoShape), value_ptr: 0x32ed00e0, value: AnyValue). Please check the node construct.4:[CNode]5{[0]: [CNode]6}, true branch: ✓construct.2, false branch: ✗construct.3. trace:
In file test_join.py(14)/        if a &gt; b:/

The function call stack (See file &#39;analyze_fail.dat&#39; for more details):
# 0 In file test_join.py(14)
        if a &gt; b:
</pre></div>
</div>
<p>When an error similar to “Shape Join Failed: shape1 = (2, 3, 4, 5), shape2 = ()” appears, it means that the shapes are inconsistent, resulting in an exception when joining abstract. The code sample is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">context</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reducesum</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">a</span> <span class="o">&gt;</span> <span class="n">b</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">reducesum</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">input_a</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">input_b</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_a</span><span class="p">,</span> <span class="n">input_b</span><span class="p">)</span>
</pre></div>
</div>
<p>The result is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>ValueError: The return values of different branches do not match. Shape Join Failed: shape1 = (2, 3, 4, 5), shape2 = (). The abstract type of the return value of the current branch is AbstractTensor(shape: (), element: AbstractScalar(Type: Float32, Value: AnyValue, Shape: NoShape), value_ptr: 0x239b5120, value: AnyValue), and that of the previous branch is AbstractTensor(shape: (2, 3, 4, 5), element: AbstractScalar(Type: Float32, Value: AnyValue, Shape: NoShape), value_ptr: 0x239b5120, value: AnyValue). Please check the node construct.4:[CNode]5{[0]: [CNode]6}, true branch: ✓construct.2, false branch: ✗construct.3. trace:
In file test_join1.py(14)/        if a &gt; b:/

The function call stack (See file &#39;analyze_fail.dat&#39; for more details):
# 0 In file test_join1.py(14)
        if a &gt; b:
</pre></div>
</div>
<p>When an error similar to “Type Join Failed: abstract type AbstractTensor can not join with AbstractTuple” appears, it means that the two abstract types are mismatched. You need to review the code and modify it based on the provided code line and other error information.</p>
<br/>
<p><font size=3><strong>Q: Can the <code class="docutils literal notranslate"><span class="pre">vgg16</span></code> model be loaded and transferred on a GPU using the Hub?</strong></font></p>
<p>A: Yes, but you need to manually modify the following two arguments:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Add the **kwargs argument as follows:</span>
<span class="k">def</span> <span class="nf">vgg16</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">phase</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Add the **kwargs argument as follows:</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Vgg</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;16&#39;</span><span class="p">],</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span> <span class="n">batch_norm</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_norm</span><span class="p">,</span> <span class="n">phase</span><span class="o">=</span><span class="n">phase</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>
</div>
<br/>
<p><font size=3><strong>Q: How to obtain middle-layer features of a VGG model?</strong></font></p>
<p>A: Obtaining the middle-layer features of a network is not closely related to the specific framework. For the <code class="docutils literal notranslate"><span class="pre">vgg</span></code> model defined in <code class="docutils literal notranslate"><span class="pre">torchvison</span></code>, the <code class="docutils literal notranslate"><span class="pre">features</span></code> field can be used to obtain the middle-layer features. The <code class="docutils literal notranslate"><span class="pre">vgg</span></code> source code of <code class="docutils literal notranslate"><span class="pre">torchvison</span></code> is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">VGG</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">init_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VGG</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">features</span> <span class="o">=</span> <span class="n">features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">((</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">vgg16</span></code> defined in ModelZoo of MindSpore can be obtained through the <code class="docutils literal notranslate"><span class="pre">layers</span></code> field as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">network</span> <span class="o">=</span> <span class="n">vgg16</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span>
</pre></div>
</div>
<br/>
<p><font size=3><strong>Q: When MindSpore is used for model training, there are four input parameters for <code class="docutils literal notranslate"><span class="pre">CTCLoss</span></code>: <code class="docutils literal notranslate"><span class="pre">inputs</span></code>, <code class="docutils literal notranslate"><span class="pre">labels_indices</span></code>, <code class="docutils literal notranslate"><span class="pre">labels_values</span></code>, and <code class="docutils literal notranslate"><span class="pre">sequence_length</span></code>. How do I use <code class="docutils literal notranslate"><span class="pre">CTCLoss</span></code> for model training?</strong></font></p>
<p>A: The <code class="docutils literal notranslate"><span class="pre">dataset</span></code> received by the defined <code class="docutils literal notranslate"><span class="pre">model.train</span></code> API can consist of multiple pieces of data, for example, (<code class="docutils literal notranslate"><span class="pre">data1</span></code>, <code class="docutils literal notranslate"><span class="pre">data2</span></code>, <code class="docutils literal notranslate"><span class="pre">data3</span></code>, …). Therefore, the <code class="docutils literal notranslate"><span class="pre">dataset</span></code> can contain <code class="docutils literal notranslate"><span class="pre">inputs</span></code>, <code class="docutils literal notranslate"><span class="pre">labels_indices</span></code>, <code class="docutils literal notranslate"><span class="pre">labels_values</span></code>, and <code class="docutils literal notranslate"><span class="pre">sequence_length</span></code> information. You only need to define the dataset in the corresponding format and transfer it to <code class="docutils literal notranslate"><span class="pre">model.train</span></code>. For details, see <a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/en/r1.3/dataset_loading.html">Data Processing API</a>.</p>
<br/>
<p><font size=3><strong>Q: How do I load the PyTorch weight to MindSpore during model transfer?</strong></font></p>
<p>A: First, enter the <code class="docutils literal notranslate"><span class="pre">PTH</span></code> file of PyTorch. Take <code class="docutils literal notranslate"><span class="pre">ResNet-18</span></code> as an example. The network structure of MindSpore is the same as that of PyTorch. After transferring, the file can be directly loaded to the network. Only <code class="docutils literal notranslate"><span class="pre">BN</span></code> and <code class="docutils literal notranslate"><span class="pre">Conv2D</span></code> are used during loading. If the network names of MindSpore and PyTorch at other layers are different, change the names to the same.</p>
<br/>
<p><font size=3><strong>Q: What are the available recommendation or text generation networks or models provided by MindSpore?</strong></font></p>
<p>A: Currently, recommendation models such as Wide &amp; Deep, DeepFM, and NCF are under development. In the natural language processing (NLP) field, Bert_NEZHA is available and models such as MASS are under development. You can rebuild the network into a text generation network based on the scenario requirements. Please stay tuned for updates on the <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.3/model_zoo">MindSpore Model Zoo</a>.</p>
<br/>
<p><font size=3><strong>Q: How do I use MindSpore to fit functions such as <span class="math notranslate nohighlight">\(f(x)=a \times sin(x)+b\)</span>?</strong></font></p>
<p>A: The following is based on the official MindSpore linear fitting case.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># The fitting function is: f(x)=2*sin(x)+3.</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dataset</span> <span class="k">as</span> <span class="n">ds</span>
<span class="kn">from</span> <span class="nn">mindspore.common.initializer</span> <span class="kn">import</span> <span class="n">Normal</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">Model</span><span class="p">,</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">mindspore.train.callback</span> <span class="kn">import</span> <span class="n">LossMonitor</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_data</span><span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="n">w</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">3.0</span><span class="p">):</span>
    <span class="c1"># f(x)=w * sin(x) + b</span>
    <span class="c1"># f(x)=2 * sin(x) +3</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="n">noise</span>
        <span class="k">yield</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">y</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">create_dataset</span><span class="p">(</span><span class="n">num_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">repeat_size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">GeneratorDataset</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">get_data</span><span class="p">(</span><span class="n">num_data</span><span class="p">)),</span> <span class="n">column_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">,</span><span class="s1">&#39;label&#39;</span><span class="p">])</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">repeat_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">input_data</span>

<span class="k">class</span> <span class="nc">LinearNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LinearNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Normal</span><span class="p">(</span><span class="mf">0.02</span><span class="p">),</span> <span class="n">Normal</span><span class="p">(</span><span class="mf">0.02</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>

    <span class="n">num_data</span> <span class="o">=</span> <span class="mi">1600</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span>
    <span class="n">repeat_size</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.005</span>
    <span class="n">momentum</span> <span class="o">=</span> <span class="mf">0.9</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">LinearNet</span><span class="p">()</span>
    <span class="n">net_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">net_loss</span><span class="p">,</span> <span class="n">opt</span><span class="p">)</span>

    <span class="n">ds_train</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">num_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">repeat_size</span><span class="o">=</span><span class="n">repeat_size</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ds_train</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">LossMonitor</span><span class="p">(),</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
<br/>
<p><font size=3><strong>Q: How do I use MindSpore to fit quadratic functions such as <span class="math notranslate nohighlight">\(f(x)=ax^2+bx+c\)</span>?</strong></font></p>
<p>A: The following code is referenced from the official <a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.3/docs/sample_code/linear_regression.py">MindSpore tutorial code</a>.</p>
<p>Modify the following items to fit <span class="math notranslate nohighlight">\(f(x) = ax^2 + bx + c\)</span>:</p>
<ol class="arabic simple">
<li><p>Dataset generation.</p></li>
<li><p>Network fitting.</p></li>
<li><p>Optimizer.</p></li>
</ol>
<p>The following explains detailed information about the modification:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># The selected optimizer does not support CPUs. Therefore, the GPU computing platform is used for training. You need to install MindSpore of the GPU version.</span>
<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">)</span>

<span class="c1"># Assume that the function to be fitted is f(x)=2x^2+3x+4. Modify the data generation function as follows:</span>
<span class="k">def</span> <span class="nf">get_data</span><span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">3.0</span> <span class="p">,</span><span class="n">c</span> <span class="o">=</span> <span class="mi">4</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mf">10.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">)</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="c1"># For details about how to generate the value of y, see the to-be-fitted objective function ax^2+bx+c.</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="n">a</span> <span class="o">+</span> <span class="n">x</span> <span class="o">*</span> <span class="n">b</span> <span class="o">+</span> <span class="n">c</span> <span class="o">+</span> <span class="n">noise</span>
        <span class="c1"># When fitting a*x^2 + b*x +c, a and b are weight parameters, and c is the offset parameter bias. The training data corresponding to the two weights is x^2 and x, respectively. Therefore, the dataset generation mode is changed as follows:</span>
        <span class="k">yield</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x</span><span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">y</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">create_dataset</span><span class="p">(</span><span class="n">num_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">repeat_size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">GeneratorDataset</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">get_data</span><span class="p">(</span><span class="n">num_data</span><span class="p">)),</span> <span class="n">column_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">,</span><span class="s1">&#39;label&#39;</span><span class="p">])</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">repeat_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">input_data</span>

<span class="k">class</span> <span class="nc">LinearNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LinearNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Two training parameters are input for the full connection function. Therefore, the input value is changed to 2. The first Normal(0.02) automatically allocates random weights to the two input parameters, and the second Normal is the random bias.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Normal</span><span class="p">(</span><span class="mf">0.02</span><span class="p">),</span> <span class="n">Normal</span><span class="p">(</span><span class="mf">0.02</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">num_data</span> <span class="o">=</span> <span class="mi">1600</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span>
    <span class="n">repeat_size</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.005</span>
    <span class="n">momentum</span> <span class="o">=</span> <span class="mf">0.9</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">LinearNet</span><span class="p">()</span>
    <span class="n">net_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
    <span class="c1"># RMSProp optimizer with better effect is selected for quadratic function fitting. Currently, Ascend and GPU computing platforms are supported.</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RMSProp</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">net_loss</span><span class="p">,</span> <span class="n">opt</span><span class="p">)</span>

    <span class="n">ds_train</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">num_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">repeat_size</span><span class="o">=</span><span class="n">repeat_size</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ds_train</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">LossMonitor</span><span class="p">(),</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
<br/>
<p><font size=3><strong>Q: How do I execute a single <code class="docutils literal notranslate"><span class="pre">ut</span></code> case in <code class="docutils literal notranslate"><span class="pre">mindspore/tests</span></code>?</strong></font></p>
<p>A: <code class="docutils literal notranslate"><span class="pre">ut</span></code> cases are usually based on the MindSpore package of the debug version, which is not provided on the official website. You can run <code class="docutils literal notranslate"><span class="pre">sh</span> <span class="pre">build.sh</span></code> to compile the source code and then run the <code class="docutils literal notranslate"><span class="pre">pytest</span></code> command. The compilation in debug mode does not depend on the backend. Run the <code class="docutils literal notranslate"><span class="pre">sh</span> <span class="pre">build.sh</span> <span class="pre">-t</span> <span class="pre">on</span></code> command. For details about how to execute cases, see the <code class="docutils literal notranslate"><span class="pre">tests/runtest.sh</span></code> script.</p>
<br/>
<p><font size=3><strong>Q: For Ascend users, how to get more detailed logs when the <code class="docutils literal notranslate"><span class="pre">run</span> <span class="pre">task</span> <span class="pre">error</span></code> is reported?</strong></font></p>
<p>A: Use the msnpureport tool to set the on-device log level. The tool is stored in <code class="docutils literal notranslate"><span class="pre">/usr/local/Ascend/driver/tools/msnpureport</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>-<span class="w"> </span>Global:<span class="w"> </span>/usr/local/Ascend/driver/tools/msnpureport<span class="w"> </span>-g<span class="w"> </span>info
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>-<span class="w"> </span>Module-level:<span class="w"> </span>/usr/local/Ascend/driver/tools/msnpureport<span class="w"> </span>-m<span class="w"> </span>SLOG:error
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>-<span class="w"> </span>Event-level:<span class="w"> </span>/usr/local/Ascend/driver/tools/msnpureport<span class="w"> </span>-e<span class="w"> </span>disable/enable
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>-<span class="w"> </span>Multi-device<span class="w"> </span>ID-level:<span class="w"> </span>/usr/local/Ascend/driver/tools/msnpureport<span class="w"> </span>-d<span class="w"> </span><span class="m">1</span><span class="w"> </span>-g<span class="w"> </span>warning
</pre></div>
</div>
<p>Assume that the value range of deviceID is [0, 7], and <code class="docutils literal notranslate"><span class="pre">devices</span> <span class="pre">0–3</span></code> and <code class="docutils literal notranslate"><span class="pre">devices</span> <span class="pre">4–7</span></code> are on the same OS. <code class="docutils literal notranslate"><span class="pre">Devices</span> <span class="pre">0–3</span></code> share the same log configuration file and <code class="docutils literal notranslate"><span class="pre">devices</span> <span class="pre">4–7</span></code> share the same configuration file. In this way, changing the log level of any device (for example device 0) will change that of other devices (for example <code class="docutils literal notranslate"><span class="pre">devices</span> <span class="pre">1–3</span></code>). This rule also applies to <code class="docutils literal notranslate"><span class="pre">devices</span> <span class="pre">4–7</span></code>.</p>
<p>After the driver package is installed (assuming that the installation path is /usr/local/HiAI and the execution file <code class="docutils literal notranslate"><span class="pre">msnpureport.exe</span></code> is in the C:\ProgramFiles\Huawei\Ascend\Driver\tools\ directory on Windows), run the command in the /home/shihangbo/ directory to export logs on the device to the current directory and store logs in a folder named after the timestamp.</p>
<br/>
<p><font size=3><strong>Q: How do I change hyperparameters for calculating loss values during neural network training?</strong></font></p>
<p>A: Sorry, this function is not available yet. You can find the optimal hyperparameters by training, redefining an optimizer, and then training.</p>
<br/>
<p><font size=3><strong>Q: What should I do when error <code class="docutils literal notranslate"><span class="pre">error</span> <span class="pre">while</span> <span class="pre">loading</span> <span class="pre">shared</span> <span class="pre">libraries:</span> <span class="pre">libge_compiler.so:</span> <span class="pre">cannot</span> <span class="pre">open</span> <span class="pre">shared</span> <span class="pre">object</span> <span class="pre">file:</span> <span class="pre">No</span> <span class="pre">such</span> <span class="pre">file</span> <span class="pre">or</span> <span class="pre">directory</span></code> prompts during application running?</strong></font></p>
<p>A: While installing Ascend 310 AI Processor software packages，the <code class="docutils literal notranslate"><span class="pre">CANN</span></code> package should install the full-featured <code class="docutils literal notranslate"><span class="pre">toolkit</span></code> version instead of the <code class="docutils literal notranslate"><span class="pre">nnrt</span></code> version.</p>
<br/>
<p><font size=3><strong>Q: Why does context.set_ps_context(enable_ps=True) in model_zoo/official/cv/resnet/train.py in the MindSpore code have to be set before init?</strong></font></p>
<p>A: In MindSpore Ascend mode, if init is called first, then all processes will be allocated cards, but in parameter server training mode, the server does not need to allocate cards, then the worker and server will use the same card, resulting in an error: Hccl dependent tsd is not open.</p>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="data_processing.html" class="btn btn-neutral float-left" title="Data Processing" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="operators_compile.html" class="btn btn-neutral float-right" title="Operators Compile" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>