<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Feature Advice &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script><script src="_static/jquery.js"></script>
        <script src="_static/js/theme.js"></script><script src="_static/underscore.js"></script><script src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Inference" href="inference.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_processing.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="implement_problem.html">Implement Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="operators_compile.html">Operators Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="usage_migrate_3rd.html">Migration from a Third-party Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="precision_tuning.html">Precision Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_configure.html">Distributed Configure</a></li>
<li class="toctree-l1"><a class="reference internal" href="inference.html">Inference</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Feature Advice</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Feature Advice</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/feature_advice.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="feature-advice">
<h1>Feature Advice<a class="headerlink" href="#feature-advice" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.5/docs/mindspore/faq/source_en/feature_advice.md"><img alt="View Source On Gitee" src="https://gitee.com/mindspore/docs/raw/r1.5/resource/_static/logo_source_en.png" /></a></p>
<p><font size=3><strong>Q: Is the <code class="docutils literal notranslate"><span class="pre">input=np.random.uniform(...)</span></code> format fixed when the MindIR format is exported?</strong></font></p>
<p>A: The format is not fixed. This step is to create an input for constructing the network structure. You only need to input the correct <code class="docutils literal notranslate"><span class="pre">shape</span></code> in <code class="docutils literal notranslate"><span class="pre">export</span></code>. You can use <code class="docutils literal notranslate"><span class="pre">np.ones</span></code> and <code class="docutils literal notranslate"><span class="pre">np.zeros</span></code> to create an input.</p>
<br/>
<p><font size=3><strong>Q: What framework models and formats can be directly read by MindSpore? Can the PTH Model Obtained Through Training in PyTorch Be Loaded to the MindSpore Framework for Use?</strong></font></p>
<p>A: MindSpore uses protocol buffers (Protobuf) to store training parameters and cannot directly read framework models. A model file stores parameters and their values. You can use APIs of other frameworks to read parameters, obtain the key-value pairs of parameters, and load the key-value pairs to MindSpore. If you want to use the .ckpt file trained by a framework, read the parameters and then call the <code class="docutils literal notranslate"><span class="pre">save_checkpoint</span></code> API of MindSpore to save the file as a .ckpt file that can be read by MindSpore.</p>
<br/>
<p><font size=3><strong>Q: What should I do if a Protobuf memory limit error is reported during the process of using ckpt or exporting a model?</strong></font></p>
<p>A: When a single Protobuf data is too large, because Protobuf itself limits the size of the data stream, a memory limit error will be reported. At this time, the restriction can be lifted by setting the environment variable <code class="docutils literal notranslate"><span class="pre">PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python</span></code>.</p>
<br/>
<p><font size=3><strong>Q: What is the difference between the PyNative and Graph modes?</strong></font></p>
<p>A: Compare through the following four aspects:</p>
<ul class="simple">
<li><p>In terms of network execution：operators used in the two modes are the same. Therefore, when the same network and operators are executed in the two modes, the accuracy is the same. As Graph mode uses graph optimization, calculation graph sinking and other technologies, it has higher performance and efficiency in executing the network.</p></li>
<li><p>In terms of application scenarios,:Graph mode requires the network structure to be built at the beginning, and then the framework performs entire graph optimization and execution. This mode is suitable to scenarios where the network is fixed and high performance is required.</p></li>
<li><p>The two modes are supported on different hardware (such as <code class="docutils literal notranslate"><span class="pre">Ascend</span></code>, <code class="docutils literal notranslate"><span class="pre">GPU</span></code>, and <code class="docutils literal notranslate"><span class="pre">CPU</span></code>).</p></li>
<li><p>In terms of code debugging,:since operators are executed line by line in PyNative mode, you can directly debug the Python code and view the <code class="docutils literal notranslate"><span class="pre">/api</span></code> output or execution result of the corresponding operator at any breakpoint in the code. In Graph mode, the network is built but not executed in the constructor function. Therefore, you cannot obtain the output of the corresponding operator at breakpoints in the <code class="docutils literal notranslate"><span class="pre">construct</span></code> function. You can only specify operators and print their output results, and then view the results after the network execution is completed.</p></li>
</ul>
<br/>
<p><font size=3><strong>Q: Does MindSpore run only on Huawei <code class="docutils literal notranslate"><span class="pre">Ascend</span></code>?</strong></font></p>
<p>A: MindSpore supports Huawei <code class="docutils literal notranslate"><span class="pre">Ascend</span></code>, <code class="docutils literal notranslate"><span class="pre">GPUs</span></code>, and <code class="docutils literal notranslate"><span class="pre">CPUs</span></code>, and supports heterogeneous computing.</p>
<br/>
<p><font size=3><strong>Q: If MindSpore and PyTorch are installed in an environment, can the syntax of the two frameworks be used together in a Python file?</strong></font></p>
<p>A: You can use the two frameworks in a python file. Pay attention to the differences between types. For example, the tensor types created by the two frameworks are different, but the basic types of Python are general.</p>
<br/>
<p><font size=3><strong>Q: Can MindSpore read a TensorFlow checkpoint?</strong></font></p>
<p>A: The checkpoint format of MindSpore is different from that of TensorFlow. Although both use the Protocol Buffers, their definitions are different. Currently, MindSpore cannot read the TensorFlow or Pytorch checkpoints.</p>
<br/>
<p><font size=3><strong>Q: How do I use models trained by MindSpore on Ascend 310? Can they be converted to models used by HiLens Kit?</strong></font></p>
<p>A: Yes. HiLens Kit uses Ascend 310 as the inference core. Therefore, the two questions are essentially the same. Ascend 310 requires a dedicated OM model. Use MindSpore to export the ONNX or AIR model and convert it into an OM model supported by Ascend 310. For details, see <a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/en/r1.5/multi_platform_inference_ascend_310.html">Multi-platform Inference</a>.</p>
<br/>
<p><font size=3><strong>Q: Can MindSpore be converted to an AIR model on Ascend 310?</strong></font></p>
<p>A: An AIR model cannot be exported from the Ascend 310. You need to load a trained checkpoint on the Ascend 910, export an AIR model, and then convert the AIR model into an OM model for inference on the Ascend 310. For details about the Ascend 910 installation, see the MindSpore Installation Guide at <a class="reference external" href="https://www.mindspore.cn/install/en">here</a>.</p>
<br/>
<p><font size=3><strong>Q: Does MindSpore need a GPU computing unit? What hardware support is needed?</strong></font></p>
<p>A: MindSpore currently supports CPU, GPU, and Ascend. Currently, you can try out MindSpore through Docker images on laptops or in environments with GPUs. Some models in MindSpore Model Zoo support GPU-based training and inference, and other models are being improved. For distributed parallel training, MindSpore supports multi-GPU training. You can obtain the latest information from <a class="reference external" href="https://www.mindspore.cn/docs/note/en/r1.5/roadmap.html">Road Map</a> and <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.5/RELEASE.md#">project release notes</a>.</p>
<br/>
<p><font size=3><strong>Q: Does MindSpore have any limitation on the input size of a single Tensor for exporting and loading models?</strong></font></p>
<p>A: Due to hardware limitations of Protobuf, when exporting to AIR and ONNX formats, the size of model parameters cannot exceed 2G; when exporting to MINDIR format, there is no limit to the size of model parameters. MindSpore only supports MINDIR, and the size of a single Tensor cannot exceed 2G.</p>
<br/>
<p><font size=3><strong>Q: Does MindSpore have any plan on supporting other types of heterogeneous computing hardwares?</strong></font></p>
<p>A: MindSpore provides pluggable device management interface so that developer could easily integrate other types of heterogeneous computing hardwares like FPGA to MindSpore. We welcome more backend support in MindSpore from the community.</p>
<br/>
<p><font size=3><strong>Q: The recent announced programming language such as taichi got Python extensions that could be directly used as <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">taichi</span> <span class="pre">as</span> <span class="pre">ti</span></code>. Does MindSpore have similar support?</strong></font></p>
<p>A: MindSpore supports Python native expression via <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">mindspore</span></code>.</p>
<br/>
<p><font size=3><strong>Q: Does MindSpore support truncated gradient?</strong></font></p>
<p>A: Yes. For details, see <a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.5/official/nlp/transformer/src/transformer_for_train.py#L35">Definition and Usage of Truncated Gradient</a>.</p>
<br/>
<p><font size=3><strong>Q: What is the MindSpore IR design concept?</strong></font></p>
<p>A: Function expression: All expressions are functions, and differentiation and automatic parallel analysis are easy to implement without side effect. <code class="docutils literal notranslate"><span class="pre">JIT</span></code> compilation capability: The graph-based IR, control flow dependency, and data flow are combined to balance the universality and usability. Graphically complete IR: More conversion <code class="docutils literal notranslate"><span class="pre">Python</span></code> flexible syntax, including recursion, etc.</p>
<br/>
<p><font size=3><strong>Q: What are the advantages and features of MindSpore parallel model training?</strong></font></p>
<p>A: In addition to data parallelism, MindSpore distributed training also supports operator-level model parallelism. The operator input tensor can be tiled and parallelized. On this basis, automatic parallelism is supported. You only need to write a single-device script to automatically tile the script to multiple nodes for parallel execution.</p>
<br/>
<p><font size=3><strong>Q: How does MindSpore implement semantic collaboration and processing? Is the popular Formal Concept Analysis (FCA) used?</strong></font></p>
<p>A: The MindSpore framework does not support FCA. For semantic models, you can call third-party tools to perform FCA in the data preprocessing phase. MindSpore supports Python therefore <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">FCA</span></code> could do the trick.</p>
<br/>
<p><font size=3><strong>Q: Does MindSpore have any plan or consideration on the edge and device when the training and inference functions on the cloud are relatively mature?</strong></font></p>
<p>A: MindSpore is a unified cloud-edge-device training and inference framework. Edge has been considered in its design, so MindSpore can perform inference at the edge. The open-source version will support Ascend 310-based inference. The optimizations supported in the current inference stage include quantization, operator fusion, and memory overcommitment.</p>
<br/>
<p><font size=3><strong>Q: How does MindSpore support automatic parallelism?</strong></font></p>
<p>A: Automatic parallelism on CPUs and GPUs are being improved. You are advised to use the automatic parallelism feature on the Ascend 910 AI processor. Follow our open source community and apply for a MindSpore developer experience environment for trial use.</p>
<br/>
<p><font size=3><strong>Q: Does MindSpore have a module that can implement object detection algorithms as TensorFlow does?</strong></font></p>
<p>A: The TensorFlow’s object detection pipeline API belongs to the TensorFlow’s Model module. After MindSpore’s detection models are complete, similar pipeline APIs will be provided.</p>
<br/>
<p><font size=3><strong>Q: How do I perform transfer learning in PyNative mode?</strong></font></p>
<p>A: PyNative mode is compatible with transfer learning. For more tutorial information, see <a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/en/r1.5/cv_mobilenetv2_fine_tune.html#code-for-loading-a-pre-trained-model">Code for Loading a Pre-Trained Model</a>.</p>
<br/>
<p><font size=3><strong>Q: What is the difference between <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.5/model_zoo">MindSpore ModelZoo</a> and <a class="reference external" href="https://www.hiascend.com/software/modelzoo">Ascend ModelZoo</a>?</strong></font></p>
<p>A: <code class="docutils literal notranslate"><span class="pre">MindSpore</span> <span class="pre">ModelZoo</span></code> contains models only implemented by MindSpore. But these models support different devices including Ascend, GPU, CPU and mobile. <code class="docutils literal notranslate"><span class="pre">Ascend</span> <span class="pre">ModelZoo</span></code> contains models only running on Ascend which are implemented by different ML platform including MindSpore, PyTorch, TensorFlow and Caffe. You can refer to the corresponding <a class="reference external" href="https://gitee.com/ascend/modelzoo">gitee repository</a>.</p>
<p>As for the models implemented by MindSpore running on Ascend, these are maintained in <code class="docutils literal notranslate"><span class="pre">MindSpore</span> <span class="pre">ModelZoo</span></code>, and will be released to <code class="docutils literal notranslate"><span class="pre">Ascend</span> <span class="pre">ModelZoo</span></code> regularly.</p>
<br/>
<p><font size=3><strong>Q: What is the relationship between Ascend and NPU?</strong></font></p>
<p>A: NPU refers to a dedicated processor for neural network algorithms. Different companies have different NPU architectures. Ascend is an NPU processor based on the DaVinci architecture developed by Huawei.</p>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="inference.html" class="btn btn-neutral float-left" title="Inference" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>