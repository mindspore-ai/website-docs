<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Network Compilation &mdash; MindSpore master documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Operators Compile" href="operators_compile.html" />
    <link rel="prev" title="Implement Problem" href="implement_problem.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_processing.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="implement_problem.html">Implement Problem</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Network Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="operators_compile.html">Operators Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="usage_migrate_3rd.html">Migration from a Third-party Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="precision_tuning.html">Precision Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_configure.html">Distributed Configure</a></li>
<li class="toctree-l1"><a class="reference internal" href="inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="feature_advice.html">Feature Advice</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Network Compilation</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/network_compilation.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="network-compilation">
<h1>Network Compilation<a class="headerlink" href="#network-compilation" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.6/docs/mindspore/faq/source_en/network_compilation.md" target="_blank"><img src="https://gitee.com/mindspore/docs/raw/r1.6/resource/_static/logo_source_en.png"></a></p>
<p><font size=3><strong>Q: What can I do if an error “Create python object `&lt;class ‘mindspore.common.tensor.Tensor’&gt;` failed, only support create Cell or Primitive object.” is reported?</strong></font></p>
<p>A: Currently in graph mode, the <code class="docutils literal notranslate"><span class="pre">construct</span></code> function (or the function decorated by the <code class="docutils literal notranslate"><span class="pre">&#64;ms_function</span></code> decorator) only supports the construction of <code class="docutils literal notranslate"><span class="pre">Cell</span></code> and <code class="docutils literal notranslate"><span class="pre">Primitive</span> <span class="pre">object</span></code>. The construction of <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> is not supported, that is, the syntax <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">Tensor(args...)</span></code> is not supported.</p>
<p>If it is a constant tensor, please define it in the function <code class="docutils literal notranslate"><span class="pre">__init__</span></code>. If not, you can use the <code class="docutils literal notranslate"><span class="pre">&#64;constexpr</span></code> decorator to modify the function and generate the <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> in the function.</p>
<p>Please see the usage of <code class="docutils literal notranslate"><span class="pre">&#64;constexpr</span></code> in <a class="reference external" href="https://www.mindspore.cn/docs/api/en/r1.6/api_python/ops/mindspore.ops.constexpr.html">https://www.mindspore.cn/docs/api/en/r1.6/api_python/ops/mindspore.ops.constexpr.html</a>.</p>
<p>The constant <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> used on the network can be used as a network attribute and defined in <code class="docutils literal notranslate"><span class="pre">init</span></code>, that is, <code class="docutils literal notranslate"><span class="pre">self.x</span> <span class="pre">=</span> <span class="pre">Tensor(args...)</span></code>. Then the constant can be used in the <code class="docutils literal notranslate"><span class="pre">construct</span></code> function (or the function decorated by the <code class="docutils literal notranslate"><span class="pre">&#64;ms_function</span></code> decorator).</p>
<p>In the following example, <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> of <code class="docutils literal notranslate"><span class="pre">shape</span> <span class="pre">=</span> <span class="pre">(3,</span> <span class="pre">4),</span> <span class="pre">dtype</span> <span class="pre">=</span> <span class="pre">int64</span></code> is generated by <code class="docutils literal notranslate"><span class="pre">&#64;constexpr</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">generate_tensor</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)))</span>
</pre></div>
</div>
<br/>
<p><font size=3><strong>Q: What can I do if an error “‘self.xx’ should be defined in the class ‘<strong>init</strong>’ function.” is reported?</strong></font></p>
<p>A: If you want to assign for a class member such as <code class="docutils literal notranslate"><span class="pre">self.xx</span></code> in the function <code class="docutils literal notranslate"><span class="pre">construct</span></code>, <code class="docutils literal notranslate"><span class="pre">self.xx</span></code> must have been defined to a <a class="reference external" href="https://www.mindspore.cn/docs/api/en/r1.6/api_python/mindspore/mindspore.Parameter.html">Parameter</a> type firstly while the other types are not supported. But the local variable <code class="docutils literal notranslate"><span class="pre">xx</span></code> is not under the regulation.</p>
<br/>
<p><font size=3><strong>Q: What can I do if an error “This comparator ‘AnyValue’ is not supported. For statement ‘is’, only support compare with ‘None’, ‘False’ or ‘True’” is reported?</strong></font></p>
<p>A: For the syntax <code class="docutils literal notranslate"><span class="pre">is</span></code> or <code class="docutils literal notranslate"><span class="pre">is</span> <span class="pre">not</span></code>, currently <code class="docutils literal notranslate"><span class="pre">MindSpore</span></code> only supports comparisons with <code class="docutils literal notranslate"><span class="pre">True</span></code>, <code class="docutils literal notranslate"><span class="pre">False</span></code> and <code class="docutils literal notranslate"><span class="pre">None</span></code>. Other types, such as strings, are not supported.</p>
<br/>
<p><font size=3><strong>Q: What can I do if an error “MindSpore does not support comparison with operators more than one now, ops size =2” is reported?</strong></font></p>
<p>A: For comparison statements, <code class="docutils literal notranslate"><span class="pre">MindSpore</span></code> supports at most one operator. Please modify your code. For example, you can use <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">&lt;</span> <span class="pre">x</span> <span class="pre">and</span> <span class="pre">x</span> <span class="pre">&lt;</span> <span class="pre">3</span></code> to take the place of <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">&lt;</span> <span class="pre">x</span> <span class="pre">&lt;</span> <span class="pre">3</span></code>.</p>
<br/>
<p><font size=3><strong>Q: What can I do if an error “TypeError: The function construct need 1 positional argument and 0 default argument, but provided 2” is reported?</strong></font></p>
<p>A: When you call the instance of a network, the function <code class="docutils literal notranslate"><span class="pre">construct</span></code> will be executed. And the program will check the number of parameters required by the function <code class="docutils literal notranslate"><span class="pre">construct</span></code> and the number of parameters actually given. If they are not equal, the above exception will be thrown.
Please check your code to make sure they are equal.</p>
<br/>
<p><font size=3><strong>Q: What can I do if an error “Type Join Failed” or “Shape Join Failed” is reported?</strong></font></p>
<p>A: In the inference stage of front-end compilation, the abstract types of nodes, including <code class="docutils literal notranslate"><span class="pre">type</span></code> and <code class="docutils literal notranslate"><span class="pre">shape</span></code>, will be inferred. Common abstract types include <code class="docutils literal notranslate"><span class="pre">AbstractScalar</span></code>, <code class="docutils literal notranslate"><span class="pre">AbstractTensor</span></code>, <code class="docutils literal notranslate"><span class="pre">AbstractFunction</span></code>, <code class="docutils literal notranslate"><span class="pre">AbstractTuple</span></code>, <code class="docutils literal notranslate"><span class="pre">AbstractList</span></code>, etc. In some scenarios, such as multi-branch scenarios, the abstract types of the return values of different branches will be joined to infer the abstract type of the returned result. If these abstract types do not match, or <code class="docutils literal notranslate"><span class="pre">type</span></code>/<code class="docutils literal notranslate"><span class="pre">shape</span></code> are inconsistent, the above exception will be thrown.</p>
<p>When an error similar to “Type Join Failed: dtype1 = Float32, dtype2 = Float16” appears, it means that the data types are inconsistent, resulting in an exception when joining abstract. According to the provided data types and code line, the error can be quickly located. In addition, the specific abstract information and node information are provided in the error message. You can view the MindIR information through the <code class="docutils literal notranslate"><span class="pre">analyze_fail.dat</span></code> file to locate and solve the problem. For specific introduction of MindIR, please refer to <a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/en/r1.6/design/mindir.html">MindSpore IR (MindIR)</a>. The code sample is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">context</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cast</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">a</span> <span class="o">&gt;</span> <span class="n">b</span><span class="p">:</span>    <span class="c1"># The type of the two branches are inconsistent.</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>    <span class="c1"># shape: (2, 3, 4, 5), dtype:Float32</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">ms</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>    <span class="c1"># shape: (2, 3, 4, 5), dtype:Float16</span>

<span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">input_a</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">input_b</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">out_me</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_a</span><span class="p">,</span> <span class="n">input_b</span><span class="p">)</span>
</pre></div>
</div>
<p>The result is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>TypeError: The return values of different branches do not match. Type Join Failed: dtype1 = Float32, dtype2 = Float16. The abstract type of the return value of the current branch is AbstractTensor(shape: (2, 3, 4, 5), element: AbstractScalar(Type: Float16, Value: AnyValue, Shape: NoShape), value_ptr: 0x32ed00e0, value: AnyValue), and that of the previous branch is AbstractTensor(shape: (2, 3, 4, 5), element: AbstractScalar(Type: Float32, Value: AnyValue, Shape: NoShape), value_ptr: 0x32ed00e0, value: AnyValue). Please check the node construct.4:[CNode]5{[0]: [CNode]6}, true branch: ✓construct.2, false branch: ✗construct.3. trace:
In file test_join.py(14)/        if a &gt; b:/

The function call stack (See file &#39;analyze_fail.dat&#39; for more details):
# 0 In file test_join.py(14)
        if a &gt; b:
</pre></div>
</div>
<p>When an error similar to “Shape Join Failed: shape1 = (2, 3, 4, 5), shape2 = ()” appears, it means that the shapes are inconsistent, resulting in an exception when joining abstract. The code sample is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">context</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reducesum</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">a</span> <span class="o">&gt;</span> <span class="n">b</span><span class="p">:</span>    <span class="c1"># The shape of the two branches are inconsistent.</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>   <span class="c1"># shape: (2, 3, 4, 5),  dtype:Float32</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">reducesum</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>    <span class="c1"># shape:(), dype: Float32</span>

<span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">input_a</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">input_b</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_a</span><span class="p">,</span> <span class="n">input_b</span><span class="p">)</span>
</pre></div>
</div>
<p>The result is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>ValueError: The return values of different branches do not match. Shape Join Failed: shape1 = (2, 3, 4, 5), shape2 = (). The abstract type of the return value of the current branch is AbstractTensor(shape: (), element: AbstractScalar(Type: Float32, Value: AnyValue, Shape: NoShape), value_ptr: 0x239b5120, value: AnyValue), and that of the previous branch is AbstractTensor(shape: (2, 3, 4, 5), element: AbstractScalar(Type: Float32, Value: AnyValue, Shape: NoShape), value_ptr: 0x239b5120, value: AnyValue). Please check the node construct.4:[CNode]5{[0]: [CNode]6}, true branch: ✓construct.2, false branch: ✗construct.3. trace:
In file test_join1.py(14)/        if a &gt; b:/

The function call stack (See file &#39;analyze_fail.dat&#39; for more details):
# 0 In file test_join1.py(14)
        if a &gt; b:
</pre></div>
</div>
<p>When an error similar to “Type Join Failed: abstract type AbstractTensor can not join with AbstractTuple” appears, it means that the two abstract types are mismatched, resulting in an exception when joining abstract. The code sample is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">ms_function</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">])</span>
<span class="n">grad</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">GradOperation</span><span class="p">(</span><span class="n">get_by_list</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sens_param</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sens</span> <span class="o">=</span> <span class="mf">1.0</span>

<span class="k">def</span> <span class="nf">test_net</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span>

<span class="nd">@ms_function</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">join_fail</span><span class="p">():</span>
    <span class="n">sens_i</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Fill</span><span class="p">()(</span><span class="n">ops</span><span class="o">.</span><span class="n">DType</span><span class="p">()(</span><span class="n">x</span><span class="p">),</span> <span class="n">ops</span><span class="o">.</span><span class="n">Shape</span><span class="p">()(</span><span class="n">x</span><span class="p">),</span> <span class="n">sens</span><span class="p">)</span>     <span class="c1"># sens_i  is a Scalar Tensor with shape: (1), dtype:Float64, value:1.0</span>
    <span class="c1"># sens_i = (sens_i, sens_i)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">test_net</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sens_i</span><span class="p">)</span>     <span class="c1"># For test_net output with type tuple(Tensor, Tensor), sens_i wih same type are needed to calculate the gradient, but sens_i is a Tensor；Setting sens_i = (sens_i, sens_i) before grad can fix the problem.</span>
    <span class="k">return</span> <span class="n">a</span>

<span class="n">join_fail</span><span class="p">()</span>
</pre></div>
</div>
<p>The result is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>TypeError: mindspore/core/abstract/abstract_value.cc:48 AbstractTypeJoinLogging] Type Join Failed: abstract type AbstractTensor cannot not join with AbstractTuple. For more details, please refer to the FAQ at https://www.mindspore.cn. this: AbstractTensor(shape: (1), element: AbstractScalar(Type: Float64, Value: AnyValue, Shape: NoShape), value_ptr: 0x55f643f283d0, value: Tensor(shape=[1], dtype=Float64, value= [ 1.00000000e+00])), other: AbstractTuple(element[0]: AbstractTensor(shape: (1), element: AbstractScalar(Type: Float64, Value: AnyValue, Shape: NoShape), value_ptr: 0x55f64473a500, value: Tensor(shape=[1], dtype=Float64, value= [ 1.00000000e+00])), element[1]: AbstractTensor(shape: (1), element: AbstractScalar(Type: Float64, Value: AnyValue, Shape: NoShape), value_ptr: 0x55f6447042c0, value: Tensor(shape=[1], dtype=Float64, value= [ 2.00000000e+00]))). Please check the node test_net.2:test_net{[0]: test_net, [1]: test_net}. trace:
In file test_shape_join_failed.py(9)/def test_net(a, b):/
In file test_shape_join_failed.py(15)/ a = grad(test_net)(x, y, sens_i)/

The function call stack (See file &#39;analyze_fail.dat&#39; for more details):
# 0 In file test_shape_join_failed.py(15)
a = grad(test_net)(x, y, sens_i)
^
# 1 In file test_shape_join_failed.py(9)
def test_net(a, b):
^
</pre></div>
</div>
<br/>
<p><font size=3><strong>Q: What can I do if an error “The params of function ‘bprop’ of Primitive or Cell requires the forward inputs as well as the ‘out’ and ‘dout” is reported?</strong></font></p>
<p>A: The inputs of user-defined back propagation function <code class="docutils literal notranslate"><span class="pre">bprop</span></code> should contain all the inputs of the forward pass, <code class="docutils literal notranslate"><span class="pre">out</span></code> and <code class="docutils literal notranslate"><span class="pre">dout</span></code>. The example is as follow:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">BpropUserDefinedNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="nb">super</span><span class="p">(</span><span class="n">BpropUserDefinedNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">zeros_like</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ZerosLike</span><span class="p">()</span>

        <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>

        <span class="k">def</span> <span class="nf">bprop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">out</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</pre></div>
</div>
<br/>
<p><font size=3><strong>Q: What can I do if an error “There isn’t any branch that can be evaluated“ is reported?</strong></font>
When an error similar to “There isn’t any branch that can be evaluated” appears.
it means that there may be infinite recursion or loop in the code, which causes each branch of the if condition to be unable to deduce the correct type and dimension information.</p>
<p>The example is as follow:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">ms_function</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="kn">import</span> <span class="nn">mindspore.context</span> <span class="k">as</span> <span class="nn">context</span>
<span class="n">ZERO</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="n">ONE</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="nd">@ms_function</span>
<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">ZERO</span>
    <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">3</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="mi">5</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">y</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">z</span>

<span class="k">def</span> <span class="nf">test_endless</span><span class="p">():</span>
    <span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mi">5</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

</pre></div>
</div>
<p>the f(x)’s each branch of the if condition cannot deduce the correct type and dimension information</p>
<br/>
<p><font size=3><strong>Q: What can I do if an error “Exceed function call depth limit 1000” is reported?</strong></font></p>
<p>This indicates that there is an infinite recursive loop in the code, or the code is too complex, that caused the stack depth exceed.</p>
<p>At this time, you can set context.set_context(max_call_depth = value) to change the maximum depth of the stack, and consider simplifying the code logic or checking whether there is infinite recursion or loop in the code.</p>
<p>Otherwise, set max_call_depth can change the recursive depth of MindSpore, it may also cause exceed the maximum depth of the system stack and cause segment fault. At this time, you may also need to set the system stack depth.</p>
<br/>
<p><font size=3><strong>Q: Why report an error that ‘could not get source code’ and ‘Mindspore can not compile temporary source code in terminal. Please write source code to a python file and run the file.’?</strong></font></p>
<p>A: When compiling a network, MindSpore use <code class="docutils literal notranslate"><span class="pre">inspect.getsourcelines(self.fn)</span></code> to get the code file. If the network is the temporary code which edited in terminal, MindSpore will report an error as the title. It can be solved if writing the network to a python file.</p>
<br/>
<p><font size=3><strong>Q: Why report an error that ‘Corresponding forward node candidate:’ and ‘Corresponding code candidate:’?</strong></font></p>
<p>A: “Corresponding forward node candidate:” is the code in the associated forward network, indicating that the backpropagation operator corresponds to the forward code. “Corresponding code candidate:” means that the operator is fused by these code, and the separator “-” is used to distinguish different code.</p>
<p>For example：</p>
<ul>
<li><p>The operator FusionOp_BNTrainingUpdate_ReLUV2 reported an error and printed the following code:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Corresponding code candidate:
 - In file /home/workspace/mindspore/build/package/mindspore/nn/layer/normalization.py(212)/                return self.bn_train(x,/
   In file /home/workspace/mindspore/tests/st/tbe_networks/resnet.py(265)/        x = self.bn1(x)/
   In file /home/workspace/mindspore/build/package/mindspore/nn/wrap/cell_wrapper.py(109)/        out = self._backbone(data)/
   In file /home/workspace/mindspore/build/package/mindspore/nn/wrap/cell_wrapper.py(356)/        loss = self.network(*inputs)/
   In file /home/workspace/mindspore/build/package/mindspore/train/dataset_helper.py(98)/        return self.network(*outputs)/
 - In file /home/workspace/mindspore/tests/st/tbe_networks/resnet.py(266)/        x = self.relu(x)/
   In file /home/workspace/mindspore/build/package/mindspore/nn/wrap/cell_wrapper.py(109)/        out = self._backbone(data)/
   In file /home/workspace/mindspore/build/package/mindspore/nn/wrap/cell_wrapper.py(356)/        loss = self.network(*inputs)/
   In file /home/workspace/mindspore/build/package/mindspore/train/dataset_helper.py(98)/        return self.network(*outputs)/
</pre></div>
</div>
<p>The code call stack of the first separator points to ‘x = self.bn1(x)’ on line 265 in the network script file, and the code call stack of the second separator points to ‘x = self.bn1(x)’ in line 266 of the network script file. It can be seen that the operator FusionOp_BNTrainingUpdate_ReLUV2 is a fusion of these two lines of code.</p>
</li>
<li><p>The operator Conv2DBackpropFilter reported an error and printed the following code:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>In file /home/workspace/mindspore/build/package/mindspore/ops/_grad/grad_nn_ops.py(65)/        dw = filter_grad(dout, x, w_shape)/
Corresponding forward node candidate:
 - In file /home/workspace/mindspore/build/package/mindspore/nn/layer/conv.py(266)/        output = self.conv2d(x, self.weight)/
   In file /home/workspace/mindspore/tests/st/tbe_networks/resnet.py(149)/        out = self.conv1(x)/
   In file /home/workspace/mindspore/tests/st/tbe_networks/resnet.py(195)/        x = self.a(x)/
   In file /home/workspace/mindspore/tests/st/tbe_networks/resnet.py(270)/        x = self.layer2(x)/
   In file /home/workspace/mindspore/build/package/mindspore/nn/wrap/cell_wrapper.py(109)/        out = self._backbone(data)/
   In file /home/workspace/mindspore/build/package/mindspore/nn/wrap/cell_wrapper.py(356)/        loss = self.network(*inputs)/
   In file /home/workspace/mindspore/build/package/mindspore/train/dataset_helper.py(98)/        return self.network(*outputs)/
</pre></div>
</div>
<p>The first line is the corresponding source code of the operator. The operator is a bprop operator realized by MindSpore. The second line indicates that the operator has an associated forward node, and points to ‘out = self.conv1(x)’ on line 149 of the network script file. In summary, the operator Conv2DBackpropFilter is a bprop operator, and the corresponding forward node is a convolution operator.</p>
</li>
</ul>
<br/>
<p><font size=3><strong>Q: What is “JIT Fallback”? What can I do if an error “Should not use Python object in runtime” is reported?</strong></font></p>
<p>A: JIT Fallback is to realize the unification of static graph mode and dynamic graph mode from the perspective of static graph. With JIT Fallback feature, the static graph mode can support as many syntaxes in the dynamic graph mode as possible, so that the static graph mode can provide a syntax experience close to that of the dynamic graph mode. The environment variable switch of JIT Fallback is <code class="docutils literal notranslate"><span class="pre">DEV_ENV_ENABLE_FALLBACK</span></code>, and JIT Fallback is enabled by default.</p>
<p>When the errors “Should not use Python object in runtime” and “We suppose all nodes generated by JIT Fallback would not return to outside of graph” appear, it means that there is an incorrect syntax in the code. When using the JIT Fallback feature to process unsupported syntax expressions, corresponding nodes will be generated, which need to be inferred and executed at compile time. Otherwise, these nodes will throw an error when passed to the runtime. The current JIT Fallback conditionally supports some constant scenes in Graph mode, and it also needs to conform to MindSpore’s programming syntax. Please refer to <a class="reference external" href="https://www.mindspore.cn/docs/note/en/r1.6/static_graph_syntax_support.html">Static Graph Syntax Support</a>.</p>
<p>For example, when calling the third-party library NumPy, JIT Fallback supports the syntax of <code class="docutils literal notranslate"><span class="pre">np.add(x,</span> <span class="pre">y)</span></code> and <code class="docutils literal notranslate"><span class="pre">Tensor(np.add(x,</span> <span class="pre">y))</span></code>, but MindSpore does not support returning the NumPy type. Therefore, the program will report an error. The code sample is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>The result is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>RuntimeError: mindspore/ccsrc/pipeline/jit/validator.cc:139 ValidateValueNode] Should not use Python object in runtime, node: ValueNode&lt;InterpretedObject&gt; InterpretedObject: &#39;2&#39;

We suppose all nodes generated by JIT Fallback not return to outside of graph.

# In file test.py(9)
        out = np.add(x, y)
        ^
</pre></div>
</div>
<p>When there is an error related to JIT Fallback, please review the code syntax and modify it according to <a class="reference external" href="https://www.mindspore.cn/docs/note/en/r1.6/static_graph_syntax_support.html">Static Graph Syntax Support</a> and the provided code line. If you need to turn off JIT Fallback, you can use <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">DEV_ENV_ENABLE_FALLBACK=0</span></code>.</p>
<p><font size=3><strong>Q: What can I do if an error  “Operator[AddN]  input(kNumberTypeBool,kNumberTypeBool) output(kNumberTypeBool) is not support. This error means the current input type is not supported, please refer to the MindSpore doc for supported types.”</strong></font></p>
<p>A: Currently, Tensor with bool data type has weak support by MindSpore, only a few primitives support Tensor (bool).  If Tensor(bool) used in forward graph correctly, but get total derivative in the backward graph will using primitive <code class="docutils literal notranslate"><span class="pre">AddN</span></code> that not support Tensor(bool),  which will raise exception.</p>
<p>The example is as follow：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span><span class="p">,</span> <span class="n">ops</span><span class="p">,</span> <span class="n">ms_function</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dtype</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">save_graphs</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">save_graphs_path</span><span class="o">=</span><span class="s1">&#39;graph_path&#39;</span><span class="p">)</span>

<span class="nd">@ms_function</span>
<span class="k">def</span> <span class="nf">test_logic</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="ow">and</span> <span class="n">y</span>
    <span class="k">return</span> <span class="n">z</span> <span class="ow">and</span> <span class="n">x</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
<span class="n">grad</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">GradOperation</span><span class="p">(</span><span class="n">get_all</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">grad_net</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">test_logic</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">grad_net</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>The forward processing of the above code can be expressed as: <code class="docutils literal notranslate"><span class="pre">r</span> <span class="pre">=</span> <span class="pre">f(z,</span> <span class="pre">x),</span> <span class="pre">z</span> <span class="pre">=</span> <span class="pre">z(x,</span> <span class="pre">y)</span></code>, the corresponding full derivative formula is: <code class="docutils literal notranslate"><span class="pre">dr/dx</span> <span class="pre">=</span> <span class="pre">df/dz</span> <span class="pre">*</span> <span class="pre">dz/dx</span> <span class="pre">+</span> <span class="pre">df/dx</span></code>,  function<code class="docutils literal notranslate"><span class="pre">f(z,x)</span></code> and <code class="docutils literal notranslate"><span class="pre">z(x,y)</span></code> are primitive <code class="docutils literal notranslate"><span class="pre">and</span></code>; Primitive <code class="docutils literal notranslate"><span class="pre">and</span></code> in the forward graph supports Tensor (bool), but primitive <code class="docutils literal notranslate"><span class="pre">AddN</span></code> in the backward graph not supports Tensor(bool).  And the error cannot be mapped to a specific forward code line.</p>
<p>The result is as follows：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Traceback (most recent call last):
  File &quot;grad_fail.py&quot;, line 14, in &lt;module&gt;
    out = grad_net(x, y)
  File &quot;/usr/local/python3.7/lib/python3.7/site-packages/mindspore/common/api.py&quot;, line 307, in staging_specialize
    out = _MindsporeFunctionExecutor(func, ms_create_time, input_signature, process_obj)(*args)
  File &quot;/usr/local/python3.7/lib/python3.7/site-packages/mindspore/common/api.py&quot;, line 79, in wrapper
    results = fn(*arg, **kwargs)
  File &quot;/usr/local/python3.7/lib/python3.7/site-packages/mindspore/common/api.py&quot;, line 221, in __call__
    phase = self.compile(args_list, arg_names, parse_method)
  File &quot;/usr/local/python3.7/lib/python3.7/site-packages/mindspore/common/api.py&quot;, line 195, in compile
    self.enable_tuple_broaden)
TypeError: mindspore/ccsrc/runtime/device/cpu/kernel_select_cpu.cc:235 KernelNotSupportException] Operator[AddN]  input(kNumberTypeBool,kNumberTypeBool) output(kNumberTypeBool) is not support. This error means the current input type is not supported, please refer to the MindSpore doc for supported types.
Trace:
In file /usr/local/python3.7/lib/python3.7/site-packages/mindspore/ops/composite/multitype_ops/add_impl.py(287)/    return F.addn((x, y))/
</pre></div>
</div>
<p>If you encounter problems like this one, please remove the use of tensor (bool). In this example, replace tensor (bool) with bool can solve the problem.</p>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="implement_problem.html" class="btn btn-neutral float-left" title="Implement Problem" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="operators_compile.html" class="btn btn-neutral float-right" title="Operators Compile" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>