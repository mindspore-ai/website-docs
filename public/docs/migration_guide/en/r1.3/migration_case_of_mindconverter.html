<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Using Mindcoverter to Perform Migration &mdash; MindSpore master documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Migration Script" href="migration_script.html" />
    <link rel="prev" title="Network Script Development" href="script_development.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="preparation.html">Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="script_analysis.html">Network Script Analysis</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="script_development.html">Network Script Development</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Using Mindcoverter to Perform Migration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#environment-preparation">Environment Preparation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#onnx-model-export">ONNX Model Export</a></li>
<li class="toctree-l3"><a class="reference internal" href="#onnx-model-validation">ONNX Model Validation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#using-mindconverter-to-perform-model-script-and-weight-migration">Using MindConverter to Perform Model Script and Weight Migration</a></li>
<li class="toctree-l3"><a class="reference internal" href="#mindspore-model-validation">MindSpore Model Validation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#frequently-asked-questions">Frequently Asked Questions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="migration_script.html">Migration Script</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="neural_network_debug.html">Network Debugging</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance_optimization.html">Using Performance Profiling Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="inference.html">Inference Execution</a></li>
<li class="toctree-l1"><a class="reference internal" href="sample_code.html">Network Migration Debugging Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">FAQ</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="script_development.html">Network Script Development</a> &raquo;</li>
      <li>Using Mindcoverter to Perform Migration</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/migration_case_of_mindconverter.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="using-mindcoverter-to-perform-migration">
<h1>Using Mindcoverter to Perform Migration<a class="headerlink" href="#using-mindcoverter-to-perform-migration" title="Permalink to this headline"></a></h1>
<p>Translator: <a class="reference external" href="https://gitee.com/ChanJiatao">ChanJiatao</a></p>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.3/docs/mindspore/migration_guide/source_en/migration_case_of_mindconverter.md" target="_blank"><img src="https://gitee.com/mindspore/docs/raw/r1.3/resource/_static/logo_source.png"></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>To convert a PyTorch model to MindSpore model, you first need to export the PyTorch model to an ONNX model, and then use the MindConverter CLI tool to perform script and weight migration.</p>
<p>HuggingFace Transformers is a mainstream natural language processing third-party library under the PyTorch framework. We take BertForMaskedLM in Transformer as an example to demonstrate the migration process.</p>
</section>
<section id="environment-preparation">
<h2>Environment Preparation<a class="headerlink" href="#environment-preparation" title="Permalink to this headline"></a></h2>
<p>In this case, the following third-party Python libraries need to be installed:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span><span class="nv">torch</span><span class="o">==</span><span class="m">1</span>.5.1
pip<span class="w"> </span>install<span class="w"> </span><span class="nv">transformers</span><span class="o">==</span><span class="m">4</span>.2.2
pip<span class="w"> </span>install<span class="w"> </span><span class="nv">mindspore</span><span class="o">==</span><span class="m">1</span>.2.0
pip<span class="w"> </span>install<span class="w"> </span><span class="nv">mindinsight</span><span class="o">==</span><span class="m">1</span>.2.0
pip<span class="w"> </span>install<span class="w"> </span>onnx
</pre></div>
</div>
<blockquote>
<div><p>When installing ‘ONNX’ third-party libraries, you need to install ‘protobuf-compiler’ and ‘libprotoc-dev’ in advance. If there are no above two libraries, you can use the command ‘apt-get install protobuf-compiler libprotoc-dev’ to install them.</p>
</div></blockquote>
</section>
<section id="onnx-model-export">
<h2>ONNX Model Export<a class="headerlink" href="#onnx-model-export" title="Permalink to this headline"></a></h2>
<p>First instantiate the BertForMaskedLM in HuggingFace and the corresponding tokenizer (you need to download the model weight, vocabulary, model configuration and other data when you use it for the first time).</p>
<p>Regarding the use of HuggingFace, this article just gives a brief presentation. For detail, please refer to the <a class="reference external" href="https://huggingface.co/transformers/model_doc/bert.html">HuggingFace user documentation</a>.</p>
<p>The model can predict the words that are masked in the sentence.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers.models.bert</span> <span class="kn">import</span> <span class="n">BertForMaskedLM</span><span class="p">,</span> <span class="n">BertTokenizer</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertForMaskedLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>We use the model for reasoning and generate several sets of test cases to verify the correctness of the model migration.</p>
<p>Here we take a sentence as an example: <code class="docutils literal notranslate"><span class="pre">china</span> <span class="pre">is</span> <span class="pre">a</span> <span class="pre">poworful</span> <span class="pre">country,</span> <span class="pre">its</span> <span class="pre">capital</span> <span class="pre">is</span> <span class="pre">beijing.</span></code></p>
<p>We mask <code class="docutils literal notranslate"><span class="pre">beijing</span></code>, and then input <code class="docutils literal notranslate"><span class="pre">china</span> <span class="pre">is</span> <span class="pre">a</span> <span class="pre">poworful</span> <span class="pre">country,</span> <span class="pre">its</span> <span class="pre">capital</span> <span class="pre">is</span> <span class="pre">[MASK].</span></code> to the model, the expected output of the model should be <code class="docutils literal notranslate"><span class="pre">beijing</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;china is a poworful country, its capital is [MASK].&quot;</span>
<span class="n">tokenized_sentence</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

<span class="n">mask_idx</span> <span class="o">=</span> <span class="n">tokenized_sentence</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="s2">&quot;[MASK]&quot;</span><span class="p">))</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">tokenized_sentence</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]])</span>
<span class="n">attention_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">tokenized_sentence</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]])</span>
<span class="n">token_type_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">tokenized_sentence</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">]])</span>

<span class="c1"># Get [MASK] token id.</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MASK TOKEN id: </span><span class="si">{</span><span class="n">mask_idx</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Tokens: </span><span class="si">{</span><span class="n">input_ids</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Attention mask: </span><span class="si">{</span><span class="n">attention_mask</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Token type ids: </span><span class="si">{</span><span class="n">token_type_ids</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">input_ids</span><span class="p">),</span>
                        <span class="n">attention_mask</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">),</span>
                        <span class="n">token_type_ids</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">token_type_ids</span><span class="p">))</span>
    <span class="n">predicted_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">mask_idx</span><span class="p">])</span>
    <span class="n">predicted_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">([</span><span class="n">predicted_index</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Pred id: </span><span class="si">{</span><span class="n">predicted_index</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Pred token: </span><span class="si">{</span><span class="n">predicted_token</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">predicted_token</span> <span class="o">==</span> <span class="s2">&quot;beijing&quot;</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>MASK TOKEN id: 12
Tokens: [[  101  2859  2003  1037 23776 16347  5313  2406  1010  2049  3007  2003
    103  1012   102]]
Attention mask: [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]
Token type ids: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]
Pred id: 7211
Pred token: beijing
</pre></div>
</div>
<p>HuggingFace provides tools to export ONNX models. The following methods can be used to export HuggingFace’s pre-trained models as ONNX models:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="nn">transformers.convert_graph_to_onnx</span> <span class="kn">import</span> <span class="n">convert</span>

<span class="c1"># Exported onnx model path.</span>
<span class="n">saved_onnx_path</span> <span class="o">=</span> <span class="s2">&quot;./exported_bert_base_uncased/bert_base_uncased.onnx&quot;</span>
<span class="n">convert</span><span class="p">(</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">Path</span><span class="p">(</span><span class="n">saved_onnx_path</span><span class="p">),</span> <span class="mi">11</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Creating folder exported_bert_base_uncased
Using framework PyTorch: 1.5.1+cu101
Found input input_ids with shape: {0: &#39;batch&#39;, 1: &#39;sequence&#39;}
Found input token_type_ids with shape: {0: &#39;batch&#39;, 1: &#39;sequence&#39;}
Found input attention_mask with shape: {0: &#39;batch&#39;, 1: &#39;sequence&#39;}
Found output output_0 with shape: {0: &#39;batch&#39;, 1: &#39;sequence&#39;}
Ensuring inputs are in correct order
position_ids is not present in the generated input list.
Generated inputs order: [&#39;input_ids&#39;, &#39;attention_mask&#39;, &#39;token_type_ids&#39;]
</pre></div>
</div>
<p>According to the print information, we can see that the exported ONNX model has 3 input nodes: <code class="docutils literal notranslate"><span class="pre">input_ids</span></code>, <code class="docutils literal notranslate"><span class="pre">token_type_ids</span></code>, <code class="docutils literal notranslate"><span class="pre">attention_mask</span></code>, and the corresponding input axis. The output node has an <code class="docutils literal notranslate"><span class="pre">output_0</span></code>.</p>
<p>So far, the ONNX model is successfully exported, and then the accuracy of the exported ONNX model is verified (the ONNX model export process is executed on the ARM environment, and the user may need to compile and install the PyTorch and Transformers third-party library).</p>
</section>
<section id="onnx-model-validation">
<h2>ONNX Model Validation<a class="headerlink" href="#onnx-model-validation" title="Permalink to this headline"></a></h2>
<p>We still use the sentence from the PyTorch model for reasoning <code class="docutils literal notranslate"><span class="pre">china</span> <span class="pre">is</span> <span class="pre">a</span> <span class="pre">poworful</span> <span class="pre">country,</span> <span class="pre">its</span> <span class="pre">capital</span> <span class="pre">is</span> <span class="pre">[MASK].</span></code> as input to observe whether the ONNX model performs as expected.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">onnx</span>
<span class="kn">import</span> <span class="nn">onnxruntime</span> <span class="k">as</span> <span class="nn">ort</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">saved_onnx_path</span><span class="p">)</span>
<span class="n">sess</span> <span class="o">=</span> <span class="n">ort</span><span class="o">.</span><span class="n">InferenceSession</span><span class="p">(</span><span class="nb">bytes</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">SerializeToString</span><span class="p">()))</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
    <span class="n">output_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">input_feed</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">input_ids</span><span class="p">,</span>
                <span class="s2">&quot;attention_mask&quot;</span><span class="p">:</span> <span class="n">attention_mask</span><span class="p">,</span>
                <span class="s2">&quot;token_type_ids&quot;</span><span class="p">:</span> <span class="n">token_type_ids</span><span class="p">}</span>
<span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">predicted_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">mask_idx</span><span class="p">])</span>
<span class="n">predicted_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">([</span><span class="n">predicted_index</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ONNX Pred id: </span><span class="si">{</span><span class="n">predicted_index</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ONNX Pred token: </span><span class="si">{</span><span class="n">predicted_token</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">predicted_token</span> <span class="o">==</span> <span class="s2">&quot;beijing&quot;</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>ONNX Pred id: 7211
ONNX Pred token: beijing
</pre></div>
</div>
<p>As you can see, the exported ONNX model functions are exactly the same as the original PyTorch model. Next, you can use MindConverter to perform script and weight migration!</p>
</section>
<section id="using-mindconverter-to-perform-model-script-and-weight-migration">
<h2>Using MindConverter to Perform Model Script and Weight Migration<a class="headerlink" href="#using-mindconverter-to-perform-model-script-and-weight-migration" title="Permalink to this headline"></a></h2>
<p>When MindConverter performs model conversion, it needs to specify the model path (<code class="docutils literal notranslate"><span class="pre">--model_file</span></code>), input node (<code class="docutils literal notranslate"><span class="pre">--input_nodes</span></code>), input node size (<code class="docutils literal notranslate"><span class="pre">--shape</span></code>), and output node (<code class="docutils literal notranslate"><span class="pre">--output_nodes</span></code>).</p>
<p>The generated script output path (<code class="docutils literal notranslate"><span class="pre">--output</span></code>) and conversion report path (<code class="docutils literal notranslate"><span class="pre">--report</span></code>) are optional parameters. The default is the output directory under the current path. If the output directory does not exist, it will be created automatically.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mindconverter<span class="w"> </span>--model_file<span class="w"> </span>./exported_bert_base_uncased/bert_base_uncased.onnx<span class="w"> </span>--shape<span class="w"> </span><span class="m">1</span>,128<span class="w"> </span><span class="m">1</span>,128<span class="w"> </span><span class="m">1</span>,128<span class="w">  </span><span class="se">\</span>
<span class="w">               </span>--input_nodes<span class="w"> </span>input_ids<span class="w"> </span>token_type_ids<span class="w"> </span>attention_mask<span class="w">  </span><span class="se">\</span>
<span class="w">               </span>--output_nodes<span class="w"> </span>output_0<span class="w">  </span><span class="se">\</span>
<span class="w">               </span>--output<span class="w"> </span>./converted_bert_base_uncased<span class="w">  </span><span class="se">\</span>
<span class="w">               </span>--report<span class="w"> </span>./converted_bert_base_uncased
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>MindConverter: conversion is completed.
</pre></div>
</div>
<p><strong>Seeing “MindConverter: conversion is completed.” means that the model has been successfully converted!</strong></p>
<p>After the conversion is completed, the following files are generated in this directory: -Model definition script(suffix is .py) -Weight ckpt file(suffix is .ckpt) -Weight mapping before and after migration(suffix is .json) -Conversion report(suffix is .txt).</p>
<p>Check the conversion result with the ls command.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ls<span class="w"> </span>./converted_bert_base_uncased
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>bert_base_uncased.ckpt  report_of_bert_base_uncased.txt
bert_base_uncased.py    weight_map_of_bert_base_uncased.json
</pre></div>
</div>
<p>You can see that all files have been generated.</p>
<p>After the migration is complete, we will verify the accuracy of the model after the migration.</p>
</section>
<section id="mindspore-model-validation">
<h2>MindSpore Model Validation<a class="headerlink" href="#mindspore-model-validation" title="Permalink to this headline"></a></h2>
<p>We still use <code class="docutils literal notranslate"><span class="pre">china</span> <span class="pre">is</span> <span class="pre">a</span> <span class="pre">poworful</span> <span class="pre">country,</span> <span class="pre">its</span> <span class="pre">capital</span> <span class="pre">is</span> <span class="pre">[MASK].</span></code> as input to observe whether the model performance after migration meets expectations.</p>
<p>Since the tool needs to freeze the size of the model during conversion, when using MindSpore for reasoning and verification, the sentence needs to be filled (Pad) to a fixed length. The sentence can be filled with the following function.</p>
<p>In reasoning, the sentence length must be less than the maximum sentence length during conversion (here our longest sentence length is 128, which is specified by <code class="docutils literal notranslate"><span class="pre">--shape</span> <span class="pre">1,128</span></code> in the conversion phase).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">padding</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">,</span> <span class="n">token_type_ids</span><span class="p">,</span> <span class="n">target_len</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
    <span class="n">length</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">target_len</span> <span class="o">-</span> <span class="n">length</span><span class="p">):</span>
        <span class="n">input_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">attn_mask</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">token_type_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">input_ids</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">attn_mask</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">token_type_ids</span><span class="p">])</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">converted_bert_base_uncased.bert_base_uncased</span> <span class="kn">import</span> <span class="n">Model</span> <span class="k">as</span> <span class="n">MsBert</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">load_checkpoint</span><span class="p">,</span> <span class="n">load_param_into_net</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">Tensor</span>


<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">)</span>
<span class="n">padded_input_ids</span><span class="p">,</span> <span class="n">padded_attention_mask</span><span class="p">,</span> <span class="n">padded_token_type</span> <span class="o">=</span> <span class="n">padding</span><span class="p">(</span><span class="n">tokenized_sentence</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">],</span>
                                                                     <span class="n">tokenized_sentence</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">],</span>
                                                                     <span class="n">tokenized_sentence</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">],</span>
                                                                     <span class="n">target_len</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
<span class="n">padded_input_ids</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">padded_input_ids</span><span class="p">)</span>
<span class="n">padded_attention_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">padded_attention_mask</span><span class="p">)</span>
<span class="n">padded_token_type</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">padded_token_type</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MsBert</span><span class="p">()</span>
<span class="n">param_dict</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span><span class="s2">&quot;./converted_bert_base_uncased/bert_base_uncased.ckpt&quot;</span><span class="p">)</span>
<span class="n">not_load_params</span> <span class="o">=</span> <span class="n">load_param_into_net</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">padded_attention_mask</span><span class="p">,</span> <span class="n">padded_input_ids</span><span class="p">,</span> <span class="n">padded_token_type</span><span class="p">)</span>

<span class="k">assert</span> <span class="ow">not</span> <span class="n">not_load_params</span>

<span class="n">predicted_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">][</span><span class="n">mask_idx</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ONNX Pred id: </span><span class="si">{</span><span class="n">predicted_index</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">predicted_index</span> <span class="o">==</span> <span class="mi">7211</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>ONNX Pred id: 7211
</pre></div>
</div>
<p>So far, the script and weight migration using MindConverter is complete.</p>
<p>Users can perform training, inference, and deployment scripts according to application scenarios to implement personal business logic.</p>
</section>
<section id="frequently-asked-questions">
<h2>Frequently Asked Questions<a class="headerlink" href="#frequently-asked-questions" title="Permalink to this headline"></a></h2>
<p><strong>Q: How to modify the shape specifications such as Batch size and Sequence length of the migrated script so that the model can support data inference and training of any size?</strong></p>
<p>A: After the migration, the script has shape restrictions, which are usually caused by the Reshape operator or other operators that involve changes in the tensor arrangement. Taking the above Bert migration as an example, first create two global variables to represent the expected batch size and sentence length, then modify the target size of the Reshape operation and replace it with the corresponding batch size and sentence length global variables.</p>
<p><strong>Q: The definition of the class name in the generated script does not conform to the developer’s habits, such as “class Module0(nn.Cell)”. Will manual modification affect the weight loading after conversion?</strong></p>
<p>A: The weight loading is only related to the variable name and class structure, so the class name can be modified without affecting the weight loading. If the structure of the class needs to be adjusted, the corresponding weight names needs to be modified synchronously to adapt to the structure of the migrated model.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="script_development.html" class="btn btn-neutral float-left" title="Network Script Development" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="migration_script.html" class="btn btn-neutral float-right" title="Migration Script" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>