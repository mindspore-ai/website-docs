<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Migration Script &mdash; MindSpore master documentation</title><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Network Debugging" href="neural_network_debug.html" />
    <link rel="prev" title="Network Script Development" href="script_development.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="preparation.html">Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="script_analysis.html">Network Script Analysis</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="script_development.html">Network Script Development</a><ul class="current">
<li class="toctree-l2"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.5/migrate_3rd_scripts_mindconverter.html">Using Mindcoverter to Perform Migration</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Migration Script</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#migrating-the-tensorflow-script-to-mindspore">Migrating the TensorFlow Script to MindSpore</a></li>
<li class="toctree-l3"><a class="reference internal" href="#migrating-the-pytorch-script-to-mindspore">Migrating the PyTorch Script to MindSpore</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="neural_network_debug.html">Network Debugging</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance_optimization.html">Using Performance Profiling Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="inference.html">Inference Execution</a></li>
<li class="toctree-l1"><a class="reference internal" href="sample_code.html">Network Migration Debugging Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_mapping.html">API Mapping</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="script_development.html">Network Script Development</a> &raquo;</li>
      <li>Migration Script</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/migration_script.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="migration-script">
<h1>Migration Script<a class="headerlink" href="#migration-script" title="Permalink to this headline"></a></h1>
<p>Translator: <a class="reference external" href="https://gitee.com/zhangxiaoxiao16">zhangxiaoxiao</a></p>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.5/docs/mindspore/migration_guide/source_en/migration_script.md" target="_blank"><img src="https://gitee.com/mindspore/docs/raw/r1.5/resource/_static/logo_source_en.png"></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>This document describes how to migrate network scripts from the TensorFlow or PyTorch framework to MindSpore。</p>
</section>
<section id="migrating-the-tensorflow-script-to-mindspore">
<h2>Migrating the TensorFlow Script to MindSpore<a class="headerlink" href="#migrating-the-tensorflow-script-to-mindspore" title="Permalink to this headline"></a></h2>
<p>Migrate scripts by reading the TensorBoard graphs。</p>
<ol class="arabic">
<li><p>The <a class="reference external" href="https://arxiv.org/pdf/1505.07427v4.pdf">PoseNet</a> implemented by TensorFlow is used as an example to show how to use TensorBoard to read graphs, write mindspore code, and migrate <a class="reference external" href="https://github.com/kentsommer/tensorflow-posenet">TensorFlow Models</a> to MindSpore.</p>
<blockquote>
<div><p>The PoseNet code mentioned here is based on Python2. You need to make some syntax changes to run on Python3. Details are not described here.</p>
</div></blockquote>
</li>
<li><p>Rewrite the code, use <code class="docutils literal notranslate"><span class="pre">tf.summary</span></code> interface to save the log required by TensorBoard, start TensorBoard.</p></li>
<li><p>The following figure shows the opened TensorBoard, it’s for reference only. The figure displayed on TensorBoard may vary in the log generation mode.</p>
<p><img alt="PoseNet TensorBoard" src="_images/pic1.png" /></p>
</li>
<li><p>Find the placeholder of three inputs, view the figure and read the code, the second and third inputs are used only for loss calculation.</p>
<p><img alt="PoseNet Placeholder" src="_images/pic3.png" /></p>
<p><img alt="PoseNet Placeholder_1 Placeholder_2" src="_images/pic2.png" /></p>
<p><img alt="PoseNet script input1 2 3" src="_images/pic4.png" /></p>
<p>So far, we can preliminarily follow three steps to construct a network model:</p>
<p>Step 1, the first three inputs of the network will compute six outputs in the backbone.</p>
<p>Step 2, the result of step 1, the second and third inputs are used to calculate the loss in the loss subnet.</p>
<p>Step 3, construct the reverse network by using <code class="docutils literal notranslate"><span class="pre">TrainOneStepCell</span></code> automatic differentiation. Use the Adam optimizer and attributes provided by TensorFlow to write the corresponding Mindspore optimizer to update parameters. The network backbone can write as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">TrainOneStepCell</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">Adam</span>

<span class="c1"># combine backbone and loss</span>
<span class="k">class</span> <span class="nc">PoseNetLossCell</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">backbone</span><span class="p">,</span> <span class="n">loss</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PoseNetLossCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pose_net</span> <span class="o">=</span> <span class="n">backbone</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_1</span><span class="p">,</span> <span class="n">input_2</span><span class="p">,</span> <span class="n">input_3</span><span class="p">):</span>
        <span class="n">p1_x</span><span class="p">,</span> <span class="n">p1_q</span><span class="p">,</span> <span class="n">p2_x</span><span class="p">,</span> <span class="n">p2_q</span><span class="p">,</span> <span class="n">p3_x</span><span class="p">,</span> <span class="n">p3_q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">poss_net</span><span class="p">(</span><span class="n">input_1</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">p1_x</span><span class="p">,</span> <span class="n">p1_q</span><span class="p">,</span> <span class="n">p2_x</span><span class="p">,</span> <span class="n">p2_q</span><span class="p">,</span> <span class="n">p3_x</span><span class="p">,</span> <span class="n">p3_q</span><span class="p">,</span> <span class="n">input_2</span><span class="p">,</span> <span class="n">input_3</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>

<span class="c1"># define backbone</span>
<span class="k">class</span> <span class="nc">PoseNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PoseNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;do something with input_1, output num 6&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">p1_x</span><span class="p">,</span> <span class="n">p1_q</span><span class="p">,</span> <span class="n">p2_x</span><span class="p">,</span> <span class="n">p2_q</span><span class="p">,</span> <span class="n">p3_x</span><span class="p">,</span> <span class="n">p3_q</span>

<span class="c1"># define loss</span>
<span class="k">class</span> <span class="nc">PoseNetLoss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PoseNetLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p1_x</span><span class="p">,</span> <span class="n">p1_q</span><span class="p">,</span> <span class="n">p2_x</span><span class="p">,</span> <span class="n">p2_q</span><span class="p">,</span> <span class="n">p3_x</span><span class="p">,</span> <span class="n">p3_q</span><span class="p">,</span> <span class="n">poses_x</span><span class="p">,</span> <span class="n">poses_q</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;do something to calc loss&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">loss</span>

<span class="c1"># define network</span>
<span class="n">backbone</span> <span class="o">=</span> <span class="n">PoseNet</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">PoseNetLoss</span><span class="p">()</span>
<span class="n">net_with_loss</span> <span class="o">=</span> <span class="n">PoseNetLossCell</span><span class="p">(</span><span class="n">backbone</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">net_with_loss</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">net_with_grad</span> <span class="o">=</span> <span class="n">TrainOneStepCell</span><span class="p">(</span><span class="n">net_with_loss</span><span class="p">,</span> <span class="n">opt</span><span class="p">)</span>


</pre></div>
</div>
</li>
<li><p>Next, let’s implement the computing logic in the backbone.</p>
<p>The first input passes through a subgraph named conv1, the computing logic can be obtained by looking at the following figure:</p>
<p><img alt="PoseNet conv1" src="_images/pic5.png" /></p>
<p>input-&gt;Conv2D-&gt;BiasAdd-&gt;ReLU, although the operator name after BiasAdd is conv1, it actually executes ReLU.</p>
<p><img alt="PoseNet Conv1 conv1 relu" src="_images/pic6.png" /></p>
<p>Then, the first subgraph conv1 can be defined as follows. Specific parameters are aligned with those in the original project:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Conv1</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Conv1</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">Conv2d</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">ReLU</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<p>By observing the TensorBoard figure and code, we find that the subnet of the conv type defined in the original TensorFlow project can be copied as the subnet of MindSpore to reduce repeated code.</p>
<p>TensorFlow project conv subnet definition:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">conv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
             <span class="nb">input</span><span class="p">,</span>
             <span class="n">k_h</span><span class="p">,</span>
             <span class="n">k_w</span><span class="p">,</span>
             <span class="n">c_o</span><span class="p">,</span>
             <span class="n">s_h</span><span class="p">,</span>
             <span class="n">s_w</span><span class="p">,</span>
             <span class="n">name</span><span class="p">,</span>
             <span class="n">relu</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
             <span class="n">padding</span><span class="o">=</span><span class="n">DEFAULT_PADDING</span><span class="p">,</span>
             <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
             <span class="n">biased</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="c1"># Verify that the padding is acceptable</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">validate_padding</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
        <span class="c1"># Get the number of channels in the input</span>
        <span class="n">c_i</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># Verify that the grouping parameter is valid</span>
        <span class="k">assert</span> <span class="n">c_i</span> <span class="o">%</span> <span class="n">group</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="k">assert</span> <span class="n">c_o</span> <span class="o">%</span> <span class="n">group</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="c1"># Convolution for a given input and kernel</span>
        <span class="n">convolve</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">s_h</span><span class="p">,</span> <span class="n">s_w</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">name</span><span class="p">)</span> <span class="k">as</span> <span class="n">scope</span><span class="p">:</span>
            <span class="n">kernel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">make_var</span><span class="p">(</span><span class="s1">&#39;weights&#39;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">k_h</span><span class="p">,</span> <span class="n">k_w</span><span class="p">,</span> <span class="n">c_i</span> <span class="o">/</span> <span class="n">group</span><span class="p">,</span> <span class="n">c_o</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">group</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="c1"># This is the common-case. Convolve the input without any further complications.</span>
                <span class="n">output</span> <span class="o">=</span> <span class="n">convolve</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Split the input into groups and then convolve each of them independently</span>
                <span class="n">input_groups</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
                <span class="n">kernel_groups</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="n">kernel</span><span class="p">)</span>
                <span class="n">output_groups</span> <span class="o">=</span> <span class="p">[</span><span class="n">convolve</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">input_groups</span><span class="p">,</span> <span class="n">kernel_groups</span><span class="p">)]</span>
                <span class="c1"># Concatenate the groups</span>
                <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">output_groups</span><span class="p">)</span>
            <span class="c1"># Add the biases</span>
            <span class="k">if</span> <span class="n">biased</span><span class="p">:</span>
                <span class="n">biases</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">make_var</span><span class="p">(</span><span class="s1">&#39;biases&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">c_o</span><span class="p">])</span>
                <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">biases</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">relu</span><span class="p">:</span>
                <span class="c1"># ReLU non-linearity</span>
                <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">scope</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
<p>The Mindspore subnet is defined as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">Conv2d</span><span class="p">,</span> <span class="n">ReLU</span>

<span class="k">class</span> <span class="nc">ConvReLU</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">channel_in</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">channel_out</span><span class="p">,</span>  <span class="n">strides</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ConvReLU</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">Conv2d</span><span class="p">(</span><span class="n">channel_in</span><span class="p">,</span> <span class="n">channel_out</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">has_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">ReLU</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<p>Based on the data flow direction and operator attributes in the TensorBoard, the backbone computing logic can be written as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">MaxPool2d</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>


<span class="k">class</span> <span class="nc">LRN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">radius</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LRN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lrn</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">LRN</span><span class="p">(</span><span class="n">radius</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lrn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">PoseNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PoseNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pool1</span> <span class="o">=</span> <span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;SAME&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">LRN</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">2e-05</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction2</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">192</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">LRN</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">2e-05</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pool2</span> <span class="o">=</span> <span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;SAME&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp1_reduction1</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">192</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">96</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp1_out1</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">96</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp1_reduction2</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">192</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp1_out2</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp1_pool</span> <span class="o">=</span> <span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;SAME&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp1_out3</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">192</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp1_out0</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">192</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">concat</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp2_reduction1</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp2_out1</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">192</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp2_reduction2</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp2_out2</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">96</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp2_pool</span> <span class="o">=</span> <span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;SAME&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp2_out3</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp2_out0</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp3_in</span> <span class="o">=</span> <span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;SAME&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp3_reduction1</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">480</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">96</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp3_out1</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">96</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">208</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp3_reduction2</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">480</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp3_out2</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp3_pool</span> <span class="o">=</span> <span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;SAME&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp3_out3</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">480</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp3_out0</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">480</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">192</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;etc&quot;&quot;&quot;</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;...&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;do something with input_1, output num 6&quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">input_1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">pool2</span> <span class="o">=</span> <span class="n">x</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">icp1_reduction1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">icp1_out1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">icp1_out1</span> <span class="o">=</span> <span class="n">x</span>

        <span class="n">icp1_reduction2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">icp1_reduction2</span><span class="p">(</span><span class="n">pool2</span><span class="p">)</span>
        <span class="n">icp1_out2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">icp1_out2</span><span class="p">(</span><span class="n">icp1_reduction2</span><span class="p">)</span>

        <span class="n">icp1_pool</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">icp1_pool</span><span class="p">(</span><span class="n">pool2</span><span class="p">)</span>
        <span class="n">icp1_out3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">icp1_out3</span><span class="p">(</span><span class="n">icp1_pool</span><span class="p">)</span>

        <span class="n">icp1_out0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">icp1_out0</span><span class="p">(</span><span class="n">pool2</span><span class="p">)</span>

        <span class="n">icp2_in</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">concat</span><span class="p">((</span><span class="n">icp1_out0</span><span class="p">,</span> <span class="n">icp1_out1</span><span class="p">,</span> <span class="n">icp1_out2</span><span class="p">,</span> <span class="n">icp1_out3</span><span class="p">))</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;etc&quot;&quot;&quot;</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;...&quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="n">p1_x</span><span class="p">,</span> <span class="n">p1_q</span><span class="p">,</span> <span class="n">p2_x</span><span class="p">,</span> <span class="n">p2_q</span><span class="p">,</span> <span class="n">p3_x</span><span class="p">,</span> <span class="n">p3_q</span>
</pre></div>
</div>
<p>Correspondingly, the loss computing logic may be written as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">PoseNetLoss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PoseNetLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sub</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Sub</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">square</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Square</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sqrt</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Sqrt</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p1_x</span><span class="p">,</span> <span class="n">p1_q</span><span class="p">,</span> <span class="n">p2_x</span><span class="p">,</span> <span class="n">p2_q</span><span class="p">,</span> <span class="n">p3_x</span><span class="p">,</span> <span class="n">p3_q</span><span class="p">,</span> <span class="n">poses_x</span><span class="p">,</span> <span class="n">poses_q</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;do something to calc loss&quot;&quot;&quot;</span>
        <span class="n">l1_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">p1_x</span><span class="p">,</span> <span class="n">poses_x</span><span class="p">))))</span> <span class="o">*</span> <span class="mf">0.3</span>
        <span class="n">l1_q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">p1_q</span><span class="p">,</span> <span class="n">poses_q</span><span class="p">))))</span> <span class="o">*</span> <span class="mi">150</span>
        <span class="n">l2_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">p2_x</span><span class="p">,</span> <span class="n">poses_x</span><span class="p">))))</span> <span class="o">*</span> <span class="mf">0.3</span>
        <span class="n">l2_q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">p2_q</span><span class="p">,</span> <span class="n">poses_q</span><span class="p">))))</span> <span class="o">*</span> <span class="mi">150</span>
        <span class="n">l3_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">p3_x</span><span class="p">,</span> <span class="n">poses_x</span><span class="p">))))</span> <span class="o">*</span> <span class="mi">1</span>
        <span class="n">l3_q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">p3_q</span><span class="p">,</span> <span class="n">poses_q</span><span class="p">))))</span> <span class="o">*</span> <span class="mi">500</span>
        <span class="k">return</span> <span class="n">l1_x</span> <span class="o">+</span> <span class="n">l1_q</span> <span class="o">+</span> <span class="n">l2_x</span> <span class="o">+</span> <span class="n">l2_q</span> <span class="o">+</span> <span class="n">l3_x</span> <span class="o">+</span> <span class="n">l3_q</span>
</pre></div>
</div>
<p>Ultimately, your training script should look like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">backbone</span> <span class="o">=</span> <span class="n">PoseNet</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">PoseNetLoss</span><span class="p">()</span>
    <span class="n">net_with_loss</span> <span class="o">=</span> <span class="n">PoseNetLossCell</span><span class="p">(</span><span class="n">backbone</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">net_with_loss</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">net_with_grad</span> <span class="o">=</span> <span class="n">TrainOneStepCell</span><span class="p">(</span><span class="n">net_with_loss</span><span class="p">,</span> <span class="n">opt</span><span class="p">)</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;dataset define&quot;&quot;&quot;</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net_with_grad</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">epoch_size</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<p>In this way, the model script is basically migrated from TensorFlow to Mindspore. Then, various Mindspore tools and computing policies are used to optimize the precision.</p>
</li>
</ol>
</section>
<section id="migrating-the-pytorch-script-to-mindspore">
<h2>Migrating the PyTorch Script to MindSpore<a class="headerlink" href="#migrating-the-pytorch-script-to-mindspore" title="Permalink to this headline"></a></h2>
<p>Read the PyTorch script to migrate directly.</p>
<ol class="arabic">
<li><p>The PyTorch subnet module usually inherits <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>, and MindSpore usually inherits <code class="docutils literal notranslate"><span class="pre">mindspore.nn.Cell</span></code>. The forward method needs to be re-written for the forward calculation logic of the PyTorch subnet module, and the construct method needs to be rewritten for the forward calculation logic of the MindSpore subnet module.</p></li>
<li><p>Take the migration of the Bottleneck class in MindSpore as an example.</p>
<p>PyTorch project code</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># defined in PyTorch</span>
<span class="k">class</span> <span class="nc">Bottleneck</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inplanes</span><span class="p">,</span> <span class="n">planes</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;NORM&#39;</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Bottleneck</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>

        <span class="n">btnk_ch</span> <span class="o">=</span> <span class="n">planes</span> <span class="o">//</span> <span class="mi">4</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">inplanes</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">inplanes</span><span class="p">,</span> <span class="n">btnk_ch</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">btnk_ch</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">btnk_ch</span><span class="p">,</span> <span class="n">btnk_ch</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span>
                               <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">bn3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">btnk_ch</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">btnk_ch</span><span class="p">,</span> <span class="n">planes</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;UP&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shortcut</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">elif</span> <span class="n">inplanes</span> <span class="o">!=</span> <span class="n">planes</span> <span class="ow">or</span> <span class="n">stride</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shortcut</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">inplanes</span><span class="p">),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">inplanes</span><span class="p">,</span> <span class="n">planes</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shortcut</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_pre_act_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn3</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;UP&#39;</span><span class="p">:</span>
            <span class="n">residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">squeeze_idt</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">shortcut</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shortcut</span><span class="p">(</span><span class="n">residual</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">+=</span> <span class="n">residual</span>

        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">squeeze_idt</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idt</span><span class="p">):</span>
        <span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">idt</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">idt</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">c</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pre_act_forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

</pre></div>
</div>
<p>Based on the differences in defining convolution parameters between PyTorch and MindSpore, the definition can be translated as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>

<span class="c1"># defined in MindSpore</span>
<span class="k">class</span> <span class="nc">Bottleneck</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inplanes</span><span class="p">,</span> <span class="n">planes</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Bottleneck</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>

        <span class="n">btnk_ch</span> <span class="o">=</span> <span class="n">planes</span> <span class="o">//</span> <span class="mi">4</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="n">inplanes</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">inplanes</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">btnk_ch</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span> <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="n">btnk_ch</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">btnk_ch</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">btnk_ch</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span> <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">bn3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="n">btnk_ch</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">btnk_ch</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">planes</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span> <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Shape</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;UP&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shortcut</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">elif</span> <span class="n">inplanes</span> <span class="o">!=</span> <span class="n">planes</span> <span class="ow">or</span> <span class="n">stride</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shortcut</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SequentialCell</span><span class="p">([</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="n">inplanes</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">inplanes</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">planes</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span> <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shortcut</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_pre_act_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn3</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">shortcut</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shortcut</span><span class="p">(</span><span class="n">residual</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">+=</span> <span class="n">residual</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pre_act_forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
</li>
<li><p>PyTorch backpropagation is usually implemented by <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code>, and parameter update is implemented by <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code>, In MindSpore, these parameters do not need to be explicitly invoked by the user and can be transferred to the <code class="docutils literal notranslate"><span class="pre">TrainOneStepCell</span></code> class for backpropagation and gradient update. Finally, the training script should look like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># define dataset</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="o">...</span>

<span class="c1"># define backbone and loss</span>
<span class="n">backbone</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">NetLoss</span><span class="p">()</span>

<span class="c1"># combine backbone and loss</span>
<span class="n">net_with_loss</span> <span class="o">=</span> <span class="n">WithLossCell</span><span class="p">(</span><span class="n">backbone</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>

<span class="c1"># define optimizer</span>
<span class="n">opt</span> <span class="o">=</span> <span class="o">...</span>

<span class="c1"># combine forward and backward</span>
<span class="n">net_with_grad</span> <span class="o">=</span> <span class="n">TrainOneStepCell</span><span class="p">(</span><span class="n">net_with_loss</span><span class="p">,</span> <span class="n">opt</span><span class="p">)</span>

<span class="c1"># define model and train</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net_with_grad</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">epoch_size</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
<p>PyTorch and mindspore have similar definitions of some basic APIs, such as <a class="reference external" href="https://www.mindspore.cn/docs/api/en/r1.5/api_python/nn/mindspore.nn.SequentialCell.html#mindspore.nn.SequentialCell">mindspore.nn.SequentialCell</a> and <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential">torch.nn.Sequential</a>, In addition, some operator APIs may be not the same. This section lists some common API comparisons. For more information, see the <a class="reference external" href="https://www.mindspore.cn/docs/note/en/r1.5/index.html#operator_api">MindSpore and PyTorch API mapping</a> on Mindspore’s official website.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="text-center head"><p>PyTorch</p></th>
<th class="text-center head"><p>MindSpore</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>tensor.view()</p></td>
<td class="text-center"><p>mindspore.ops.operations.Reshape()(tensor)</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>tensor.size()</p></td>
<td class="text-center"><p>mindspore.ops.operations.Shape()(tensor)</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>tensor.sum(axis)</p></td>
<td class="text-center"><p>mindspore.ops.operations.ReduceSum()(tensor, axis)</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>torch.nn.Upsample[mode: nearest]</p></td>
<td class="text-center"><p>mindspore.ops.operations.ResizeNearestNeighbor</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>torch.nn.Upsample[mode: bilinear]</p></td>
<td class="text-center"><p>mindspore.ops.operations.ResizeBilinear</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>torch.nn.Linear</p></td>
<td class="text-center"><p>mindspore.nn.Dense</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>torch.nn.PixelShuffle</p></td>
<td class="text-center"><p>mindspore.ops.operations.DepthToSpace</p></td>
</tr>
</tbody>
</table>
<p>It should be noticed that although <code class="docutils literal notranslate"><span class="pre">torch.nn.MaxPool2d</span></code> and <code class="docutils literal notranslate"><span class="pre">mindspore.nn.MaxPool2d</span></code> are similar in interface definition, Mindspore actually invokes the <code class="docutils literal notranslate"><span class="pre">MaxPoolWithArgMax</span></code> operator during training on Ascend. The function of this operator is the same as that of TensorFlow, during the migration, the MindSpore output after the MaxPool layer is inconsistent with that of PyTorch, theoretically, it’s not affect the final training result.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="script_development.html" class="btn btn-neutral float-left" title="Network Script Development" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="neural_network_debug.html" class="btn btn-neutral float-right" title="Network Debugging" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>