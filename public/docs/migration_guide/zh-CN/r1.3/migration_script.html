<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>迁移脚本 &mdash; MindSpore master documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="网络调试" href="neural_network_debug.html" />
    <link rel="prev" title="使用MindConverter迁移脚本" href="migration_case_of_mindconverter.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="preparation.html">准备工作</a></li>
<li class="toctree-l1"><a class="reference internal" href="script_analysis.html">网络脚本分析</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="script_development.html">网络脚本开发</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="migration_case_of_mindconverter.html">使用MindConverter迁移脚本</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">迁移脚本</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id2">概述</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tensorflowmindspore">TensorFlow脚本迁移MindSpore</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pytorchmindspore">PyTorch脚本迁移MindSpore</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="neural_network_debug.html">网络调试</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/zh-CN/r1.3/accuracy_optimization.html">精度调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance_optimization.html">性能调试</a></li>
<li class="toctree-l1"><a class="reference internal" href="inference.html">推理执行</a></li>
<li class="toctree-l1"><a class="reference internal" href="sample_code.html">网络迁移调试实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">常见问题</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="script_development.html">网络脚本开发</a> &raquo;</li>
      <li>迁移脚本</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/migration_script.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section class="tex2jax_ignore mathjax_ignore" id="id1">
<h1>迁移脚本<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.3/docs/mindspore/migration_guide/source_zh_cn/migration_script.md" target="_blank"><img src="https://gitee.com/mindspore/docs/raw/r1.3/resource/_static/logo_source.png"></a></p>
<section id="id2">
<h2>概述<a class="headerlink" href="#id2" title="Permalink to this headline"></a></h2>
<p>本文档主要介绍，怎样将网络脚本从TensorFlow或PyTorch框架迁移至MindSpore。</p>
</section>
<section id="tensorflowmindspore">
<h2>TensorFlow脚本迁移MindSpore<a class="headerlink" href="#tensorflowmindspore" title="Permalink to this headline"></a></h2>
<p>通过读TensorBoard图，进行脚本迁移。</p>
<ol class="arabic">
<li><p>以TensorFlow实现的<a class="reference external" href="https://arxiv.org/pdf/1505.07427v4.pdf">PoseNet</a>为例，演示如何利用TensorBoard读图，编写MindSpore代码，将<a class="reference external" href="https://github.com/kentsommer/tensorflow-posenet">TensorFlow模型</a>迁移到MindSpore上。</p>
<blockquote>
<div><p>此处提到的PoseNet代码为基于Python2的代码，需要对Python3做一些语法更改才能在Python3上运行，具体修改内容不予赘述。</p>
</div></blockquote>
</li>
<li><p>改写代码，利用<code class="docutils literal notranslate"><span class="pre">tf.summary</span></code>接口，保存TensorBoard需要的log，并启动TensorBoard。</p></li>
<li><p>打开的TensorBoard如图所示，图例仅供参考，可能因log生成方式的差异，TensorBoard展示的图也有所差异。</p>
<p><img alt="PoseNet TensorBoard" src="_images/pic1.png" /></p>
</li>
<li><p>找到3个输入的Placeholder，通过看图并阅读代码得知，第二、第三个输入都只在计算loss时使用。</p>
<p><img alt="PoseNet Placeholder" src="_images/pic3.png" /></p>
<p><img alt="PoseNet Placeholder_1 Placeholder_2" src="_images/pic2.png" /></p>
<p><img alt="PoseNet script input1 2 3" src="_images/pic4.png" /></p>
<p>至此，我们可以初步划分出，构造网络模型三步：</p>
<p>第一步，在网络的三个输入中，第一个输入将在backbone中计算出六个输出；</p>
<p>第二步，上一步结果与第二、第三个输入在loss子网中计算loss；</p>
<p>第三步，利用<code class="docutils literal notranslate"><span class="pre">TrainOneStepCell</span></code>自动微分构造反向网络；利用TensorFlow工程中提供的Adam优化器及属性，写出对应的MindSpore优化器来更新参数，网络脚本骨干可写作：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">TrainOneStepCell</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">Adam</span>

<span class="c1"># combine backbone and loss</span>
<span class="k">class</span> <span class="nc">PoseNetLossCell</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">backbone</span><span class="p">,</span> <span class="n">loss</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PoseNetLossCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pose_net</span> <span class="o">=</span> <span class="n">backbone</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_1</span><span class="p">,</span> <span class="n">input_2</span><span class="p">,</span> <span class="n">input_3</span><span class="p">):</span>
        <span class="n">p1_x</span><span class="p">,</span> <span class="n">p1_q</span><span class="p">,</span> <span class="n">p2_x</span><span class="p">,</span> <span class="n">p2_q</span><span class="p">,</span> <span class="n">p3_x</span><span class="p">,</span> <span class="n">p3_q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">poss_net</span><span class="p">(</span><span class="n">input_1</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">p1_x</span><span class="p">,</span> <span class="n">p1_q</span><span class="p">,</span> <span class="n">p2_x</span><span class="p">,</span> <span class="n">p2_q</span><span class="p">,</span> <span class="n">p3_x</span><span class="p">,</span> <span class="n">p3_q</span><span class="p">,</span> <span class="n">input_2</span><span class="p">,</span> <span class="n">input_3</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>

<span class="c1"># define backbone</span>
<span class="k">class</span> <span class="nc">PoseNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PoseNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;do something with input_1, output num 6&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">p1_x</span><span class="p">,</span> <span class="n">p1_q</span><span class="p">,</span> <span class="n">p2_x</span><span class="p">,</span> <span class="n">p2_q</span><span class="p">,</span> <span class="n">p3_x</span><span class="p">,</span> <span class="n">p3_q</span>

<span class="c1"># define loss</span>
<span class="k">class</span> <span class="nc">PoseNetLoss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PoseNetLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p1_x</span><span class="p">,</span> <span class="n">p1_q</span><span class="p">,</span> <span class="n">p2_x</span><span class="p">,</span> <span class="n">p2_q</span><span class="p">,</span> <span class="n">p3_x</span><span class="p">,</span> <span class="n">p3_q</span><span class="p">,</span> <span class="n">poses_x</span><span class="p">,</span> <span class="n">poses_q</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;do something to calc loss&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">loss</span>

<span class="c1"># define network</span>
<span class="n">backbone</span> <span class="o">=</span> <span class="n">PoseNet</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">PoseNetLoss</span><span class="p">()</span>
<span class="n">net_with_loss</span> <span class="o">=</span> <span class="n">PoseNetLossCell</span><span class="p">(</span><span class="n">backbone</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">net_with_loss</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">net_with_grad</span> <span class="o">=</span> <span class="n">TrainOneStepCell</span><span class="p">(</span><span class="n">net_with_loss</span><span class="p">,</span> <span class="n">opt</span><span class="p">)</span>


</pre></div>
</div>
</li>
<li><p>接下来，我们来具体实现backbone中的计算逻辑。</p>
<p>第一个输入首先经过了一个名为conv1的子图，通过看图可得，其中计算逻辑为：</p>
<p><img alt="PoseNet conv1 子图" src="_images/pic5.png" /></p>
<p>输入-&gt;Conv2D-&gt;BiasAdd-&gt;ReLU，虽然图上看起来，BiasAdd后的算子名虽然为conv1，但其实际执行的是ReLU。</p>
<p><img alt="PoseNet Conv1 conv1 relu" src="_images/pic6.png" /></p>
<p>这样一来，第一个子图conv1，可以定义如下，具体参数，与原工程中的参数对齐：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Conv1</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Conv1</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">Conv2d</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">ReLU</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<p>通过观察TensorBoard图和代码，我们不难发现，原TensorFlow工程中定义的conv这一类型的子网，可以复写为MindSpore的子网，减少重复代码。</p>
<p>TensorFlow工程conv子网定义：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">conv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
             <span class="nb">input</span><span class="p">,</span>
             <span class="n">k_h</span><span class="p">,</span>
             <span class="n">k_w</span><span class="p">,</span>
             <span class="n">c_o</span><span class="p">,</span>
             <span class="n">s_h</span><span class="p">,</span>
             <span class="n">s_w</span><span class="p">,</span>
             <span class="n">name</span><span class="p">,</span>
             <span class="n">relu</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
             <span class="n">padding</span><span class="o">=</span><span class="n">DEFAULT_PADDING</span><span class="p">,</span>
             <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
             <span class="n">biased</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="c1"># Verify that the padding is acceptable</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">validate_padding</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
        <span class="c1"># Get the number of channels in the input</span>
        <span class="n">c_i</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># Verify that the grouping parameter is valid</span>
        <span class="k">assert</span> <span class="n">c_i</span> <span class="o">%</span> <span class="n">group</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="k">assert</span> <span class="n">c_o</span> <span class="o">%</span> <span class="n">group</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="c1"># Convolution for a given input and kernel</span>
        <span class="n">convolve</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">s_h</span><span class="p">,</span> <span class="n">s_w</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">name</span><span class="p">)</span> <span class="k">as</span> <span class="n">scope</span><span class="p">:</span>
            <span class="n">kernel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">make_var</span><span class="p">(</span><span class="s1">&#39;weights&#39;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">k_h</span><span class="p">,</span> <span class="n">k_w</span><span class="p">,</span> <span class="n">c_i</span> <span class="o">/</span> <span class="n">group</span><span class="p">,</span> <span class="n">c_o</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">group</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="c1"># This is the common-case. Convolve the input without any further complications.</span>
                <span class="n">output</span> <span class="o">=</span> <span class="n">convolve</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Split the input into groups and then convolve each of them independently</span>
                <span class="n">input_groups</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
                <span class="n">kernel_groups</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="n">kernel</span><span class="p">)</span>
                <span class="n">output_groups</span> <span class="o">=</span> <span class="p">[</span><span class="n">convolve</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">input_groups</span><span class="p">,</span> <span class="n">kernel_groups</span><span class="p">)]</span>
                <span class="c1"># Concatenate the groups</span>
                <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">output_groups</span><span class="p">)</span>
            <span class="c1"># Add the biases</span>
            <span class="k">if</span> <span class="n">biased</span><span class="p">:</span>
                <span class="n">biases</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">make_var</span><span class="p">(</span><span class="s1">&#39;biases&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">c_o</span><span class="p">])</span>
                <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">biases</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">relu</span><span class="p">:</span>
                <span class="c1"># ReLU non-linearity</span>
                <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">scope</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
<p>则对应MindSpore子网定义如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">Conv2d</span><span class="p">,</span> <span class="n">ReLU</span>

<span class="k">class</span> <span class="nc">ConvReLU</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">channel_in</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">channel_out</span><span class="p">,</span>  <span class="n">strides</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ConvReLU</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">Conv2d</span><span class="p">(</span><span class="n">channel_in</span><span class="p">,</span> <span class="n">channel_out</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">has_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">ReLU</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<p>那么，对照着TensorBoard中的数据流向与算子属性，backbone计算逻辑可编写如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">MaxPool2d</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>


<span class="k">class</span> <span class="nc">LRN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">radius</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LRN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lrn</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">LRN</span><span class="p">(</span><span class="n">radius</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lrn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">PoseNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PoseNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pool1</span> <span class="o">=</span> <span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;SAME&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">LRN</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">2e-05</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction2</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">192</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">LRN</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">2e-05</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pool2</span> <span class="o">=</span> <span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;SAME&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp1_reduction1</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">192</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">96</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp1_out1</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">96</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp1_reduction2</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">192</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp1_out2</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp1_pool</span> <span class="o">=</span> <span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;SAME&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp1_out3</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">192</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp1_out0</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">192</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">concat</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp2_reduction1</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp2_out1</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">192</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp2_reduction2</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp2_out2</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">96</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp2_pool</span> <span class="o">=</span> <span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;SAME&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp2_out3</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp2_out0</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp3_in</span> <span class="o">=</span> <span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;SAME&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp3_reduction1</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">480</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">96</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp3_out1</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">96</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">208</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp3_reduction2</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">480</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp3_out2</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp3_pool</span> <span class="o">=</span> <span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;SAME&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp3_out3</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">480</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">icp3_out0</span> <span class="o">=</span> <span class="n">ConvReLU</span><span class="p">(</span><span class="mi">480</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">192</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;etc&quot;&quot;&quot;</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;...&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;do something with input_1, output num 6&quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">input_1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">pool2</span> <span class="o">=</span> <span class="n">x</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">icp1_reduction1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">icp1_out1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">icp1_out1</span> <span class="o">=</span> <span class="n">x</span>

        <span class="n">icp1_reduction2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">icp1_reduction2</span><span class="p">(</span><span class="n">pool2</span><span class="p">)</span>
        <span class="n">icp1_out2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">icp1_out2</span><span class="p">(</span><span class="n">icp1_reduction2</span><span class="p">)</span>

        <span class="n">icp1_pool</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">icp1_pool</span><span class="p">(</span><span class="n">pool2</span><span class="p">)</span>
        <span class="n">icp1_out3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">icp1_out3</span><span class="p">(</span><span class="n">icp1_pool</span><span class="p">)</span>

        <span class="n">icp1_out0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">icp1_out0</span><span class="p">(</span><span class="n">pool2</span><span class="p">)</span>

        <span class="n">icp2_in</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">concat</span><span class="p">((</span><span class="n">icp1_out0</span><span class="p">,</span> <span class="n">icp1_out1</span><span class="p">,</span> <span class="n">icp1_out2</span><span class="p">,</span> <span class="n">icp1_out3</span><span class="p">))</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;etc&quot;&quot;&quot;</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;...&quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="n">p1_x</span><span class="p">,</span> <span class="n">p1_q</span><span class="p">,</span> <span class="n">p2_x</span><span class="p">,</span> <span class="n">p2_q</span><span class="p">,</span> <span class="n">p3_x</span><span class="p">,</span> <span class="n">p3_q</span>
</pre></div>
</div>
<p>相应的，loss计算逻辑可编写如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">PoseNetLoss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PoseNetLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sub</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Sub</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">square</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Square</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sqrt</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Sqrt</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p1_x</span><span class="p">,</span> <span class="n">p1_q</span><span class="p">,</span> <span class="n">p2_x</span><span class="p">,</span> <span class="n">p2_q</span><span class="p">,</span> <span class="n">p3_x</span><span class="p">,</span> <span class="n">p3_q</span><span class="p">,</span> <span class="n">poses_x</span><span class="p">,</span> <span class="n">poses_q</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;do something to calc loss&quot;&quot;&quot;</span>
        <span class="n">l1_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">p1_x</span><span class="p">,</span> <span class="n">poses_x</span><span class="p">))))</span> <span class="o">*</span> <span class="mf">0.3</span>
        <span class="n">l1_q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">p1_q</span><span class="p">,</span> <span class="n">poses_q</span><span class="p">))))</span> <span class="o">*</span> <span class="mi">150</span>
        <span class="n">l2_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">p2_x</span><span class="p">,</span> <span class="n">poses_x</span><span class="p">))))</span> <span class="o">*</span> <span class="mf">0.3</span>
        <span class="n">l2_q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">p2_q</span><span class="p">,</span> <span class="n">poses_q</span><span class="p">))))</span> <span class="o">*</span> <span class="mi">150</span>
        <span class="n">l3_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">p3_x</span><span class="p">,</span> <span class="n">poses_x</span><span class="p">))))</span> <span class="o">*</span> <span class="mi">1</span>
        <span class="n">l3_q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">p3_q</span><span class="p">,</span> <span class="n">poses_q</span><span class="p">))))</span> <span class="o">*</span> <span class="mi">500</span>
        <span class="k">return</span> <span class="n">l1_x</span> <span class="o">+</span> <span class="n">l1_q</span> <span class="o">+</span> <span class="n">l2_x</span> <span class="o">+</span> <span class="n">l2_q</span> <span class="o">+</span> <span class="n">l3_x</span> <span class="o">+</span> <span class="n">l3_q</span>
</pre></div>
</div>
<p>最终，你的训练脚本应该类似如下所示：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">backbone</span> <span class="o">=</span> <span class="n">PoseNet</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">PoseNetLoss</span><span class="p">()</span>
    <span class="n">net_with_loss</span> <span class="o">=</span> <span class="n">PoseNetLossCell</span><span class="p">(</span><span class="n">backbone</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">net_with_loss</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">net_with_grad</span> <span class="o">=</span> <span class="n">TrainOneStepCell</span><span class="p">(</span><span class="n">net_with_loss</span><span class="p">,</span> <span class="n">opt</span><span class="p">)</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;dataset define&quot;&quot;&quot;</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net_with_grad</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">epoch_size</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<p>这样，就基本完成了模型脚本从TensorFlow到MindSpore的迁移，接下来就是利用丰富的MindSpore工具和计算策略，对精度进行调优，在此不予详述。</p>
</li>
</ol>
</section>
<section id="pytorchmindspore">
<h2>PyTorch脚本迁移MindSpore<a class="headerlink" href="#pytorchmindspore" title="Permalink to this headline"></a></h2>
<p>通过读PyTorch脚本，直接进行迁移。</p>
<ol class="arabic">
<li><p>PyTorch子网模块通常继承<code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>，MindSpore通常继承<code class="docutils literal notranslate"><span class="pre">mindspore.nn.Cell</span></code>；PyTorch子网模块正向计算逻辑需要重写forward方法，MindSpore子网模块正向计算逻辑需要重写construct方法。</p></li>
<li><p>以常见的Bottleneck类在MindSpore下的迁移为例。</p>
<p>PyTorch工程代码</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># defined in PyTorch</span>
<span class="k">class</span> <span class="nc">Bottleneck</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inplanes</span><span class="p">,</span> <span class="n">planes</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;NORM&#39;</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Bottleneck</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>

        <span class="n">btnk_ch</span> <span class="o">=</span> <span class="n">planes</span> <span class="o">//</span> <span class="mi">4</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">inplanes</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">inplanes</span><span class="p">,</span> <span class="n">btnk_ch</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">btnk_ch</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">btnk_ch</span><span class="p">,</span> <span class="n">btnk_ch</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span>
                               <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">bn3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">btnk_ch</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">btnk_ch</span><span class="p">,</span> <span class="n">planes</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;UP&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shortcut</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">elif</span> <span class="n">inplanes</span> <span class="o">!=</span> <span class="n">planes</span> <span class="ow">or</span> <span class="n">stride</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shortcut</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">inplanes</span><span class="p">),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">inplanes</span><span class="p">,</span> <span class="n">planes</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shortcut</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_pre_act_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn3</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;UP&#39;</span><span class="p">:</span>
            <span class="n">residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">squeeze_idt</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">shortcut</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shortcut</span><span class="p">(</span><span class="n">residual</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">+=</span> <span class="n">residual</span>

        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">squeeze_idt</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idt</span><span class="p">):</span>
        <span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">idt</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">idt</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">c</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pre_act_forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

</pre></div>
</div>
<p>根据PyTorch和MindSpore对卷积参数定义的区别，可以翻译成如下定义：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>

<span class="c1"># defined in MindSpore</span>
<span class="k">class</span> <span class="nc">Bottleneck</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inplanes</span><span class="p">,</span> <span class="n">planes</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Bottleneck</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>

        <span class="n">btnk_ch</span> <span class="o">=</span> <span class="n">planes</span> <span class="o">//</span> <span class="mi">4</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="n">inplanes</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">inplanes</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">btnk_ch</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span> <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="n">btnk_ch</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">btnk_ch</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">btnk_ch</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span> <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">bn3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="n">btnk_ch</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">btnk_ch</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">planes</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span> <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Shape</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;UP&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shortcut</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">elif</span> <span class="n">inplanes</span> <span class="o">!=</span> <span class="n">planes</span> <span class="ow">or</span> <span class="n">stride</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shortcut</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SequentialCell</span><span class="p">([</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="n">inplanes</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">inplanes</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">planes</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span> <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shortcut</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_pre_act_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn3</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">shortcut</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shortcut</span><span class="p">(</span><span class="n">residual</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">+=</span> <span class="n">residual</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pre_act_forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
</li>
<li><p>PyTorch的反向传播通常使用<code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code>实现，参数更新通过<code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code>实现，在MindSpore中，这些不需要用户显式调用执行，可以交给<code class="docutils literal notranslate"><span class="pre">TrainOneStepCell</span></code>类进行反向传播和梯度更新。最后，训练脚本结构应如下所示：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># define dataset</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="o">...</span>

<span class="c1"># define backbone and loss</span>
<span class="n">backbone</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">NetLoss</span><span class="p">()</span>

<span class="c1"># combine backbone and loss</span>
<span class="n">net_with_loss</span> <span class="o">=</span> <span class="n">WithLossCell</span><span class="p">(</span><span class="n">backbone</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>

<span class="c1"># define optimizer</span>
<span class="n">opt</span> <span class="o">=</span> <span class="o">...</span>

<span class="c1"># combine forward and backward</span>
<span class="n">net_with_grad</span> <span class="o">=</span> <span class="n">TrainOneStepCell</span><span class="p">(</span><span class="n">net_with_loss</span><span class="p">,</span> <span class="n">opt</span><span class="p">)</span>

<span class="c1"># define model and train</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net_with_grad</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">epoch_size</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
<p>PyTorch和mindspore在一些基础API的定义上比较相似，比如<a class="reference external" href="https://www.mindspore.cn/docs/api/zh-CN/r1.3/api_python/nn/mindspore.nn.SequentialCell.html#mindspore.nn.SequentialCell">mindspore.nn.SequentialCell</a>和<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential">torch.nn.Sequential</a>，另外，一些算子API可能不尽相同，此处列举一些常见的API对照，更多信息可以参考MindSpore官网的<a class="reference external" href="https://www.mindspore.cn/docs/note/zh-CN/r1.3/index.html#operator_api">MindSpore与PyTorch对照表</a>。</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="text-center head"><p>PyTorch</p></th>
<th class="text-center head"><p>MindSpore</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>tensor.view()</p></td>
<td class="text-center"><p>mindspore.ops.operations.Reshape()(tensor)</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>tensor.size()</p></td>
<td class="text-center"><p>mindspore.ops.operations.Shape()(tensor)</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>tensor.sum(axis)</p></td>
<td class="text-center"><p>mindspore.ops.operations.ReduceSum()(tensor, axis)</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>torch.nn.Upsample[mode: nearest]</p></td>
<td class="text-center"><p>mindspore.ops.operations.ResizeNearestNeighbor</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>torch.nn.Upsample[mode: bilinear]</p></td>
<td class="text-center"><p>mindspore.ops.operations.ResizeBilinear</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>torch.nn.Linear</p></td>
<td class="text-center"><p>mindspore.nn.Dense</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>torch.nn.PixelShuffle</p></td>
<td class="text-center"><p>mindspore.ops.operations.DepthToSpace</p></td>
</tr>
</tbody>
</table>
<p>值得注意的是，尽管<code class="docutils literal notranslate"><span class="pre">torch.nn.MaxPool2d</span></code>和<code class="docutils literal notranslate"><span class="pre">mindspore.nn.MaxPool2d</span></code>在接口定义上较为相似，但在Ascend上的训练过程中，MindSpore实际调用了<code class="docutils literal notranslate"><span class="pre">MaxPoolWithArgMax</span></code>算子，该算子与TensorFlow的同名算子功能相同，在迁移过程中MaxPool层后的输出MindSpore与PyTorch不一致是正常现象，理论上不影响最终训练结果。</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="migration_case_of_mindconverter.html" class="btn btn-neutral float-left" title="使用MindConverter迁移脚本" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="neural_network_debug.html" class="btn btn-neutral float-right" title="网络调试" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>