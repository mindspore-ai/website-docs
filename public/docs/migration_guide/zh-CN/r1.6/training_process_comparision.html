<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>基本执行流程横向对比 &mdash; MindSpore master documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="典型算子或接口区别介绍" href="typical_api_comparision.html" />
    <link rel="prev" title="迁移脚本" href="migration_script.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="preparation.html">准备工作</a></li>
<li class="toctree-l1"><a class="reference internal" href="script_analysis.html">网络脚本分析</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="script_development.html">网络脚本开发</a><ul class="current">
<li class="toctree-l2"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/zh-CN/r1.6/migrate_3rd_scripts_mindconverter.html">使用MindConverter迁移脚本↗</a></li>
<li class="toctree-l2"><a class="reference internal" href="migration_script.html">迁移脚本</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">基本执行流程横向对比</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id2">总体流程</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id3">构建数据集</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id4">反向传播</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id5">反向传播原理</a></li>
<li class="toctree-l4"><a class="reference internal" href="#trainonestepcell">TrainOneStepCell</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="typical_api_comparision.html">典型算子或接口区别介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="optim.html">优化器迁移指南</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="neural_network_debug.html">网络调试</a></li>
<li class="toctree-l1"><a class="reference internal" href="accuracy_optimization.html">精度调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance_optimization.html">性能调试</a></li>
<li class="toctree-l1"><a class="reference internal" href="inference.html">推理执行</a></li>
<li class="toctree-l1"><a class="reference internal" href="sample_code.html">网络迁移调试实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_mapping.html">API映射</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">常见问题</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="script_development.html">网络脚本开发</a> &raquo;</li>
      <li>基本执行流程横向对比</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/training_process_comparision.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="id1">
<h1>基本执行流程横向对比<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.6/docs/mindspore/migration_guide/source_zh_cn/training_process_comparision.md" target="_blank"><img src="https://gitee.com/mindspore/docs/raw/r1.6/resource/_static/logo_source.png"></a></p>
<section id="id2">
<h2>总体流程<a class="headerlink" href="#id2" title="Permalink to this headline"></a></h2>
<p>MindSpore 的模型训练和推理的总体执行流程，基本与主流的 AI 框架（如 TensorFlow 和 PyTorch）类似，主要分为以下几个步骤：</p>
<ol class="arabic simple">
<li><p>构建数据集对象</p></li>
<li><p>定义正向网络结构</p></li>
<li><p>定义 Loss</p></li>
<li><p>定义优化器</p></li>
<li><p>迭代数据集对象，执行正向传播，输出 Loss</p></li>
<li><p>执行反向传播计算梯度</p></li>
<li><p>优化器更新参数</p></li>
</ol>
<p>以下是典型的 MindSpore、PyTorch 和 TensorFlow 的训练代码：</p>
<ul>
<li><p>MindSpore</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">src.dataset</span> <span class="kn">import</span> <span class="n">create_dataset</span>
<span class="kn">from</span> <span class="nn">src.lenet</span> <span class="kn">import</span> <span class="n">LeNet</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span><span class="p">,</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">mindspore.train.callback</span> <span class="kn">import</span> <span class="n">ModelCheckpoint</span><span class="p">,</span> <span class="n">CheckpointConfig</span><span class="p">,</span> <span class="n">LossMonitor</span><span class="p">,</span> <span class="n">TimeMonitor</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">Accuracy</span>

<span class="c1"># set context, including device type, device number...</span>
<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">device_target</span><span class="p">)</span>
<span class="c1"># 1. define dataset object</span>
<span class="n">ds_train</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
<span class="c1"># 2. define forward network</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">LeNet</span><span class="p">(</span><span class="n">num_classes</span><span class="p">)</span>
<span class="c1"># 3. define loss</span>
<span class="n">net_loss</span> <span class="o">=</span> <span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">()</span>
<span class="c1"># 4. define optimizer</span>
<span class="n">net_opt</span> <span class="o">=</span> <span class="n">Momentum</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="p">)</span>
<span class="c1"># define callbacks</span>
<span class="n">config_ck</span> <span class="o">=</span> <span class="n">CheckpointConfig</span><span class="p">(</span><span class="n">save_checkpoint_steps</span><span class="p">)</span>
<span class="n">ckpt_cb</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;lenet&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config_ck</span><span class="p">)</span>
<span class="n">loss_cb</span> <span class="o">=</span> <span class="n">LossMonitor</span><span class="p">()</span>

<span class="c1"># define model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">net_loss</span><span class="p">,</span> <span class="n">net_opt</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;Accuracy&quot;</span><span class="p">:</span> <span class="n">Accuracy</span><span class="p">()})</span>
<span class="c1"># 5-7. start training</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">epoch_size</span><span class="p">,</span> <span class="n">ds_train</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">loss_cb</span><span class="p">,</span> <span class="n">ckpoint_cb</span> <span class="p">])</span>
</pre></div>
</div>
<p>代码来源： <a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.6/official/cv/lenet/train.py">ModelZoo/LeNet5</a></p>
</li>
<li><p>PyTorch</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">from</span> <span class="nn">model</span> <span class="kn">import</span> <span class="n">LeNet</span>
<span class="c1"># 1. define dataset object and DataLoader</span>
<span class="n">train_set</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
<span class="c1"># 2. define forward network</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">LeNet</span><span class="p">()</span>
<span class="c1"># 3. define loss</span>
<span class="n">net_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="c1"># 4. define optimizer</span>
<span class="n">net_opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="p">)</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="c1"># 5. forward propagation and output loss</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">net_loss</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="c1"># 6. backward propagation</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="c1"># 7. update parameters</span>
        <span class="n">net_opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Epoch </span><span class="si">{}</span><span class="s1">, Loss: </span><span class="si">{}</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">loss</span><span class="p">,))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Finished Training&#39;</span><span class="p">)</span>
<span class="n">save_path</span> <span class="o">=</span> <span class="s1">&#39;./Lenet.pth&#39;</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">path</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>TensorFlow</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">model</span> <span class="kn">import</span> <span class="n">LeNet</span>
<span class="n">mnist</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">mnist</span>
<span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="c1"># 1. define dataset</span>
<span class="n">train_ds</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>
<span class="c1"># 2. define forward network</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LeNet</span><span class="p">()</span>
<span class="c1"># 3. define loss</span>
<span class="n">net_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">()</span>
<span class="c1"># 4. define optimizer</span>
<span class="n">net_opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">()</span>

<span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="c1"># 5. forward execution and output loss</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">net_loss</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
    <span class="c1"># 6. backward propagation</span>
    <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
    <span class="c1"># 7. update parameters</span>
    <span class="n">net_opt</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">loss</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">EPOCHS</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">train_ds</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Epoch </span><span class="si">{}</span><span class="s1">, Loss: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">loss</span><span class="p">))</span>

</pre></div>
</div>
</li>
</ul>
<p>可以看到，MindSpore、PyTorch 和 TensorFlow 在步骤 1-4 的执行流程上几乎一致，API 也很相似，开发者在做网络迁移时可以很容易进行脚本适配。但在步骤 5-7 上，三者却有一定的差异，主要是因为 MindSpore、TensorFlow 和 PyTorch 实现自动微分的逻辑不同。下面我们就对其中的几点差异做详细说明。</p>
</section>
<section id="id3">
<h2>构建数据集<a class="headerlink" href="#id3" title="Permalink to this headline"></a></h2>
<p>在构造自定义数据集时，MindSpore 和 PyTorch 有很多相似的 API，构造自定义 Dataset 对象的基本流程如下：</p>
<ol class="arabic">
<li><p>首先创建一个类，在该类中定义  <code class="docutils literal notranslate"><span class="pre">__init__</span></code> 、 <code class="docutils literal notranslate"><span class="pre">__getitem__</span></code> 、 <code class="docutils literal notranslate"><span class="pre">__len__</span></code> 三个方法。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="k">class</span> <span class="nc">MyDataset</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Self Defined dataset.&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
            <span class="n">label</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">label</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">label</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">data</span><span class="p">,</span> <span class="n">label</span>
</pre></div>
</div>
<p>这里的 <code class="docutils literal notranslate"><span class="pre">__len__</span></code> 和  <code class="docutils literal notranslate"><span class="pre">__getitem__</span></code> 在 MindSpore 和 PyTorch 中都具有相同的意义，  <code class="docutils literal notranslate"><span class="pre">__len__</span></code>  表示数据集的总大小，<code class="docutils literal notranslate"><span class="pre">__getitem__</span></code>  通过传入的 idx 获取相应索引的数据。虽然   <code class="docutils literal notranslate"><span class="pre">MyDataset</span></code> 类可以帮助我们在给定索引的情况下获取数据，但实际训练的过程中，我们还需要更加复杂的功能，例如：按照某种自定义顺序索引数据、将不同索引的数据组合成一个 batch、为分布式训练做数据切分等。为了实现这些复杂的功能，MindSpore 提供了一个更高层的 API  <code class="docutils literal notranslate"><span class="pre">mindspore.dataset.GeneratorDataset</span></code> （对应到 PyTorch 上为  <code class="docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> ）。</p>
</li>
<li><p>使用 <code class="docutils literal notranslate"><span class="pre">GeneratorDataset</span></code> 封装 <code class="docutils literal notranslate"><span class="pre">MyDataset</span></code>，提供更多数据预处理操作。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">my_dataset</span> <span class="o">=</span> <span class="n">MyDataset</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="c1"># corresponding to torch.utils.data.DataLoader(my_dataset)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">GeneratorDataset</span><span class="p">(</span><span class="n">my_dataset</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
<p>可以发现，无论是 MindSpore 的 <code class="docutils literal notranslate"><span class="pre">GeneratorDataset</span></code> 还是 PyTorch 的  <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>，都提供了很多参数，我们将其中常用的参数进行对比：</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="text-left head"><p>GeneratorDataset</p></th>
<th class="text-left head"><p>DataLoader</p></th>
<th class="text-left head"><p>参数意义</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>source</p></td>
<td class="text-left"><p>dataset</p></td>
<td class="text-left"><p>数据集对象</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>column_names</p></td>
<td class="text-left"><p>/</p></td>
<td class="text-left"><p>数据输入每一列的名字</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>column_types</p></td>
<td class="text-left"><p>/</p></td>
<td class="text-left"><p>数据输入每一列的类型</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>num_samples</p></td>
<td class="text-left"><p>/</p></td>
<td class="text-left"><p>实际加载的数据数量</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>num_parallel_workers</p></td>
<td class="text-left"><p>num_workers</p></td>
<td class="text-left"><p>并行处理数据的子进程数量</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>shuffle</p></td>
<td class="text-left"><p>/</p></td>
<td class="text-left"><p>是否对数据序列进行 shuffle</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>sampler</p></td>
<td class="text-left"><p>sampler</p></td>
<td class="text-left"><p>定义索引生成函数</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>num_shards</p></td>
<td class="text-left"><p>/</p></td>
<td class="text-left"><p>数据集的均分数量（分布式运算涉及）</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>shard_id</p></td>
<td class="text-left"><p>/</p></td>
<td class="text-left"><p>数据集的均分后的序号（分布式运算涉及）</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>/</p></td>
<td class="text-left"><p>batch_size</p></td>
<td class="text-left"><p>批大小</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>/</p></td>
<td class="text-left"><p>batch_sampler</p></td>
<td class="text-left"><p>批索引生成函数</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>/</p></td>
<td class="text-left"><p>collate_fn</p></td>
<td class="text-left"><p>组成批数据的映射函数</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>/</p></td>
<td class="text-left"><p>drop_last</p></td>
<td class="text-left"><p>是否丢弃无法 batch 的数据</p></td>
</tr>
</tbody>
</table>
<p><code class="docutils literal notranslate"><span class="pre">column_name</span></code>、<code class="docutils literal notranslate"><span class="pre">column_names</span></code> 是 MindSpore 增加的参数项，主要是为更好的描述数据列。<code class="docutils literal notranslate"><span class="pre">num_samples</span></code>  是限制实际读取的数据量，当数据集很大时可以只加载部分数据。</p>
<p>在 PyTorch 中，数据 shffule 和分布式数据分发的操作主要由对应的  <code class="docutils literal notranslate"><span class="pre">sampler</span></code>  实现，MindSpore 沿用了这一设计，但考虑到这两个操作使用频率较高，MindSpore 直接在  <code class="docutils literal notranslate"><span class="pre">GeneratorDataSet</span></code> 类中增加新参数 <code class="docutils literal notranslate"><span class="pre">shuffle</span></code> 和  <code class="docutils literal notranslate"><span class="pre">num_shards</span></code>、<code class="docutils literal notranslate"><span class="pre">shard_id</span></code> 来实现这两个功能。</p>
<p>对于 PyTorch 中额外的四个参数  <code class="docutils literal notranslate"><span class="pre">batch_size</span></code>、<code class="docutils literal notranslate"><span class="pre">batch_sampler</span></code>、<code class="docutils literal notranslate"><span class="pre">collate_fn</span></code> 和  <code class="docutils literal notranslate"><span class="pre">drop_last</span></code>，考虑到它们均与批处理有关，MindSpore 将这四个参数全部移动到了成员函数 <code class="docutils literal notranslate"><span class="pre">batch</span></code> 中，这种设计使得参数分组更加清晰。</p>
<p>需要注意的是，MindSpore 的 GeneratorDataset 和 PyTorch 的 Dataloader 并不是同一个概念。GenerateDataset 加载数据集后生成最基本的数据流，输出为单个样本数据，我们还可以继续使用 MindData 提供的接口进行其他预处理的数据增强操作，例如 map、batch、shuffle、repeat 等。而 Dataloader 通常是数据处理的最终出口，输出 batch size 个样本，然后直接送入网络。</p>
<ol class="arabic simple" start="3">
<li><p>迭代 Dataset</p></li>
</ol>
<ul>
<li><p>PyTorch</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>MindSpore</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="c1"># method 1: create a tuple iterator</span>
<span class="k">for</span> <span class="n">columns</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">():</span>
    <span class="c1"># type of columns is dict</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">columns</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">]</span>
    <span class="n">label</span> <span class="o">=</span> <span class="n">columns</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span>
    <span class="o">...</span>

<span class="c1"># method 2: create a dict iterator</span>
<span class="k">for</span> <span class="n">columns</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_tuple_iterator</span><span class="p">():</span>
    <span class="c1"># type of columns is tuple</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">columns</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">label</span> <span class="o">=</span> <span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="o">...</span>
</pre></div>
</div>
</li>
</ul>
<p>MindSpore 和 PyTorch 的数据集迭代过程基本相同，而 MindSpore 可以以  <code class="docutils literal notranslate"><span class="pre">dict</span></code> 和  <code class="docutils literal notranslate"><span class="pre">tuple</span></code> 两种形式输出数据。</p>
<p>需要注意的是，MindSpore 中的  <code class="docutils literal notranslate"><span class="pre">GeneratorDataset</span></code> 类除了用于迭代输出数据，它也为 MindSpore 特有的数据下沉模式做了适配，使得基于 NPU Ascend910 设备训练时，NPU 无需与 Host 交互便可获取训练数据。</p>
</section>
<section id="id4">
<h2>反向传播<a class="headerlink" href="#id4" title="Permalink to this headline"></a></h2>
<p>MindSpore 和 PyTorch 都提供了自动微分功能，让我们在定义了正向网络后，可以通过简单的接口调用实现自动反向传播以及梯度更新。但需要注意的是，MindSpore 和 PyTorch 构建反向图的逻辑是不同的，这个差异也会带来 API 设计上的不同。</p>
<section id="id5">
<h3>反向传播原理<a class="headerlink" href="#id5" title="Permalink to this headline"></a></h3>
<ul>
<li><p>PyTorch 的自动微分</p>
<p>我们知道 PyTorch 是基于计算路径追踪的自动微分，当我们定义一个网络结构后， 并不会建立反向图，而是在执行正向图的过程中，<code class="docutils literal notranslate"><span class="pre">Variable</span></code> 或  <code class="docutils literal notranslate"><span class="pre">Parameter</span></code> 记录每一个正向计算对应的反向函数，并生成一个动态计算图，用于后续的梯度计算。当在最终的输出处调用  <code class="docutils literal notranslate"><span class="pre">backward</span></code> 时，就会从根节点到叶节点应用链式法则计算梯度。PyTorch 的动态计算图所存储的节点实际是 Function 函数对象，每当对 Tensor 执行一步运算后，就会产生一个 Function 对象，它记录了反向传播中必要的信息。反向传播过程中，autograd 引擎会按照逆序，通过 Function 的 backward 依次计算梯度。 这一点我们可以通过 Tensor 的隐藏属性查看。</p>
<p>例如，运行以下代码：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">1</span>
<span class="n">x</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>此时我们查看 x 的属性，可以发现它已记录了整个反向传播过程所需要的信息：</p>
<p><img alt="图片" src="_images/pytorch_backward.png" /></p>
<p>上图中的  <code class="docutils literal notranslate"><span class="pre">grad_fn</span></code>  记录了 x 反向传播所需要的函数，而由于 x 是经过多次正向计算得到的，因此  <code class="docutils literal notranslate"><span class="pre">grad_fn</span></code>  不只记录了一条反向函数。</p>
</li>
<li><p>MindSpore 的自动微分</p>
<p>在图模式下，MindSpore 的自动微分是基于图结构的微分，和 PyTorch 不同，它不会在正向计算过程中记录任何信息，仅仅执行正常的计算流程（在PyNative模式下和 PyTorch 类似）。那么问题来了，如果整个正向计算都结束了，MindSpore 也没有记录任何信息，那它是如何知道反向传播怎么执行的呢？</p>
<p>MindSpore 在做自动微分时，需要传入正向图结构，自动微分的过程就是通过对正向图的分析从而得到反向传播信息，自动微分的结果与正向计算中具体的数值无关，仅和正向图结构有关。通过对正向图的自动微分，我们得到了反向传播过程，而这个反向传播过程其实也是通过一个图结构来表达，也就是反向图。将反向图添加到用户定义的正向图之后，组成一个最终的计算图。不过后添加的反向图和其中的反向算子我们并不感知，也无法手动添加，只能通过 MindSpore 为我们提供的接口自动添加，这样做也避免了我们在反向构图时引入错误。</p>
<p>最终，我们看似仅执行了正向图，其实图结构里既包含了正向算子，又包含了 MindSpore 为我们添加的反向算子，也就是说，MindSpore 在我们定义的正向图后面又新加了一个看不见的  <code class="docutils literal notranslate"><span class="pre">Cell</span></code>，这个  <code class="docutils literal notranslate"><span class="pre">Cell</span></code> 里都是根据正向图推导出来的反向算子。</p>
<p>而这个帮助我们构建反向图的接口就是 <a class="reference external" href="https://www.mindspore.cn/docs/api/zh-CN/r1.6/api_python/ops/mindspore.ops.GradOperation.html?highlight=gradopera#mindspore.ops.GradOperation">GradOperation</a> ：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GradNetWrtX</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GradNetWrtX</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">net</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad_op</span> <span class="o">=</span> <span class="n">GradOperation</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">gradient_function</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_op</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">gradient_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
<p>查看文档介绍我们可以发现，<code class="docutils literal notranslate"><span class="pre">GradOperation</span></code> 并不是一个算子，它的输入输出并不是 Tensor，而是 <code class="docutils literal notranslate"><span class="pre">Cell</span></code>，也就是我们定义的正向图和自动微分得到的反向图。为什么输入是一个图结构呢？因为构建反向图并不需要知道具体的输入数据是什么，只要知道正向图的结构就行了，有了正向图就可以推算出反向图结构，之后我们可以把正向图+反向图当成一个新的计算图来对待，这个新的计算图就像是一个函数，对于你输入的任何一组数据，它不仅能计算出正向的输出，还能计算出所有权重的梯度，由于图结构是固定的，并不保存中间变量，所以这个图结构可以被反复调用。</p>
<p>同理，之后我们再给网络加上优化器结构时，优化器也会加上优化器相关的算子，也就是再给这个计算图加点我们不感知的优化器算子，最终，计算图就构建完成。</p>
<p>在 MindSpore 中，大部分操作都会最终转换成真实的算子操作，最终加入到计算图中，因此，我们实际执行的计算图中算子的数量远多于我们最开始定义的计算图中算子的数量。</p>
</section>
<section id="trainonestepcell">
<h3>TrainOneStepCell<a class="headerlink" href="#trainonestepcell" title="Permalink to this headline"></a></h3>
<p>当我们定义的网络结构比较简单，只需要基本的反向传播流程，且无需对梯度做额外操作时，直接使用 MindSpore 封装的高级接口  <code class="docutils literal notranslate"><span class="pre">mindspore.Model</span></code>  即可，代码如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># define forward network</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">LeNet</span><span class="p">(</span><span class="n">num_classes</span><span class="p">)</span>
<span class="c1"># define loss</span>
<span class="n">net_loss</span> <span class="o">=</span> <span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">()</span>
<span class="c1"># define optimizer</span>
<span class="n">net_opt</span> <span class="o">=</span> <span class="n">Momentum</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="p">)</span>

<span class="c1"># define model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">net_loss</span><span class="p">,</span> <span class="n">net_opt</span><span class="p">)</span>
</pre></div>
</div>
<p>使用  <code class="docutils literal notranslate"><span class="pre">Model</span></code> 封装后，网络会自动添加反向、优化器相关算子。但有时网络结构复杂，有多个输入输出，或者在求梯度后，还需要对梯度进行额外的操作（例如梯度裁剪、使用 Loss Scale 等），此时我们需要自行掌控反向流程，为此，MindSpore 提供了  <code class="docutils literal notranslate"><span class="pre">nn.TrainOneStepCell</span></code> 模板，<code class="docutils literal notranslate"><span class="pre">nn.TrainOneStepCell</span></code>  的功能是添加反向算子和优化器算子，当需要自定义反向结构时，我们可以基于该模板重写反向流程（Model 实际也是调用  <code class="docutils literal notranslate"><span class="pre">TrainOneStepCell</span></code> 完成自动微分）。</p>
<p><code class="docutils literal notranslate"><span class="pre">nn.TrainOneStepCell</span></code> 继承了 <code class="docutils literal notranslate"><span class="pre">nn.Cell</span></code>，所以它构建反向网络结构的逻辑和我们构建正向网络的保持一致。这里我们对 <code class="docutils literal notranslate"><span class="pre">nn.TrainOneStepCell</span></code> 的源码进行分析：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">TrainOneStepCell</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Network training package class.</span>

<span class="sd">    Wraps the network with an optimizer. The resulting Cell is trained with input &#39;\*inputs&#39;.</span>
<span class="sd">    The backward graph will be created in the construct function to update the parameter. Different</span>
<span class="sd">    parallel modes are available for training.</span>

<span class="sd">    Args:</span>
<span class="sd">        network (Cell): The training network. The network only supports single output.</span>
<span class="sd">        optimizer (Union[Cell]): Optimizer for updating the weights.</span>
<span class="sd">        sens (numbers.Number): The scaling number to be filled as the input of backpropagation. Default value is 1.0.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **(\*inputs)** (Tuple(Tensor)) - Tuple of input tensors with shape :math:`(N, \ldots)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, a tensor means the loss value, the shape of which is usually :math:`()`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `sens` is not a number.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; loss_fn = nn.SoftmaxCrossEntropyWithLogits()</span>
<span class="sd">        &gt;&gt;&gt; optim = nn.Momentum(net.trainable_params(), learning_rate=0.1, momentum=0.9)</span>
<span class="sd">        &gt;&gt;&gt; #1) Using the WithLossCell existing provide</span>
<span class="sd">        &gt;&gt;&gt; loss_net = nn.WithLossCell(net, loss_fn)</span>
<span class="sd">        &gt;&gt;&gt; train_net = nn.TrainOneStepCell(loss_net, optim)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; #2) Using user-defined WithLossCell</span>
<span class="sd">        &gt;&gt;&gt; class MyWithLossCell(Cell):</span>
<span class="sd">        ...    def __init__(self, backbone, loss_fn):</span>
<span class="sd">        ...        super(MyWithLossCell, self).__init__(auto_prefix=False)</span>
<span class="sd">        ...        self._backbone = backbone</span>
<span class="sd">        ...        self._loss_fn = loss_fn</span>
<span class="sd">        ...</span>
<span class="sd">        ...    def construct(self, x, y, label):</span>
<span class="sd">        ...        out = self._backbone(x, y)</span>
<span class="sd">        ...        return self._loss_fn(out, label)</span>
<span class="sd">        ...</span>
<span class="sd">        ...    @property</span>
<span class="sd">        ...    def backbone_network(self):</span>
<span class="sd">        ...        return self._backbone</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; loss_net = MyWithLossCell(net, loss_fn)</span>
<span class="sd">        &gt;&gt;&gt; train_net = nn.TrainOneStepCell(loss_net, optim)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">sens</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TrainOneStepCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">auto_prefix</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">network</span> <span class="o">=</span> <span class="n">network</span>
        <span class="c1"># set require_grad=True for Parameters in network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">set_grad</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">GradOperation</span><span class="p">(</span><span class="n">get_by_list</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sens_param</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="c1"># set init value for back propagation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sens</span> <span class="o">=</span> <span class="n">sens</span>
        <span class="c1"># reduce_flag, used in distributed training</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reducer_flag</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad_reducer</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">identity</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">parallel_mode</span> <span class="o">=</span> <span class="n">_get_parallel_mode</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reducer_flag</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parallel_mode</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">DATA_PARALLEL</span><span class="p">,</span> <span class="n">ParallelMode</span><span class="o">.</span><span class="n">HYBRID_PARALLEL</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reducer_flag</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">_get_gradients_mean</span><span class="p">()</span>
            <span class="c1"># number of devices</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">degree</span> <span class="o">=</span> <span class="n">_get_device_num</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">grad_reducer</span> <span class="o">=</span> <span class="n">DistributedGradReducer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">degree</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
        <span class="c1"># get the loss of forward</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">)</span>
        <span class="c1"># set init value for backward propagation</span>
        <span class="n">sens</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sens</span><span class="p">)</span>
        <span class="c1"># perform backward propagation</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">sens</span><span class="p">)</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_reducer</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">(</span><span class="n">grads</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<p>如果理解了刚才介绍的 MindSpore 反向传播原理，就会很容易理解  <code class="docutils literal notranslate"><span class="pre">TrainOneStepCell</span></code> 的每一步在做什么，其实就是一个基于计算图求导的过程。</p>
<p>这里需要特别解释最后几个语句：</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">sens</span></code> 表示初始梯度，类似 PyTorch 中 <code class="docutils literal notranslate"><span class="pre">loss.backward(sens)</span></code> 的作用，为反向传播过程设置一个初始梯度。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">DistributedGradReducer</span></code> 是负责一个分布式梯度计算的  <code class="docutils literal notranslate"><span class="pre">Cell</span></code> （相当于又是一个计算图），在多卡训练时，它会将所有卡的梯度先求和，然后除以卡的数量（ <code class="docutils literal notranslate"><span class="pre">self.degree</span></code> ），最终得到全局平均的梯度。</p></li>
</ul>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="o">...</span>
<span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">sens</span><span class="p">)</span>
<span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_reducer</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">(</span><span class="n">grads</span><span class="p">))</span>
<span class="o">...</span>
</pre></div>
</div>
<p>除了 <code class="docutils literal notranslate"><span class="pre">TrainOneStepCell</span></code>，对于混合精度场景，MindSpore 还提供了带有 <a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/zh-CN/r1.6/enable_mixed_precision.html">Loss Scale</a> 的 <code class="docutils literal notranslate"><span class="pre">TrainOneStepWithLossScale</span></code>，原理其实是一样的，感兴趣的读者可以阅读混合精度的原理以及查看该方法的实现。</p>
<p>当我们使用  <code class="docutils literal notranslate"><span class="pre">TrainOneStepCell</span></code> 添加反向网络结构后，仍可以使用 <code class="docutils literal notranslate"><span class="pre">Model</span></code> 类进行封装，但此时不需要再给 <code class="docutils literal notranslate"><span class="pre">Model</span></code> 传入 loss、优化器这两个参数了，因为传入的网络已经包含了正向+反向结构。最后，通过调用  <code class="docutils literal notranslate"><span class="pre">model.train()</span></code> ，开始正常的训练流程。</p>
<p>参考链接： <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.6/mindspore/python/mindspore/nn/wrap/cell_wrapper.py">TrainOneStepCell</a> 、 <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.6/mindspore/python/mindspore/nn/wrap/loss_scale.py">TrainOneStepWithLossScale</a></p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="migration_script.html" class="btn btn-neutral float-left" title="迁移脚本" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="typical_api_comparision.html" class="btn btn-neutral float-right" title="典型算子或接口区别介绍" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>