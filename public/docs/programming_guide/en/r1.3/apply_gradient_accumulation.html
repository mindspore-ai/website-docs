<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Applying a Gradient Accumulation Algorithm &mdash; MindSpore master documentation</title><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Applying Quantization Aware Training" href="apply_quantization_aware_training.html" />
    <link rel="prev" title="Enabling Graph Kernel Fusion" href="enable_graph_kernel_fusion.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="architecture.html">Overall Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_structure.html">MindSpore API Overview</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quickstart</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="quick_start/linear_regression.html">Implementing Simple Linear Function Fitting</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start/quick_start.html">Implementing an Image Classification Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start/quick_video.html">Hands-on Installation and Experience</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Basic Concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dtype.html">dtype</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor.html">Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="operators.html">Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="cell.html">Cell</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Numpy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="numpy.html">NumPy Interfaces in MindSpore</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Execution Management</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="context.html">Configuring Running Information</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Pipeline</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dataset_sample.html">Quick Start of Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset.html">Loading Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline.html">Processing Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset_advanced.html">Advanced Usage of Pipeline</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build the Network</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="initializer.html">Initialization of Network Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter.html">Updating Network Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="layer.html">Model Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="loss.html">Loss Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">Optimization Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_net.html">Building a Customized Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="network_component.html">Common Network Components</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Train Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="run.html">Running Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="callback.html">Callback Mechanism</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_model.html">Saving Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="load_model_for_inference_and_transfer.html">Loading a Model for Inference and Transfer Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="train.html">Advanced Usage of Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference_ascend_910.html">Inference on the Ascend 910 AI processor</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference_ascend_310.html">Inference on Ascend 310</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference_gpu.html">Inference on a GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference_cpu.html">Inference on a CPU</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Distributed Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="distributed_training.html">Distributed Training Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_training_ascend.html">Parallel Distributed Training (Ascend)</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_training_gpu.html">Distributed Parallel Training (GPU)</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_pipeline_parallel.html">Pipeline Parallelism Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_host_device_training.html">Applying Host&amp;Device Hybrid Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_parameter_server_training.html">Training with Parameter Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_load_model_hybrid_parallel.html">Saving and Loading Models in Hybrid Parallel Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_inference.html">Distributed Inference With Multi Devices</a></li>
<li class="toctree-l1"><a class="reference internal" href="auto_parallel.html">Parallel Distributed Training Interfaces</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Function Debugging</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="debug_in_pynative_mode.html">Debugging in PyNative Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="dump_in_graph_mode.html">Using Dump in the Graph Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_debugging_info.html">Custom Debugging Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="evaluate_the_model_during_training.html">Evaluating the Model during Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="incremental_operator_build.html">Incremental Operator Build</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="optimize_data_processing.html">Optimizing the Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_mixed_precision.html">Enabling Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_graph_kernel_fusion.html">Enabling Graph Kernel Fusion</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Applying a Gradient Accumulation Algorithm</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#standalone-mode">Standalone Mode</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#importing-library-files">Importing Library Files</a></li>
<li class="toctree-l3"><a class="reference internal" href="#loading-the-dataset">Loading the Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="#defining-the-network">Defining the Network</a></li>
<li class="toctree-l3"><a class="reference internal" href="#defining-the-training-process">Defining the Training Process</a></li>
<li class="toctree-l3"><a class="reference internal" href="#defining-the-training-model">Defining the Training Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#training-and-saving-the-model">Training and Saving the Model</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#experiment-result">Experiment Result</a></li>
<li class="toctree-l2"><a class="reference internal" href="#parallel-mode">Parallel Mode</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#defining-the-parallel-training-process">Defining the Parallel Training Process</a></li>
<li class="toctree-l3"><a class="reference internal" href="#defining-the-parallel-training-model">Defining the Parallel Training Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#training-the-model">Training the Model</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="apply_quantization_aware_training.html">Applying Quantization Aware Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_post_training_quantization.html">Applying Post Training Quantization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cv.html">Computer Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp.html">Natural Language Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="hpc.html">High Performance Computing</a></li>
<li class="toctree-l1"><a class="reference internal" href="use_on_the_cloud.html">Using MindSpore on the Cloud</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Applying a Gradient Accumulation Algorithm</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/apply_gradient_accumulation.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="applying-a-gradient-accumulation-algorithm">
<h1>Applying a Gradient Accumulation Algorithm<a class="headerlink" href="#applying-a-gradient-accumulation-algorithm" title="Permalink to this headline"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Linux</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">Model</span> <span class="pre">Optimization</span></code> <code class="docutils literal notranslate"><span class="pre">Intermediate</span></code> <code class="docutils literal notranslate"><span class="pre">Expert</span></code></p>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.3/docs/mindspore/programming_guide/source_en/apply_gradient_accumulation.md" target="_blank"><img src="https://gitee.com/mindspore/docs/raw/r1.3/resource/_static/logo_source.png"></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>This tutorial describes the gradient accumulation training methods to solve the problem that some large-scale networks cannot train large batch_size due to insufficient memory.</p>
<p>In a traditional training method, after a loss and a gradient are computed each time, a parameter is directly updated by using the obtained gradient.</p>
<p>Compared to the traditional training method, mini-batch is introduced to the gradient accumulation. The loss and gradient are computed for each mini-batch data, but the model parameters are not updated immediately. Instead, the obtained gradients are accumulated first, and then after a specified number (N) of mini-batches, the accumulated gradient is used to update the network parameters. Before the next training, the accumulated gradients are cleared and re-accumulated. The ultimate objective is to achieve the same effect as training with N x Mini-batch data.</p>
<p>This tutorial describes how to implement gradient accumulation training in standalone mode and parallel mode, respectively.</p>
</section>
<section id="standalone-mode">
<h2>Standalone Mode<a class="headerlink" href="#standalone-mode" title="Permalink to this headline"></a></h2>
<p>In standalone mode, the training process consists of three parts: forward and backward training, parameter update, and accumulated gradient clearance. MNIST is used as an example dataset. To customize a simple model to implement gradient accumulation, perform the following steps:</p>
<blockquote>
<div><p>Download the main training sample code: <a class="reference external" href="https://gitee.com/mindspore/docs/tree/r1.3/docs/sample_code/gradient_accumulation">https://gitee.com/mindspore/docs/tree/r1.3/docs/sample_code/gradient_accumulation</a></p>
</div></blockquote>
<section id="importing-library-files">
<h3>Importing Library Files<a class="headerlink" href="#importing-library-files" title="Permalink to this headline"></a></h3>
<p>The following are the required public modules and MindSpore modules and library files.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">collections.abc</span> <span class="kn">import</span> <span class="n">Iterable</span>

<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ParameterTuple</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span><span class="p">,</span> <span class="n">DatasetHelper</span><span class="p">,</span> <span class="n">save_checkpoint</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">Cell</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">model_zoo.official.cv.lenet.src.dataset</span> <span class="kn">import</span> <span class="n">create_dataset</span>
<span class="kn">from</span> <span class="nn">model_zoo.official.cv.lenet.src.lenet</span> <span class="kn">import</span> <span class="n">LeNet5</span>
</pre></div>
</div>
</section>
<section id="loading-the-dataset">
<h3>Loading the Dataset<a class="headerlink" href="#loading-the-dataset" title="Permalink to this headline"></a></h3>
<p>Use the <code class="docutils literal notranslate"><span class="pre">MnistDataset</span></code> API provided by <code class="docutils literal notranslate"><span class="pre">dataset</span></code> of MindSpore to load the MNIST dataset. The code is imported from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.3/model_zoo/official/cv/lenet/src/dataset.py">dataset.py</a> in the <code class="docutils literal notranslate"><span class="pre">lenet</span></code> directory of <code class="docutils literal notranslate"><span class="pre">model_zoo</span></code>.</p>
</section>
<section id="defining-the-network">
<h3>Defining the Network<a class="headerlink" href="#defining-the-network" title="Permalink to this headline"></a></h3>
<p>LeNet is used as an example network. You can also use other networks, such as ResNet-50 and BERT. The code is imported from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.3/model_zoo/official/cv/lenet/src/lenet.py">lenet.py</a> in the <code class="docutils literal notranslate"><span class="pre">lenet</span></code> directory of <code class="docutils literal notranslate"><span class="pre">model_zoo</span></code>.</p>
</section>
<section id="defining-the-training-process">
<h3>Defining the Training Process<a class="headerlink" href="#defining-the-training-process" title="Permalink to this headline"></a></h3>
<p>The training process consists of three parts: forward and backward training, parameter update, and accumulated gradient clearance.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">TrainForwardBackward</span></code> calculates the loss and gradient, and uses grad_sum to implement gradient accumulation.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">TrainOptim</span></code> updates parameters.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">TrainClear</span></code> clears the gradient accumulation variable grad_sum.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">_sum_op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MultitypeFuncGraph</span><span class="p">(</span><span class="s2">&quot;grad_sum_op&quot;</span><span class="p">)</span>
<span class="n">_clear_op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MultitypeFuncGraph</span><span class="p">(</span><span class="s2">&quot;clear_op&quot;</span><span class="p">)</span>


<span class="nd">@_sum_op</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_cumulative_grad</span><span class="p">(</span><span class="n">grad_sum</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Apply grad sum to cumulative gradient.&quot;&quot;&quot;</span>
    <span class="n">add</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">AssignAdd</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">add</span><span class="p">(</span><span class="n">grad_sum</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>


<span class="nd">@_clear_op</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_clear_grad_sum</span><span class="p">(</span><span class="n">grad_sum</span><span class="p">,</span> <span class="n">zero</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Apply zero to clear grad_sum.&quot;&quot;&quot;</span>
    <span class="n">success</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">success</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">success</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">grad_sum</span><span class="p">,</span> <span class="n">zero</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">success</span>


<span class="k">class</span> <span class="nc">TrainForwardBackward</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">grad_sum</span><span class="p">,</span> <span class="n">sens</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TrainForwardBackward</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">auto_prefix</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">network</span> <span class="o">=</span> <span class="n">network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">set_grad</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">add_flags</span><span class="p">(</span><span class="n">defer_inline</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">ParameterTuple</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad_sum</span> <span class="o">=</span> <span class="n">grad_sum</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">GradOperation</span><span class="p">(</span><span class="n">get_by_list</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sens_param</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sens</span> <span class="o">=</span> <span class="n">sens</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hyper_map</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">HyperMap</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">sens</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Fill</span><span class="p">()(</span><span class="n">ops</span><span class="o">.</span><span class="n">DType</span><span class="p">()(</span><span class="n">loss</span><span class="p">),</span> <span class="n">ops</span><span class="o">.</span><span class="n">Shape</span><span class="p">()(</span><span class="n">loss</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">sens</span><span class="p">)</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">,</span> <span class="n">weights</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">sens</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hyper_map</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_sum_op</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_sum</span><span class="p">,</span> <span class="n">grads</span><span class="p">))</span>


<span class="k">class</span> <span class="nc">TrainOptim</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">grad_sum</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TrainOptim</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">auto_prefix</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad_sum</span> <span class="o">=</span> <span class="n">grad_sum</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">grad_sum</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">TrainClear</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_sum</span><span class="p">,</span> <span class="n">zeros</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TrainClear</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">auto_prefix</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad_sum</span> <span class="o">=</span> <span class="n">grad_sum</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">zeros</span> <span class="o">=</span> <span class="n">zeros</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hyper_map</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">HyperMap</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">success</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hyper_map</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_clear_op</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_sum</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">zeros</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">success</span>
</pre></div>
</div>
</section>
<section id="defining-the-training-model">
<h3>Defining the Training Model<a class="headerlink" href="#defining-the-training-model" title="Permalink to this headline"></a></h3>
<p>Each mini-batch computes the loss and gradient through forward and backward training, and uses mini_steps to control the accumulated times before each parameter update. After the number of accumulation times is reached, the parameter is updated
and the accumulated gradient variable is cleared.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GradientAccumulation</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_network</span> <span class="o">=</span> <span class="n">network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_loss_fn</span> <span class="o">=</span> <span class="n">loss_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>

        <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="o">.</span><span class="n">parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_grad_sum</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;grad_sum&quot;</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_zeros</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;zeros&quot;</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_train_forward_backward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_train_forward_backward_network</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_train_optim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_train_optim</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_train_clear</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_train_clear</span><span class="p">()</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_transform_callbacks</span><span class="p">(</span><span class="n">callbacks</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Transform callback to a list.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">callbacks</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[]</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">callbacks</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">):</span>
            <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">callbacks</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">[</span><span class="n">callbacks</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_build_train_forward_backward_network</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Build forward and backward network&quot;&quot;&quot;</span>
        <span class="n">network</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_network</span>
        <span class="n">network</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">WithLossCell</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_fn</span><span class="p">)</span>
        <span class="n">loss_scale</span> <span class="o">=</span> <span class="mf">1.0</span>
        <span class="n">network</span> <span class="o">=</span> <span class="n">TrainForwardBackward</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_grad_sum</span><span class="p">,</span> <span class="n">loss_scale</span><span class="p">)</span><span class="o">.</span><span class="n">set_train</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">network</span>

    <span class="k">def</span> <span class="nf">_build_train_optim</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Build optimizer network&quot;&quot;&quot;</span>
        <span class="n">network</span> <span class="o">=</span> <span class="n">TrainOptim</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_grad_sum</span><span class="p">)</span><span class="o">.</span><span class="n">set_train</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">network</span>

    <span class="k">def</span> <span class="nf">_build_train_clear</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Build clear network&quot;&quot;&quot;</span>
        <span class="n">network</span> <span class="o">=</span> <span class="n">TrainClear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_grad_sum</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_zeros</span><span class="p">)</span><span class="o">.</span><span class="n">set_train</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">network</span>

    <span class="k">def</span> <span class="nf">train_process</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">train_dataset</span><span class="p">,</span> <span class="n">mini_steps</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Training process. The data would be passed to network directly.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">dataset_helper</span> <span class="o">=</span> <span class="n">DatasetHelper</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">epoch_num</span><span class="o">=</span><span class="n">epoch</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>
            <span class="n">step</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">next_element</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataset_helper</span><span class="p">):</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_forward_backward</span><span class="p">(</span><span class="o">*</span><span class="n">next_element</span><span class="p">)</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">mini_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">step</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;epoch:&quot;</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;step:&quot;</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="s2">&quot;loss is &quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_train_optim</span><span class="p">()</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_train_clear</span><span class="p">()</span>

            <span class="n">train_dataset</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

        <span class="n">save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_train_forward_backward</span><span class="p">,</span> <span class="s2">&quot;gradient_accumulation.ckpt&quot;</span><span class="p">,</span> <span class="p">)</span>
</pre></div>
</div>
</section>
<section id="training-and-saving-the-model">
<h3>Training and Saving the Model<a class="headerlink" href="#training-and-saving-the-model" title="Permalink to this headline"></a></h3>
<p>Call the network, optimizer, and loss function, and then customize the <code class="docutils literal notranslate"><span class="pre">train_process</span></code> API of <code class="docutils literal notranslate"><span class="pre">GradientAccumulation</span></code> to train the model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;MindSpore Grad Cumulative Example&#39;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--device_target&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">,</span> <span class="n">choices</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;GPU&#39;</span><span class="p">],</span>
                        <span class="n">help</span><span class="o">=</span><span class="s1">&#39;device where the code will be implemented (default: GPU)&#39;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--data_path&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;./Data&quot;</span><span class="p">,</span>
                        <span class="n">help</span><span class="o">=</span><span class="s1">&#39;path where the dataset is saved&#39;</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">device_target</span><span class="p">)</span>
    <span class="n">ds_train</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data_path</span><span class="p">,</span> <span class="s2">&quot;train&quot;</span><span class="p">),</span> <span class="mi">32</span><span class="p">)</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">LeNet5</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">net_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">)</span>
    <span class="n">net_opt</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">GradientAccumulation</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">net_loss</span><span class="p">,</span> <span class="n">net_opt</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;============== Starting Training ==============&quot;</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train_process</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">ds_train</span><span class="p">,</span> <span class="n">mini_steps</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="experiment-result">
<h2>Experiment Result<a class="headerlink" href="#experiment-result" title="Permalink to this headline"></a></h2>
<p>After 10 epochs, the accuracy on the test set is about 96.31%.</p>
<p><strong>Start training.</strong></p>
<ol class="arabic">
<li><p>Run the training code and view the running result.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>train.py<span class="w"> </span>--data_path<span class="o">=</span>./MNIST_Data
</pre></div>
</div>
<p>The output is as follows. You can see that the loss value decreases with the training.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>epoch: 1 step: 27 loss is  0.3660637
epoch: 1 step: 28 loss is  0.25238192
...
epoch: 3 step: 2 loss is  0.12296932
epoch: 3 step: 3 loss is  0.15799297
...
epoch: 10 step: 448 loss is  0.06443884
epoch: 10 step: 449 loss is  0.0067842817
</pre></div>
</div>
</li>
<li><p>Check the saved checkpoint files.</p>
<p>The checkpoint file <code class="docutils literal notranslate"><span class="pre">gradient_accumulation.ckpt</span></code>, that is, the model file, is saved during training.</p>
</li>
</ol>
<p><strong>Validate the model.</strong></p>
<p>Use the saved checkpoint file to load the validation dataset through <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.3/model_zoo/official/cv/lenet/train.py">eval.py</a> in the lenet directory of model_zoo.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>eval.py<span class="w"> </span>--data_path<span class="o">=</span>./MNIST_Data<span class="w"> </span>--ckpt_path<span class="o">=</span>./gradient_accumulation.ckpt<span class="w"> </span>--device_target<span class="o">=</span>GPU
</pre></div>
</div>
<p>The output is as follows. The accuracy of the validation dataset is about 96.31%, which is the same as the result when the value of batch_size is 32.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>============== Starting Testing ==============
============== {&#39;Accuracy&#39;: 0.9631730769230769} ==============
</pre></div>
</div>
</section>
<section id="parallel-mode">
<h2>Parallel Mode<a class="headerlink" href="#parallel-mode" title="Permalink to this headline"></a></h2>
<p>If gradient accumulation is used in <code class="docutils literal notranslate"><span class="pre">SEMI_AUTO_PARALLEL</span></code> and <code class="docutils literal notranslate"><span class="pre">AUTO_PARALLEL</span></code> modes, the accumulation steps and update steps are delivered as two graphs and executed alternately. In an accumulation step graph, only the forward and backward operations and gradient accumulation are performed. In an update step graph, the forward and backward operations and parameter updates are performed. The example in <a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/en/r1.3/distributed_training_ascend.html">Parallel Distributed Training</a> is used to describe the procedure.</p>
<blockquote>
<div><p>Download the main training sample code: <a class="reference external" href="https://gitee.com/mindspore/docs/tree/r1.3/docs/sample_code/distributed_training">https://gitee.com/mindspore/docs/tree/r1.3/docs/sample_code/distributed_training</a></p>
</div></blockquote>
<section id="defining-the-parallel-training-process">
<h3>Defining the Parallel Training Process<a class="headerlink" href="#defining-the-parallel-training-process" title="Permalink to this headline"></a></h3>
<p>Generally, after the forward network is defined, <a class="reference external" href="https://www.mindspore.cn/docs/api/en/r1.3/api_python/nn/mindspore.nn.TrainOneStepCell.html"><code class="docutils literal notranslate"><span class="pre">TrainOneStepCell</span></code></a> is used to associate the forward and backward networks with the optimizer. However, two different situations, accumulation and update, exist during gradient accumulation. We need to make some modifications based on the original class definition. The sample code is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.common.dtype</span> <span class="k">as</span> <span class="nn">mstype</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">TrainOneStepCell</span>
<span class="kn">from</span> <span class="nn">mindspore.common.initializer</span> <span class="kn">import</span> <span class="n">initializer</span>

<span class="n">zeroslike</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ZerosLike</span><span class="p">()</span>
<span class="n">reset_accu_grads</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MultitypeFuncGraph</span><span class="p">(</span><span class="s2">&quot;reset_accu_grads&quot;</span><span class="p">)</span>

<span class="nd">@reset_accu_grads</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;Tensor&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_reset_accu_grads</span><span class="p">(</span><span class="n">accu_grad</span><span class="p">):</span>
    <span class="n">succ</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">succ</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">accu_grad</span><span class="p">,</span> <span class="n">zeroslike</span><span class="p">(</span><span class="n">accu_grad</span><span class="p">)))</span>

<span class="n">cast</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>
<span class="n">update_accu_grads</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MultitypeFuncGraph</span><span class="p">(</span><span class="s2">&quot;update_accu_grads&quot;</span><span class="p">)</span>


<span class="nd">@update_accu_grads</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_update_accu_grads</span><span class="p">(</span><span class="n">accu_grad</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
    <span class="n">succ</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">succ</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">assign_add</span><span class="p">(</span><span class="n">accu_grad</span><span class="p">,</span> <span class="n">cast</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)))</span>

<span class="k">class</span> <span class="nc">TrainAccuStepsCell</span><span class="p">(</span><span class="n">TrainOneStepCell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">sens</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TrainAccuStepsCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">sens</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">accumulation</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">accumulation_steps</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;grad_accumulation_step&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">accu_grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;accu_grads&quot;</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hyper_map</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">HyperMap</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Defines the computation performed.&quot;&quot;&quot;</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">sens</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Fill</span><span class="p">()(</span><span class="n">ops</span><span class="o">.</span><span class="n">DType</span><span class="p">()(</span><span class="n">loss</span><span class="p">),</span> <span class="n">ops</span><span class="o">.</span><span class="n">Shape</span><span class="p">()(</span><span class="n">loss</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">sens</span><span class="p">)</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">,</span> <span class="n">weights</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">sens</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">accumulation</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">accumulation_steps</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">accu_succ</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hyper_map</span><span class="p">(</span><span class="n">update_accu_grads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">accu_grads</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">accu_succ</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">accumulation</span><span class="p">:</span>
            <span class="n">succ</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_reducer</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
            <span class="n">accu_grads</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">accu_grads</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span>
            <span class="n">accu_succ</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hyper_map</span><span class="p">(</span><span class="n">reset_accu_grads</span><span class="p">,</span> <span class="n">accu_grads</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">accu_succ</span><span class="p">)</span>
            <span class="n">succ</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">succ</span><span class="p">)</span>
</pre></div>
</div>
<p>On the basis of <code class="docutils literal notranslate"><span class="pre">TrainOneStepCell</span></code>, definitions of the accumulation flag <code class="docutils literal notranslate"><span class="pre">accumulation</span></code> and the accumulation gradient parameter <code class="docutils literal notranslate"><span class="pre">accu_grads</span></code> are added to distinguish the training process and save the accumulation gradient value, respectively. In an accumulation step graph, if <code class="docutils literal notranslate"><span class="pre">accumulation</span></code> is set to True, only the forward and backward operations are performed and gradients are accumulated to the parameter <code class="docutils literal notranslate"><span class="pre">accu_grads</span></code>. In an update step graph, if <code class="docutils literal notranslate"><span class="pre">accumulation</span></code> is set to False, the forward and backward operations and parameter updates are performed.</p>
<blockquote>
<div><p>The gradient accumulation in parallel mode needs to be implemented based on the internal graph optimization of the framework. Therefore, <code class="docutils literal notranslate"><span class="pre">accumulation</span></code> and <code class="docutils literal notranslate"><span class="pre">accu_grads</span></code> defined on the network are specific characters and cannot be modified.</p>
</div></blockquote>
<p>In the dynamic loss scale scenario, in addition to the gradient, the overflow flag status also needs to be accumulated. The code can be modified based on <a class="reference external" href="https://www.mindspore.cn/docs/api/en/r1.3/api_python/nn/mindspore.nn.TrainOneStepWithLossScaleCell.html#mindspore.nn.TrainOneStepWithLossScaleCell"><code class="docutils literal notranslate"><span class="pre">TrainOneStepWithLossScaleCell</span></code></a>. The implementation code is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.common.dtype</span> <span class="k">as</span> <span class="nn">mstype</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">TrainOneStepWithLossScaleCell</span>
<span class="kn">from</span> <span class="nn">mindspore.nn.wrap.loss_scale</span> <span class="kn">import</span> <span class="n">_grad_scale</span>
<span class="kn">from</span> <span class="nn">mindspore.common.initializer</span> <span class="kn">import</span> <span class="n">initializer</span>

<span class="n">zeroslike</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ZerosLike</span><span class="p">()</span>
<span class="n">reset_accu_grads</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MultitypeFuncGraph</span><span class="p">(</span><span class="s2">&quot;reset_accu_grads&quot;</span><span class="p">)</span>

<span class="nd">@reset_accu_grads</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;Tensor&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_reset_accu_grads</span><span class="p">(</span><span class="n">accu_grad</span><span class="p">):</span>
    <span class="n">succ</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">succ</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">accu_grad</span><span class="p">,</span> <span class="n">zeroslike</span><span class="p">(</span><span class="n">accu_grad</span><span class="p">)))</span>

<span class="n">cast</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>
<span class="n">update_accu_grads</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MultitypeFuncGraph</span><span class="p">(</span><span class="s2">&quot;update_accu_grads&quot;</span><span class="p">)</span>


<span class="nd">@update_accu_grads</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_update_accu_grads</span><span class="p">(</span><span class="n">accu_grad</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
    <span class="n">succ</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">succ</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">assign_add</span><span class="p">(</span><span class="n">accu_grad</span><span class="p">,</span> <span class="n">cast</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)))</span>


<span class="k">class</span> <span class="nc">TrainAccuStepsWithLossScaleCell</span><span class="p">(</span><span class="n">TrainOneStepWithLossScaleCell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scale_sense</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TrainAccuStepsWithLossScaleCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scale_sense</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">accumulation</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">accumulation_steps</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;grad_accumulation_step&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">one</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">zero</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">accu_grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;accu_grads&quot;</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">accu_overflow</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">accu_loss</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cast</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logical_or</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">LogicalOr</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">not_equal</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">NotEqual</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">select</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Select</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Defines the computation performed.&quot;&quot;&quot;</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">scaling_sens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_sense</span>
        <span class="n">status</span><span class="p">,</span> <span class="n">scaling_sens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_overflow_check</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">scaling_sens</span><span class="p">)</span>
        <span class="n">scaling_sens_filled</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span> <span class="o">*</span> <span class="n">ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">scaling_sens</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">loss</span><span class="p">))</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">,</span> <span class="n">weights</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">scaling_sens_filled</span><span class="p">)</span>
        <span class="c1"># accumulate gradients</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">accumulation</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">accumulation_steps</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">accu_succ</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hyper_map</span><span class="p">(</span><span class="n">update_accu_grads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">accu_grads</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">accu_succ</span><span class="p">)</span>
        <span class="n">overflow</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_overflow_status</span><span class="p">(</span><span class="n">status</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span>
        <span class="n">overflow</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">logical_or</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">not_equal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">accu_overflow</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero</span><span class="p">),</span> <span class="n">overflow</span><span class="p">)</span>
        <span class="n">accu_overflow</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">overflow</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">one</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">accumulation</span><span class="p">:</span>
            <span class="n">succ</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">accu_overflow</span> <span class="o">=</span> <span class="n">accu_overflow</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">accu_overflow</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero</span>
            <span class="c1"># apply grad reducer on grads</span>
            <span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_reducer</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
            <span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hyper_map</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_grad_scale</span><span class="p">,</span> <span class="n">scaling_sens</span><span class="p">),</span> <span class="n">grads</span><span class="p">)</span>
            <span class="n">accu_overflow</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">allreduce</span><span class="p">(</span><span class="n">accu_overflow</span><span class="p">)</span>
            <span class="n">overflow</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">less_equal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base</span><span class="p">,</span> <span class="n">accu_overflow</span><span class="p">)</span>
            <span class="n">accu_grads</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">accu_grads</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span>
            <span class="n">accu_succ</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hyper_map</span><span class="p">(</span><span class="n">reset_accu_grads</span><span class="p">,</span> <span class="n">accu_grads</span><span class="p">)</span>
            <span class="n">overflow</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">overflow</span><span class="p">,</span> <span class="n">accu_succ</span><span class="p">)</span>
            <span class="n">overflow</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">overflow</span><span class="p">,</span> <span class="p">(()))</span>
            <span class="n">overflow</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">process_loss_scale</span><span class="p">(</span><span class="n">overflow</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">overflow</span><span class="p">:</span>
                <span class="n">succ</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">succ</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>

        <span class="n">ret</span> <span class="o">=</span> <span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">overflow</span><span class="p">,</span> <span class="n">scaling_sens</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="n">succ</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">accu_overflow</span></code> is a parameter used to store the accumulation overflow flag status.</p>
</section>
<section id="defining-the-parallel-training-model">
<h3>Defining the Parallel Training Model<a class="headerlink" href="#defining-the-parallel-training-model" title="Permalink to this headline"></a></h3>
<p>The network encapsulated by <code class="docutils literal notranslate"><span class="pre">cell_wrapper</span></code> contains the forward and backward operations and optimizer implementation. You need to connect the dataset to the network and execute the two graphs alternately. The preceding functions are implemented based on the <a class="reference external" href="https://www.mindspore.cn/docs/api/en/r1.3/api_python/mindspore.html?highlight=model#mindspore.Model"><code class="docutils literal notranslate"><span class="pre">Model</span></code></a> API in the framework.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>
<span class="kn">from</span> <span class="nn">mindspore.train.callback</span> <span class="kn">import</span> <span class="n">RunContext</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">mindspore.context</span> <span class="kn">import</span> <span class="n">ParallelMode</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Model</span><span class="p">,</span> <span class="n">connect_network_with_dataset</span>
<span class="kn">from</span> <span class="nn">mindspore.common.dtype</span> <span class="kn">import</span> <span class="n">pytype_to_dtype</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">init_exec_dataset</span>
<span class="kn">from</span> <span class="nn">mindspore.train.train_thor.dataset_helper</span> <span class="kn">import</span> <span class="n">DatasetHelper</span>


<span class="k">def</span> <span class="nf">_convert_type</span><span class="p">(</span><span class="n">types</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Convert from numpy type to tensor type.</span>

<span class="sd">    Args:</span>
<span class="sd">        types (list): Numpy type list of element in dataset.</span>

<span class="sd">    Returns:</span>
<span class="sd">        list, list of element in dataset.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ms_types</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">np_type</span> <span class="ow">in</span> <span class="n">types</span><span class="p">:</span>
        <span class="n">ms_type</span> <span class="o">=</span> <span class="n">pytype_to_dtype</span><span class="p">(</span><span class="n">np_type</span><span class="p">)</span>
        <span class="n">ms_types</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ms_type</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ms_types</span>


<span class="k">def</span> <span class="nf">_get_types_and_shapes</span><span class="p">(</span><span class="n">dataset</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get dataset types and shapes.&quot;&quot;&quot;</span>
    <span class="n">dataset_types</span> <span class="o">=</span> <span class="n">_convert_type</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">output_types</span><span class="p">())</span>
    <span class="n">dataset_shapes</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">output_shapes</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">dataset_types</span><span class="p">,</span> <span class="n">dataset_shapes</span>


<span class="k">def</span> <span class="nf">_exec_datagraph</span><span class="p">(</span><span class="n">exec_dataset</span><span class="p">,</span> <span class="n">dataset_size</span><span class="p">,</span> <span class="n">phase</span><span class="o">=</span><span class="s1">&#39;dataset&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize and execute the dataset graph.&quot;&quot;&quot;</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">exec_dataset</span><span class="o">.</span><span class="n">get_batch_size</span><span class="p">()</span>
    <span class="n">input_indexs</span> <span class="o">=</span> <span class="n">exec_dataset</span><span class="o">.</span><span class="n">input_indexs</span>

    <span class="c1"># transform data format</span>
    <span class="n">dataset_types</span><span class="p">,</span> <span class="n">dataset_shapes</span> <span class="o">=</span> <span class="n">_get_types_and_shapes</span><span class="p">(</span><span class="n">exec_dataset</span><span class="p">)</span>
    <span class="n">init_exec_dataset</span><span class="p">(</span><span class="n">exec_dataset</span><span class="o">.</span><span class="n">__transfer_dataset__</span><span class="o">.</span><span class="n">queue_name</span><span class="p">,</span>
                      <span class="n">dataset_size</span><span class="p">,</span>
                      <span class="n">batch_size</span><span class="p">,</span>
                      <span class="n">dataset_types</span><span class="p">,</span>
                      <span class="n">dataset_shapes</span><span class="p">,</span>
                      <span class="n">input_indexs</span><span class="p">,</span>
                      <span class="n">phase</span><span class="o">=</span><span class="n">phase</span><span class="p">,</span>
                      <span class="n">need_run</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Model_ACCU</span><span class="p">(</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eval_network</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">eval_indexes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">amp_level</span><span class="o">=</span><span class="s2">&quot;O0&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Model_ACCU</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="n">eval_network</span><span class="p">,</span>
                                         <span class="n">eval_indexes</span><span class="p">,</span> <span class="n">amp_level</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_frequency</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;grad_accumulation_step&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_train_network</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_train_network</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_exec_preprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">,</span> <span class="n">is_train</span><span class="p">,</span> <span class="n">phase</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">dataset_sink_mode</span><span class="p">,</span> <span class="n">sink_size</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                         <span class="n">epoch_num</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">iter_first_order</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initializes dataset.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">dataset_sink_mode</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_train</span><span class="p">:</span>
            <span class="n">dataset</span><span class="o">.</span><span class="n">__loop_size__</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">dataset_helper</span> <span class="o">=</span> <span class="n">DatasetHelper</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">dataset_sink_mode</span><span class="p">,</span> <span class="n">sink_size</span><span class="p">,</span> <span class="n">epoch_num</span><span class="p">,</span> <span class="n">iter_first_order</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">dataset_sink_mode</span> <span class="ow">and</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span> <span class="o">!=</span> <span class="s2">&quot;GPU&quot;</span><span class="p">:</span>
            <span class="n">network</span> <span class="o">=</span> <span class="n">connect_network_with_dataset</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">dataset_helper</span><span class="p">)</span>
        <span class="n">network</span><span class="o">.</span><span class="n">set_train</span><span class="p">(</span><span class="n">is_train</span><span class="p">)</span>
        <span class="n">network</span><span class="o">.</span><span class="n">phase</span> <span class="o">=</span> <span class="n">phase</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parallel_mode</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">SEMI_AUTO_PARALLEL</span><span class="p">,</span> <span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">):</span>
            <span class="n">network</span><span class="o">.</span><span class="n">set_auto_parallel</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">dataset_helper</span><span class="p">,</span> <span class="n">network</span>

    <span class="k">def</span> <span class="nf">_train_dataset_sink_process</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">train_dataset</span><span class="p">,</span> <span class="n">list_callback</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">cb_params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sink_size</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Training process. The data would be passed to network through dataset channel.</span>

<span class="sd">        Args:</span>
<span class="sd">            epoch (int): Total number of iterations on the data.</span>
<span class="sd">            train_dataset (Dataset): A training dataset iterator. If there is no</span>
<span class="sd">                                     loss_fn, a tuple with multiple data (data1, data2, data3, ...) should be</span>
<span class="sd">                                     returned and passed to the network. Otherwise, a tuple (data, label) should</span>
<span class="sd">                                     be returned. The data and label would be passed to the network and loss</span>
<span class="sd">                                     function respectively.</span>
<span class="sd">            list_callback (Callback): Executor of callback list. Default: None.</span>
<span class="sd">            cb_params (_InternalCallbackParam): Callback parameters. Default: None.</span>
<span class="sd">            sink_size (int): Control the amount of data in each sink. Default: -1.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">sink_size</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="n">epoch_num</span> <span class="o">=</span> <span class="n">epoch</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">epoch_num</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">epoch</span> <span class="o">*</span> <span class="n">sink_size</span> <span class="o">/</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">get_dataset_size</span><span class="p">())</span>

        <span class="n">iter_first_order</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">iter_second_order</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_frequency</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">train_dataset</span><span class="o">.</span><span class="n">__loop_size__</span> <span class="o">=</span> <span class="n">iter_second_order</span>
        <span class="n">dataset_helper</span><span class="p">,</span> <span class="n">train_network</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_exec_preprocess</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_train_network</span><span class="p">,</span>
                                                              <span class="n">is_train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                                              <span class="n">phase</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">,</span>
                                                              <span class="n">dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
                                                              <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                                              <span class="n">sink_size</span><span class="o">=</span><span class="n">sink_size</span><span class="p">,</span>
                                                              <span class="n">epoch_num</span><span class="o">=</span><span class="n">epoch_num</span><span class="p">,</span>
                                                              <span class="n">iter_first_order</span><span class="o">=</span><span class="n">iter_first_order</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_train_network</span> <span class="o">=</span> <span class="n">train_network</span>
        <span class="n">cb_params</span><span class="o">.</span><span class="n">train_network</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_network</span>
        <span class="n">cb_params</span><span class="o">.</span><span class="n">cur_step_num</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="n">run_context</span> <span class="o">=</span> <span class="n">RunContext</span><span class="p">(</span><span class="n">cb_params</span><span class="p">)</span>
        <span class="n">list_callback</span><span class="o">.</span><span class="n">begin</span><span class="p">(</span><span class="n">run_context</span><span class="p">)</span>

        <span class="c1"># used to stop training for early stop, such as stopAtTIme or stopATStep</span>
        <span class="n">should_stop</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">switch_branch_one</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">index_first_order</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">train_network_init_flag</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">has_do_dataset_init</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>
            <span class="n">cb_params</span><span class="o">.</span><span class="n">cur_epoch_num</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="n">list_callback</span><span class="o">.</span><span class="n">epoch_begin</span><span class="p">(</span><span class="n">run_context</span><span class="p">)</span>
            <span class="c1"># for data sink dataset_helper only iter once, other wise iter epoch_size times.</span>
            <span class="k">for</span> <span class="n">inputs</span> <span class="ow">in</span> <span class="n">dataset_helper</span><span class="p">:</span>
                <span class="n">list_callback</span><span class="o">.</span><span class="n">step_begin</span><span class="p">(</span><span class="n">run_context</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">switch_branch_one</span><span class="p">:</span>
                    <span class="n">cb_params</span><span class="o">.</span><span class="n">cur_step_num</span> <span class="o">+=</span> <span class="n">iter_second_order</span>
                    <span class="k">if</span> <span class="n">train_network_init_flag</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_train_network</span><span class="o">.</span><span class="n">add_flags_recursive</span><span class="p">(</span><span class="n">accumulation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_train_network</span><span class="o">.</span><span class="n">phase</span> <span class="o">=</span> <span class="s1">&#39;train0&#39;</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">cb_params</span><span class="o">.</span><span class="n">cur_step_num</span> <span class="o">+=</span> <span class="n">iter_first_order</span>
                    <span class="k">if</span> <span class="n">train_network_init_flag</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_train_network</span><span class="o">.</span><span class="n">add_flags_recursive</span><span class="p">(</span><span class="n">accumulation</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                        <span class="n">train_network_init_flag</span> <span class="o">=</span> <span class="kc">False</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_train_network</span><span class="o">.</span><span class="n">phase</span> <span class="o">=</span> <span class="s1">&#39;train1&#39;</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">has_do_dataset_init</span><span class="p">:</span>
                        <span class="n">_exec_datagraph</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">iter_first_order</span><span class="p">,</span> <span class="n">phase</span><span class="o">=</span><span class="s1">&#39;train1_dataset&#39;</span><span class="p">)</span>
                        <span class="n">has_do_dataset_init</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="n">switch_branch_one</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">switch_branch_one</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_network</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">)</span>
                <span class="n">cb_params</span><span class="o">.</span><span class="n">net_outputs</span> <span class="o">=</span> <span class="n">outputs</span>
                <span class="n">list_callback</span><span class="o">.</span><span class="n">step_end</span><span class="p">(</span><span class="n">run_context</span><span class="p">)</span>

            <span class="n">list_callback</span><span class="o">.</span><span class="n">epoch_end</span><span class="p">(</span><span class="n">run_context</span><span class="p">)</span>
            <span class="n">should_stop</span> <span class="o">=</span> <span class="n">should_stop</span> <span class="ow">or</span> <span class="n">run_context</span><span class="o">.</span><span class="n">get_stop_requested</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">should_stop</span><span class="p">:</span>
                <span class="k">break</span>
        <span class="n">dataset_helper</span><span class="o">.</span><span class="n">stop_send</span><span class="p">()</span>

        <span class="n">list_callback</span><span class="o">.</span><span class="n">end</span><span class="p">(</span><span class="n">run_context</span><span class="p">)</span>
</pre></div>
</div>
<p>In the sample code, the subclass <code class="docutils literal notranslate"><span class="pre">Model_ACCU</span></code> rewrites the <code class="docutils literal notranslate"><span class="pre">_exec_preprocess</span></code> dataset encapsulation and the <code class="docutils literal notranslate"><span class="pre">_train_dataset_sink_process</span></code> training offload method of the base class, and delivers the data subgraph and training graph of the accumulation step graph <code class="docutils literal notranslate"><span class="pre">(accumulation=True)</span></code> and the update step graph <code class="docutils literal notranslate"><span class="pre">(accumulation=False)</span></code>, respectively. The training process is alternately executed. The number of offloaded steps of the accumulation step graph is the value of <code class="docutils literal notranslate"><span class="pre">grad_accumulation_step</span></code> minus 1 and that of the update step graph is 1.</p>
</section>
<section id="training-the-model">
<h3>Training the Model<a class="headerlink" href="#training-the-model" title="Permalink to this headline"></a></h3>
<p>After the preceding definition is complete, you can use the training API to complete model training. Configure the <code class="docutils literal notranslate"><span class="pre">grad_accumulation_step</span></code> parameter in <code class="docutils literal notranslate"><span class="pre">context.set_auto_parallel_context</span></code> to enable gradient accumulation. Then, use the modified <code class="docutils literal notranslate"><span class="pre">cell_wrapper</span></code> to encapsulate the network structure and transfer it to the <code class="docutils literal notranslate"><span class="pre">Model_ACCU</span></code> to initialize the model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">,</span> <span class="n">gradients_mean</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">grad_accumulation_step</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
<span class="n">loss_cb</span> <span class="o">=</span> <span class="n">LossMonitor</span><span class="p">()</span>
<span class="n">data_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;DATA_PATH&#39;</span><span class="p">)</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">resnet50</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">SoftmaxCrossEntropyExpand</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">Momentum</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()),</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">)</span>
<span class="n">net_with_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">WithLossCell</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
<span class="n">net_with_loss</span> <span class="o">=</span> <span class="n">VirtualDatasetCell</span><span class="p">(</span><span class="n">net_with_loss</span><span class="p">)</span>
<span class="n">wrap_net</span> <span class="o">=</span> <span class="n">TrainAccuStepsCell</span><span class="p">(</span><span class="n">net_with_loss</span><span class="p">,</span> <span class="n">opt</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model_ACCU</span><span class="p">(</span><span class="n">wrap_net</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">epoch_size</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">loss_cb</span><span class="p">],</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>The following information can be found in the logs:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>epoch: 1 step: 234, loss is 1.7588712
epoch: 2 step: 234, loss is 1.7275971
epoch: 3 step: 234, loss is 1.5423206
epoch: 4 step: 234, loss is 1.2762429
epoch: 5 step: 234, loss is 1.0915408
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="enable_graph_kernel_fusion.html" class="btn btn-neutral float-left" title="Enabling Graph Kernel Fusion" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="apply_quantization_aware_training.html" class="btn btn-neutral float-right" title="Applying Quantization Aware Training" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>