<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Pipeline Parallelism Application &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script><script src="_static/jquery.js"></script>
        <script src="_static/js/theme.js"></script><script src="_static/underscore.js"></script><script src="_static/doctools.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Applying Host&amp;Device Hybrid Training" href="apply_host_device_training.html" />
    <link rel="prev" title="Distributed Parallel Training (GPU)" href="distributed_training_gpu.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="architecture.html">Overall Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_structure.html">MindSpore API Overview</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quickstart</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="quick_start/linear_regression.html">Implementing Simple Linear Function Fitting</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start/quick_start.html">Implementing an Image Classification Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start/quick_video.html">Hands-on Installation and Experience</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Basic Concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dtype.html">dtype</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor.html">Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="operators.html">Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="cell.html">Cell</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Numpy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="numpy.html">NumPy Interfaces in MindSpore</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Execution Management</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="context.html">Configuring Running Information</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Pipeline</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dataset_sample.html">Quick Start of Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset.html">Loading Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline.html">Processing Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset_advanced.html">Advanced Usage of Pipeline</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build the Network</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="initializer.html">Initialization of Network Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter.html">Updating Network Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="layer.html">Model Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="loss.html">Loss Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">Optimization Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_net.html">Building a Customized Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="network_component.html">Common Network Components</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Train Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="run.html">Running Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="callback.html">Callback Mechanism</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_model.html">Saving Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="load_model_for_inference_and_transfer.html">Loading a Model for Inference and Transfer Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="train.html">Advanced Usage of Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference_ascend_910.html">Inference on the Ascend 910 AI processor</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference_ascend_310.html">Inference on Ascend 310</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference_gpu.html">Inference on a GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference_cpu.html">Inference on a CPU</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Distributed Training</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="distributed_training.html">Distributed Training Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_training_ascend.html">Parallel Distributed Training (Ascend)</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_training_gpu.html">Distributed Parallel Training (GPU)</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Pipeline Parallelism Application</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#preparations">Preparations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#downloading-the-dataset">Downloading the Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="#configuring-the-distributed-environment">Configuring the Distributed Environment</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#defining-the-network">Defining the Network</a></li>
<li class="toctree-l2"><a class="reference internal" href="#training-the-network">Training the Network</a></li>
<li class="toctree-l2"><a class="reference internal" href="#running-the-multi-node-script">Running the Multi-node Script</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="apply_host_device_training.html">Applying Host&amp;Device Hybrid Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_parameter_server_training.html">Training with Parameter Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_load_model_hybrid_parallel.html">Saving and Loading Models in Hybrid Parallel Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_inference.html">Distributed Inference With Multi Devices</a></li>
<li class="toctree-l1"><a class="reference internal" href="auto_parallel.html">Parallel Distributed Training Interfaces</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Function Debugging</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="debug_in_pynative_mode.html">Debugging in PyNative Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="dump_in_graph_mode.html">Using Dump in the Graph Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_debugging_info.html">Custom Debugging Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="evaluate_the_model_during_training.html">Evaluating the Model during Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="incremental_operator_build.html">Incremental Operator Build</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="optimize_data_processing.html">Optimizing the Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_mixed_precision.html">Enabling Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_graph_kernel_fusion.html">Enabling Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_gradient_accumulation.html">Applying a Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_quantization_aware_training.html">Applying Quantization Aware Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_post_training_quantization.html">Applying Post Training Quantization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cv.html">Computer Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp.html">Natural Language Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="hpc.html">High Performance Computing</a></li>
<li class="toctree-l1"><a class="reference internal" href="use_on_the_cloud.html">Using MindSpore on the Cloud</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Pipeline Parallelism Application</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/apply_pipeline_parallel.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="pipeline-parallelism-application">
<h1>Pipeline Parallelism Application<a class="headerlink" href="#pipeline-parallelism-application" title="Permalink to this headline"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Linux</span></code> <code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">Model</span> <span class="pre">Training</span></code> <code class="docutils literal notranslate"><span class="pre">Intermediate</span></code> <code class="docutils literal notranslate"><span class="pre">Expert</span></code></p>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.3/docs/mindspore/programming_guide/source_en/apply_pipeline_parallel.md"><img alt="View Source On Gitee" src="https://gitee.com/mindspore/docs/raw/r1.3/resource/_static/logo_source.png" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>In recent years, the scale of neural networks has increased exponentially. Limited by the memory on a single device, the number of devices used for training large models is also increasing. Due to the low communication bandwidth between servers, the performance of the conventional hybrid parallelism (data parallel + model parallel) is poor. Therefore, pipeline parallelism needs to be introduced. Pipeline parallelism can divide a model in space based on <code class="docutils literal notranslate"><span class="pre">stage</span></code>. Each <code class="docutils literal notranslate"><span class="pre">stage</span></code> needs to execute only a part of the network, which greatly reduces memory overheads, shrinks the communication domain, and shortens the communication time. MindSpore can automatically convert a standalone model to the pipeline parallel mode based on user configurations.</p>
<blockquote>
<div><p>Download address of the complete sample code:</p>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/tree/r1.3/docs/sample_code/distributed_training">https://gitee.com/mindspore/docs/tree/r1.3/docs/sample_code/distributed_training</a>.</p>
</div></blockquote>
</section>
<section id="preparations">
<h2>Preparations<a class="headerlink" href="#preparations" title="Permalink to this headline"></a></h2>
<section id="downloading-the-dataset">
<h3>Downloading the Dataset<a class="headerlink" href="#downloading-the-dataset" title="Permalink to this headline"></a></h3>
<p>This example uses the <code class="docutils literal notranslate"><span class="pre">CIFAR-10</span></code> dataset. For details about how to download and load the dataset, visit <a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/en/r1.3/distributed_training_ascend.html">https://www.mindspore.cn/docs/programming_guide/en/r1.3/distributed_training_ascend.html</a>.</p>
</section>
<section id="configuring-the-distributed-environment">
<h3>Configuring the Distributed Environment<a class="headerlink" href="#configuring-the-distributed-environment" title="Permalink to this headline"></a></h3>
<blockquote>
<div><p>Currently, pipeline parallelism supports only Ascend.</p>
<p>Due to the impact of HCCL, pipeline parallelism can only be performed across nodes.</p>
</div></blockquote>
<p>For details about how to configure the distributed environment and call the HCCL, visit <a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/en/r1.3/distributed_training_ascend.html">https://www.mindspore.cn/docs/programming_guide/en/r1.3/distributed_training_ascend.html</a>.</p>
</section>
</section>
<section id="defining-the-network">
<h2>Defining the Network<a class="headerlink" href="#defining-the-network" title="Permalink to this headline"></a></h2>
<p>The network definition is the same as that of the Ascend 910 AI Processor.</p>
<p>For details about the definitions of the network, optimizer, and loss function, visit <a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/en/r1.3/distributed_training_ascend.html">https://www.mindspore.cn/docs/programming_guide/en/r1.3/distributed_training_ascend.html</a>.</p>
<blockquote>
<div><p>To implement pipeline parallelism, you need to define the parallel strategy and call the <code class="docutils literal notranslate"><span class="pre">pipeline_stage</span></code> API to specify the stage on which each layer is to be executed. The granularity of the <code class="docutils literal notranslate"><span class="pre">pipeline_stage</span></code> API is <code class="docutils literal notranslate"><span class="pre">Cell</span></code>. <code class="docutils literal notranslate"><span class="pre">pipeline_stage</span></code> must be configured for all <code class="docutils literal notranslate"><span class="pre">Cells</span></code> that contain training parameters.</p>
</div></blockquote>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ResNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ResNet&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">block</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;init&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ResNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="n">num_classes</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">Head</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">MakeLayer0</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="n">in_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="n">MakeLayer1</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="n">in_channels</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span> <span class="o">=</span> <span class="n">MakeLayer2</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="n">in_channels</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer4</span> <span class="o">=</span> <span class="n">MakeLayer3</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="n">in_channels</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ReduceMean</span><span class="p">(</span><span class="n">keep_dims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">squeeze</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Squeeze</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">fc_with_initialize</span><span class="p">(</span><span class="mi">512</span> <span class="o">*</span> <span class="n">block</span><span class="o">.</span><span class="n">expansion</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>

        <span class="c1"># pipeline parallel config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="o">.</span><span class="n">pipeline_stage</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span><span class="o">.</span><span class="n">pipeline_stage</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span><span class="o">.</span><span class="n">pipeline_stage</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span><span class="o">.</span><span class="n">pipeline_stage</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer4</span><span class="o">.</span><span class="n">pipeline_stage</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="o">.</span><span class="n">pipeline_stage</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;construct&quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</section>
<section id="training-the-network">
<h2>Training the Network<a class="headerlink" href="#training-the-network" title="Permalink to this headline"></a></h2>
<p>To enable pipeline parallelism, you need to add the following configurations to the training script:</p>
<ul class="simple">
<li><p>Set <code class="docutils literal notranslate"><span class="pre">pipeline_stages</span></code> in <code class="docutils literal notranslate"><span class="pre">set_auto_parallel_context</span></code> to specify the total number of <code class="docutils literal notranslate"><span class="pre">stages</span></code>.</p></li>
<li><p>Set the <code class="docutils literal notranslate"><span class="pre">SEMI_AUTO_PARALLEL</span></code> mode. Currently, the pipeline parallelism supports only this mode.</p></li>
<li><p>Define the LossCell. In this example, the <code class="docutils literal notranslate"><span class="pre">nn.WithLossCell</span></code> API is called.</p></li>
<li><p>Pass the <code class="docutils literal notranslate"><span class="pre">parameters</span></code> used in this <code class="docutils literal notranslate"><span class="pre">stage</span></code> to the optimizer. Call the <code class="docutils literal notranslate"><span class="pre">add_pipeline_stage</span></code> method of <code class="docutils literal notranslate"><span class="pre">Parameter</span></code> to pass all <code class="docutils literal notranslate"><span class="pre">stage</span></code> information to <code class="docutils literal notranslate"><span class="pre">Parameter</span></code> if multiple <code class="docutils literal notranslate"><span class="pre">stages</span></code> share a parameter. Then, call the <code class="docutils literal notranslate"><span class="pre">infer_param_pipeline_stage</span></code> API of the <code class="docutils literal notranslate"><span class="pre">Cell</span></code> to obtain the training parameters of the <code class="docutils literal notranslate"><span class="pre">stage</span></code>.</p></li>
<li><p>Finally, wrap the LossCell with <code class="docutils literal notranslate"><span class="pre">PipelineCell</span></code>, and specify the Micro_batch size. To improve machine utilization, MindSpore divides Mini_batch into finer-grained Micro_batch to streamline the entire cluster. The final loss value is the sum of the loss values computed by all Micro_batch. The size of Micro_batch must be greater than or equal to the number of <code class="docutils literal notranslate"><span class="pre">stages</span></code>.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span><span class="p">,</span> <span class="n">Model</span><span class="p">,</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">Momentum</span>
<span class="kn">from</span> <span class="nn">mindspore.train.callback</span> <span class="kn">import</span> <span class="n">LossMonitor</span>
<span class="kn">from</span> <span class="nn">mindspore.context</span> <span class="kn">import</span> <span class="n">ParallelMode</span>
<span class="kn">from</span> <span class="nn">resnet</span> <span class="kn">import</span> <span class="n">resnet50</span>

<span class="k">def</span> <span class="nf">test_train_cifar</span><span class="p">(</span><span class="n">epoch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">SEMI_AUTO_PARALLEL</span><span class="p">,</span> <span class="n">gradients_mean</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">pipeline_stages</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">save_graphs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loss_cb</span> <span class="o">=</span> <span class="n">LossMonitor</span><span class="p">()</span>
    <span class="n">data_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;DATA_PATH&#39;</span><span class="p">)</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">data_path</span><span class="p">)</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
    <span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">resnet50</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">SoftmaxCrossEntropyExpand</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">net_with_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">WithLossCell</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
    <span class="n">net_pipeline</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PipelineCell</span><span class="p">(</span><span class="n">net_with_loss</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">infer_param_pipeline_stage</span><span class="p">(),</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net_pipeline</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">epoch_size</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">loss_cb</span><span class="p">],</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="running-the-multi-node-script">
<h2>Running the Multi-node Script<a class="headerlink" href="#running-the-multi-node-script" title="Permalink to this headline"></a></h2>
<p>Pipeline parallelism requires cross-node execution. For details about multi-node multi-device training, visit <a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/en/r1.3/distributed_training_ascend.html">https://www.mindspore.cn/docs/programming_guide/en/r1.3/distributed_training_ascend.html</a>.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="distributed_training_gpu.html" class="btn btn-neutral float-left" title="Distributed Parallel Training (GPU)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="apply_host_device_training.html" class="btn btn-neutral float-right" title="Applying Host&amp;Device Hybrid Training" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>