<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Application of Single-Node Tensor Cache &mdash; MindSpore master documentation</title><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="architecture.html">Overall Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_structure.html">MindSpore API Overview</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quickstart</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="quick_start/linear_regression.html">Implementing Simple Linear Function Fitting</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start/quick_start.html">Implementing an Image Classification Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start/quick_video.html">Hands-on Installation and Experience</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Basic Concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dtype.html">dtype</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor.html">Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="operators.html">Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="cell.html">Cell</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Numpy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="numpy.html">NumPy Interfaces in MindSpore</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Execution Management</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="context.html">Configuring Running Information</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Pipeline</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dataset_sample.html">Quick Start of Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset.html">Loading Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline.html">Processing Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset_advanced.html">Advanced Usage of Pipeline</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build the Network</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="initializer.html">Initialization of Network Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter.html">Updating Network Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="layer.html">Model Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="loss.html">Loss Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">Optimization Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_net.html">Building a Customized Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="network_component.html">Common Network Components</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Train Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="run.html">Running Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="callback.html">Callback Mechanism</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_model.html">Saving Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="load_model_for_inference_and_transfer.html">Loading a Model for Inference and Transfer Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="train.html">Advanced Usage of Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference_ascend_910.html">Inference on the Ascend 910 AI processor</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference_ascend_310.html">Inference on Ascend 310</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference_gpu.html">Inference on a GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference_cpu.html">Inference on a CPU</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Distributed Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="distributed_training.html">Distributed Training Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_training_ascend.html">Parallel Distributed Training (Ascend)</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_training_gpu.html">Distributed Parallel Training (GPU)</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_pipeline_parallel.html">Pipeline Parallelism Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_host_device_training.html">Applying Host&amp;Device Hybrid Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_parameter_server_training.html">Training with Parameter Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_load_model_hybrid_parallel.html">Saving and Loading Models in Hybrid Parallel Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_inference.html">Distributed Inference With Multi Devices</a></li>
<li class="toctree-l1"><a class="reference internal" href="auto_parallel.html">Parallel Distributed Training Interfaces</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Function Debugging</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="debug_in_pynative_mode.html">Debugging in PyNative Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="dump_in_graph_mode.html">Using Dump in the Graph Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_debugging_info.html">Custom Debugging Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="evaluate_the_model_during_training.html">Evaluating the Model during Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="incremental_operator_build.html">Incremental Operator Build</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="optimize_data_processing.html">Optimizing the Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_mixed_precision.html">Enabling Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_graph_kernel_fusion.html">Enabling Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_gradient_accumulation.html">Applying a Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_quantization_aware_training.html">Applying Quantization Aware Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_post_training_quantization.html">Applying Post Training Quantization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cv.html">Computer Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp.html">Natural Language Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="hpc.html">High Performance Computing</a></li>
<li class="toctree-l1"><a class="reference internal" href="use_on_the_cloud.html">Using MindSpore on the Cloud</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Application of Single-Node Tensor Cache</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/enable_cache.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="application-of-single-node-tensor-cache">
<h1>Application of Single-Node Tensor Cache<a class="headerlink" href="#application-of-single-node-tensor-cache" title="Permalink to this headline"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Linux</span></code> <code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code> <code class="docutils literal notranslate"><span class="pre">Data</span> <span class="pre">Preparation</span></code> <code class="docutils literal notranslate"><span class="pre">Intermediate</span></code> <code class="docutils literal notranslate"><span class="pre">Expert</span></code></p>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.3/docs/mindspore/programming_guide/source_en/enable_cache.md" target="_blank"><img src="https://gitee.com/mindspore/docs/raw/r1.3/resource/_static/logo_source.png"></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>If you need to repeatedly access remote datasets or read datasets from disks, you can use the single-node cache operator to cache datasets in the local memory to accelerate dataset reading.</p>
<p>This tutorial demonstrates how to use the single-node cache service, and shows several best practices of using cache to improve the performance of network training or evaluating.</p>
</section>
<section id="quick-start">
<h2>Quick Start<a class="headerlink" href="#quick-start" title="Permalink to this headline"></a></h2>
<ol class="arabic">
<li><p>Configuring the Environment</p>
<p>Before using the cache service, you need to install MindSpore and set related environment variables. The Conda environment is used as an example. The setting method is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:{path_to_conda}/envs/{your_env_name}/lib/python3.7/site-packages/mindspore:{path_to_conda}/envs/{your_env_name}/lib/python3.7/site-packages/mindspore/lib
export PATH=$PATH:{path_to_conda}/envs/{your_env_name}/bin
</pre></div>
</div>
</li>
<li><p>Starting the Cache Server</p>
<p>Before using the single-node cache service, you need to start the cache server.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ cache_admin --start
Cache server startup completed successfully!
The cache server daemon has been created as process id 10394 and is listening on port 50052

Recommendation:
Since the server is detached into its own daemon process, monitor the server logs (under /tmp/mindspore/cache/log) for any issues that may happen after startup
</pre></div>
</div>
<p>If the system displays a message indicating that the <code class="docutils literal notranslate"><span class="pre">libpython3.7m.so.1.0</span></code> file cannot be found, search for the file path in the virtual environment and set environment variables.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:{path_to_conda}/envs/{your_env_name}/lib
</pre></div>
</div>
</li>
<li><p>Creating a Cache Session</p>
<p>If no cache session exists on the cache server, a cache session needs to be created to obtain the cache session ID.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ cache_admin -g
Session created for server on port 50052: 1493732251
</pre></div>
</div>
<p>The cache session ID is randomly allocated by the server.</p>
</li>
<li><p>Creating a Cache Instance</p>
<p>Create the Python script <code class="docutils literal notranslate"><span class="pre">my_training_script.py</span></code>, use the <code class="docutils literal notranslate"><span class="pre">DatasetCache</span></code> API to define a cache instance named <code class="docutils literal notranslate"><span class="pre">some_cache</span></code> in the script, and specify the <code class="docutils literal notranslate"><span class="pre">session_id</span></code> parameter to a cache session ID created in the previous step.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>

<span class="n">some_cache</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">DatasetCache</span><span class="p">(</span><span class="n">session_id</span><span class="o">=</span><span class="mi">1493732251</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">spilling</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Inserting a Cache Instance</p>
<p>The following uses the CIFAR-10 dataset as an example. Before running the sample, download and store the CIFAR-10 dataset by referring to <a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/en/r1.3/dataset_loading.html#cifar-10-100-dataset">Loading Dataset</a>. The directory structure is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>├─my_training_script.py
└─cifar-10-batches-bin
    ├── batches.meta.txt
    ├── data_batch_1.bin
    ├── data_batch_2.bin
    ├── data_batch_3.bin
    ├── data_batch_4.bin
    ├── data_batch_5.bin
    ├── readme.html
    └── test_batch.bin
</pre></div>
</div>
<p>To cache the enhanced data processed by data augmentation of the map operator, the created <code class="docutils literal notranslate"><span class="pre">some_cache</span></code> instance is used as the input parameter of the <code class="docutils literal notranslate"><span class="pre">cache</span></code> API in the map operator.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.dataset.vision.c_transforms</span> <span class="k">as</span> <span class="nn">c_vision</span>

<span class="n">dataset_dir</span> <span class="o">=</span> <span class="s2">&quot;cifar-10-batches-bin/&quot;</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">Cifar10Dataset</span><span class="p">(</span><span class="n">dataset_dir</span><span class="o">=</span><span class="n">dataset_dir</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># apply cache to map</span>
<span class="n">rescale_op</span> <span class="o">=</span> <span class="n">c_vision</span><span class="o">.</span><span class="n">Rescale</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">input_columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;image&quot;</span><span class="p">],</span> <span class="n">operations</span><span class="o">=</span><span class="n">rescale_op</span><span class="p">,</span> <span class="n">cache</span><span class="o">=</span><span class="n">some_cache</span><span class="p">)</span>

<span class="n">num_iter</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>  <span class="c1"># each data is a dictionary</span>
    <span class="c1"># in this example, each dictionary has a key &quot;image&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> image shape: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_iter</span><span class="p">,</span> <span class="n">item</span><span class="p">[</span><span class="s2">&quot;image&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="n">num_iter</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
</div>
<p>Run the Python script <code class="docutils literal notranslate"><span class="pre">my_training_script.py</span></code>. The following information is displayed:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>0 image shape: (32, 32, 3)
1 image shape: (32, 32, 3)
2 image shape: (32, 32, 3)
3 image shape: (32, 32, 3)
4 image shape: (32, 32, 3)
</pre></div>
</div>
<p>You can run the <code class="docutils literal notranslate"><span class="pre">cache_admin</span> <span class="pre">--list_sessions</span></code> command to check whether there are five data records in the current session. If yes, the data is successfully cached.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ cache_admin --list_sessions
Listing sessions for server on port 50052

     Session    Cache Id  Mem cached  Disk cached  Avg cache size  Numa hit
  1493732251  3618046178       5          n/a          12442         5
</pre></div>
</div>
</li>
<li><p>Destroying a Cache Session</p>
<p>After the training is complete, you can destroy the current cache and release the memory.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ cache_admin --destroy_session 1493732251
Drop session successfully for server on port 50052
</pre></div>
</div>
<p>The preceding command is used to destroy the cache whose session ID is 1493732251.</p>
</li>
<li><p>Stopping the Cache Server</p>
<p>After using the cache server, you can stop the cache server. This operation will destroy all cache sessions on the current server and release the memory.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ cache_admin --stop
Cache server on port 50052 has been stopped successfully.
</pre></div>
</div>
</li>
</ol>
</section>
<section id="best-practices">
<h2>Best Practices<a class="headerlink" href="#best-practices" title="Permalink to this headline"></a></h2>
<section id="using-cache-to-speed-up-resnet-evaluation-during-training">
<h3>Using Cache to Speed Up ResNet Evaluation During Training<a class="headerlink" href="#using-cache-to-speed-up-resnet-evaluation-during-training" title="Permalink to this headline"></a></h3>
<p>For a complex network, epoch training usually needs to be performed for dozens or even hundreds of times. Before training, it is difficult to know when a model can achieve required accuracy in epoch training. Therefore, the accuracy of the model is usually validated at a fixed epoch interval during training and the corresponding model is saved. After the training is completed, users can quickly select the optimal model by viewing the change of the corresponding model accuracy.</p>
<p>Therefore, the performance of evaluation during training will have a great impact on the total end-to-end time required. In this section, we will show an example of leveraging the cache service and caching data after augmentation in Tensor format in memory to speed up the evaluation procedure.</p>
<p>The inference data processing procedure usually does not contain random operations. For example, the dataset processing in ResNet50 evaluation only contains augmentations like <code class="docutils literal notranslate"><span class="pre">Decode</span></code>, <code class="docutils literal notranslate"><span class="pre">Resize</span></code>, <code class="docutils literal notranslate"><span class="pre">CenterCrop</span></code>, <code class="docutils literal notranslate"><span class="pre">Normalize</span></code>, <code class="docutils literal notranslate"><span class="pre">HWC2CHW</span></code>, <code class="docutils literal notranslate"><span class="pre">TypeCast</span></code>. Therefore, it’s usually better to inject cache after the last augmentation step and directly cache data that’s fully augmented, to minimize repeated computations and to yield the most performance benefits. In this section, we will follow this approach and take ResNet as an example.</p>
<p>For the complete sample code, please refer to <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.3/model_zoo/official/cv/resnet">ResNet</a> in ModelZoo.</p>
<ol class="arabic">
<li><p>Create a Shell script named <code class="docutils literal notranslate"><span class="pre">cache_util.sh</span></code> for cache management:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bootup_cache_server<span class="o">()</span>
<span class="o">{</span>
<span class="w">  </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Booting up cache server...&quot;</span>
<span class="w">  </span><span class="nv">result</span><span class="o">=</span><span class="k">$(</span>cache_admin<span class="w"> </span>--start<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="k">)</span>
<span class="w">  </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">result</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="o">}</span>

generate_cache_session<span class="o">()</span>
<span class="o">{</span>
<span class="w">  </span><span class="nv">result</span><span class="o">=</span><span class="k">$(</span>cache_admin<span class="w"> </span>-g<span class="w"> </span><span class="p">|</span><span class="w"> </span>awk<span class="w"> </span><span class="s1">&#39;END {print $NF}&#39;</span><span class="k">)</span>
<span class="w">  </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">result</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="o">}</span>
</pre></div>
</div>
<blockquote>
<div><p>Complete sample code: <a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.3/docs/sample_code/cache/cache_util.sh">cache_util.sh</a></p>
</div></blockquote>
</li>
<li><p>In the Shell script for starting the distributed training i.e., <code class="docutils literal notranslate"><span class="pre">run_distributed_train.sh</span></code>, start a cache server for evaluation during training scenarios and generate a cache session, saved in <code class="docutils literal notranslate"><span class="pre">CACHE_SESSION_ID</span></code> Shell variable:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">source</span><span class="w"> </span>cache_util.sh

<span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="s2">&quot;x</span><span class="si">${</span><span class="nv">RUN_EVAL</span><span class="si">}</span><span class="s2">&quot;</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s2">&quot;xTrue&quot;</span><span class="w"> </span><span class="o">]</span>
<span class="k">then</span>
<span class="w">  </span>bootup_cache_server
<span class="w">  </span><span class="nv">CACHE_SESSION_ID</span><span class="o">=</span><span class="k">$(</span>generate_cache_session<span class="k">)</span>
<span class="k">fi</span>
</pre></div>
</div>
</li>
<li><p>Pass the <code class="docutils literal notranslate"><span class="pre">CACHE_SESSION_ID</span></code> as well as other arguments when start the Python training script:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>python train.py \
--net=$1 \
--dataset=$2 \
--run_distribute=True \
--device_num=$DEVICE_NUM \
--dataset_path=$PATH2 \
--run_eval=$RUN_EVAL \
--eval_dataset_path=$EVAL_DATASET_PATH \
--enable_cache=True \
--cache_session_id=$CACHE_SESSION_ID \
&amp;&gt; log &amp;
</pre></div>
</div>
</li>
<li><p>In Python training script <code class="docutils literal notranslate"><span class="pre">train.py</span></code>, use the following code to receive <code class="docutils literal notranslate"><span class="pre">cache_session_id</span></code> that’s passed in and use it when defining a eval dataset <code class="docutils literal notranslate"><span class="pre">eval_dataset</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">argparse</span>

<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--enable_cache&#39;</span><span class="p">,</span>
    <span class="nb">type</span><span class="o">=</span><span class="n">ast</span><span class="o">.</span><span class="n">literal_eval</span><span class="p">,</span>
    <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Caching the eval dataset in memory to speedup evaluation, default is False.&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--cache_session_id&#39;</span><span class="p">,</span>
    <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
    <span class="n">default</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;The session id for cache service.&#39;</span><span class="p">)</span>
<span class="n">args_opt</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

<span class="n">eval_dataset</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span>
    <span class="n">dataset_path</span><span class="o">=</span><span class="n">args_opt</span><span class="o">.</span><span class="n">eval_dataset_path</span><span class="p">,</span>
    <span class="n">do_train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
    <span class="n">enable_cache</span><span class="o">=</span><span class="n">args_opt</span><span class="o">.</span><span class="n">enable_cache</span><span class="p">,</span>
    <span class="n">cache_session_id</span><span class="o">=</span><span class="n">args_opt</span><span class="o">.</span><span class="n">cache_session_id</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>In Python <code class="docutils literal notranslate"><span class="pre">dataset.py</span></code> script which creates the dataset processing pipeline，create a <code class="docutils literal notranslate"><span class="pre">DatasetCache</span></code> instance according to <code class="docutils literal notranslate"><span class="pre">enable_cache</span></code> and <code class="docutils literal notranslate"><span class="pre">cache_session_id</span></code> arguments, and inject the cache instance after the last step of data augmentation, i.e., after <code class="docutils literal notranslate"><span class="pre">TyepCast</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_dataset2</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">,</span> <span class="n">do_train</span><span class="p">,</span> <span class="n">repeat_num</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="s2">&quot;Ascend&quot;</span><span class="p">,</span> <span class="n">distribute</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">enable_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">cache_session_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="o">...</span>
    <span class="k">if</span> <span class="n">enable_cache</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">cache_session_id</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;A cache session_id must be provided to use cache.&quot;</span><span class="p">)</span>
        <span class="n">eval_cache</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">DatasetCache</span><span class="p">(</span><span class="n">session_id</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">cache_session_id</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">data_set</span> <span class="o">=</span> <span class="n">data_set</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">type_cast_op</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;label&quot;</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">cache</span><span class="o">=</span><span class="n">eval_cache</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">data_set</span> <span class="o">=</span> <span class="n">data_set</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">type_cast_op</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="s2">&quot;label&quot;</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Execute the training script:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>...
epoch: 40, acc: 0.5665486653645834, eval_cost:30.54
epoch: 41, acc: 0.6212361653645834, eval_cost:2.80
epoch: 42, acc: 0.6523844401041666, eval_cost:3.77
...
</pre></div>
</div>
<p>By default, the evaluation starts after the 40th epoch, and <code class="docutils literal notranslate"><span class="pre">eval_cost</span></code> shows how much time it costs for each evaluation run, measured by seconds.</p>
<p>The following table compares the average evaluation time with/without cache:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>|                            | without cache | with cache |
| -------------------------- | ------------- | ---------- |
| 4p, resnet50, imagenet2012 | 10.59s        | 3.62s      |
</pre></div>
</div>
<p>On Ascend machine with 4 parallel pipelines, it generally takes around 88 seconds for each training epoch and ResNet training usually requires 90 epochs. Therefore, using cache can shorten the total end-to-end time from 8849 seconds to 8101 seconds, thus bringing 348 seconds total time reduction.</p>
</li>
<li><p>After the training run is completed, you can destroy the current cache and release the memory:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ cache_admin --stop
Cache server on port 50052 has been stopped successfully.
</pre></div>
</div>
</li>
</ol>
</section>
<section id="using-cache-to-speed-up-training-with-datasets-on-nfs">
<h3>Using Cache to Speed Up Training with Datasets on NFS<a class="headerlink" href="#using-cache-to-speed-up-training-with-datasets-on-nfs" title="Permalink to this headline"></a></h3>
<p>To share a large dataset across multiple servers, many users resort to NFS (Network File System) to store their datasets (Please check <a class="reference external" href="https://support.huaweicloud.com/intl/en-us/usermanual-functiongraph/functiongraph_01_0561.html">Huawei cloud - Creating an NFS Shared Directory on ECS</a> for how to setup and config an NFS server).</p>
<p>However, due to the fact that the cost of accessing NFS is usually large, running training with a dataset located on NFS is relatively slow. To improve training performance for this scenario, we can leverage cache service to cache the dataset in the form of Tensor in memory. After caching, the following training epochs can directly access from memory, thus avoiding costly remote dataset access.</p>
<p>Note that typically after reading the dataset, certain random operations such as <code class="docutils literal notranslate"><span class="pre">RandomCropDecodeResize</span></code> would be performed in the dataset processing procedure. Caching after these random operations would result in the loss of randomness of the data, and therefore affect the final accuracy. As a result, we choose to directly cache the source dataset. In this section, we will follow this approach and take MobileNetV2 as an example.</p>
<p>For the complete sample code, please refer to <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.3/model_zoo/official/cv/mobilenetv2">MobileNetV2</a>  in ModelZoo.</p>
<ol class="arabic">
<li><p>Create a Shell script namely <code class="docutils literal notranslate"><span class="pre">cache_util.sh</span></code> for cache management:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bootup_cache_server<span class="o">()</span>
<span class="o">{</span>
<span class="w">  </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Booting up cache server...&quot;</span>
<span class="w">  </span><span class="nv">result</span><span class="o">=</span><span class="k">$(</span>cache_admin<span class="w"> </span>--start<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="k">)</span>
<span class="w">  </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">result</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="o">}</span>

generate_cache_session<span class="o">()</span>
<span class="o">{</span>
<span class="w">  </span><span class="nv">result</span><span class="o">=</span><span class="k">$(</span>cache_admin<span class="w"> </span>-g<span class="w"> </span><span class="p">|</span><span class="w"> </span>awk<span class="w"> </span><span class="s1">&#39;END {print $NF}&#39;</span><span class="k">)</span>
<span class="w">  </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">result</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="o">}</span>
</pre></div>
</div>
<blockquote>
<div><p>Complete sample code: <a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.3/docs/sample_code/cache/cache_util.sh">cache_util.sh</a></p>
</div></blockquote>
</li>
<li><p>In the Shell script for starting the distributed training with NFS dataset i.e., <code class="docutils literal notranslate"><span class="pre">run_train_nfs_cache.sh</span></code>, start a cache server for scenarios where dataset is on NFS. Then generate a cache session, saved in <code class="docutils literal notranslate"><span class="pre">CACHE_SESSION_ID</span></code> Shell variable:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">source</span><span class="w"> </span>cache_util.sh

bootup_cache_server
<span class="nv">CACHE_SESSION_ID</span><span class="o">=</span><span class="k">$(</span>generate_cache_session<span class="k">)</span>
</pre></div>
</div>
</li>
<li><p>Pass the <code class="docutils literal notranslate"><span class="pre">CACHE_SESSION_ID</span></code> as well as other arguments when start the Python training script:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>python train.py \
--platform=$1 \
--dataset_path=$5 \
--pretrain_ckpt=$PRETRAINED_CKPT \
--freeze_layer=$FREEZE_LAYER \
--filter_head=$FILTER_HEAD \
--enable_cache=True \
--cache_session_id=$CACHE_SESSION_ID \
&amp;&gt; log$i.log &amp;
</pre></div>
</div>
</li>
<li><p>In the <code class="docutils literal notranslate"><span class="pre">train_parse_args()</span></code> function of Python argument-parsing script <code class="docutils literal notranslate"><span class="pre">args.py</span></code>, use the following code to receive <code class="docutils literal notranslate"><span class="pre">cache_session_id</span></code> that’s passed in:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">argparse</span>

<span class="k">def</span> <span class="nf">train_parse_args</span><span class="p">():</span>
<span class="o">...</span>
    <span class="n">train_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--enable_cache&#39;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="n">ast</span><span class="o">.</span><span class="n">literal_eval</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Caching the dataset in memory to speedup dataset processing, default is False.&#39;</span><span class="p">)</span>
    <span class="n">train_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--cache_session_id&#39;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s1">&#39;The session id for cache service.&#39;</span><span class="p">)</span>
<span class="n">train_args</span> <span class="o">=</span> <span class="n">train_parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
</pre></div>
</div>
<p>In Python training script<code class="docutils literal notranslate"><span class="pre">train.py</span></code>，call <code class="docutils literal notranslate"><span class="pre">train_parse_args()</span></code> to parse the arguments that’s passed in such as <code class="docutils literal notranslate"><span class="pre">cache_session_id</span></code>, and use it when defining the training dataset:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">src.args</span> <span class="kn">import</span> <span class="n">train_parse_args</span>
<span class="n">args_opt</span> <span class="o">=</span> <span class="n">train_parse_args</span><span class="p">()</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span>
    <span class="n">dataset_path</span><span class="o">=</span><span class="n">args_opt</span><span class="o">.</span><span class="n">dataset_path</span><span class="p">,</span>
    <span class="n">do_train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
    <span class="n">enable_cache</span><span class="o">=</span><span class="n">args_opt</span><span class="o">.</span><span class="n">enable_cache</span><span class="p">,</span>
    <span class="n">cache_session_id</span><span class="o">=</span><span class="n">args_opt</span><span class="o">.</span><span class="n">cache_session_id</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>In Python <code class="docutils literal notranslate"><span class="pre">dataset.py</span></code> script which creates the dataset processing pipeline，create a <code class="docutils literal notranslate"><span class="pre">DatasetCache</span></code> instance according to <code class="docutils literal notranslate"><span class="pre">enable_cache</span></code> and <code class="docutils literal notranslate"><span class="pre">cache_session_id</span></code> arguments, and inject the cache instance directly after the <code class="docutils literal notranslate"><span class="pre">ImageFolderDataset</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_dataset</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">,</span> <span class="n">do_train</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">repeat_num</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">enable_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">cache_session_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="o">...</span>
    <span class="k">if</span> <span class="n">enable_cache</span><span class="p">:</span>
        <span class="n">nfs_dataset_cache</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">DatasetCache</span><span class="p">(</span><span class="n">session_id</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">cache_session_id</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">nfs_dataset_cache</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">platform</span> <span class="o">==</span> <span class="s2">&quot;Ascend&quot;</span><span class="p">:</span>
        <span class="n">rank_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;RANK_SIZE&quot;</span><span class="p">,</span> <span class="s1">&#39;1&#39;</span><span class="p">))</span>
        <span class="n">rank_id</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;RANK_ID&quot;</span><span class="p">,</span> <span class="s1">&#39;0&#39;</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">rank_size</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">data_set</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">ImageFolderDataset</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cache</span><span class="o">=</span><span class="n">nfs_dataset_cache</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">data_set</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">ImageFolderDataset</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_shards</span><span class="o">=</span><span class="n">rank_size</span><span class="p">,</span> <span class="n">shard_id</span><span class="o">=</span><span class="n">rank_id</span><span class="p">,</span> <span class="n">cache</span><span class="o">=</span><span class="n">nfs_dataset_cache</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Execute the training run via <code class="docutils literal notranslate"><span class="pre">run_train_nfs_cache.sh</span></code>:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>epoch: [  0/ 200], step:[ 2134/ 2135], loss:[4.682/4.682], time:[3364893.166], lr:[0.780]
epoch time: 3384387.999, per step time: 1585.193, avg loss: 4.682
epoch: [  1/ 200], step:[ 2134/ 2135], loss:[3.750/3.750], time:[430495.242], lr:[0.724]
epoch time: 431005.885, per step time: 201.876, avg loss: 4.286
epoch: [  2/ 200], step:[ 2134/ 2135], loss:[3.922/3.922], time:[420104.849], lr:[0.635]
epoch time: 420669.174, per step time: 197.035, avg loss: 3.534
epoch: [  3/ 200], step:[ 2134/ 2135], loss:[3.581/3.581], time:[420825.587], lr:[0.524]
epoch time: 421494.842, per step time: 197.421, avg loss: 3.417
...
</pre></div>
</div>
<p>The following table compares the average epoch time with/without cache:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>| 4p, MobileNetV2, imagenet2012            | without cache | with cache |
| ---------------------------------------- | ------------- | ---------- |
| first epoch time                         | 1649s         | 3384s      |
| average epoch time (exclude first epoch) | 458s          | 421s       |
</pre></div>
</div>
<p>With cache, the first epoch time increases significantly due to cache writing overhead, but all later epochs can benefit from caching the dataset in memory. Therefore, the more epochs, the more cache case shows benefits due to per-step-time savings.</p>
<p>MobileNetV2 generally requires 200 epochs in total, therefore, using cache can shorten the total end-to-end time from 92791 seconds to 87163 seconds, thus bringing 5628 seconds total time reduction.</p>
</li>
<li><p>After the training run is completed, you can destroy the current cache and release the memory:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ cache_admin --stop
Cache server on port 50052 has been stopped successfully.
</pre></div>
</div>
</li>
</ol>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>