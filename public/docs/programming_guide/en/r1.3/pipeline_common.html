<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>General Data Processing &mdash; MindSpore master documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Image Data Processing and Enhancement" href="augmentation.html" />
    <link rel="prev" title="Processing Data" href="pipeline.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="architecture.html">Overall Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_structure.html">MindSpore API Overview</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quickstart</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="quick_start/linear_regression.html">Implementing Simple Linear Function Fitting</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start/quick_start.html">Implementing an Image Classification Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start/quick_video.html">Hands-on Installation and Experience</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Basic Concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dtype.html">dtype</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor.html">Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="operators.html">Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="cell.html">Cell</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Numpy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="numpy.html">NumPy Interfaces in MindSpore</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Execution Management</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="context.html">Configuring Running Information</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Pipeline</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="dataset_sample.html">Quick Start of Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset.html">Loading Dataset</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="pipeline.html">Processing Data</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">General Data Processing</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#data-processing-operators">Data Processing Operators</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#shuffle">shuffle</a></li>
<li class="toctree-l4"><a class="reference internal" href="#map">map</a></li>
<li class="toctree-l4"><a class="reference internal" href="#batch">batch</a></li>
<li class="toctree-l4"><a class="reference internal" href="#repeat">repeat</a></li>
<li class="toctree-l4"><a class="reference internal" href="#zip">zip</a></li>
<li class="toctree-l4"><a class="reference internal" href="#concat">concat</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="augmentation.html">Image Data Processing and Enhancement</a></li>
<li class="toctree-l2"><a class="reference internal" href="tokenizer.html">Text Data Processing and Enhancement</a></li>
<li class="toctree-l2"><a class="reference internal" href="eager.html">Lightweight Data Processing</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="dataset_advanced.html">Advanced Usage of Pipeline</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build the Network</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="initializer.html">Initialization of Network Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter.html">Updating Network Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="layer.html">Model Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="loss.html">Loss Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">Optimization Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_net.html">Building a Customized Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="network_component.html">Common Network Components</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Train Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="run.html">Running Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="callback.html">Callback Mechanism</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_model.html">Saving Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="load_model_for_inference_and_transfer.html">Loading a Model for Inference and Transfer Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="train.html">Advanced Usage of Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference_ascend_910.html">Inference on the Ascend 910 AI processor</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference_ascend_310.html">Inference on Ascend 310</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference_gpu.html">Inference on a GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference_cpu.html">Inference on a CPU</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Distributed Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="distributed_training.html">Distributed Training Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_training_ascend.html">Parallel Distributed Training (Ascend)</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_training_gpu.html">Distributed Parallel Training (GPU)</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_pipeline_parallel.html">Pipeline Parallelism Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_host_device_training.html">Applying Host&amp;Device Hybrid Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_parameter_server_training.html">Training with Parameter Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_load_model_hybrid_parallel.html">Saving and Loading Models in Hybrid Parallel Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_inference.html">Distributed Inference With Multi Devices</a></li>
<li class="toctree-l1"><a class="reference internal" href="auto_parallel.html">Parallel Distributed Training Interfaces</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Function Debugging</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="debug_in_pynative_mode.html">Debugging in PyNative Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="dump_in_graph_mode.html">Using Dump in the Graph Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_debugging_info.html">Custom Debugging Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="evaluate_the_model_during_training.html">Evaluating the Model during Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="incremental_operator_build.html">Incremental Operator Build</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="optimize_data_processing.html">Optimizing the Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_mixed_precision.html">Enabling Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_graph_kernel_fusion.html">Enabling Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_gradient_accumulation.html">Applying a Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_quantization_aware_training.html">Applying Quantization Aware Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_post_training_quantization.html">Applying Post Training Quantization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cv.html">Computer Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp.html">Natural Language Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="hpc.html">High Performance Computing</a></li>
<li class="toctree-l1"><a class="reference internal" href="use_on_the_cloud.html">Using MindSpore on the Cloud</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="pipeline.html">Processing Data</a> &raquo;</li>
      <li>General Data Processing</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/pipeline_common.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section class="tex2jax_ignore mathjax_ignore" id="general-data-processing">
<h1>General Data Processing<a class="headerlink" href="#general-data-processing" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.3/docs/mindspore/programming_guide/source_en/pipeline_common.md" target="_blank"><img src="https://gitee.com/mindspore/docs/raw/r1.3/resource/_static/logo_source.png"></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>Data is the basis of deep learning. Good data input can play a positive role in the entire deep neural network training. Before training, data processing is performed on a loaded dataset for resolving problems such as an excessively large data volume and uneven sample distribution, thereby obtaining a more optimized data input.</p>
<p>Each dataset class of MindSpore provides multiple data processing operators. You can build a data processing pipeline to define the data processing operations to be used. In this way, data can be continuously transferred to the training system through the data processing pipeline during the training process.</p>
<p>The following table lists part of the common data processing operators supported by MindSpore. For more data processing operations, see <a class="reference external" href="https://www.mindspore.cn/docs/api/en/r1.3/api_python/mindspore.dataset.html">MindSpore API</a>.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Data Processing Operator</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>shuffle</p></td>
<td><p>Randomly shuffles datasets.</p></td>
</tr>
<tr class="row-odd"><td><p>map</p></td>
<td><p>Provides customized functions or operators for specified column data in a dataset.</p></td>
</tr>
<tr class="row-even"><td><p>batch</p></td>
<td><p>Divides datasets into batches to reduce the number of training steps and accelerate the training process.</p></td>
</tr>
<tr class="row-odd"><td><p>repeat</p></td>
<td><p>Repeats a dataset to expand the data volume.</p></td>
</tr>
<tr class="row-even"><td><p>zip</p></td>
<td><p>Zips two datasets into one vertically.</p></td>
</tr>
<tr class="row-odd"><td><p>concat</p></td>
<td><p>Concatenates two datasets into one horizontally.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="data-processing-operators">
<h2>Data Processing Operators<a class="headerlink" href="#data-processing-operators" title="Permalink to this headline"></a></h2>
<section id="shuffle">
<h3>shuffle<a class="headerlink" href="#shuffle" title="Permalink to this headline"></a></h3>
<p>Randomly shuffles datasets.</p>
<blockquote>
<div><p>The larger the value of <code class="docutils literal notranslate"><span class="pre">buffer_size</span></code>, the higher the shuffle degree, but the more time and computing resources are consumed.</p>
</div></blockquote>
<p><img alt="shuffle" src="_images/shuffle.png" /></p>
<p>The following example builds a random dataset, then shuffles it, and finally shows the shuffled data result.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>

<span class="n">ds</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">generator_func</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
        <span class="k">yield</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">2</span><span class="p">]),)</span>

<span class="n">dataset1</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">GeneratorDataset</span><span class="p">(</span><span class="n">generator_func</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">])</span>

<span class="n">dataset1</span> <span class="o">=</span> <span class="n">dataset1</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataset1</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<p>The output is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>{&#39;data&#39;: Tensor(shape=[3], dtype=Int64, value= [0, 1, 2])}
{&#39;data&#39;: Tensor(shape=[3], dtype=Int64, value= [2, 3, 4])}
{&#39;data&#39;: Tensor(shape=[3], dtype=Int64, value= [3, 4, 5])}
{&#39;data&#39;: Tensor(shape=[3], dtype=Int64, value= [1, 2, 3])}
{&#39;data&#39;: Tensor(shape=[3], dtype=Int64, value= [4, 5, 6])}
</pre></div>
</div>
</section>
<section id="map">
<h3>map<a class="headerlink" href="#map" title="Permalink to this headline"></a></h3>
<p>Applies a specified function or operator to specified columns in a dataset to implement data mapping. You can customize the mapping function or use operators in <code class="docutils literal notranslate"><span class="pre">c_transforms</span></code> or <code class="docutils literal notranslate"><span class="pre">py_transforms</span></code> to augment image and text data.</p>
<blockquote>
<div><p>For details about how to use data augmentation, see <a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/en/r1.3/augmentation.html">Data Augmentation</a> in the Programming Guide.</p>
</div></blockquote>
<p><img alt="map" src="_images/map.png" /></p>
<p>The following example builds a random dataset, defines a mapping function for data augmentation, applies the function to the dataset, and compares the data results before and after the mapping.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>

<span class="k">def</span> <span class="nf">generator_func</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
        <span class="k">yield</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">2</span><span class="p">]),)</span>

<span class="k">def</span> <span class="nf">pyfunc</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">*</span><span class="mi">2</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">GeneratorDataset</span><span class="p">(</span><span class="n">generator_func</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">])</span>

<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------ after processing ------&quot;</span><span class="p">)</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">pyfunc</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">])</span>

<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<p>The output is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>{&#39;data&#39;: Tensor(shape=[3], dtype=Int64, value= [0, 1, 2])}
{&#39;data&#39;: Tensor(shape=[3], dtype=Int64, value= [1, 2, 3])}
{&#39;data&#39;: Tensor(shape=[3], dtype=Int64, value= [2, 3, 4])}
{&#39;data&#39;: Tensor(shape=[3], dtype=Int64, value= [3, 4, 5])}
{&#39;data&#39;: Tensor(shape=[3], dtype=Int64, value= [4, 5, 6])}
------ after processing ------
{&#39;data&#39;: Tensor(shape=[3], dtype=Int64, value= [0, 2, 4])}
{&#39;data&#39;: Tensor(shape=[3], dtype=Int64, value= [2, 4, 6])}
{&#39;data&#39;: Tensor(shape=[3], dtype=Int64, value= [4, 6, 8])}
{&#39;data&#39;: Tensor(shape=[3], dtype=Int64, value= [ 6,  8, 10])}
{&#39;data&#39;: Tensor(shape=[3], dtype=Int64, value= [ 8, 10, 12])}
</pre></div>
</div>
</section>
<section id="batch">
<h3>batch<a class="headerlink" href="#batch" title="Permalink to this headline"></a></h3>
<p>Divides datasets into batches in the training system to reduce the number of training steps and accelerate the training process.</p>
<p><img alt="batch" src="_images/batch.png" /></p>
<p>The following example builds a random dataset, and then displays the batching results of the datasets that retain and do not retain redundant data. The batch size is 2.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>

<span class="k">def</span> <span class="nf">generator_func</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
        <span class="k">yield</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">2</span><span class="p">]),)</span>

<span class="n">dataset1</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">GeneratorDataset</span><span class="p">(</span><span class="n">generator_func</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">])</span>

<span class="n">dataset1</span> <span class="o">=</span> <span class="n">dataset1</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataset1</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------ drop remainder ------&quot;</span><span class="p">)</span>

<span class="n">dataset2</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">GeneratorDataset</span><span class="p">(</span><span class="n">generator_func</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">])</span>

<span class="n">dataset2</span> <span class="o">=</span> <span class="n">dataset2</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataset2</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<p>The output is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>{&#39;data&#39;: Tensor(shape=[2, 3], dtype=Int64, value=
[[0, 1, 2],
 [1, 2, 3]])}
{&#39;data&#39;: Tensor(shape=[2, 3], dtype=Int64, value=
[[2, 3, 4],
 [3, 4, 5]])}
{&#39;data&#39;: Tensor(shape=[1, 3], dtype=Int64, value=
[[4, 5, 6]])}
------ drop remainder ------
{&#39;data&#39;: Tensor(shape=[2, 3], dtype=Int64, value=
[[0, 1, 2],
 [1, 2, 3]])}
{&#39;data&#39;: Tensor(shape=[2, 3], dtype=Int64, value=
[[2, 3, 4],
 [3, 4, 5]])}
</pre></div>
</div>
</section>
<section id="repeat">
<h3>repeat<a class="headerlink" href="#repeat" title="Permalink to this headline"></a></h3>
<p>Repeats a dataset to expand the data volume.</p>
<blockquote>
<div><p>The operation sequence of <code class="docutils literal notranslate"><span class="pre">repeat</span></code> and <code class="docutils literal notranslate"><span class="pre">batch</span></code> affects the number of training batches. You are advised to place <code class="docutils literal notranslate"><span class="pre">repeat</span></code> after <code class="docutils literal notranslate"><span class="pre">batch</span></code>.</p>
</div></blockquote>
<p><img alt="repeat" src="_images/repeat.png" /></p>
<p>The following example builds a random dataset, repeats it twice, and finally shows the data result after repetition.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>

<span class="k">def</span> <span class="nf">generator_func</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
        <span class="k">yield</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">2</span><span class="p">]),)</span>

<span class="n">dataset1</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">GeneratorDataset</span><span class="p">(</span><span class="n">generator_func</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">])</span>

<span class="n">dataset1</span> <span class="o">=</span> <span class="n">dataset1</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">count</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataset1</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<p>The output is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>{&#39;data&#39;: Tensor(shape=[3], dtype=Int64, value= [0, 1, 2])}
{&#39;data&#39;: Tensor(shape=[3], dtype=Int64, value= [1, 2, 3])}
{&#39;data&#39;: Tensor(shape=[3], dtype=Int64, value= [2, 3, 4])}
{&#39;data&#39;: Tensor(shape=[3], dtype=Int64, value= [3, 4, 5])}
{&#39;data&#39;: Tensor(shape=[3], dtype=Int64, value= [4, 5, 6])}
{&#39;data&#39;: Tensor(shape=[3], dtype=Int64, value= [0, 1, 2])}
{&#39;data&#39;: Tensor(shape=[3], dtype=Int64, value= [1, 2, 3])}
{&#39;data&#39;: Tensor(shape=[3], dtype=Int64, value= [2, 3, 4])}
{&#39;data&#39;: Tensor(shape=[3], dtype=Int64, value= [3, 4, 5])}
{&#39;data&#39;: Tensor(shape=[3], dtype=Int64, value= [4, 5, 6])}
</pre></div>
</div>
</section>
<section id="zip">
<h3>zip<a class="headerlink" href="#zip" title="Permalink to this headline"></a></h3>
<p>Zips two datasets into one vertically.</p>
<blockquote>
<div><p>If the column names in the two datasets are the same, the two datasets will not be zipped. Therefore, ensure that the column name is unique.<br>If the number of rows in the two datasets is different, the number of rows after zipping is the same as the smaller number of rows.</p>
</div></blockquote>
<p><img alt="zip" src="_images/zip.png" /></p>
<p>The following example builds two random datasets with different samples, zips columns, and displays the data result after zipping.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>

<span class="k">def</span> <span class="nf">generator_func</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">7</span><span class="p">):</span>
        <span class="k">yield</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">2</span><span class="p">]),)</span>

<span class="k">def</span> <span class="nf">generator_func2</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
        <span class="k">yield</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),)</span>

<span class="n">dataset1</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">GeneratorDataset</span><span class="p">(</span><span class="n">generator_func</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;data1&quot;</span><span class="p">])</span>
<span class="n">dataset2</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">GeneratorDataset</span><span class="p">(</span><span class="n">generator_func2</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;data2&quot;</span><span class="p">])</span>

<span class="n">dataset3</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">zip</span><span class="p">((</span><span class="n">dataset1</span><span class="p">,</span> <span class="n">dataset2</span><span class="p">))</span>

<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataset3</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<p>The output is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>{&#39;data1&#39;: Tensor(shape=[3], dtype=Int64, value= [0, 1, 2]), &#39;data2&#39;: Tensor(shape=[2], dtype=Int64, value= [1, 2])}
{&#39;data1&#39;: Tensor(shape=[3], dtype=Int64, value= [1, 2, 3]), &#39;data2&#39;: Tensor(shape=[2], dtype=Int64, value= [1, 2])}
{&#39;data1&#39;: Tensor(shape=[3], dtype=Int64, value= [2, 3, 4]), &#39;data2&#39;: Tensor(shape=[2], dtype=Int64, value= [1, 2])}
{&#39;data1&#39;: Tensor(shape=[3], dtype=Int64, value= [3, 4, 5]), &#39;data2&#39;: Tensor(shape=[2], dtype=Int64, value= [1, 2])}
</pre></div>
</div>
</section>
<section id="concat">
<h3>concat<a class="headerlink" href="#concat" title="Permalink to this headline"></a></h3>
<p>Concatenates two datasets into one horizontally.</p>
<blockquote>
<div><p>Enter the column name in the dataset. The column data type and column data sequence must be the same.</p>
</div></blockquote>
<p><img alt="concat" src="_images/concat.png" /></p>
<p>The following example builds two random datasets, concatenates them by row, and displays the concatenated data result. Note that the same effect can be achieved by using the <code class="docutils literal notranslate"><span class="pre">+</span></code> operator.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>

<span class="k">def</span> <span class="nf">generator_func</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
        <span class="k">yield</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]),)</span>

<span class="k">def</span> <span class="nf">generator_func2</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
        <span class="k">yield</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),)</span>

<span class="n">dataset1</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">GeneratorDataset</span><span class="p">(</span><span class="n">generator_func</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;data1&quot;</span><span class="p">])</span>
<span class="n">dataset2</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">GeneratorDataset</span><span class="p">(</span><span class="n">generator_func2</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;data1&quot;</span><span class="p">])</span>

<span class="n">dataset3</span> <span class="o">=</span> <span class="n">dataset1</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">dataset2</span><span class="p">)</span>

<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataset3</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<p>The output is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>{&#39;data1&#39;: Tensor(shape=[3], dtype=Int64, value= [0, 0, 0])}
{&#39;data1&#39;: Tensor(shape=[3], dtype=Int64, value= [0, 0, 0])}
{&#39;data1&#39;: Tensor(shape=[3], dtype=Int64, value= [1, 2, 3])}
{&#39;data1&#39;: Tensor(shape=[3], dtype=Int64, value= [1, 2, 3])}
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="pipeline.html" class="btn btn-neutral float-left" title="Processing Data" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="augmentation.html" class="btn btn-neutral float-right" title="Image Data Processing and Enhancement" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>