<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Using MindSpore on the Cloud &mdash; MindSpore master documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="SPONGE Molecular Simulation Practice" href="hpc_sponge.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="architecture.html">Overall Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_structure.html">MindSpore API Overview</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quickstart</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="quick_start/linear_regression.html">Implementing Simple Linear Function Fitting</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start/quick_start.html">Implementing an Image Classification Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start/quick_video.html">Hands-on Installation and Experience</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Basic Concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dtype.html">dtype</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor.html">Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="operators.html">Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="cell.html">Cell</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Numpy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="numpy.html">NumPy Interfaces in MindSpore</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Execution Management</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="context.html">Configuring Running Information</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Pipeline</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dataset_sample.html">Quick Start of Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset.html">Loading Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline.html">Processing Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset_advanced.html">Advanced Usage of Pipeline</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build the Network</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="initializer.html">Initialization of Network Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter.html">Updating Network Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="layer.html">Model Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="loss.html">Loss Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">Optimization Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_net.html">Building a Customized Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="network_component.html">Common Network Components</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Train Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="run.html">Running Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="callback.html">Callback Mechanism</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_model.html">Saving Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="load_model_for_inference_and_transfer.html">Loading a Model for Inference and Transfer Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="train.html">Advanced Usage of Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference_ascend_910.html">Inference on the Ascend 910 AI processor</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference_ascend_310.html">Inference on Ascend 310</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference_gpu.html">Inference on a GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference_cpu.html">Inference on a CPU</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Distributed Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="distributed_training.html">Distributed Training Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_training_ascend.html">Parallel Distributed Training (Ascend)</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_training_gpu.html">Distributed Parallel Training (GPU)</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_pipeline_parallel.html">Pipeline Parallelism Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_host_device_training.html">Applying Host&amp;Device Hybrid Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_parameter_server_training.html">Training with Parameter Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_load_model_hybrid_parallel.html">Saving and Loading Models in Hybrid Parallel Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_inference.html">Distributed Inference With Multi Devices</a></li>
<li class="toctree-l1"><a class="reference internal" href="auto_parallel.html">Parallel Distributed Training Interfaces</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Function Debugging</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="debug_in_pynative_mode.html">Debugging in PyNative Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="dump_in_graph_mode.html">Using Dump in the Graph Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_debugging_info.html">Custom Debugging Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="evaluate_the_model_during_training.html">Evaluating the Model during Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="incremental_operator_build.html">Incremental Operator Build</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="optimize_data_processing.html">Optimizing the Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_mixed_precision.html">Enabling Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_graph_kernel_fusion.html">Enabling Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_gradient_accumulation.html">Applying a Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_quantization_aware_training.html">Applying Quantization Aware Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_post_training_quantization.html">Applying Post Training Quantization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="cv.html">Computer Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp.html">Natural Language Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="hpc.html">High Performance Computing</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Using MindSpore on the Cloud</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#preparations">Preparations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#preparing-modelarts">Preparing ModelArts</a></li>
<li class="toctree-l3"><a class="reference internal" href="#accessing-ascend-ai-processor-resources-on-huawei-cloud">Accessing Ascend AI Processor Resources on HUAWEI CLOUD</a></li>
<li class="toctree-l3"><a class="reference internal" href="#preparing-data">Preparing Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#preparing-for-script-execution">Preparing for Script Execution</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#running-the-mindspore-script-on-modelarts-after-simple-adaptation">Running the MindSpore Script on ModelArts After Simple Adaptation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#adapting-to-script-arguments">Adapting to Script Arguments</a></li>
<li class="toctree-l3"><a class="reference internal" href="#adapting-to-obs-data">Adapting to OBS Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#adapting-to-8-device-training-jobs">Adapting to 8-Device Training Jobs</a></li>
<li class="toctree-l3"><a class="reference internal" href="#sample-code">Sample Code</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#creating-a-training-job">Creating a Training Job</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#opening-the-modelarts-console">Opening the ModelArts Console</a></li>
<li class="toctree-l3"><a class="reference internal" href="#using-a-common-framework-to-create-a-training-job">Using a Common Framework to Create a Training Job</a></li>
<li class="toctree-l3"><a class="reference internal" href="#using-mindspore-as-a-common-framework-to-create-a-training-job">Using MindSpore as a Common Framework to Create a Training Job</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#viewing-the-execution-result">Viewing the Execution Result</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Using MindSpore on the Cloud</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/use_on_the_cloud.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section class="tex2jax_ignore mathjax_ignore" id="using-mindspore-on-the-cloud">
<h1>Using MindSpore on the Cloud<a class="headerlink" href="#using-mindspore-on-the-cloud" title="Permalink to this headline"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Linux</span></code> <code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">Whole</span> <span class="pre">Process</span></code> <code class="docutils literal notranslate"><span class="pre">Beginner</span></code> <code class="docutils literal notranslate"><span class="pre">Intermediate</span></code> <code class="docutils literal notranslate"><span class="pre">Expert</span></code></p>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.3/docs/mindspore/programming_guide/source_en/use_on_the_cloud.md" target="_blank"><img src="https://gitee.com/mindspore/docs/raw/r1.3/resource/_static/logo_source.png"></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>ModelArts is a one-stop AI development platform provided by HUAWEI CLOUD. It integrates the Ascend AI Processor resource pool. Developers can experience MindSpore on this platform.</p>
<p>ResNet-50 is used as an example to describe how to use MindSpore to complete a training task on ModelArts.</p>
</section>
<section id="preparations">
<h2>Preparations<a class="headerlink" href="#preparations" title="Permalink to this headline"></a></h2>
<section id="preparing-modelarts">
<h3>Preparing ModelArts<a class="headerlink" href="#preparing-modelarts" title="Permalink to this headline"></a></h3>
<p>Create an account, configure ModelArts, and create an Object Storage Service (OBS) bucket by referring to the “Preparations” section in the ModelArts tutorial.</p>
<blockquote>
<div><p>For more information about ModelArts, visit <a class="reference external" href="https://support.huaweicloud.com/wtsnew-modelarts/index.html">https://support.huaweicloud.com/wtsnew-modelarts/index.html</a>. Prepare ModelArts by referring to the “Preparations” section.</p>
</div></blockquote>
</section>
<section id="accessing-ascend-ai-processor-resources-on-huawei-cloud">
<h3>Accessing Ascend AI Processor Resources on HUAWEI CLOUD<a class="headerlink" href="#accessing-ascend-ai-processor-resources-on-huawei-cloud" title="Permalink to this headline"></a></h3>
<p>You can click <a class="reference external" href="https://console.huaweicloud.com/modelarts/?region=cn-north-4#/dashboard/applyModelArtsAscend910Beta">here</a> to join the beta testing program of the ModelArts Ascend Compute Service.</p>
</section>
<section id="preparing-data">
<h3>Preparing Data<a class="headerlink" href="#preparing-data" title="Permalink to this headline"></a></h3>
<p>ModelArts uses OBS to store data. Therefore, before starting a training job, you need to upload the data to OBS. The CIFAR-10 dataset in binary format is used as an example.</p>
<ol class="arabic">
<li><p>Download and decompress the CIFAR-10 dataset.</p>
<blockquote>
<div><p>Download the CIFAR-10 dataset at <a class="reference external" href="http://www.cs.toronto.edu/~kriz/cifar.html">http://www.cs.toronto.edu/~kriz/cifar.html</a>. Among the three dataset versions provided on the page, select CIFAR-10 binary version.</p>
</div></blockquote>
</li>
<li><p>Create an OBS bucket (for example, ms-dataset), create a data directory (for example, cifar-10) in the bucket, and upload the CIFAR-10 data to the data directory according to the following structure.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>└─Object storage/ms-dataset/cifar-10
    ├─train
    │      data_batch_1.bin
    │      data_batch_2.bin
    │      data_batch_3.bin
    │      data_batch_4.bin
    │      data_batch_5.bin
    │
    └─eval
        test_batch.bin
</pre></div>
</div>
</li>
</ol>
</section>
<section id="preparing-for-script-execution">
<h3>Preparing for Script Execution<a class="headerlink" href="#preparing-for-script-execution" title="Permalink to this headline"></a></h3>
<p>Create an OBS bucket (for example, <code class="docutils literal notranslate"><span class="pre">resnet50-train</span></code>), create a code directory (for example, <code class="docutils literal notranslate"><span class="pre">resnet50_cifar10_train</span></code>) in the bucket, and upload all scripts in the following directories to the code directory:</p>
<blockquote>
<div><p>ResNet-50 is used in scripts in <a class="reference external" href="https://gitee.com/mindspore/docs/tree/r1.3/docs/sample_code/sample_for_cloud/">https://gitee.com/mindspore/docs/tree/r1.3/docs/sample_code/sample_for_cloud/</a> to train the CIFAR-10 dataset and validate the accuracy after training is complete. <code class="docutils literal notranslate"><span class="pre">1*Ascend</span></code> or <code class="docutils literal notranslate"><span class="pre">8*Ascend</span></code> can be used in scripts on ModelArts for training.</p>
<p>Note that the script version must be the same as the MindSpore version selected in “Creating a Training Task.” For example, if you use scripts provided for MindSpore 1.1, you need to select MindSpore 1.1 when creating a training job.</p>
</div></blockquote>
<p>To facilitate subsequent training job creation, you need to create a training output directory and a log output directory. The directory structure created in this example is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>└─Object storage/resnet50-train
    ├─resnet50_cifar10_train
    │      dataset.py
    │      resnet.py
    │      resnet50_train.py
    │
    ├─output
    └─log
</pre></div>
</div>
</section>
</section>
<section id="running-the-mindspore-script-on-modelarts-after-simple-adaptation">
<h2>Running the MindSpore Script on ModelArts After Simple Adaptation<a class="headerlink" href="#running-the-mindspore-script-on-modelarts-after-simple-adaptation" title="Permalink to this headline"></a></h2>
<p>Scripts provided in section “Preparing for Script Execution” can directly run on ModelArts. If you want to experience how to use ResNet-50 to train CIFAR-10, skip this section. If you need to run customized MindSpore scripts or more MindSpore sample code on ModelArts, perform simple adaptation on the MindSpore code as follows:</p>
<section id="adapting-to-script-arguments">
<h3>Adapting to Script Arguments<a class="headerlink" href="#adapting-to-script-arguments" title="Permalink to this headline"></a></h3>
<ol class="arabic">
<li><p>Set <code class="docutils literal notranslate"><span class="pre">data_url</span></code> and <code class="docutils literal notranslate"><span class="pre">train_url</span></code>. They are necessary for running the script on ModelArts, corresponding to the data storage path (an OBS path) and training output path (an OBS path), respectively.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">argparse</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;ResNet-50 train.&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--data_url&#39;</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Location of data.&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--train_url&#39;</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Location of training outputs.&#39;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>ModelArts allows you to pass arguments to the configuration options in the script. For details, see “Creating a Training Job.”</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--epoch_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Train epoch size.&#39;</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="adapting-to-obs-data">
<h3>Adapting to OBS Data<a class="headerlink" href="#adapting-to-obs-data" title="Permalink to this headline"></a></h3>
<p>MindSpore does not provide APIs for directly accessing OBS data. You need to use APIs provided by MoXing to interact with OBS. ModelArts training scripts are executed in containers. Generally, the <code class="docutils literal notranslate"><span class="pre">/cache</span></code> directory is used to store the container data.</p>
<blockquote>
<div><p>HUAWEI CLOUD MoXing provides various APIs for users: <a class="reference external" href="https://github.com/huaweicloud/ModelArts-Lab/tree/master/docs/moxing_api_doc">https://github.com/huaweicloud/ModelArts-Lab/tree/master/docs/moxing_api_doc</a>. In this example, only the <code class="docutils literal notranslate"><span class="pre">copy_parallel</span></code> API is used.</p>
</div></blockquote>
<ol class="arabic">
<li><p>Download the data stored in OBS to an execution container.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">moxing</span> <span class="k">as</span> <span class="nn">mox</span>
<span class="n">mox</span><span class="o">.</span><span class="n">file</span><span class="o">.</span><span class="n">copy_parallel</span><span class="p">(</span><span class="n">src_url</span><span class="o">=</span><span class="s1">&#39;s3://dataset_url/&#39;</span><span class="p">,</span> <span class="n">dst_url</span><span class="o">=</span><span class="s1">&#39;/cache/data_path&#39;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Upload the training output from the container to OBS.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">moxing</span> <span class="k">as</span> <span class="nn">mox</span>
<span class="n">mox</span><span class="o">.</span><span class="n">file</span><span class="o">.</span><span class="n">copy_parallel</span><span class="p">(</span><span class="n">src_url</span><span class="o">=</span><span class="s1">&#39;/cache/output_path&#39;</span><span class="p">,</span> <span class="n">dst_url</span><span class="o">=</span><span class="s1">&#39;s3://output_url/&#39;</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="adapting-to-8-device-training-jobs">
<h3>Adapting to 8-Device Training Jobs<a class="headerlink" href="#adapting-to-8-device-training-jobs" title="Permalink to this headline"></a></h3>
<p>To run scripts in the <code class="docutils literal notranslate"><span class="pre">8*Ascend</span></code> environment, you need to adapt dataset creation code and a local data path, and configure a distributed policy. By obtaining the environment variables <code class="docutils literal notranslate"><span class="pre">DEVICE_ID</span></code> and <code class="docutils literal notranslate"><span class="pre">RANK_SIZE</span></code>, you can build training scripts applicable to <code class="docutils literal notranslate"><span class="pre">1*Ascend</span></code> and <code class="docutils literal notranslate"><span class="pre">8*Ascend</span></code>.</p>
<ol class="arabic">
<li><p>Adapt a local path.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>

<span class="n">device_num</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;RANK_SIZE&#39;</span><span class="p">))</span>
<span class="n">device_id</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;DEVICE_ID&#39;</span><span class="p">))</span>
<span class="c1"># define local data path</span>
<span class="n">local_data_path</span> <span class="o">=</span> <span class="s1">&#39;/cache/data&#39;</span>

<span class="k">if</span> <span class="n">device_num</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="c1"># define distributed local data path</span>
    <span class="n">local_data_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">local_data_path</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">device_id</span><span class="p">))</span>
</pre></div>
</div>
</li>
<li><p>Adapt datasets.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.engine</span> <span class="k">as</span> <span class="nn">de</span>

<span class="n">device_id</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;DEVICE_ID&#39;</span><span class="p">))</span>
<span class="n">device_num</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;RANK_SIZE&#39;</span><span class="p">))</span>
<span class="k">if</span> <span class="n">device_num</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="c1"># create train data for 1 Ascend situation</span>
    <span class="n">ds</span> <span class="o">=</span> <span class="n">de</span><span class="o">.</span><span class="n">Cifar10Dataset</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># create train data for 1 Ascend situation, split train data for 8 Ascend situation</span>
    <span class="n">ds</span> <span class="o">=</span> <span class="n">de</span><span class="o">.</span><span class="n">Cifar10Dataset</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                           <span class="n">num_shards</span><span class="o">=</span><span class="n">device_num</span><span class="p">,</span> <span class="n">shard_id</span><span class="o">=</span><span class="n">device_id</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Configure a distributed policy.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">mindspore.context</span> <span class="kn">import</span> <span class="n">ParallelMode</span>

<span class="n">device_num</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;RANK_SIZE&#39;</span><span class="p">))</span>
<span class="k">if</span> <span class="n">device_num</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">device_num</span><span class="o">=</span><span class="n">device_num</span><span class="p">,</span>
                                      <span class="n">parallel_mode</span><span class="o">=</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">DATA_PARALLEL</span><span class="p">,</span>
                                      <span class="n">gradients_mean</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="sample-code">
<h3>Sample Code<a class="headerlink" href="#sample-code" title="Permalink to this headline"></a></h3>
<p>Perform simple adaptation on the MindSpore script based on the preceding three points. The following pseudocode is used as an example:</p>
<p>Original MindSpore script:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">mindspore.context</span> <span class="kn">import</span> <span class="n">ParallelMode</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.engine</span> <span class="k">as</span> <span class="nn">de</span>

<span class="n">device_id</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;DEVICE_ID&#39;</span><span class="p">))</span>
<span class="n">device_num</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;RANK_SIZE&#39;</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">create_dataset</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">device_num</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">ds</span> <span class="o">=</span> <span class="n">de</span><span class="o">.</span><span class="n">Cifar10Dataset</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ds</span> <span class="o">=</span> <span class="n">de</span><span class="o">.</span><span class="n">Cifar10Dataset</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                               <span class="n">num_shards</span><span class="o">=</span><span class="n">device_num</span><span class="p">,</span> <span class="n">shard_id</span><span class="o">=</span><span class="n">device_id</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ds</span>

<span class="k">def</span> <span class="nf">resnet50_train</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">device_num</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">device_num</span><span class="o">=</span><span class="n">device_num</span><span class="p">,</span>
                                          <span class="n">parallel_mode</span><span class="o">=</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">DATA_PARALLEL</span><span class="p">,</span>
                                          <span class="n">gradients_mean</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">local_data_path</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;ResNet-50 train.&#39;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--local_data_path&#39;</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Location of data.&#39;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--epoch_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Train epoch size.&#39;</span><span class="p">)</span>

    <span class="n">args_opt</span><span class="p">,</span> <span class="n">unknown</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_known_args</span><span class="p">()</span>

    <span class="n">resnet50_train</span><span class="p">(</span><span class="n">args_opt</span><span class="p">)</span>
</pre></div>
</div>
<p>Adapted MindSpore script:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">mindspore.context</span> <span class="kn">import</span> <span class="n">ParallelMode</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.engine</span> <span class="k">as</span> <span class="nn">de</span>

<span class="c1"># adapt to cloud: used for downloading data</span>
<span class="kn">import</span> <span class="nn">moxing</span> <span class="k">as</span> <span class="nn">mox</span>

<span class="n">device_id</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;DEVICE_ID&#39;</span><span class="p">))</span>
<span class="n">device_num</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;RANK_SIZE&#39;</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">create_dataset</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">device_num</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">ds</span> <span class="o">=</span> <span class="n">de</span><span class="o">.</span><span class="n">Cifar10Dataset</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ds</span> <span class="o">=</span> <span class="n">de</span><span class="o">.</span><span class="n">Cifar10Dataset</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                               <span class="n">num_shards</span><span class="o">=</span><span class="n">device_num</span><span class="p">,</span> <span class="n">shard_id</span><span class="o">=</span><span class="n">device_id</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ds</span>

<span class="k">def</span> <span class="nf">resnet50_train</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
    <span class="c1"># adapt to cloud: define local data path</span>
    <span class="n">local_data_path</span> <span class="o">=</span> <span class="s1">&#39;/cache/data&#39;</span>

    <span class="k">if</span> <span class="n">device_num</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">device_num</span><span class="o">=</span><span class="n">device_num</span><span class="p">,</span>
                                          <span class="n">parallel_mode</span><span class="o">=</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">DATA_PARALLEL</span><span class="p">,</span>
                                          <span class="n">gradients_mean</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="c1"># adapt to cloud: define distributed local data path</span>
        <span class="n">local_data_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">local_data_path</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">device_id</span><span class="p">))</span>

    <span class="c1"># adapt to cloud: download data from obs to local location</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Download data.&#39;</span><span class="p">)</span>
    <span class="n">mox</span><span class="o">.</span><span class="n">file</span><span class="o">.</span><span class="n">copy_parallel</span><span class="p">(</span><span class="n">src_url</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">data_url</span><span class="p">,</span> <span class="n">dst_url</span><span class="o">=</span><span class="n">local_data_path</span><span class="p">)</span>

    <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">local_data_path</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;ResNet-50 train.&#39;</span><span class="p">)</span>
    <span class="c1"># adapt to cloud: get obs data path</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--data_url&#39;</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Location of data.&#39;</span><span class="p">)</span>
    <span class="c1"># adapt to cloud: get obs output path</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--train_url&#39;</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Location of training outputs.&#39;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--epoch_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Train epoch size.&#39;</span><span class="p">)</span>
    <span class="n">args_opt</span><span class="p">,</span> <span class="n">unknown</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_known_args</span><span class="p">()</span>

    <span class="n">resnet50_train</span><span class="p">(</span><span class="n">args_opt</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="creating-a-training-job">
<h2>Creating a Training Job<a class="headerlink" href="#creating-a-training-job" title="Permalink to this headline"></a></h2>
<p>Create a training job to run the MindSpore script. The following provides step-by-step instructions for creating a training job on ModelArts.</p>
<section id="opening-the-modelarts-console">
<h3>Opening the ModelArts Console<a class="headerlink" href="#opening-the-modelarts-console" title="Permalink to this headline"></a></h3>
<p>Click Console on the HUAWEI CLOUD ModelArts home page at <a class="reference external" href="https://www.huaweicloud.com/product/modelarts.html">https://www.huaweicloud.com/product/modelarts.html</a>.</p>
</section>
<section id="using-a-common-framework-to-create-a-training-job">
<h3>Using a Common Framework to Create a Training Job<a class="headerlink" href="#using-a-common-framework-to-create-a-training-job" title="Permalink to this headline"></a></h3>
<p>ModelArts Tutorial <a class="reference external" href="https://support.huaweicloud.com/engineers-modelarts/modelarts_23_0238.html">https://support.huaweicloud.com/engineers-modelarts/modelarts_23_0238.html</a> shows how to use a common framework to create a training job.</p>
</section>
<section id="using-mindspore-as-a-common-framework-to-create-a-training-job">
<h3>Using MindSpore as a Common Framework to Create a Training Job<a class="headerlink" href="#using-mindspore-as-a-common-framework-to-create-a-training-job" title="Permalink to this headline"></a></h3>
<p>Training scripts and data in this tutorial are used as an example to describe how to configure arguments on the training job creation page.</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Algorithm</span> <span class="pre">Source</span></code>: Click <code class="docutils literal notranslate"><span class="pre">Frameworks</span></code>, and then select <code class="docutils literal notranslate"><span class="pre">Ascend-Powered-Engine</span></code> and the required MindSpore version. (<code class="docutils literal notranslate"><span class="pre">Mindspore-0.5-python3.7-aarch64</span></code> is used as an example here. Use scripts corresponding to the selected version.)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Code</span> <span class="pre">Directory</span></code>: Select a code directory created in an OBS bucket. Set <code class="docutils literal notranslate"><span class="pre">Startup</span> <span class="pre">File</span></code> to a startup script in the code directory.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Data</span> <span class="pre">Source</span></code>: Click <code class="docutils literal notranslate"><span class="pre">Data</span> <span class="pre">Storage</span> <span class="pre">Path</span></code> and enter the CIFAR-10 dataset path in OBS.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Argument</span></code>: Set <code class="docutils literal notranslate"><span class="pre">data_url</span></code> and <code class="docutils literal notranslate"><span class="pre">train_url</span></code> to the values of <code class="docutils literal notranslate"><span class="pre">Data</span> <span class="pre">Storage</span> <span class="pre">Path</span></code> and <code class="docutils literal notranslate"><span class="pre">Training</span> <span class="pre">Output</span> <span class="pre">Path</span></code>, respectively. Click the add icon to pass values to other arguments in the script, for example, <code class="docutils literal notranslate"><span class="pre">epoch_size</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Resource</span> <span class="pre">Pool</span></code>: Click <code class="docutils literal notranslate"><span class="pre">Public</span> <span class="pre">Resource</span> <span class="pre">Pool</span> <span class="pre">&gt;</span> <span class="pre">Ascend</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Specification</span></code>: Select <code class="docutils literal notranslate"><span class="pre">Ascend:</span> <span class="pre">1</span> <span class="pre">*</span> <span class="pre">Ascend</span> <span class="pre">910</span> <span class="pre">CPU:</span> <span class="pre">24-core</span> <span class="pre">96</span> <span class="pre">GiB</span></code> or <code class="docutils literal notranslate"><span class="pre">Ascend:</span> <span class="pre">8</span> <span class="pre">*</span> <span class="pre">Ascend</span> <span class="pre">910</span> <span class="pre">CPU:</span> <span class="pre">192-core</span> <span class="pre">768</span> <span class="pre">GiB</span></code>, which indicate single-node single-device and single-node 8-device specifications, respectively.</p></li>
</ol>
</section>
</section>
<section id="viewing-the-execution-result">
<h2>Viewing the Execution Result<a class="headerlink" href="#viewing-the-execution-result" title="Permalink to this headline"></a></h2>
<ol class="arabic">
<li><p>You can view run logs on the Training Jobs page.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">8*Ascend</span></code> specification is used to execute the ResNet-50 training job. The total number of epochs is 92, the accuracy is about 92%, and the number of images trained per second is about 12,000.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">1*Ascend</span></code> specification is used to execute the ResNet-50 training job. The total number of epochs is 92, the accuracy is about 95%, and the number of images trained per second is about 1800.</p>
</li>
<li><p>If you specify a log path when creating a training job, you can download log files from OBS and view them.</p></li>
</ol>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="hpc_sponge.html" class="btn btn-neutral float-left" title="SPONGE Molecular Simulation Practice" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>