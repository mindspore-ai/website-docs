<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Parallel Distributed Training Interfaces &mdash; MindSpore master documentation</title><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Distributed Parallel Training Mode" href="distributed_training_mode.html" />
    <link rel="prev" title="Distributed Parallel Overview" href="distributed_training.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="architecture.html">Overall Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_structure.html">MindSpore API Overview</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="design/technical_white_paper.html">Technical White Paper</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/gradient.html">MindSpore Automatic Differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/distributed_training_design.html">Distributed Training Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/mindir.html">MindSpore IR (MindIR)</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.5/training_visual_design.html">Design of Visualization↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quickstart</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/tutorials/en/r1.5/linear_regression.html">Implementing Simple Linear Function Fitting↗</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/tutorials/en/r1.5/quick_start.html">Implementing an Image Classification Application↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Basic Concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dtype.html">DataType</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor.html">Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter_introduction.html">Parameter</a></li>
<li class="toctree-l1"><a class="reference internal" href="operators.html">Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="cell.html">Cell</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset_introduction.html">Dataset</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Pipeline</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dataset_sample.html">Quick Start of Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset.html">Loading Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline.html">Processing Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset_advanced.html">Advanced Usage of Pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset_usage.html">Data Iteration</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build the Network</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="build_net.html">Constructing Single Operator Network and Multi-layer Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="initializer.html">Initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter.html">Network Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="control_flow.html">Using the Process Control Statement</a></li>
<li class="toctree-l1"><a class="reference internal" href="loss.html">Loss Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="grad_operation.html">Gradient Operation</a></li>
<li class="toctree-l1"><a class="reference internal" href="indefinite_parameter.html">Parameter Passing</a></li>
<li class="toctree-l1"><a class="reference internal" href="constexpr.html">Construct Constants In the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="hypermap.html">Operation Overloading</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">Optimization Algorithms</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Running</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="context.html">Configuring Running Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="run.html">Running Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_and_load_models.html">Save and Load Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="model.html">Application of Model</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="online_inference.html">Online Inference with Checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="offline_inference.html">Using Offline Model for Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Distributed Training</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="distributed_training.html">Distributed Parallel Overview</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Parallel Distributed Training Interfaces</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#parallel-distributed-training-configuration">Parallel Distributed Training Configuration</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#general-configuration">General Configuration</a></li>
<li class="toctree-l4"><a class="reference internal" href="#automatic-parallel-configuration">Automatic Parallel Configuration</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#distributed-communication-interface">Distributed Communication Interface</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#init">init</a></li>
<li class="toctree-l4"><a class="reference internal" href="#get-group-size">get_group_size</a></li>
<li class="toctree-l4"><a class="reference internal" href="#get-rank">get_rank</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#distributed-attribute-configuration">Distributed Attribute Configuration</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#cross-batch">cross_batch</a></li>
<li class="toctree-l4"><a class="reference internal" href="#fusion">fusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="#layerwise-parallel">layerwise_parallel</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="distributed_training_mode.html">Distributed Parallel Training Mode</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="distributed_advanced.html">Distributed Parallel Advanced Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_example.html">Distributed Parallel Usage Example</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">PyNative</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="debug_in_pynative_mode.html">Debugging in PyNative Mode</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Numpy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="numpy.html">NumPy Interfaces in MindSpore</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Features</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="second_order_optimizer.html">Second Order Optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_quantization_aware_training.html">Applying Quantization Aware Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Function Debugging</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="read_ir_files.html">Reading IR</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/en/r1.5/debug_in_pynative_mode.html">Debugging in PyNative Mode↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="dump_in_graph_mode.html">Using Dump in the Graph Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_debugging_info.html">Custom Debugging Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="incremental_operator_build.html">Incremental Operator Build</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="enable_mixed_precision.html">Enabling Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_graph_kernel_fusion.html">Enabling Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_auto_tune.html">Enabling AutoTune</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_gradient_accumulation.html">Applying a Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.5/performance_profiling.html">Debugging performance with Profiler↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cv.html">Computer Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp.html">Natural Language Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="hpc.html">High Performance Computing</a></li>
<li class="toctree-l1"><a class="reference internal" href="use_on_the_cloud.html">Using MindSpore on the Cloud</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="distributed_training.html">Distributed Parallel Overview</a> &raquo;</li>
      <li>Parallel Distributed Training Interfaces</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/auto_parallel.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="parallel-distributed-training-interfaces">
<h1>Parallel Distributed Training Interfaces<a class="headerlink" href="#parallel-distributed-training-interfaces" title="Permalink to this headline"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">Distributed</span> <span class="pre">Parallel</span></code></p>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.5/docs/mindspore/programming_guide/source_en/auto_parallel.md" target="_blank"><img src="https://gitee.com/mindspore/docs/raw/r1.5/resource/_static/logo_source_en.png"></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>In deep learning, as the number of datasets and parameters increases, the time and hardware resources required for training increase, and finally become a bottleneck to the training. Parallel distributed training can reduce the requirements on hardware such as memory and computing performance and is an important optimization method for training.</p>
<p>MindSpore provides the parallel distributed training function and supports multiple parallel modes, including data parallel and automatic parallel.</p>
</section>
<section id="parallel-distributed-training-configuration">
<h2>Parallel Distributed Training Configuration<a class="headerlink" href="#parallel-distributed-training-configuration" title="Permalink to this headline"></a></h2>
<p>The parallel distributed training configuration of MindSpore is managed by <code class="docutils literal notranslate"><span class="pre">auto_parallel_context</span></code> in a centralized manner. You can customize the configuration based on the actual situation. These configurations can be classified into three types:</p>
<ul class="simple">
<li><p>General configuration: takes effect on both data parallel and automatic parallel, for example, <code class="docutils literal notranslate"><span class="pre">device_num</span></code> and <code class="docutils literal notranslate"><span class="pre">global_rank</span></code> etc.</p></li>
<li><p>Automatic parallel configuration: takes effect only in automatic parallel mode, for example, <code class="docutils literal notranslate"><span class="pre">gradient_fp32_sync</span></code> etc.</p></li>
</ul>
<p>You can use <code class="docutils literal notranslate"><span class="pre">context.set_auto_parallel_context</span></code> to configure the preceding parameters and use <code class="docutils literal notranslate"><span class="pre">context.get_auto_parallel_context</span></code> to obtain the parameters.</p>
<section id="general-configuration">
<h3>General Configuration<a class="headerlink" href="#general-configuration" title="Permalink to this headline"></a></h3>
<section id="device-num">
<h4>device_num<a class="headerlink" href="#device-num" title="Permalink to this headline"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">device_num</span></code> indicates the number of available machines. The default value is 0. The value is of the int type and must range from 1 to 4096. If you do not set this parameter, the <code class="docutils literal notranslate"><span class="pre">Model</span></code> interface obtains the value by using the <code class="docutils literal notranslate"><span class="pre">get_group_size</span></code> method. If you set this parameter, your configuration is used. This configuration allows you to manually transfer <code class="docutils literal notranslate"><span class="pre">device_num</span></code> without using the <code class="docutils literal notranslate"><span class="pre">Model</span></code> interface.</p>
<p>The following is a code example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">device_num</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">context</span><span class="o">.</span><span class="n">get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;device_num&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="global-rank">
<h4>global_rank<a class="headerlink" href="#global-rank" title="Permalink to this headline"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">global_rank</span></code> indicates the logical sequence number of the current device. The default value is 0. The value is of the int type and must range from 0 to 4095. If you do not set this parameter, the <code class="docutils literal notranslate"><span class="pre">Model</span></code> interface obtains the value by using the <code class="docutils literal notranslate"><span class="pre">get_rank</span></code> method. If you set this parameter, your configuration is used. This configuration allows you to manually transfer <code class="docutils literal notranslate"><span class="pre">global_rank</span></code> without using the <code class="docutils literal notranslate"><span class="pre">Model</span></code> interface.</p>
<p>The following is a code example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">global_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">context</span><span class="o">.</span><span class="n">get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;global_rank&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="gradients-mean">
<h4>gradients_mean<a class="headerlink" href="#gradients-mean" title="Permalink to this headline"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">gradients_mean</span></code> indicates whether to perform the averaging operation during reverse gradient aggregation. The value is of the Boolean type. The default value is False, indicating that only the SUM operation of AllReduce is performed for gradient aggregation. <code class="docutils literal notranslate"><span class="pre">gradients_means</span></code> affects network convergence. The setting of <code class="docutils literal notranslate"><span class="pre">gradients_means</span></code> may vary in different scenarios. Therefore, MindSpore provides this interface for users to configure parameters based on the actual situation.</p>
<p>The following is a code example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">gradients_mean</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">context</span><span class="o">.</span><span class="n">get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;gradients_mean&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="parallel-mode">
<h4>parallel_mode<a class="headerlink" href="#parallel-mode" title="Permalink to this headline"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">parallel_mode</span></code> indicates the parallel mode. The value is a character string. The options are as follows:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">stand_alone</span></code>: standalone mode.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">data_parallel</span></code>: data parallel mode.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">hybrid_parallel</span></code>: hybrid parallel mode.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">semi_auto_parallel</span></code>: semi-automatic parallel mode. In this mode, you can use the <code class="docutils literal notranslate"><span class="pre">shard</span></code> method to configure a segmentation policy for an operator. If no policy is configured, the data parallel policy is used by default.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">auto_parallel</span></code>: automatic parallel mode. In this mode, the framework automatically creates a cost model and selects the optimal segmentation policy for users.</p></li>
</ul>
<p>The complete examples of <code class="docutils literal notranslate"><span class="pre">auto_parallel</span></code> and <code class="docutils literal notranslate"><span class="pre">data_parallel</span></code> are provided in <a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/zh-CN/r1.5/distributed_training.html">Distributed Training</a>.</p>
<p>The following is a code example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="s2">&quot;semi_auto_parallel&quot;</span><span class="p">)</span>
<span class="n">mul</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">context</span><span class="o">.</span><span class="n">get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;parallel_mode&quot;</span><span class="p">)</span>
</pre></div>
</div>
<blockquote>
<div><p>In semi_auto_parallel mode, if a parameter is used by multiple operators, please ensure that the parameter layout in each operator is consistent, otherwise an error will be reported during compilation. In the following example, mul1 and mul2 share the weight, but mul1 splits weight into 8 slices by row, however, mul2 splits the weight into 8 slices by column. The layout of weight in the two operators is inconsistent, compilation will be failed.</p>
</div></blockquote>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">Cell</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Net definition&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mul1</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mul2</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="s2">&quot;weight1&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul2</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
</section>
<section id="all-reduce-fusion-config">
<h4>all_reduce_fusion_config<a class="headerlink" href="#all-reduce-fusion-config" title="Permalink to this headline"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">all_reduce_fusion_config</span></code> allows users to customize the AllReduce segmentation policy by gradient aggregation. To reduce resource consumption and operator execution gaps, the framework fusions all the reverse gradient aggregation AllReduce operators into one by default. However, when the model is large, the iteration smearing time increases. You can set this parameter based on the actual network to manually tune and find the optimal segmentation policy by gradient aggregation.</p>
<p>The following is a code example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">all_reduce_fusion_config</span><span class="o">=</span><span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">35</span><span class="p">])</span>
<span class="n">context</span><span class="o">.</span><span class="n">get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;all_reduce_fusion_config&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>In the example, the value range of <code class="docutils literal notranslate"><span class="pre">all_reduce_fusion_config</span></code> is [20,35]. The first 20 AllReduce operators, the 20th to 35th AllReduce operators, and the remaining AllReduce operators are fused into three operators, respectively.</p>
</section>
<section id="enable-parallel-optimizer">
<h4>enable_parallel_optimizer<a class="headerlink" href="#enable-parallel-optimizer" title="Permalink to this headline"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">enable_parallel_optimizer</span></code> is a feature under development. The default value is False. In data parallel, weight update has redundant computation among devices. Parallel optimizer shards the computation of optimizer to each device. For large-scale networks like Bert and GPT, this feature could reduce requirements on memory and improve the performance efficiently.</p>
<p>When the <code class="docutils literal notranslate"><span class="pre">enable_parallel_optimizer</span></code> is enabled in the data_parallel mode, MindSpore will split the parameters that need to be updated into different devices, and then use the Broadcast operator to share weights between clusters after each update. It should be noted that the number of parameters should be greater than the number of machines. Currently, only the <code class="docutils literal notranslate"><span class="pre">Lamb</span></code> and <code class="docutils literal notranslate"><span class="pre">AdamWeightDecay</span></code> optimizers are supported.</p>
<p>In the <code class="docutils literal notranslate"><span class="pre">auto_parallel</span></code> or <code class="docutils literal notranslate"><span class="pre">semi_auto_parallel</span></code> mode, the optimizer parallel is enabled. If one parameter which has been sliced by shard strategy still has repeated slices among devices, and the highest dimension of the shape can be divided by the number of devices, MindSpore would save parameters and update them by the smallest slice shapes. All optimizers are supported under this two modes.</p>
<p>No matter which parallel mode is selected, parallel optimizer would not influence the forward and backward graph. Only the computation of weight update would be influenced.</p>
<p>The following is a code example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">enable_parallel_optimizer</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">context</span><span class="o">.</span><span class="n">get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;enable_parallel_optimizer&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="parameter-broadcast">
<h4>parameter_broadcast<a class="headerlink" href="#parameter-broadcast" title="Permalink to this headline"></a></h4>
<p>Parameter broadcast shares the value of data parallel weights among devices, in the purpose of synchronization of weights. The default value is False and only the graph mode is supported.</p>
<p>The following is a code example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parameter_broadcast</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">context</span><span class="o">.</span><span class="n">get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;parameter_broadcast&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="automatic-parallel-configuration">
<h3>Automatic Parallel Configuration<a class="headerlink" href="#automatic-parallel-configuration" title="Permalink to this headline"></a></h3>
<section id="gradient-fp32-sync">
<h4>gradient_fp32_sync<a class="headerlink" href="#gradient-fp32-sync" title="Permalink to this headline"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">gradient_fp32_sync</span></code> indicates whether gradients are aggregated based on the FP32 type. The value is of the Boolean type. The default value is True, indicating that gradients are aggregated based on the FP32 type. Due to the special structure of the <code class="docutils literal notranslate"><span class="pre">Ascend</span></code> AI processor, the speed of aggregating FP32 data is higher than that of aggregating FP16 data, but the precision may be affected. Therefore, MindSpore provides the <code class="docutils literal notranslate"><span class="pre">gradient_fp32_sync</span></code> interface for users to make choices based on the actual situation.</p>
<p>The following is a code example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">gradient_fp32_sync</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">context</span><span class="o">.</span><span class="n">get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;gradient_fp32_sync&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="auto-parallel-search-mode">
<h4>auto_parallel_search_mode<a class="headerlink" href="#auto-parallel-search-mode" title="Permalink to this headline"></a></h4>
<p>MindSpore provides two search policies: <code class="docutils literal notranslate"><span class="pre">dynamic_programming</span></code> and <code class="docutils literal notranslate"><span class="pre">recursive_programming</span></code>. <code class="docutils literal notranslate"><span class="pre">dynamic_programming</span></code> can search for the optimal policy depicted by the cost model, but it takes a long time to search for the parallel policy of a huge network model. <code class="docutils literal notranslate"><span class="pre">recursive_programming</span></code> can quickly search out parallel policies, but the found policies may not have the optimal running performance. MindSpore provides parameters to allow users to select a search algorithm. The default value is <code class="docutils literal notranslate"><span class="pre">dynamic_programming</span></code>.</p>
<p>The following is a code example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">auto_parallel_search_mode</span><span class="o">=</span><span class="s2">&quot;dynamic_programming&quot;</span><span class="p">)</span>
<span class="n">context</span><span class="o">.</span><span class="n">get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;auto_parallel_search_mode&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="strategy-ckpt-load-file">
<h4>strategy_ckpt_load_file<a class="headerlink" href="#strategy-ckpt-load-file" title="Permalink to this headline"></a></h4>
<p>Specifies a path to load the segmentation information of all operators with weights in automatic parallel mode.</p>
<p>The following is a code example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">strategy_ckpt_load_file</span><span class="o">=</span><span class="s2">&quot;./&quot;</span><span class="p">)</span>
<span class="n">context</span><span class="o">.</span><span class="n">get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;strategy_ckpt_load_file&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="strategy-ckpt-save-file">
<h4>strategy_ckpt_save_file<a class="headerlink" href="#strategy-ckpt-save-file" title="Permalink to this headline"></a></h4>
<p>Specifies a path for storing the segmentation information of all operators with weights in automatic parallel mode.</p>
<p>The following is a code example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">strategy_ckpt_save_file</span><span class="o">=</span><span class="s2">&quot;./&quot;</span><span class="p">)</span>
<span class="n">context</span><span class="o">.</span><span class="n">get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;strategy_ckpt_save_file&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="full-batch">
<h4>full_batch<a class="headerlink" href="#full-batch" title="Permalink to this headline"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">full_batch</span></code> allows users to determine whether to import datasets in full mode. The default value is False. That is, datasets are imported in data parallel mode. In special scenarios, the performance of full dataset import is better than that of import in data parallel mode. For example, the WideDeep network is used in uneven segmentation scenarios. Therefore, MindSpore provides the <code class="docutils literal notranslate"><span class="pre">full_batch</span></code> configurable interface.</p>
<p>The following is a code example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">full_batch</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">context</span><span class="o">.</span><span class="n">get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;full_batch&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="pipeline-stages">
<h4>pipeline_stages<a class="headerlink" href="#pipeline-stages" title="Permalink to this headline"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">pipeline_stages</span></code> is used to set parallel stage information of  pipeline. It is used to show how the machines are distributed in <code class="docutils literal notranslate"><span class="pre">auto_parallel</span></code> mode in pipeline. Currently pipeline parallel is still under development.</p>
<p>The following is a code example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">pipeline_stages</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">context</span><span class="o">.</span><span class="n">get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;pipeline_stages&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="grad-accumulation-step">
<h4>grad_accumulation_step<a class="headerlink" href="#grad-accumulation-step" title="Permalink to this headline"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">grad_accumulation_step</span></code> means the steps where the gradients are accumulated.</p>
<p>The following is a code example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">grad_accumulation_step</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">context</span><span class="o">.</span><span class="n">get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;grad_accumulation_step&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="distributed-communication-interface">
<h2>Distributed Communication Interface<a class="headerlink" href="#distributed-communication-interface" title="Permalink to this headline"></a></h2>
<p><code class="docutils literal notranslate"><span class="pre">mindspore.communication</span></code> encapsulates a collection of communication interfaces used by parallel distributed training, facilitating users to configure distributed information.</p>
<section id="init">
<h3>init<a class="headerlink" href="#init" title="Permalink to this headline"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">init</span></code> enables MindSpore communication and initializes distributed training. <code class="docutils literal notranslate"><span class="pre">init</span></code> must be invoked after <code class="docutils literal notranslate"><span class="pre">context.set_context</span></code>. You can transfer the communication backend information to the <code class="docutils literal notranslate"><span class="pre">init</span></code>. The <code class="docutils literal notranslate"><span class="pre">init</span></code> performs initialization based on the backend information.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">hccl</span></code>: short for <code class="docutils literal notranslate"><span class="pre">Huawei</span> <span class="pre">Collective</span> <span class="pre">Communication</span> <span class="pre">Library</span></code>, used for the <code class="docutils literal notranslate"><span class="pre">Ascend</span></code> processor platform.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">nccl</span></code>: short for <code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">Collective</span> <span class="pre">Communication</span> <span class="pre">Library</span></code>, used for the <code class="docutils literal notranslate"><span class="pre">GPU</span></code> processor platform.</p></li>
</ul>
<p>If you do not configure the communication backend, MindSpore automatically configures it based on the <code class="docutils literal notranslate"><span class="pre">device_target</span></code> information in <code class="docutils literal notranslate"><span class="pre">context</span></code>.</p>
<p>The following is a code example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s1">&#39;GPU&#39;</span><span class="p">)</span>
<span class="n">init</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="get-group-size">
<h3>get_group_size<a class="headerlink" href="#get-group-size" title="Permalink to this headline"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">get_group_size</span></code> allows users to obtain the number of clusters. Invoke <code class="docutils literal notranslate"><span class="pre">init</span></code> before using the <code class="docutils literal notranslate"><span class="pre">get_group_size</span></code> interface.</p>
<p>The following is a code example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span><span class="p">,</span> <span class="n">get_group_size</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s1">&#39;GPU&#39;</span><span class="p">)</span>
<span class="n">init</span><span class="p">()</span>
<span class="n">group_size</span> <span class="o">=</span> <span class="n">get_group_size</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="get-rank">
<h3>get_rank<a class="headerlink" href="#get-rank" title="Permalink to this headline"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">get_rank</span></code> allows users to obtain the ID of the current device in the cluster. Invoke <code class="docutils literal notranslate"><span class="pre">init</span></code> before using the <code class="docutils literal notranslate"><span class="pre">get_rank</span></code> interface.</p>
<p>The following is a code example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span><span class="p">,</span> <span class="n">get_rank</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s1">&#39;GPU&#39;</span><span class="p">)</span>
<span class="n">init</span><span class="p">()</span>
<span class="n">rank_id</span> <span class="o">=</span> <span class="n">get_rank</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<section id="distributed-attribute-configuration">
<h2>Distributed Attribute Configuration<a class="headerlink" href="#distributed-attribute-configuration" title="Permalink to this headline"></a></h2>
<section id="cross-batch">
<h3>cross_batch<a class="headerlink" href="#cross-batch" title="Permalink to this headline"></a></h3>
<p>In specific scenarios, the calculation logic of <code class="docutils literal notranslate"><span class="pre">data_parallel</span></code> is different from that of <code class="docutils literal notranslate"><span class="pre">stand_alone</span></code>. The calculation logic of <code class="docutils literal notranslate"><span class="pre">auto_parallel</span></code> is the same as that of <code class="docutils literal notranslate"><span class="pre">stand_alone</span></code> in any scenario. The convergence effect of <code class="docutils literal notranslate"><span class="pre">data_parallel</span></code> may be better. Therefore, MindSpore provides the <code class="docutils literal notranslate"><span class="pre">cross_batch</span></code> parameter to ensure that the calculation logic of <code class="docutils literal notranslate"><span class="pre">auto_parallel</span></code> is consistent with that of <code class="docutils literal notranslate"><span class="pre">data_parallel</span></code>. You can use the <code class="docutils literal notranslate"><span class="pre">add_prim_attr</span></code> method to configure the logic. The default value is False.</p>
<p>The following is a code example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>

<span class="n">mul</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;cross_batch&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="fusion">
<h3>fusion<a class="headerlink" href="#fusion" title="Permalink to this headline"></a></h3>
<p>To ensure performance, MindSpore provides the fusion function for the <code class="docutils literal notranslate"><span class="pre">AllGather</span></code> and <code class="docutils literal notranslate"><span class="pre">AllReduce</span></code> operators. Operators of the same type (of the same operator type and in the same communication domain) with the same <code class="docutils literal notranslate"><span class="pre">fusion</span></code> value will be fused together. The value of <code class="docutils literal notranslate"><span class="pre">fusion</span></code> must be greater than or equal to 0. When the value of <code class="docutils literal notranslate"><span class="pre">fusion</span></code> is 0, operators will not be fused together. Only <code class="docutils literal notranslate"><span class="pre">Ascend</span></code> backend is supported.</p>
<p>There are two ways for configuration. If the communication operators are called explicitly, <code class="docutils literal notranslate"><span class="pre">add_prim_attr</span></code> could be used to configure. The following is a code example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>

<span class="n">allreduce1</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">AllReduce</span><span class="p">()</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;fusion&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">allreduce2</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">AllReduce</span><span class="p">()</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;fusion&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">allreduce1</span></code> and <code class="docutils literal notranslate"><span class="pre">allreduce2</span></code> will be fused into one operator during execution.</p>
<p>In <code class="docutils literal notranslate"><span class="pre">AUTO_PARALLEL</span></code> and <code class="docutils literal notranslate"><span class="pre">SEMI_AUTO_PARALLEL</span></code> mode, some communication operators used for parameters or gradients aggregation are inserted automatically. So the attribute should be added on a <code class="docutils literal notranslate"><span class="pre">Cell</span></code> or a <code class="docutils literal notranslate"><span class="pre">Parameter</span></code>. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Net definition&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p1</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">48</span><span class="p">,</span> <span class="mi">64</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;weight1&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p1</span><span class="o">.</span><span class="n">comm_fusion</span> <span class="o">=</span> <span class="mi">2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p2</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">64</span><span class="p">,</span> <span class="mi">16</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;weight2&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">p1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">p2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">-</span> <span class="n">y</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">)</span>
<span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="s2">&quot;auto_parallel&quot;</span><span class="p">,</span> <span class="n">device_num</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span><span class="o">.</span><span class="n">set_comm_fusion</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>Here the <code class="docutils literal notranslate"><span class="pre">comm_fusion</span></code> of parameter <code class="docutils literal notranslate"><span class="pre">Net.p1</span></code> is 2, which means the attribute <code class="docutils literal notranslate"><span class="pre">fusion</span></code> is 2 for the communication operators generated for this parameter. When you need to manipulate the parameters in batches, it is recommended to call <code class="docutils literal notranslate"><span class="pre">set_comm_fusion</span></code> to set <code class="docutils literal notranslate"><span class="pre">comm_fusion</span></code> for all the parameters in the Net. The value of attribute will be overwritten when the function is called multiply.</p>
<blockquote>
<div><p>When a parameter is shared, the operators connected with the parameter should have the same data type. Otherwise, fusion would failed.</p>
</div></blockquote>
</section>
<section id="layerwise-parallel">
<h3>layerwise_parallel<a class="headerlink" href="#layerwise-parallel" title="Permalink to this headline"></a></h3>
<p>In <code class="docutils literal notranslate"><span class="pre">HYBRID_PARALLEL</span></code> mode, you need to manually split the model. You need to manually add the <code class="docutils literal notranslate"><span class="pre">layerwise_parallel</span></code> flag to the parallel parameters of the model. The framework filters out the gradient aggregation operation for the parallel parameters of the model based on the flag.</p>
<p>The following is a code example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Parameter</span><span class="p">,</span> <span class="n">Tensor</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])),</span> <span class="n">layerwise_parallel</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="distributed_training.html" class="btn btn-neutral float-left" title="Distributed Parallel Overview" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="distributed_training_mode.html" class="btn btn-neutral float-right" title="Distributed Parallel Training Mode" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>