<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Custom Operators (Ascend) &mdash; MindSpore master documentation</title><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Custom Operators (GPU)" href="custom_operator_gpu.html" />
    <link rel="prev" title="Custom Operator" href="custom_operator.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="architecture.html">Overall Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_structure.html">MindSpore API Overview</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="design/technical_white_paper.html">Technical White Paper</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/gradient.html">MindSpore Automatic Differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/distributed_training_design.html">Distributed Training Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/mindir.html">MindSpore IR (MindIR)</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.5/training_visual_design.html">Design of Visualization↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quickstart</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/tutorials/en/r1.5/linear_regression.html">Implementing Simple Linear Function Fitting↗</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/tutorials/en/r1.5/quick_start.html">Implementing an Image Classification Application↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Basic Concepts</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="dtype.html">DataType</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor.html">Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter_introduction.html">Parameter</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="operators.html">Operators</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="operators_usage.html">Operators Usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators_classification.html">Operators Classification</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="custom_operator.html">Custom Operator</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Custom Operators (Ascend)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="#registering-the-operator-primitive">Registering the Operator Primitive</a></li>
<li class="toctree-l4"><a class="reference internal" href="#implementing-a-tbe-operator-and-registering-the-operator-information">Implementing a TBE Operator and Registering the Operator Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="#using-custom-operators">Using Custom Operators</a></li>
<li class="toctree-l4"><a class="reference internal" href="#defining-the-bprop-function-for-an-operator">Defining the bprop Function for an Operator</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="custom_operator_gpu.html">Custom Operators (GPU)</a></li>
<li class="toctree-l3"><a class="reference internal" href="custom_operator_cpu.html">Custom Operators (CPU)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="cell.html">Cell</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset_introduction.html">Dataset</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Pipeline</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dataset_sample.html">Quick Start of Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset.html">Loading Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline.html">Processing Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset_advanced.html">Advanced Usage of Pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset_usage.html">Data Iteration</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build the Network</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="build_net.html">Constructing Single Operator Network and Multi-layer Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="initializer.html">Initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter.html">Network Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="control_flow.html">Using the Process Control Statement</a></li>
<li class="toctree-l1"><a class="reference internal" href="loss.html">Loss Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="grad_operation.html">Gradient Operation</a></li>
<li class="toctree-l1"><a class="reference internal" href="indefinite_parameter.html">Parameter Passing</a></li>
<li class="toctree-l1"><a class="reference internal" href="constexpr.html">Construct Constants In the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="hypermap.html">Operation Overloading</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">Optimization Algorithms</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Running</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="context.html">Configuring Running Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="run.html">Running Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_and_load_models.html">Save and Load Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="model.html">Application of Model</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="online_inference.html">Online Inference with Checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="offline_inference.html">Using Offline Model for Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Distributed Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="distributed_training.html">Distributed Parallel Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_advanced.html">Distributed Parallel Advanced Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_example.html">Distributed Parallel Usage Example</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">PyNative</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="debug_in_pynative_mode.html">Debugging in PyNative Mode</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Numpy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="numpy.html">NumPy Interfaces in MindSpore</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Features</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="second_order_optimizer.html">Second Order Optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_quantization_aware_training.html">Applying Quantization Aware Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Function Debugging</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="read_ir_files.html">Reading IR</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/en/r1.5/debug_in_pynative_mode.html">Debugging in PyNative Mode↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="dump_in_graph_mode.html">Using Dump in the Graph Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_debugging_info.html">Custom Debugging Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="incremental_operator_build.html">Incremental Operator Build</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="enable_mixed_precision.html">Enabling Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_graph_kernel_fusion.html">Enabling Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_auto_tune.html">Enabling AutoTune</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_gradient_accumulation.html">Applying a Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.5/performance_profiling.html">Debugging performance with Profiler↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cv.html">Computer Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp.html">Natural Language Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="hpc.html">High Performance Computing</a></li>
<li class="toctree-l1"><a class="reference internal" href="use_on_the_cloud.html">Using MindSpore on the Cloud</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="operators.html">Operators</a> &raquo;</li>
          <li><a href="custom_operator.html">Custom Operator</a> &raquo;</li>
      <li>Custom Operators (Ascend)</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/custom_operator_ascend.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="custom-operators-ascend">
<h1>Custom Operators (Ascend)<a class="headerlink" href="#custom-operators-ascend" title="Permalink to this headline"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">Model</span> <span class="pre">Development</span></code></p>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.5/docs/mindspore/programming_guide/source_en/custom_operator_ascend.md" target="_blank"><img src="https://gitee.com/mindspore/docs/raw/r1.5/resource/_static/logo_source_en.png"></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>When built-in operators cannot meet requirements during network development, you can call the Python API of MindSpore to quickly extend custom operators of the Ascend AI processor.</p>
<p>To add a custom operator, you need to register the operator primitive, implement the operator, and register the operator information.</p>
<p>The related concepts are as follows:</p>
<ul class="simple">
<li><p>Operator primitive: defines the frontend API prototype of an operator on the network. It is the basic unit for forming a network model and includes the operator name, attribute (optional), input and output names, output shape inference method, and output dtype inference method.</p></li>
<li><p>Operator implementation: describes the implementation of the internal computation logic for an operator through the DSL API provided by the Tensor Boost Engine (TBE). The TBE supports the development of custom operators based on the Ascend AI chip.</p></li>
<li><p>Operator information: describes basic information about a TBE operator, such as the operator name and supported input and output types. It is the basis for the backend to select and map operators.</p></li>
</ul>
<p>This section takes a Square operator as an example to describe how to customize an operator.</p>
<blockquote>
<div><p>For details, see cases in <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.5/tests/st/ops/custom_ops_tbe">tests/st/ops/custom_ops_tbe</a> in the MindSpore source code.</p>
</div></blockquote>
</section>
<section id="registering-the-operator-primitive">
<h2>Registering the Operator Primitive<a class="headerlink" href="#registering-the-operator-primitive" title="Permalink to this headline"></a></h2>
<p>The primitive of an operator is a subclass inherited from <code class="docutils literal notranslate"><span class="pre">PrimitiveWithInfer</span></code>. The type name of the subclass is the operator name.</p>
<p>The definition of the custom operator primitive is the same as that of the built-in operator primitive.</p>
<ul class="simple">
<li><p>The attribute is defined by the input parameter of the constructor function <code class="docutils literal notranslate"><span class="pre">__init__</span></code>. The operator in this test case has no attribute. Therefore, <code class="docutils literal notranslate"><span class="pre">__init__</span></code> has only one input parameter. For details about test cases in which operators have attributes, see <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.5/tests/st/ops/custom_ops_tbe/cus_add3.py">custom add3</a> in the MindSpore source code.</p></li>
<li><p>The input and output names are defined by the <code class="docutils literal notranslate"><span class="pre">init_prim_io_names</span></code> function.</p></li>
<li><p>The shape inference method of the output tensor is defined in the <code class="docutils literal notranslate"><span class="pre">infer_shape</span></code> function, and the dtype inference method of the output tensor is defined in the <code class="docutils literal notranslate"><span class="pre">infer_dtype</span></code> function.</p></li>
</ul>
<p>The only difference between a custom operator and a built-in operator is that the operator implementation function (<code class="docutils literal notranslate"><span class="pre">from</span> <span class="pre">square_impl</span> <span class="pre">import</span> <span class="pre">CusSquareImpl</span></code>) needs to be imported to the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> function to register the operator implementation with the backend for the custom operator. In this test case, the operator implementation and information are defined in <code class="docutils literal notranslate"><span class="pre">square_impl.py</span></code>, and the definition will be described in the following parts.</p>
<p>The following code takes the Square operator primitive <code class="docutils literal notranslate"><span class="pre">cus_square.py</span></code> as an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">prim_attr_register</span><span class="p">,</span> <span class="n">PrimitiveWithInfer</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="c1"># y = x^2</span>
<span class="k">class</span> <span class="nc">CusSquare</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The definition of the CusSquare primitive.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="kn">from</span> <span class="nn">square_impl</span> <span class="kn">import</span> <span class="n">CusSquareImpl</span> <span class="c1"># Import the entry function of the kernel implementation from relative path or PYTHONPATH.</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">data_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_dtype</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">data_dtype</span>
</pre></div>
</div>
</section>
<section id="implementing-a-tbe-operator-and-registering-the-operator-information">
<h2>Implementing a TBE Operator and Registering the Operator Information<a class="headerlink" href="#implementing-a-tbe-operator-and-registering-the-operator-information" title="Permalink to this headline"></a></h2>
<section id="implementing-a-tbe-operator">
<h3>Implementing a TBE Operator<a class="headerlink" href="#implementing-a-tbe-operator" title="Permalink to this headline"></a></h3>
<p>To compile an operator implementation, you need to compile a computable function and an entry function first.</p>
<p>The computable function of an operator is mainly used to encapsulate the computation logic of the operator for the main function to call. The computation logic is implemented by calling the combined API of the TBE.</p>
<p>The entry function of an operator describes the internal process of compiling the operator. The process is as follows:</p>
<ol class="arabic simple">
<li><p>Prepare placeholders to be input. A placeholder will return a tensor object that represents a group of input data.</p></li>
<li><p>Call the computable function. The computable function uses the API provided by the TBE to describe the computation logic of the operator.</p></li>
<li><p>Call the scheduling module. The model tiles the operator data based on the scheduling description and specifies the data transfer process to ensure optimal hardware execution. By default, the automatic scheduling module (<code class="docutils literal notranslate"><span class="pre">auto_schedule</span></code>) can be used.</p></li>
<li><p>Call <code class="docutils literal notranslate"><span class="pre">cce_build_code</span></code> to compile and generate an operator binary file.</p></li>
</ol>
<blockquote>
<div><p>The input parameters of the entry function require the input information of each operator, output information of each operator, operator attributes (optional), and <code class="docutils literal notranslate"><span class="pre">kernel_name</span></code> (name of the generated operator binary file). The input and output information is encapsulated in dictionaries, including the input and output shape and dtype when the operator is called on the network.</p>
</div></blockquote>
<p>For details about TBE operator development, visit the <a class="reference external" href="https://support.huaweicloud.com/odevg-A800_3000_3010/atlaste_10_0063.html">TBE website</a>. For details about how to debug and optimize the TBE operator, visit the <a class="reference external" href="https://support.huaweicloud.com/usermanual-mindstudioc73/atlasmindstudio_02_0043.html">Mind Studio website</a>.</p>
</section>
<section id="registering-the-operator-information">
<h3>Registering the Operator Information<a class="headerlink" href="#registering-the-operator-information" title="Permalink to this headline"></a></h3>
<p>The operator information is key for the backend to select the operator implementation and guides the backend to insert appropriate type and format conversion operators. It uses the <code class="docutils literal notranslate"><span class="pre">TBERegOp</span></code> API for definition and uses the <code class="docutils literal notranslate"><span class="pre">op_info_register</span></code> decorator to bind the operator information to the entry function of the operator implementation. When the .py operator implementation file is imported, the <code class="docutils literal notranslate"><span class="pre">op_info_register</span></code> decorator registers the operator information to the operator information library at the backend. For details about how to use the operator information, see comments for the member method of <code class="docutils literal notranslate"><span class="pre">TBERegOp</span></code>.</p>
<blockquote>
<div><p>The numbers and sequences of the input and output information defined in the operator information must be the same as those in the parameters of the entry function of the operator implementation and those listed in the operator primitive.</p>
<p>If an operator has attributes, use <code class="docutils literal notranslate"><span class="pre">attr</span></code> to describe the attribute information in the operator information. The attribute names must be the same as those in the operator primitive definition.</p>
</div></blockquote>
</section>
<section id="example">
<h3>Example<a class="headerlink" href="#example" title="Permalink to this headline"></a></h3>
<p>The following takes the TBE implementation <code class="docutils literal notranslate"><span class="pre">square_impl.py</span></code> of the <code class="docutils literal notranslate"><span class="pre">Square</span></code> operator as an example. <code class="docutils literal notranslate"><span class="pre">square_compute</span></code> is a computable function of the operator implementation. It describes the computation logic of <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">*</span> <span class="pre">x</span></code> by calling the API provided by <code class="docutils literal notranslate"><span class="pre">te.lang.cce</span></code>. <code class="docutils literal notranslate"><span class="pre">cus_square_op_info</span></code> is the operator information, which is defined by <code class="docutils literal notranslate"><span class="pre">TBERegOp</span></code>. For the specific field meaning of the operator information, visit the <a class="reference external" href="https://support.huaweicloud.com/odevg-A800_3000_3010/atlaste_10_0096.html">TBE website</a>.</p>
<p>Note the following parameters when setting <code class="docutils literal notranslate"><span class="pre">TBERegOp</span></code>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">OPAQUE</span></code> in <code class="docutils literal notranslate"><span class="pre">fusion_type(&quot;OPAQUE&quot;)</span></code> indicates that the custom operator uses the non-fusion strategy.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">CusSquareImpl</span></code> in <code class="docutils literal notranslate"><span class="pre">kernel_name(&quot;CusSquareImpl&quot;)</span></code> must be the same as the name of the operator entry function.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dtype_format</span></code> is used to describe data types supported by the operator. In the following example, two types are registered, indicating that the operator supports two data types. Each type describes the supported format in order of input and output. The first <code class="docutils literal notranslate"><span class="pre">dtype_format</span></code> indicates that the data type input0 is in F32_Default format and the data type output0 is in F32_Default format. The second <code class="docutils literal notranslate"><span class="pre">dtype_format</span></code> indicates that the data type input0 is in F16_Default format and the data type output0 is in F16_Default format.</p></li>
<li><p>About the interfaces <code class="docutils literal notranslate"><span class="pre">auto_schedule</span></code> and <code class="docutils literal notranslate"><span class="pre">cce_build_code</span></code>, please see the TBE documents <a class="reference external" href="https://support.huaweicloud.com/odevg-A800_3000_3010/atlaste_07_0071.html">auto_schedule</a> and <a class="reference external" href="https://support.huaweicloud.com/odevg-A800_3000_3010/atlaste_07_0072.html">cce_build_code</a> for details.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">te</span> <span class="kn">import</span> <span class="n">tvm</span>
<span class="kn">from</span> <span class="nn">topi</span> <span class="kn">import</span> <span class="n">generic</span>
<span class="kn">import</span> <span class="nn">te.lang.cce</span>
<span class="kn">from</span> <span class="nn">topi.cce</span> <span class="kn">import</span> <span class="n">util</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">op_info_register</span><span class="p">,</span> <span class="n">TBERegOp</span><span class="p">,</span> <span class="n">DataType</span>

<span class="k">def</span> <span class="nf">square_compute</span><span class="p">(</span><span class="n">input_x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The compute function of the CusSquare implementation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">lang</span><span class="o">.</span><span class="n">cce</span><span class="o">.</span><span class="n">vmul</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span>

<span class="c1"># Define the kernel info of CusSquare.</span>
<span class="n">cus_square_op_info</span> <span class="o">=</span> <span class="n">TBERegOp</span><span class="p">(</span><span class="s2">&quot;CusSquare&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">fusion_type</span><span class="p">(</span><span class="s2">&quot;OPAQUE&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">partial_flag</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">async_flag</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">binfile_name</span><span class="p">(</span><span class="s2">&quot;square.so&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">compute_cost</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">kernel_name</span><span class="p">(</span><span class="s2">&quot;CusSquareImpl&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">input</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;required&quot;</span><span class="p">,</span> <span class="s2">&quot;all&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;required&quot;</span><span class="p">,</span> <span class="s2">&quot;all&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">dtype_format</span><span class="p">(</span><span class="n">DataType</span><span class="o">.</span><span class="n">F32_Default</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">F32_Default</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">dtype_format</span><span class="p">(</span><span class="n">DataType</span><span class="o">.</span><span class="n">F16_Default</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">F16_Default</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">get_op_info</span><span class="p">()</span>

<span class="c1"># Binding kernel info with the kernel implementation.</span>
<span class="nd">@op_info_register</span><span class="p">(</span><span class="n">cus_square_op_info</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">CusSquareImpl</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">output_y</span><span class="p">,</span> <span class="n">kernel_name</span><span class="o">=</span><span class="s2">&quot;CusSquareImpl&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The entry function of the CusSquare implementation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">input_x</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;shape&quot;</span><span class="p">)</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">input_x</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;dtype&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>

    <span class="n">shape</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">shape_refine</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>

    <span class="k">with</span> <span class="n">tvm</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">cce</span><span class="p">():</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">square_compute</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">sch</span> <span class="o">=</span> <span class="n">generic</span><span class="o">.</span><span class="n">auto_schedule</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>

    <span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;print_ir&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
              <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="n">kernel_name</span><span class="p">,</span>
              <span class="s2">&quot;tensor_list&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">data</span><span class="p">,</span> <span class="n">res</span><span class="p">]}</span>

    <span class="n">te</span><span class="o">.</span><span class="n">lang</span><span class="o">.</span><span class="n">cce</span><span class="o">.</span><span class="n">cce_build_code</span><span class="p">(</span><span class="n">sch</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="using-custom-operators">
<h2>Using Custom Operators<a class="headerlink" href="#using-custom-operators" title="Permalink to this headline"></a></h2>
<p>The usage of custom operators is the same as that of built-in operators in the network. The operators can be directly used by importing primitives. The following takes the single-operator network test of <code class="docutils literal notranslate"><span class="pre">CusSquare</span></code> as an example.</p>
<p>Define the network in the <code class="docutils literal notranslate"><span class="pre">test_square.py</span></code> file.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore.context</span> <span class="k">as</span> <span class="nn">context</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="c1"># Import the definition of the CusSquare primitive.</span>
<span class="kn">from</span> <span class="nn">cus_square</span> <span class="kn">import</span> <span class="n">CusSquare</span>
<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;Ascend&quot;</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">square</span> <span class="o">=</span> <span class="n">CusSquare</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">test_net</span><span class="p">():</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">9.0</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">square</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">square</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;x: &quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;output: &quot;</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<p>Execute the test case.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest<span class="w"> </span>-s<span class="w"> </span>tests/st/ops/custom_ops_tbe/test_square.py::test_net
</pre></div>
</div>
<p>The execution result is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>x: [1. 4. 9.]
output: [1. 16. 81.]
</pre></div>
</div>
</section>
<section id="defining-the-bprop-function-for-an-operator">
<h2>Defining the bprop Function for an Operator<a class="headerlink" href="#defining-the-bprop-function-for-an-operator" title="Permalink to this headline"></a></h2>
<p>If an operator needs to support automatic differentiation, the bprop function needs to be defined in the primitive of the operator. In the bprop function, you need to describe the backward computation logic that uses the forward input, forward output, and output gradients to obtain the input gradients. The backward computation logic can be composed of built-in operators or custom backward operators.</p>
<p>Note the following points when defining the bprop function:</p>
<ul class="simple">
<li><p>The input parameter sequence of the bprop function is the forward input, forward output, and output gradients. For a multi-output operator, the forward output and output gradients are provided in the form of tuples.</p></li>
<li><p>The return value of the bprop function is tuples consisting of input gradients. The sequence of elements in a tuple is the same as that of the forward input parameters. Even if there is only one input gradient, the return value must be a tuple.</p></li>
</ul>
<p>For example, the <code class="docutils literal notranslate"><span class="pre">CusSquare</span></code> primitive after the bprop function is added is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">CusSquare</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;init CusSquare&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="kn">from</span> <span class="nn">square_impl</span> <span class="kn">import</span> <span class="n">CusSquareImpl</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">data_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_dtype</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">data_dtype</span>

    <span class="k">def</span> <span class="nf">get_bprop</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">bprop</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
            <span class="n">twos_like</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">OnesLike</span><span class="p">()(</span><span class="n">data</span><span class="p">)</span> <span class="o">*</span> <span class="mf">2.0</span>
            <span class="n">gradient</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Mul</span><span class="p">()(</span><span class="n">data</span><span class="p">,</span> <span class="n">twos_like</span><span class="p">)</span>
            <span class="n">dx</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Mul</span><span class="p">()(</span><span class="n">gradient</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">dx</span><span class="p">,)</span>
        <span class="k">return</span> <span class="n">bprop</span>
</pre></div>
</div>
<p>Define backward cases in the <code class="docutils literal notranslate"><span class="pre">test_square.py</span></code> file.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="k">def</span> <span class="nf">test_grad_net</span><span class="p">():</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">9.0</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">sens</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">square</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">GradOperation</span><span class="p">(</span><span class="n">sens_param</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">square</span><span class="p">)(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">sens</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;x: &quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;dx: &quot;</span><span class="p">,</span> <span class="n">dx</span><span class="p">)</span>
</pre></div>
</div>
<p>Execute the test case.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest<span class="w"> </span>-s<span class="w"> </span>tests/st/ops/custom_ops_tbe/test_square.py::test_grad_net
</pre></div>
</div>
<p>The execution result is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>x: [1. 4. 9.]
dx: [2. 8. 18.]
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="custom_operator.html" class="btn btn-neutral float-left" title="Custom Operator" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="custom_operator_gpu.html" class="btn btn-neutral float-right" title="Custom Operators (GPU)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>