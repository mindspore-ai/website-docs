<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Inference on the Ascend 310 AI Processor &mdash; MindSpore master documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Inference on a GPU" href="multi_platform_inference_gpu.html" />
    <link rel="prev" title="Inference Using the MindIR Model on Ascend 310 AI Processors" href="multi_platform_inference_ascend_310_mindir.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="architecture.html">Overall Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_structure.html">MindSpore API Overview</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="design/technical_white_paper.html">Technical White Paper</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/gradient.html">MindSpore Automatic Differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/distributed_training_design.html">Distributed Training Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/mindir.html">MindSpore IR (MindIR)</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.5/training_visual_design.html">Design of Visualization↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quickstart</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/tutorials/en/r1.5/linear_regression.html">Implementing Simple Linear Function Fitting↗</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/tutorials/en/r1.5/quick_start.html">Implementing an Image Classification Application↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Basic Concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dtype.html">DataType</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor.html">Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter_introduction.html">Parameter</a></li>
<li class="toctree-l1"><a class="reference internal" href="operators.html">Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="cell.html">Cell</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset_introduction.html">Dataset</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Pipeline</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dataset_sample.html">Quick Start of Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset.html">Loading Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline.html">Processing Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset_advanced.html">Advanced Usage of Pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset_usage.html">Data Iteration</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build the Network</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="build_net.html">Constructing Single Operator Network and Multi-layer Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="initializer.html">Initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter.html">Network Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="control_flow.html">Using the Process Control Statement</a></li>
<li class="toctree-l1"><a class="reference internal" href="loss.html">Loss Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="grad_operation.html">Gradient Operation</a></li>
<li class="toctree-l1"><a class="reference internal" href="indefinite_parameter.html">Parameter Passing</a></li>
<li class="toctree-l1"><a class="reference internal" href="constexpr.html">Construct Constants In the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="hypermap.html">Operation Overloading</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">Optimization Algorithms</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Running</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="context.html">Configuring Running Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="run.html">Running Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_and_load_models.html">Save and Load Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="model.html">Application of Model</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Inference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="online_inference.html">Online Inference with Checkpoint</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="offline_inference.html">Using Offline Model for Inference</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="multi_platform_inference_ascend_910.html">Inference on the Ascend 910 AI processor</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="multi_platform_inference_ascend_310.html">Inference on Ascend 310</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="multi_platform_inference_ascend_310_mindir.html">Inference Using the MindIR Model on Ascend 310 AI Processors</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Inference on the Ascend 310 AI Processor</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="#preparing-the-development-environment">Preparing the Development Environment</a></li>
<li class="toctree-l4"><a class="reference internal" href="#inference-directory-structure">Inference Directory Structure</a></li>
<li class="toctree-l4"><a class="reference internal" href="#exporting-the-air-model">Exporting the AIR Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="#converting-the-air-model-file-into-an-om-model">Converting the AIR Model File into an OM Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="#building-inference-code">Building Inference Code</a></li>
<li class="toctree-l4"><a class="reference internal" href="#performing-inference-and-viewing-the-result">Performing Inference and Viewing the Result</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="multi_platform_inference_gpu.html">Inference on a GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="multi_platform_inference_cpu.html">Inference on a CPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="apply_post_training_quantization.html">Applying Post Training Quantization</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Distributed Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="distributed_training.html">Distributed Parallel Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_advanced.html">Distributed Parallel Advanced Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_example.html">Distributed Parallel Usage Example</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">PyNative</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="debug_in_pynative_mode.html">Debugging in PyNative Mode</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Numpy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="numpy.html">NumPy Interfaces in MindSpore</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Features</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="second_order_optimizer.html">Second Order Optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_quantization_aware_training.html">Applying Quantization Aware Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Function Debugging</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="read_ir_files.html">Reading IR</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/en/r1.5/debug_in_pynative_mode.html">Debugging in PyNative Mode↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="dump_in_graph_mode.html">Using Dump in the Graph Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_debugging_info.html">Custom Debugging Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="incremental_operator_build.html">Incremental Operator Build</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="enable_mixed_precision.html">Enabling Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_graph_kernel_fusion.html">Enabling Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_auto_tune.html">Enabling AutoTune</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_gradient_accumulation.html">Applying a Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.5/performance_profiling.html">Debugging performance with Profiler↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cv.html">Computer Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp.html">Natural Language Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="hpc.html">High Performance Computing</a></li>
<li class="toctree-l1"><a class="reference internal" href="use_on_the_cloud.html">Using MindSpore on the Cloud</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="offline_inference.html">Using Offline Model for Inference</a> &raquo;</li>
          <li><a href="multi_platform_inference_ascend_310.html">Inference on Ascend 310</a> &raquo;</li>
      <li>Inference on the Ascend 310 AI Processor</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/multi_platform_inference_ascend_310_air.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section class="tex2jax_ignore mathjax_ignore" id="inference-on-the-ascend-310-ai-processor">
<h1>Inference on the Ascend 310 AI Processor<a class="headerlink" href="#inference-on-the-ascend-310-ai-processor" title="Permalink to this headline"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">Inference</span> <span class="pre">Application</span></code></p>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.5/docs/mindspore/programming_guide/source_en/multi_platform_inference_ascend_310_air.md" target="_blank"><img src="https://gitee.com/mindspore/docs/raw/r1.5/resource/_static/logo_source_en.png"></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>Ascend 310 is a highly efficient and integrated AI processor oriented to edge scenarios. The Atlas 200 Developer Kit (Atlas 200 DK) is a developer board that uses the Atlas 200 AI accelerator module. Integrated with the HiSilicon Ascend 310 AI processor, the Atlas 200 allows data analysis, inference, and computing for various data such as images and videos, and can be widely used in scenarios such as intelligent surveillance, robots, drones, and video servers.</p>
<p>This tutorial describes how to use MindSpore to perform inference on the Atlas 200 DK based on the AIR model file. The process is as follows:</p>
<ol class="arabic simple">
<li><p>Prepare the development environment, including creating an SD card for the Atlas 200 DK, configuring the Python environment, and updating the development software package.</p></li>
<li><p>Export the AIR model file. The ResNet-50 model is used as an example.</p></li>
<li><p>Use the ATC tool to convert the AIR model file into an OM model.</p></li>
<li><p>Build the inference code to generate an executable <code class="docutils literal notranslate"><span class="pre">main</span></code> file.</p></li>
<li><p>Load the saved OM model, perform inference, and view the result.</p></li>
</ol>
<blockquote>
<div><p>You can obtain the complete executable sample code at <a class="reference external" href="https://gitee.com/mindspore/docs/tree/r1.5/docs/sample_code/acl_resnet50_sample">https://gitee.com/mindspore/docs/tree/r1.5/docs/sample_code/acl_resnet50_sample</a>.</p>
</div></blockquote>
</section>
<section id="preparing-the-development-environment">
<h2>Preparing the Development Environment<a class="headerlink" href="#preparing-the-development-environment" title="Permalink to this headline"></a></h2>
<section id="hardware-preparation">
<h3>Hardware Preparation<a class="headerlink" href="#hardware-preparation" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>A server or PC with the Ubuntu OS is used to prepare a bootable SD card for the Atlas 200 DK and deploy the development environment.</p></li>
<li><p>An SD card with a capacity of at least 16 GB.</p></li>
</ul>
</section>
<section id="software-package-preparation">
<h3>Software Package Preparation<a class="headerlink" href="#software-package-preparation" title="Permalink to this headline"></a></h3>
<p>The following five types of scripts and software packages are required for configuring the development environment:</p>
<ol class="arabic simple">
<li><p>Entry script for SD card preparation: <a class="reference external" href="https://gitee.com/ascend/tools/blob/master/makesd/for_1.0.9.alpha/make_sd_card.py">make_sd_card.py</a></p></li>
<li><p>Script for preparing a bootable SD card: <a class="reference external" href="https://gitee.com/ascend/tools/blob/master/makesd/for_1.0.9.alpha/make_ubuntu_sd.sh">make_ubuntu_sd.sh</a></p></li>
<li><p>Ubuntu OS image package: <a class="reference external" href="http://cdimage.ubuntu.com/ubuntu/releases/18.04/release/ubuntu-18.04.6-server-arm64.iso">ubuntu-18.04.xx-server-arm64.iso</a></p></li>
<li><p>Driver package and running package of Atlas 200 DK:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Ascend310-driver-*{software</span> <span class="pre">version}*-ubuntu18.04.aarch64-minirc.tar.gz</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Ascend310-aicpu_kernels-*{software</span> <span class="pre">version}*-minirc.tar.gz</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Ascend-acllib-*{software</span> <span class="pre">version}*-ubuntu18.04.aarch64-minirc.run</span></code></p></li>
</ul>
</li>
<li><p>Package for installing the development kit: <code class="docutils literal notranslate"><span class="pre">Ascend-Toolkit-*{version}*-arm64-linux_gcc7.3.0.run</span></code></p></li>
</ol>
<p>In the preceding information:</p>
<ul class="simple">
<li><p>For details about the first three items, see <a class="reference external" href="https://support.huaweicloud.com/intl/en-us//usermanual-A200dk_3000/atlas200dk_02_0011.html">Creating an SD Card with a Card Reader</a>.</p></li>
<li><p>You are advised to obtain other software packages from <a class="reference external" href="https://ascend.huawei.com/en/#/hardware/firmware-drivers">Firmware and Driver</a>. On this page, select <code class="docutils literal notranslate"><span class="pre">Atlas</span> <span class="pre">200</span> <span class="pre">DK</span></code> from the product series and product model and select the required files to download.</p></li>
</ul>
</section>
<section id="preparing-the-sd-card">
<h3>Preparing the SD Card<a class="headerlink" href="#preparing-the-sd-card" title="Permalink to this headline"></a></h3>
<p>A card reader is connected to the Ubuntu server through a USB port, and the SD card is prepared using the script for SD card preparation. For details, see <a class="reference external" href="https://support.huaweicloud.com/intl/en-us/usermanual-A200dk_3000/atlas200dk_02_0011.html#section2">Procedure</a>.</p>
</section>
<section id="connecting-the-atlas-200-dk-to-the-ubuntu-server">
<h3>Connecting the Atlas 200 DK to the Ubuntu Server<a class="headerlink" href="#connecting-the-atlas-200-dk-to-the-ubuntu-server" title="Permalink to this headline"></a></h3>
<p>The Atlas 200 DK can be connected to the Ubuntu server through a USB port or network cable. For details, see <a class="reference external" href="https://support.huaweicloud.com/intl/en-us/usermanual-A200dk_3000/atlas200dk_02_0013.html">Connecting the Atlas 200 DK to the Ubuntu Server</a>.</p>
</section>
<section id="configuring-the-python-environment">
<h3>Configuring the Python Environment<a class="headerlink" href="#configuring-the-python-environment" title="Permalink to this headline"></a></h3>
<p>Install Python and GCC software. For details, see <a class="reference external" href="https://support.huaweicloud.com/intl/en-us/usermanual-A200dk_3000/atlas200dk_02_0016.html#section4">Installing Dependencies</a>.</p>
</section>
<section id="installing-the-development-kit">
<h3>Installing the Development Kit<a class="headerlink" href="#installing-the-development-kit" title="Permalink to this headline"></a></h3>
<p>Install the development kit software package <code class="docutils literal notranslate"><span class="pre">Ascend-Toolkit-*{version}*-arm64-linux_gcc7.3.0.run</span></code>. For details, see <a class="reference external" href="https://support.huaweicloud.com/intl/en-us/usermanual-A200dk_3000/atlas200dk_02_0017.html">Installing the Development Kit</a>.</p>
</section>
</section>
<section id="inference-directory-structure">
<h2>Inference Directory Structure<a class="headerlink" href="#inference-directory-structure" title="Permalink to this headline"></a></h2>
<p>Create a directory to store the inference code project, for example, <code class="docutils literal notranslate"><span class="pre">/home/HwHiAiUser/Ascend/ascend-toolkit/20.0.RC1/acllib_linux.arm64/sample/acl_execute_model/acl_resnet50_sample</span></code>. The <code class="docutils literal notranslate"><span class="pre">inc</span></code>, <code class="docutils literal notranslate"><span class="pre">src</span></code>, and <code class="docutils literal notranslate"><span class="pre">test_data</span></code> <a class="reference external" href="https://gitee.com/mindspore/docs/tree/r1.5/docs/sample_code/acl_resnet50_sample">sample code</a> can be obtained from the official website, and the <code class="docutils literal notranslate"><span class="pre">model</span></code> directory stores the exported <code class="docutils literal notranslate"><span class="pre">AIR</span></code> model file and the converted <code class="docutils literal notranslate"><span class="pre">OM</span></code> model file. The <code class="docutils literal notranslate"><span class="pre">out</span></code> directory stores the executable file generated after building and the output result directory. The directory structure of the inference code project is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>└─acl_resnet50_sample
    ├── inc
    │   ├── model_process.h                   // Header file that declares functions related to resource initialization/destruction
    │   ├── sample_process.h                  // Header file that declares functions related to model processing
    │   ├── utils.h                           // Header file that declares common functions (such as the file reading function)
    ├── model
    │   ├── resnet50_export.air               // AIR model file
    │   ├── resnet50_export.om                // Converted OM model file
    ├── src
    │   ├── acl.json                          // Configuration file for system initialization
    │   ├── CMakeLists.txt                    // Build script
    │   ├── main.cpp                          // /Main function, which is the implementation file of image classification
    │   ├── model_process.cpp                 // Implementation file of model processing functions
    │   ├── sample_process.cpp                // Implementation file of functions related to resource initialization and destruction
    │   ├── utils.cpp                         // Implementation file of common functions (such as the file reading function)
    ├── test_data
    │   ├── test_data_1x3x224x224_1.bin       // Input sample data 1
    │   ├── test_data_1x3x224x224_2.bin       // input sample data 2
    ├── out
    │   ├── main                              // Executable file generated during building
    │   ├── result                            // Directory for storing the output result
</pre></div>
</div>
<blockquote>
<div><p>The output result directory <code class="docutils literal notranslate"><span class="pre">acl_resnet50_sample/out/result</span></code> must be created before inference.</p>
</div></blockquote>
</section>
<section id="exporting-the-air-model">
<h2>Exporting the AIR Model<a class="headerlink" href="#exporting-the-air-model" title="Permalink to this headline"></a></h2>
<p>Train the target network on the Ascend 910 AI Processor, save it as a checkpoint file, and export the model file in AIR format through the network and checkpoint file. For details about the export process, see <a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/en/r1.5/save_model.html#export-air-model">Export AIR Model</a>.</p>
<blockquote>
<div><p>The <a class="reference external" href="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com:443/sample_resources/acl_resnet50_sample/resnet50_export.air">resnet50_export.air</a> is a sample AIR file exported using the ResNet-50 model.</p>
</div></blockquote>
</section>
<section id="converting-the-air-model-file-into-an-om-model">
<h2>Converting the AIR Model File into an OM Model<a class="headerlink" href="#converting-the-air-model-file-into-an-om-model" title="Permalink to this headline"></a></h2>
<p>Log in to the Atlas 200 DK environment, create the <code class="docutils literal notranslate"><span class="pre">model</span></code> directory for storing the AIR file <code class="docutils literal notranslate"><span class="pre">resnet50_export.air</span></code>, for example, <code class="docutils literal notranslate"><span class="pre">/home/HwHiAiUser/Ascend/ascend-toolkit/20.0.RC1/acllib_linux.arm64/sample/acl_execute_model/acl_resnet50_sample/model</span></code>, go to the directory, and set the following environment variables where <code class="docutils literal notranslate"><span class="pre">install_path</span></code> specifies the actual installation path:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">install_path</span><span class="o">=</span>/home/HwHiAiUser/Ascend/ascend-toolkit/20.0.RC1
<span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span>/usr/local/python3.7.5/bin:<span class="si">${</span><span class="nv">install_path</span><span class="si">}</span>/atc/ccec_compiler/bin:<span class="si">${</span><span class="nv">install_path</span><span class="si">}</span>/atc/bin:<span class="nv">$PATH</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">PYTHONPATH</span><span class="o">=</span><span class="si">${</span><span class="nv">install_path</span><span class="si">}</span>/atc/python/site-packages/te:<span class="si">${</span><span class="nv">install_path</span><span class="si">}</span>/atc/python/site-packages/topi:<span class="nv">$PYTHONPATH</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">install_path</span><span class="si">}</span>/atc/lib64:<span class="nv">$LD_LIBRARY_PATH</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">ASCEND_OPP_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">install_path</span><span class="si">}</span>/opp
</pre></div>
</div>
<p>Take <code class="docutils literal notranslate"><span class="pre">resnet50_export.air</span></code> as an example. Run the following command to convert the model and generate the <code class="docutils literal notranslate"><span class="pre">resnet50_export.om</span></code> file in the current directory.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>/home/HwHiAiUser/Ascend/ascend-toolkit/20.0.RC1/atc/bin/atc<span class="w"> </span>--framework<span class="o">=</span><span class="m">1</span><span class="w"> </span>--model<span class="o">=</span>./resnet50_export.air<span class="w"> </span>--output<span class="o">=</span>./resnet50_export<span class="w"> </span>--input_format<span class="o">=</span>NCHW<span class="w"> </span>--soc_version<span class="o">=</span>Ascend310
</pre></div>
</div>
<p>In the preceding information:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--model</span></code>: path of the original model file</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--output</span></code>: path of the converted OM model file</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--input_format</span></code>: input image format</p></li>
</ul>
<p>For detailed information about ATC tools, please select the corresponding CANN version in the <a class="reference external" href="https://ascend.huawei.com/en/#/document?tag=developer">Developer Documentation(Community Edition)</a>, and then search for the chapter of “ATC Tool Instructions”.</p>
</section>
<section id="building-inference-code">
<h2>Building Inference Code<a class="headerlink" href="#building-inference-code" title="Permalink to this headline"></a></h2>
<p>Go to the project directory <code class="docutils literal notranslate"><span class="pre">acl_resnet50_sample</span></code> and set the following environment variables:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">DDK_PATH</span><span class="o">=</span>/home/HwHiAiUser/Ascend/ascend-toolkit/20.0.RC1
<span class="nb">export</span><span class="w"> </span><span class="nv">NPU_HOST_LIB</span><span class="o">=</span>/home/HwHiAiUser/Ascend/ascend-toolkit/20.0.RC1/acllib_linux.arm64/lib64/stub/
</pre></div>
</div>
<blockquote>
<div><p>The <code class="docutils literal notranslate"><span class="pre">include</span></code> directory of the <code class="docutils literal notranslate"><span class="pre">acllib</span></code> package in the <code class="docutils literal notranslate"><span class="pre">CMakeLists.txt</span></code> file must be correctly specified. Otherwise, an error indicating that <code class="docutils literal notranslate"><span class="pre">acl/acl.h</span></code> cannot be found is reported. The code location of the <code class="docutils literal notranslate"><span class="pre">include</span></code> directory is as follows. If the location is inconsistent with the actual installation directory, modify it.</p>
</div></blockquote>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>...
#Header path

 include_directories(

  ${INC_PATH}/acllib_linux.arm64/include/

  ../

 )
...
</pre></div>
</div>
<p>Run the following command to create a build directory:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>-p<span class="w"> </span>build/intermediates/minirc
</pre></div>
</div>
<p>Run the following command to switch to the build directory:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>build/intermediates/minirc
</pre></div>
</div>
<p>Run the <code class="docutils literal notranslate"><span class="pre">cmake</span></code> command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cmake<span class="w"> </span>../../../src<span class="w"> </span>-DCMAKE_CXX_COMPILER<span class="o">=</span>aarch64-linux-gnu-g++<span class="w"> </span>-DCMAKE_SKIP_RPATH<span class="o">=</span>TRUE
</pre></div>
</div>
<p>Run the <code class="docutils literal notranslate"><span class="pre">make</span></code> command for building:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>make
</pre></div>
</div>
<p>After building, the executable <code class="docutils literal notranslate"><span class="pre">main</span></code> file is generated in <code class="docutils literal notranslate"><span class="pre">acl_resnet50_sample/out</span></code>.</p>
</section>
<section id="performing-inference-and-viewing-the-result">
<h2>Performing Inference and Viewing the Result<a class="headerlink" href="#performing-inference-and-viewing-the-result" title="Permalink to this headline"></a></h2>
<p>Copy the generated OM model file <code class="docutils literal notranslate"><span class="pre">resnet50_export.om</span></code> to the <code class="docutils literal notranslate"><span class="pre">acl_resnet50_sample/out</span></code> directory (the same path as the executable <code class="docutils literal notranslate"><span class="pre">main</span></code> file) and ensure that the input data sample is ready in the <code class="docutils literal notranslate"><span class="pre">acl_resnet50_sample/test_data</span></code> directory. Then, you can perform inference.</p>
<p>Note that the following environment variables must be set. Otherwise, the inference fails.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>/home/HwHiAiUser/Ascend/acllib/lib64/
</pre></div>
</div>
<p>Go to the <code class="docutils literal notranslate"><span class="pre">acl_resnet50_sample/out</span></code> directory. If the <code class="docutils literal notranslate"><span class="pre">result</span></code> directory does not exist in the current directory, run the <code class="docutils literal notranslate"><span class="pre">mkdir</span> <span class="pre">result</span></code> command to create one and run the following command to perform inference:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./main<span class="w">  </span>./resnet50_export.om<span class="w">  </span>../test_data
</pre></div>
</div>
<p>After the execution is successful, the following inference result is displayed. The <code class="docutils literal notranslate"><span class="pre">top5</span></code> probability label is displayed, and the output result is saved in the <code class="docutils literal notranslate"><span class="pre">.bin</span></code> file format in the <code class="docutils literal notranslate"><span class="pre">acl_resnet50_sample/out/result</span></code> directory.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[INFO]  acl init success
[INFO]  open device 0 success
[INFO]  create context success
[INFO]  create stream success
[INFO]  get run mode success
[INFO]  load model ./resnet50_export.om success
[INFO]  create model description success
[INFO]  create model output success
[INFO]  start to process file:../test_data/test_data_1x3x224x224_1.bin
[INFO]  model execute success
[INFO]  top 1: index[2] value[0.941406]
[INFO]  top 2: index[3] value[0.291992]
[INFO]  top 3: index[1] value[0.067139]
[INFO]  top 4: index[0] value[0.013519]
[INFO]  top 5: index[4] value[-0.226685]
[INFO]  output data success
[INFO]  dump data success
[INFO]  start to process file:../test_data/test_data_1x3x224x224_2.bin
[INFO]  model execute success
[INFO]  top 1: index[2] value[0.946289]
[INFO]  top 2: index[3] value[0.296143]
[INFO]  top 3: index[1] value[0.072083]
[INFO]  top 4: index[0] value[0.014549]
[INFO]  top 5: index[4] value[-0.225098]
[INFO]  output data success
[INFO]  dump data success
[INFO]  unload model success, modelId is 1
[INFO]  execute sample success
[INFO]  end to destroy stream
[INFO]  end to destroy context
[INFO]  end to reset device is 0
[INFO]  end to finalize acl
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="multi_platform_inference_ascend_310_mindir.html" class="btn btn-neutral float-left" title="Inference Using the MindIR Model on Ascend 310 AI Processors" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="multi_platform_inference_gpu.html" class="btn btn-neutral float-right" title="Inference on a GPU" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>