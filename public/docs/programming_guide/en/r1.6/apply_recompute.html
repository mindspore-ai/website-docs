<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Recomputation &mdash; MindSpore master documentation</title>
      <link rel="stylesheet" href="_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/training.css" type="text/css" /><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/js/training.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Distributed Parallel Usage Example" href="distributed_example.html" />
    <link rel="prev" title="Distributed Inference" href="distributed_inference.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="architecture.html">Overall Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_structure.html">MindSpore API Overview</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="design/technical_white_paper.html">Technical White Paper</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/gradient.html">MindSpore Automatic Differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/distributed_training_design.html">Distributed Training Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/mindir.html">MindSpore IR (MindIR)</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.6/training_visual_design.html">Design of Visualization↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quickstart</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/tutorials/en/r1.6/linear_regression.html">Implementing Simple Linear Function Fitting↗</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/tutorials/en/r1.6/quick_start.html">Implementing an Image Classification Application↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Basic Concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dtype.html">DataType</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor.html">Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter_introduction.html">Parameter</a></li>
<li class="toctree-l1"><a class="reference internal" href="operators.html">Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="cell.html">Cell</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset_introduction.html">Dataset</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Pipeline</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dataset_sample.html">Quick Start of Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset.html">Loading Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline.html">Processing Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset_advanced.html">Advanced Usage of Pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset_usage.html">Data Iteration</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build the Network</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="build_net.html">Constructing Single Operator Network and Multi-layer Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="initializer.html">Initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter.html">Network Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="control_flow.html">Using the Process Control Statement</a></li>
<li class="toctree-l1"><a class="reference internal" href="indefinite_parameter.html">Parameter Passing</a></li>
<li class="toctree-l1"><a class="reference internal" href="loss.html">Loss Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="grad_operation.html">Gradient Operation</a></li>
<li class="toctree-l1"><a class="reference internal" href="constexpr.html">Construct Constants In the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="hypermap.html">Operation Overloading</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">Optimization Algorithms</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Running</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="context.html">Configuring Running Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="run.html">Running Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_and_load_models.html">Save and Load Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="model.html">Application of Model</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="online_inference.html">Online Inference with Checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="offline_inference.html">Using Offline Model for Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Distributed Training</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="distributed_training.html">Distributed Parallel Overview</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="distributed_advanced.html">Distributed Parallel Advanced Features</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="apply_pipeline_parallel.html">Pipeline Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="apply_host_device_training.html">Host&amp;Device Heterogeneous</a></li>
<li class="toctree-l2"><a class="reference internal" href="apply_parameter_server_training.html">Parameter Server Mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed_inference.html">Distributed Inference</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Recomputation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#preliminaries">Preliminaries</a></li>
<li class="toctree-l3"><a class="reference internal" href="#configuring-for-recomputation">Configuring for Recomputation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#training-the-model">Training the Model</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="distributed_example.html">Distributed Parallel Usage Example</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">PyNative</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="debug_in_pynative_mode.html">Debugging in PyNative Mode</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Numpy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="numpy.html">NumPy Interfaces in MindSpore</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Function Debugging</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="read_ir_files.html">Reading IR</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/en/r1.6/debug_in_pynative_mode.html">Debugging in PyNative Mode↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="dump_in_graph_mode.html">Using Dump in the Graph Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_debugging_info.html">Custom Debugging Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="incremental_operator_build.html">Incremental Operator Build</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="enable_mixed_precision.html">Enabling Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_auto_tune.html">AutoTune</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_dataset_autotune.html">Dataset AutoTune for Dataset Pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_dataset_offload.html">Enabling Offload for Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.6/performance_profiling.html">Debugging performance with Profiler↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Features</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="second_order_optimizer.html">Second Order Optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="graph_kernel_fusion.html">Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_quantization_aware_training.html">Quantization Aware Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cv.html">Computer Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp.html">Natural Language Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="hpc.html">High Performance Computing</a></li>
<li class="toctree-l1"><a class="reference internal" href="use_on_the_cloud.html">Using MindSpore on the Cloud</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="distributed_advanced.html">Distributed Parallel Advanced Features</a> &raquo;</li>
      <li>Recomputation</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/apply_recompute.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="recomputation">
<h1>Recomputation<a class="headerlink" href="#recomputation" title="Permalink to this headline"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code> <code class="docutils literal notranslate"><span class="pre">Model</span> <span class="pre">Running</span></code> <code class="docutils literal notranslate"><span class="pre">Intermediate</span></code> <code class="docutils literal notranslate"><span class="pre">Expert</span></code></p>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.6/docs/mindspore/programming_guide/source_en/apply_recompute.md" target="_blank"><img src="https://gitee.com/mindspore/docs/raw/r1.6/resource/_static/logo_source_en.png"></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>The automatic differential of MindSpore is in reverse-mode, which derives the backward pass according to the forward pass. Before some backward operators are computed, the results of some forward operators should be ready. It leads to the problem that the memory occupied by these results of the forward operators, can not be reused until the computation of the backward operators are completed. This problem can  drive up the peak of memory, which is particularly significant in the large model.</p>
<p>In order to solve this problem, Mindspore provides the recomputation function. It will recompute the forward operators before computing the backward operators rather than storing the results of forward operators, which can help the memory be reused. This tutorial takes the model ResNet-50 for example to explain how to configure recomputation to train your model in MindSpore.</p>
</section>
<section id="preliminaries">
<h2>Preliminaries<a class="headerlink" href="#preliminaries" title="Permalink to this headline"></a></h2>
<ol class="arabic simple">
<li><p>Prepare the model. The ResNet-50 code can be found at: <a class="reference external" href="https://gitee.com/mindspore/models/tree/r1.6/official/cv/resnet">https://gitee.com/mindspore/models/tree/r1.6/official/cv/resnet</a>, in which <code class="docutils literal notranslate"><span class="pre">train.py</span></code> is the main function for training, <code class="docutils literal notranslate"><span class="pre">src/</span></code> directory contains the model definition and configuration files of ResNet-50, <code class="docutils literal notranslate"><span class="pre">script/</span></code> directory contains the training and evaluation scripts.</p></li>
<li><p>Prepare the dataset. This example uses the <code class="docutils literal notranslate"><span class="pre">CIFAR-10</span></code> dataset. For details about how to download and load the dataset, visit <a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/en/r1.6/distributed_training_ascend.html">https://www.mindspore.cn/docs/programming_guide/en/r1.6/distributed_training_ascend.html</a>.</p></li>
</ol>
</section>
<section id="configuring-for-recomputation">
<h2>Configuring for Recomputation<a class="headerlink" href="#configuring-for-recomputation" title="Permalink to this headline"></a></h2>
<p>We can call two kinds of interface to configure the recomputation. Take <code class="docutils literal notranslate"><span class="pre">src/resnet.py</span></code> for example:</p>
<ol class="arabic">
<li><p>The <a class="reference external" href="https://www.mindspore.cn/docs/api/en/r1.6/api_python/ops/mindspore.ops.Primitive.html#mindspore.ops.Primitive.recompute">recompute api</a> of <code class="docutils literal notranslate"><span class="pre">Primitive</span></code>. It can set an operator to be recomputed. After setting, the operator will be recomputed in the backward pass.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ResNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">block</span><span class="p">,</span>
                 <span class="n">layer_nums</span><span class="p">,</span>
                 <span class="n">in_channels</span><span class="p">,</span>
                 <span class="n">out_channels</span><span class="p">,</span>
                 <span class="n">strides</span><span class="p">,</span>
                 <span class="n">num_classes</span><span class="p">,</span>
                 <span class="n">use_se</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">res_base</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ResNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="o">...</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="o">.</span><span class="n">recompute</span><span class="p">()</span>
        <span class="o">...</span>
</pre></div>
</div>
</li>
<li><p>Call the <a class="reference external" href="https://www.mindspore.cn/docs/api/en/r1.6/api_python/nn/mindspore.nn.Cell.html#mindspore.nn.Cell.recompute">recompute api</a> of <code class="docutils literal notranslate"><span class="pre">Cell</span></code>. It can set the whole <code class="docutils literal notranslate"><span class="pre">Cell</span></code> to be recomputed. After setting, except the output of the <code class="docutils literal notranslate"><span class="pre">Cell</span></code>, all the operators in this cell and its sub <code class="docutils literal notranslate"><span class="pre">Cell</span></code> will be recomputed in the backward pass.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ResNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">block</span><span class="p">,</span>
                 <span class="n">layer_nums</span><span class="p">,</span>
                 <span class="n">in_channels</span><span class="p">,</span>
                 <span class="n">out_channels</span><span class="p">,</span>
                 <span class="n">strides</span><span class="p">,</span>
                 <span class="n">num_classes</span><span class="p">,</span>
                 <span class="n">use_se</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">res_base</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ResNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="o">...</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">block</span><span class="p">,</span>
                                       <span class="n">layer_nums</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                       <span class="n">in_channel</span><span class="o">=</span><span class="n">in_channels</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                       <span class="n">out_channel</span><span class="o">=</span><span class="n">out_channels</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                       <span class="n">stride</span><span class="o">=</span><span class="n">strides</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                       <span class="n">use_se</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">use_se</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_make_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">block</span><span class="p">,</span> <span class="n">layer_num</span><span class="p">,</span> <span class="n">in_channel</span><span class="p">,</span> <span class="n">out_channel</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">use_se</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">se_block</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="o">...</span>
        <span class="k">if</span> <span class="n">se_block</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">layer_num</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
                <span class="n">resnet_block</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span><span class="n">out_channel</span><span class="p">,</span> <span class="n">out_channel</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">use_se</span><span class="o">=</span><span class="n">use_se</span><span class="p">)</span>
                <span class="n">resnet_block</span><span class="o">.</span><span class="n">recompute</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">layer_num</span><span class="p">):</span>
                <span class="n">resnet_block</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span><span class="n">out_channel</span><span class="p">,</span> <span class="n">out_channel</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">use_se</span><span class="o">=</span><span class="n">use_se</span><span class="p">)</span>
                <span class="n">resnet_block</span><span class="o">.</span><span class="n">recompute</span><span class="p">()</span>
        <span class="o">...</span>

<span class="k">class</span> <span class="nc">ResidualBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">in_channel</span><span class="p">,</span>
                 <span class="n">out_channel</span><span class="p">,</span>
                 <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">use_se</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">se_block</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ResidualBlock</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="o">...</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="o">...</span>

<span class="k">def</span> <span class="nf">resnet50</span><span class="p">(</span><span class="n">class_num</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">ResNet</span><span class="p">(</span><span class="n">ResidualBlock</span><span class="p">,</span>
                  <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">2048</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                  <span class="n">class_num</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="training-the-model">
<h2>Training the Model<a class="headerlink" href="#training-the-model" title="Permalink to this headline"></a></h2>
<p>We take the GPU environment for example, use the script <code class="docutils literal notranslate"><span class="pre">script/run_standalone_train_gpu.sh</span></code>. Run the command <code class="docutils literal notranslate"><span class="pre">bash</span> <span class="pre">scripts/run_standalone_train_gpu.sh</span> <span class="pre">$date_set_path</span> <span class="pre">config/resnet50_cifar10_config.yaml</span></code>.
We can set the context: <code class="docutils literal notranslate"><span class="pre">save_graph=True</span></code> in <code class="docutils literal notranslate"><span class="pre">src/train.py</span></code> to print the construction of the computation graph to do comparison.</p>
<p>The graph before setting recomputation is as follow:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>...
%56(equivoutput) = Conv2D(%53, %55) {instance name: conv2d} primitive_attrs: {pad_list: (0, 0, 0, 0), stride: (1, 1, 1, 1), pad: (0, 0, 0, 0), pad_mode: 1, out_channel: 64, kernel_size: (1, 1), input_names: [x, w], format: NCHW, groups: 1, mode: 1, group: 1, dilation: (1, 1, 1, 1), output_names: [output]}
      : (&lt;Tensor[Float16], (32, 256, 56, 56)&gt;, &lt;Tensor[Float16], (64, 256, 1, 1)&gt;) -&gt; (&lt;Tensor[Float16], (32, 64, 56, 56)&gt;)
...
%61(equiv[CNode]707) = BatchNorm(%56, %57, %58, %59, %60) {instance name: bn_train} primitive_attrs: {epsilon: 0.000100, is_training: true, momentum: 0.100000, format: NCHW, output_names: [y, batch_mean, batch_variance, reserve_space_1, reserve_space_2], input_names: [x, scale, offset, mean, variance]}
      : (&lt;Tensor[Float16], (32, 64, 56, 56)&gt;, &lt;Tensor[Float32], (64)&gt;, &lt;Tensor[Float32], (64)&gt;, &lt;Tensor[Float32], (64)&gt;, &lt;Tensor[Float32], (64)&gt;) -&gt; (&lt;Tuple[Tensor[Float16],Tensor[Float32]*4]&gt;)
...
%927(out) = BatchNormGrad(%923, %56, %57, %924, %925, %926) primitive_attrs: {epsilon: 0.000100, format: NCHW, is_training: true} cnode_primal_attrs: {forward_node_name: BatchNorm_102499}
      : (&lt;Tensor[Float16], (32, 64, 56, 56)&gt;, &lt;Tensor[Float16], (32, 64, 56, 56)&gt;, &lt;Tensor[Float32], (64)&gt;, &lt;Tensor[Float32], (64)&gt;, &lt;Tensor[Float32], (64)&gt;, &lt;Tensor[Float32], (64)&gt;) -&gt; (&lt;Tuple[Tensor[Float16],Tensor[Float32]*2]&gt;)
...
</pre></div>
</div>
<p>The graph after setting recomputation is as follow:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>...
%56(equivoutput) = Conv2D(%53, %55) {instance name: conv2d} primitive_attrs: {pad_list: (0, 0, 0, 0), stride: (1, 1, 1, 1), pad: (0, 0, 0, 0), pad_mode: 1, out_channel: 64, kernel_size: (1, 1), input_names: [x, w], format: NCHW, groups: 1, mode: 1, group: 1, dilation: (1, 1, 1, 1), output_names: [output]} cnode_attrs: {need_cse_after_recompute: true, recompute: true}
      : (&lt;Tensor[Float16], (32, 256, 56, 56)&gt;, &lt;Tensor[Float16], (64, 256, 1, 1)&gt;) -&gt; (&lt;Tensor[Float16], (32, 64, 56, 56)&gt;)
...
%61(equiv[CNode]707) = BatchNorm(%56, %57, %58, %59, %60) {instance name: bn_train} primitive_attrs: {epsilon: 0.000100, is_training: true, momentum: 0.100000, format: NCHW, output_names: [y, batch_mean, batch_variance, reserve_space_1, reserve_space_2], input_names: [x, scale, offset, mean, variance]}
      : (&lt;Tensor[Float16], (32, 64, 56, 56)&gt;, &lt;Tensor[Float32], (64)&gt;, &lt;Tensor[Float32], (64)&gt;, &lt;Tensor[Float32], (64)&gt;, &lt;Tensor[Float32], (64)&gt;) -&gt; (&lt;Tuple[Tensor[Float16],Tensor[Float32]*4]&gt;)
...
%1094([CNode]15682) = Conv2D(%1091, %1093) {instance name: conv2d} primitive_attrs: {pad_list: (1, 1, 1, 1), stride: (1, 1, 1, 1), pad: (0, 0, 0, 0), pad_mode: 1, out_channel: 64, kernel_size: (3, 3), input_names: [x, w], format: NCHW, groups: 1, mode: 1, group: 1, dilation: (1, 1, 1, 1), output_names: [output]} cnode_attrs: {need_cse_after_recompute: true, duplicated: true}
      : (&lt;Tensor[Float16], (32, 64, 56, 56)&gt;, &lt;Tensor[Float16], (64, 64, 3, 3)&gt;) -&gt; (&lt;Tensor[Float16], (32, 64, 56, 56)&gt;)
...
%1095([CNode]15681) = BatchNormGrad(%1085, %1094, %98, %1086, %1087, %1088) primitive_attrs: {epsilon: 0.000100, format: NCHW, is_training: true} cnode_attrs: {target_grad: true} cnode_primal_attrs: {forward_node_name: BatchNorm_102499}
      : (&lt;Tensor[Float16], (32, 64, 56, 56)&gt;, &lt;Tensor[Float16], (32, 64, 56, 56)&gt;, &lt;Tensor[Float32], (64)&gt;, &lt;Tensor[Float32], (64)&gt;, &lt;Tensor[Float32], (64)&gt;, &lt;Tensor[Float32], (64)&gt;) -&gt; (&lt;Tuple[Tensor[Float16],Tensor[Float32]*2]&gt;)
...
</pre></div>
</div>
<p>We can see that <code class="docutils literal notranslate"><span class="pre">Conv2D</span></code> is replicated to become the input to <code class="docutils literal notranslate"><span class="pre">BatchNormGrad</span></code>.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="distributed_inference.html" class="btn btn-neutral float-left" title="Distributed Inference" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="distributed_example.html" class="btn btn-neutral float-right" title="Distributed Parallel Usage Example" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>