<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Single-Node Tensor Cache &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script>
      <link rel="stylesheet" href="_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/training.css" type="text/css" /><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script><script src="_static/jquery.js"></script>
        <script src="_static/js/theme.js"></script><script src="_static/underscore.js"></script><script src="_static/doctools.js"></script><script src="_static/js/training.js"></script><script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Optimizing the Data Processing" href="optimize_data_processing.html" />
    <link rel="prev" title="Auto Augmentation" href="auto_augmentation.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="architecture.html">Overall Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_structure.html">MindSpore API Overview</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="design/technical_white_paper.html">Technical White Paper</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/gradient.html">MindSpore Automatic Differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/distributed_training_design.html">Distributed Training Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/mindir.html">MindSpore IR (MindIR)</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.6/training_visual_design.html">Design of Visualization↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quickstart</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/tutorials/en/r1.6/linear_regression.html">Implementing Simple Linear Function Fitting↗</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/tutorials/en/r1.6/quick_start.html">Implementing an Image Classification Application↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Basic Concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dtype.html">DataType</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor.html">Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter_introduction.html">Parameter</a></li>
<li class="toctree-l1"><a class="reference internal" href="operators.html">Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="cell.html">Cell</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset_introduction.html">Dataset</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Pipeline</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="dataset_sample.html">Quick Start of Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset.html">Loading Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline.html">Processing Data</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="dataset_advanced.html">Advanced Usage of Pipeline</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="auto_augmentation.html">Auto Augmentation</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Single-Node Tensor Cache</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#basic-cache-usage">Basic Cache Usage</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cache-sharing">Cache Sharing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#limitations">Limitations</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cache-performance-tuning">Cache Performance Tuning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="optimize_data_processing.html">Optimizing the Data Processing</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="dataset_usage.html">Data Iteration</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build the Network</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="build_net.html">Constructing Single Operator Network and Multi-layer Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="initializer.html">Initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter.html">Network Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="control_flow.html">Using the Process Control Statement</a></li>
<li class="toctree-l1"><a class="reference internal" href="indefinite_parameter.html">Parameter Passing</a></li>
<li class="toctree-l1"><a class="reference internal" href="loss.html">Loss Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="grad_operation.html">Gradient Operation</a></li>
<li class="toctree-l1"><a class="reference internal" href="constexpr.html">Construct Constants In the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="hypermap.html">Operation Overloading</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">Optimization Algorithms</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Running</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="context.html">Configuring Running Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="run.html">Running Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_and_load_models.html">Save and Load Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="model.html">Application of Model</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="online_inference.html">Online Inference with Checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="offline_inference.html">Using Offline Model for Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Distributed Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="distributed_training.html">Distributed Parallel Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_advanced.html">Distributed Parallel Advanced Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_example.html">Distributed Parallel Usage Example</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">PyNative</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="debug_in_pynative_mode.html">Debugging in PyNative Mode</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Numpy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="numpy.html">NumPy Interfaces in MindSpore</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Function Debugging</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="read_ir_files.html">Reading IR</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/en/r1.6/debug_in_pynative_mode.html">Debugging in PyNative Mode↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="dump_in_graph_mode.html">Using Dump in the Graph Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_debugging_info.html">Custom Debugging Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="incremental_operator_build.html">Incremental Operator Build</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="enable_mixed_precision.html">Enabling Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_auto_tune.html">AutoTune</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_dataset_autotune.html">Dataset AutoTune for Dataset Pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_dataset_offload.html">Enabling Offload for Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.6/performance_profiling.html">Debugging performance with Profiler↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Features</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="second_order_optimizer.html">Second Order Optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="graph_kernel_fusion.html">Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_quantization_aware_training.html">Quantization Aware Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cv.html">Computer Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp.html">Natural Language Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="hpc.html">High Performance Computing</a></li>
<li class="toctree-l1"><a class="reference internal" href="use_on_the_cloud.html">Using MindSpore on the Cloud</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="dataset_advanced.html">Advanced Usage of Pipeline</a> &raquo;</li>
      <li>Single-Node Tensor Cache</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/cache.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="single-node-tensor-cache">
<h1>Single-Node Tensor Cache<a class="headerlink" href="#single-node-tensor-cache" title="Permalink to this headline"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code> <code class="docutils literal notranslate"><span class="pre">Data</span> <span class="pre">Preparation</span></code></p>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.6/docs/mindspore/programming_guide/source_en/cache.md"><img alt="View Source On Gitee" src="https://gitee.com/mindspore/docs/raw/r1.6/resource/_static/logo_source_en.png" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>If you need to repeatedly access remote datasets or load datasets from disks, you can use the single-node cache operator to cache datasets in the local memory to accelerate dataset loading.</p>
<p>The cache operator depends on the cache server started on the current node. Functioning as a daemon process and independent of the training script, the cache server is mainly used to manage cached data, including storing, querying, and loading data, and writing cached data when the cache is not hit.</p>
<p>If the memory space is insufficient to cache all datasets, you can configure a cache operator to cache the remaining data to disks.</p>
<p>Currently, the cache service supports only single-node cache. That is, the client and server are deployed on the same machine. This service can be used in the following scenarios:</p>
<ul>
<li><p>Cache the loaded original dataset.</p>
<p>You can use the cache in the dataset loading operator. The loaded data is stored in the cache server. If the same data is required subsequently, the data can be directly load from the cache server, avoiding repeated loading from the disk.</p>
<p><img alt="cache on leaf pipeline" src="_images/cache_dataset.png" /></p>
</li>
<li><p>Cache the data processed by argumentation.</p>
<p>You can also use the cache in the <code class="docutils literal notranslate"><span class="pre">map</span></code> operator. The data processed by argumentation (such as image cropping or resizing) is directly cached, avoiding repeated data argumentation operations and reducing unnecessary computations.</p>
<p><img alt="cache on map pipeline" src="_images/cache_processed_data.png" /></p>
<blockquote>
<div><p>You are advised to cache image data in <code class="docutils literal notranslate"><span class="pre">decode</span></code> + <code class="docutils literal notranslate"><span class="pre">resize</span></code> + <code class="docutils literal notranslate"><span class="pre">cache</span></code> mode. The data processed by <code class="docutils literal notranslate"><span class="pre">decode</span></code> can be directly cached only in single-node single-device mode.</p>
</div></blockquote>
</li>
</ul>
<blockquote>
<div><p>For a complete example, see <a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/en/r1.6/enable_cache.html">Application of Single-Node Tensor Cache</a>.</p>
</div></blockquote>
</section>
<section id="basic-cache-usage">
<h2>Basic Cache Usage<a class="headerlink" href="#basic-cache-usage" title="Permalink to this headline"></a></h2>
<ol class="arabic">
<li><p>Configure the environment.</p>
<p>Before using the cache service, you need to install MindSpore and set related environment variables. The Conda environment is used as an example. The setting method is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:{path_to_conda}/envs/{your_env_name}/lib/python3.7/site-packages/mindspore:{path_to_conda}/envs/{your_env_name}/lib/python3.7/site-packages/mindspore/lib
export PATH=$PATH:{path_to_conda}/envs/{your_env_name}/bin
</pre></div>
</div>
<p>You can also set the environment with the following code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">mindspore</span>

<span class="n">python_path</span> <span class="o">=</span> <span class="s2">&quot;/&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">executable</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">mindspore_path</span> <span class="o">=</span> <span class="s2">&quot;/&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="vm">__file__</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">mindspore_lib_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">mindspore_path</span><span class="p">,</span> <span class="s2">&quot;lib&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="s1">&#39;PATH&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">:</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;PATH&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">python_path</span>
<span class="k">elif</span> <span class="n">python_path</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;PATH&#39;</span><span class="p">]:</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;PATH&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="s2">&quot;:&quot;</span> <span class="o">+</span> <span class="n">python_path</span>
<span class="nb">print</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;PATH&#39;</span><span class="p">])</span>

<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;LD_LIBRARY_PATH&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="si">{}</span><span class="s2">:</span><span class="si">{}</span><span class="s2">:</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mindspore_path</span><span class="p">,</span> <span class="n">mindspore_lib_path</span><span class="p">,</span> <span class="n">mindspore_lib_path</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;python3.7&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;LD_LIBRARY_PATH&#39;</span><span class="p">])</span>
</pre></div>
</div>
<blockquote>
<div><p>When the cache is used, the server memory may be insufficient. Therefore, you are advised to increase the swap memory space of the server to more than 100 GB before using the cache. For details about how to increase the swap memory space on Ubuntu, EulerOS, or CentOS, see <a class="reference external" href="https://help.ubuntu.com/community/SwapFaq#How_do_I_add_a_swap_file.3F">related tutorials</a>.</p>
</div></blockquote>
</li>
<li><p>Start the cache server.</p>
<p>Before using the single-node cache service, run the following command to start the cache server:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cache_admin<span class="w"> </span>--start
</pre></div>
</div>
<p>If the following information is displayed, the cache server is started successfully:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Cache server startup completed successfully!
The cache server daemon has been created as process id 10394 and is listening on port 50052

Recommendation:
Since the server is detached into its own daemon process, monitor the server logs (under /tmp/mindspore/cache/log) for any issues that may happen after startup
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">cache_admin</span></code> supports the following commands and options:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--start</span></code>: starts the cache server. The following options are supported:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">--workers</span></code> or <code class="docutils literal notranslate"><span class="pre">-w</span></code>: specifies the number of worker threads on the cache server. By default, the number of worker threads is half of the number of CPUs. This parameter relies on the NUMA architecture of the server. The value will be adjusted automatically by the server if it’s not a multiple of number of NUMA nodes in the machine.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--spilldir</span></code> or <code class="docutils literal notranslate"><span class="pre">-s</span></code>: specifies the disk file path for storing remaining data when the cached data size exceeds the memory space. The default value is ‘’ (which means disabling spilling).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--hostname</span></code> or <code class="docutils literal notranslate"><span class="pre">-h</span></code>: specifies the IP address of the cache server. The default value is 127.0.0.1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--port</span></code> or <code class="docutils literal notranslate"><span class="pre">-p</span></code>: specifies the port number of the cache server. The default value is 50052.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--loglevel</span></code> or <code class="docutils literal notranslate"><span class="pre">-l</span></code>: sets the log level. The default value is 1 (WARNING). If this option is set to 0 (INFO), excessive logs will be generated, resulting in performance deterioration.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">--stop</span></code>: stops the cache server.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--generate_session</span></code> or <code class="docutils literal notranslate"><span class="pre">-g</span></code>: generates a cache session.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--destroy_session</span></code> or <code class="docutils literal notranslate"><span class="pre">-d</span></code>: deletes a cache session.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--list_sessions</span></code>: displays the list of currently cached sessions and their details.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--server_info</span></code>：displays the configuration parameters and active session list of current server.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--help</span></code>: displays the help information.</p></li>
</ul>
<p>In the preceding options, you can use <code class="docutils literal notranslate"><span class="pre">-h</span></code> and <code class="docutils literal notranslate"><span class="pre">-p</span></code> to specify a server. Users can also set environment variables <code class="docutils literal notranslate"><span class="pre">MS_CACHE_HOST</span></code> and <code class="docutils literal notranslate"><span class="pre">MS_CACHE_PORT</span></code> to specify it. If hostname and port are not set, operations are performed on the server with the IP address 127.0.0.1 and port number 50052 by default.</p>
<p>You can run the <code class="docutils literal notranslate"><span class="pre">ps</span> <span class="pre">-ef|grep</span> <span class="pre">cache_server</span></code> command to check whether the server is started and query server parameters.</p>
<p>You can also run the <code class="docutils literal notranslate"><span class="pre">cache_admin</span> <span class="pre">--server_info</span></code> command to get the full list of configuration of cache server.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ cache_admin --server_info
Cache Server Configuration:
----------------------------------------
         config name          value
----------------------------------------
            hostname      127.0.0.1
                port          50052
   number of workers             16
           log level              1
           spill dir           None
----------------------------------------
Active sessions:
No active sessions.
</pre></div>
</div>
<p>Where, the table of Cache Server Configuration lists five detailed configuration information. Active sessions shows the list of active session ID in current server if any.</p>
<p>Cache server generates log files with filename “cache_server.&lt;hostname&gt;.&lt;username&gt;.log.&lt;severity level&gt;.&lt;date-time&gt;.&lt;pid&gt;”. Note that there might be masses of DEBUG logs printed to the screen when <code class="docutils literal notranslate"><span class="pre">GLOG_v=0</span></code> is set.</p>
<blockquote>
<div><ul class="simple">
<li><p>To enable data spilling, you need to use <code class="docutils literal notranslate"><span class="pre">-s</span></code> to set spilling path when starting cache server. Otherwise, this feature is default to be disabled and it will bring up a memory-only cache server.</p></li>
</ul>
</div></blockquote>
</li>
<li><p>Create a cache session.</p>
<p>If no cache session exists on the cache server, a cache session needs to be created to obtain the cache session ID.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ cache_admin -g
Session created for server on port 50052: 1456416665
</pre></div>
</div>
<p>In the preceding command, 1456416665 is the cache session ID allocated by the server with port number 50052.</p>
<p>You can run the <code class="docutils literal notranslate"><span class="pre">cache_admin</span> <span class="pre">--list_sessions</span></code> command to view all cache sessions on the current server.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ cache_admin --list_sessions
Listing sessions for server on port 50052

     Session    Cache Id  Mem cached Disk cached  Avg cache size  Numa hit
  1456416665         n/a         n/a         n/a             n/a       n/a
</pre></div>
</div>
<p>Output parameter description:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Session</span></code>: specifies the cache session ID.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Cache</span> <span class="pre">Id</span></code>: specifies the ID of the cache instance in the current cache session. <code class="docutils literal notranslate"><span class="pre">n/a</span></code> indicates that no cache instance is created.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Mem</span> <span class="pre">cached</span></code>: specifies the cached data volume in the memory.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Disk</span> <span class="pre">cached</span></code>: specifies the cached data volume in the disk.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Avg</span> <span class="pre">cache</span> <span class="pre">size</span></code>: specifies the average size of each line of data in the current cache.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Numa</span> <span class="pre">hit</span></code>: specifies the number of NUMA hits. A larger value indicates better time performance.</p></li>
</ul>
</li>
<li><p>Create a cache instance.</p>
<p>In the Python training script, use the <code class="docutils literal notranslate"><span class="pre">DatasetCache</span></code> API to define a cache instance named <code class="docutils literal notranslate"><span class="pre">test_cache</span></code>, and specify the <code class="docutils literal notranslate"><span class="pre">session_id</span></code> parameter to a cache session ID created in the previous step.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>

<span class="n">test_cache</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">DatasetCache</span><span class="p">(</span><span class="n">session_id</span><span class="o">=</span><span class="mi">1456416665</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">spilling</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">DatasetCache</span></code> supports the following parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">session_id</span></code>: specifies the cache session ID, which can be created and obtained by running the <code class="docutils literal notranslate"><span class="pre">cache_admin</span> <span class="pre">-g</span></code> command.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">size</span></code>: specifies the maximum memory space occupied by the cache. The unit is MB. For example, if the cache space is 512 GB, set <code class="docutils literal notranslate"><span class="pre">size</span></code> to <code class="docutils literal notranslate"><span class="pre">524288</span></code>. The default value is 0.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">spilling</span></code>: determines whether to spill the remaining data to disks when the memory space exceeds the upper limit. The default value is False.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">hostname</span></code>: specifies the IP address for connecting to the cache server. The default value is 127.0.0.1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">port</span></code>: specifies the port number for connecting to the cache server. The default value is 50052.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_connections</span></code>: specifies the number of established TCP/IP connections. The default value is 12.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">prefetch_size</span></code>: specifies the number of prefetched rows. The default value is 20.</p></li>
</ul>
<blockquote>
<div><ul class="simple">
<li><p>In actual use, you are advised to run the <code class="docutils literal notranslate"><span class="pre">cache_admin</span> <span class="pre">-g</span></code> command to obtain a cache session ID from the cache server and use it as the parameter of <code class="docutils literal notranslate"><span class="pre">session_id</span></code> to prevent errors caused by cache session nonexistence.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">size=0</span></code> indicates that the memory space used by the cache is not limited manually, but automically controlled by the cache_server according to system’s total memory resources, and cache server’s memory usage would be limited to within 80% of the total system memory.</p></li>
<li><p>Users can also manually set <code class="docutils literal notranslate"><span class="pre">size</span></code> to a proper value based on the idle memory of the machine. Note that before setting the <code class="docutils literal notranslate"><span class="pre">size</span></code> parameter, make sure to check the available memory of the system and the size of the dataset to be loaded. If the memory of cache_server or the dataset size exceeds the available memory of the system, the server may break down or restart, it may also automatically shut down, or the training process may fail.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">spilling=True</span></code> indicates that the remaining data is written to disks when the memory space is insufficient. Therefore, ensure that you have the write permission on the configured disk path and the disk space is sufficient to store the remaining cache data. Note that if no spilling path is set when cache server starts, setting <code class="docutils literal notranslate"><span class="pre">spilling=True</span></code> will raise an error when calling the API.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">spilling=False</span></code> indicates that no data is written once the configured memory space is used up on the cache server.</p></li>
<li><p>If a dataset that does not support random access (such as <code class="docutils literal notranslate"><span class="pre">TFRecordDataset</span></code>) is used to load data and the cache service is enabled, ensure that the entire dataset is stored locally. In this scenario, if the local memory space is insufficient to store all data, spilling must be enabled to spill data to disks.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_connections</span></code> and <code class="docutils literal notranslate"><span class="pre">prefetch_size</span></code> are internal performance tuning parameters. Generally, you do not need to set these two parameters.</p></li>
</ul>
</div></blockquote>
</li>
<li><p>Insert a cache instance.</p>
<p>Currently, the cache service can be used to cache both original datasets and datasets processed by argumentation. The following example shows two usage methods.</p>
<p>Note that you need to create a cache instance for each of the two examples according to step 4, and use the created <code class="docutils literal notranslate"><span class="pre">test_cache</span></code> as the <code class="docutils literal notranslate"><span class="pre">cache</span></code> parameter in the dataset loading operator or map operator.</p>
<p>CIFAR-10 dataset is used in the following two examples. Before running the sample, download and store the CIFAR-10 dataset by referring to <a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/en/r1.6/dataset_loading.html#cifar-10-100">Loading Dataset</a>.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>./datasets/cifar-10-batches-bin
├── readme.html
├── test
│   └── test_batch.bin
└── train
    ├── batches.meta.txt
    ├── data_batch_1.bin
    ├── data_batch_2.bin
    ├── data_batch_3.bin
    ├── data_batch_4.bin
    └── data_batch_5.bin
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">tarfile</span>
<span class="kn">import</span> <span class="nn">zipfile</span>
<span class="kn">import</span> <span class="nn">shutil</span>

<span class="n">requests</span><span class="o">.</span><span class="n">packages</span><span class="o">.</span><span class="n">urllib3</span><span class="o">.</span><span class="n">disable_warnings</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">download_dataset</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">target_path</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; download and unzip the dataset &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">target_path</span><span class="p">):</span>
        <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">target_path</span><span class="p">)</span>
    <span class="n">download_file</span> <span class="o">=</span> <span class="n">url</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>\<span class="s2">&quot;/</span><span class="se">\&quot;</span><span class="s2">)[-1]</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">download_file</span><span class="p">):</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">verify</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">download_file</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>\<span class="s2">&quot;.</span><span class="se">\&quot;</span><span class="s2">)[-1] not in [</span><span class="se">\&quot;</span><span class="s2">tgz</span><span class="se">\&quot;</span><span class="s2">, </span><span class="se">\&quot;</span><span class="s2">zip</span><span class="se">\&quot;</span><span class="s2">, </span><span class="se">\&quot;</span><span class="s2">tar</span><span class="se">\&quot;</span><span class="s2">, </span><span class="se">\&quot;</span><span class="s2">gz</span><span class="se">\&quot;</span><span class="s2">]:</span>
            <span class="n">download_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">target_path</span><span class="p">,</span> <span class="n">download_file</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">download_file</span><span class="p">,</span> \<span class="s2">&quot;wb</span><span class="se">\&quot;</span><span class="s2">) as f:</span>
            <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">iter_content</span><span class="p">(</span><span class="n">chunk_size</span><span class="o">=</span><span class="mi">512</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">chunk</span><span class="p">:</span>
                    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">chunk</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">download_file</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span>\<span class="s2">&quot;zip</span><span class="se">\&quot;</span><span class="s2">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">zipfile</span><span class="o">.</span><span class="n">ZipFile</span><span class="p">(</span><span class="n">download_file</span><span class="p">,</span> \<span class="s2">&quot;r</span><span class="se">\&quot;</span><span class="s2">)</span>
        <span class="n">z</span><span class="o">.</span><span class="n">extractall</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="n">target_path</span><span class="p">)</span>
        <span class="n">z</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">download_file</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span>\<span class="s2">&quot;.tar.gz</span><span class="se">\&quot;</span><span class="s2">) or download_file.endswith(</span><span class="se">\&quot;</span><span class="s2">.tar</span><span class="se">\&quot;</span><span class="s2">) or download_file.endswith(</span><span class="se">\&quot;</span><span class="s2">.tgz</span><span class="se">\&quot;</span><span class="s2">):</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">tarfile</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">download_file</span><span class="p">)</span>
        <span class="n">names</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">getnames</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">names</span><span class="p">:</span>
            <span class="n">t</span><span class="o">.</span><span class="n">extract</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">target_path</span><span class="p">)</span>
        <span class="n">t</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span>\<span class="s2">&quot;The </span><span class="si">{}</span><span class="s2"> file is downloaded and saved in the path </span><span class="si">{}</span><span class="s2"> after processing</span><span class="se">\&quot;</span><span class="s2">.format(os.path.basename(url), target_path))</span>

<span class="n">download_dataset</span><span class="p">(</span>\<span class="s2">&quot;https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/datasets/cifar-10-binary.tar.gz</span><span class="se">\&quot;</span><span class="s2">, </span><span class="se">\&quot;</span><span class="s2">./datasets</span><span class="se">\&quot;</span><span class="s2">)</span>
<span class="n">test_path</span> <span class="o">=</span> \<span class="s2">&quot;./datasets/cifar-10-batches-bin/test</span><span class="se">\&quot;</span>
<span class="n">train_path</span> <span class="o">=</span> \<span class="s2">&quot;./datasets/cifar-10-batches-bin/train</span><span class="se">\&quot;</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">test_path</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">train_path</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">test_path</span><span class="p">,</span> \<span class="s2">&quot;test_batch.bin</span><span class="se">\&quot;</span><span class="s2">)):</span>
    <span class="n">shutil</span><span class="o">.</span><span class="n">move</span><span class="p">(</span>\<span class="s2">&quot;./datasets/cifar-10-batches-bin/test_batch.bin</span><span class="se">\&quot;</span><span class="s2">, test_path)</span>
<span class="p">[</span><span class="n">shutil</span><span class="o">.</span><span class="n">move</span><span class="p">(</span>\<span class="s2">&quot;./datasets/cifar-10-batches-bin/</span><span class="se">\&quot;</span><span class="s2">+i, train_path) for i in os.listdir(</span><span class="se">\&quot;</span><span class="s2">./datasets/cifar-10-batches-bin/</span><span class="se">\&quot;</span><span class="s2">) if os.path.isfile(</span><span class="se">\&quot;</span><span class="s2">./datasets/cifar-10-batches-bin/</span><span class="se">\&quot;</span><span class="s2">+i) and not i.endswith(</span><span class="se">\&quot;</span><span class="s2">.html</span><span class="se">\&quot;</span><span class="s2">) and not os.path.exists(os.path.join(train_path, i))]</span>
</pre></div>
</div>
<ul>
<li><p>Cache the original loaded dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dataset_dir</span> <span class="o">=</span> <span class="s2">&quot;./datasets/cifar-10-batches-bin/train&quot;</span>

<span class="c1"># apply cache to dataset</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">Cifar10Dataset</span><span class="p">(</span><span class="n">dataset_dir</span><span class="o">=</span><span class="n">dataset_dir</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cache</span><span class="o">=</span><span class="n">test_cache</span><span class="p">)</span>

<span class="n">num_iter</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>  <span class="c1"># each data is a dictionary</span>
    <span class="c1"># in this example, each dictionary has a key &quot;image&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> image shape: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_iter</span><span class="p">,</span> <span class="n">item</span><span class="p">[</span><span class="s2">&quot;image&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="n">num_iter</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
</div>
<p>The output is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>0 image shape: (32, 32, 3)
1 image shape: (32, 32, 3)
2 image shape: (32, 32, 3)
3 image shape: (32, 32, 3)
</pre></div>
</div>
<p>You can run the <code class="docutils literal notranslate"><span class="pre">cache_admin</span> <span class="pre">--list_sessions</span></code> command to check whether there are four data records in the current session. If yes, the data is successfully cached.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ cache_admin --list_sessions
Listing sessions for server on port 50052

     Session    Cache Id  Mem cached Disk cached  Avg cache size  Numa hit
  1456416665   821590605       4         n/a          3226           4
</pre></div>
</div>
</li>
<li><p>Cache the data processed by argumentation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.dataset.vision.c_transforms</span> <span class="k">as</span> <span class="nn">c_vision</span>

<span class="n">dataset_dir</span> <span class="o">=</span> <span class="s2">&quot;cifar-10-batches-bin/&quot;</span>

<span class="c1"># apply cache to dataset</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">Cifar10Dataset</span><span class="p">(</span><span class="n">dataset_dir</span><span class="o">=</span><span class="n">dataset_dir</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># apply cache to map</span>
<span class="n">rescale_op</span> <span class="o">=</span> <span class="n">c_vision</span><span class="o">.</span><span class="n">Rescale</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">input_columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;image&quot;</span><span class="p">],</span> <span class="n">operations</span><span class="o">=</span><span class="n">rescale_op</span><span class="p">,</span> <span class="n">cache</span><span class="o">=</span><span class="n">test_cache</span><span class="p">)</span>

<span class="n">num_iter</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>  <span class="c1"># each data is a dictionary</span>
    <span class="c1"># in this example, each dictionary has a keys &quot;image&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> image shape: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_iter</span><span class="p">,</span> <span class="n">item</span><span class="p">[</span><span class="s2">&quot;image&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="n">num_iter</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
</div>
<p>The output is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>0 image shape: (32, 32, 3)
1 image shape: (32, 32, 3)
2 image shape: (32, 32, 3)
3 image shape: (32, 32, 3)
4 image shape: (32, 32, 3)
</pre></div>
</div>
<p>You can run the <code class="docutils literal notranslate"><span class="pre">cache_admin</span> <span class="pre">--list_sessions</span></code> command to check whether there are five data records in the current session. If yes, the data is successfully cached.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ cache_admin --list_sessions
Listing sessions for server on port 50052

     Session    Cache Id  Mem cached Disk cached  Avg cache size  Numa hit
  1456416665  3618046178       5         n/a          12442         5
</pre></div>
</div>
</li>
</ul>
</li>
<li><p>Destroy the cache session.</p>
<p>After the training is complete, you can destroy the current cache and release the memory.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ cache_admin --destroy_session 1456416665
Drop session successfully for server on port 50052
</pre></div>
</div>
<p>The preceding command is used to destroy the cache with the session ID 1456416665 on the server with the port number 50052.</p>
<p>If you choose not to destroy the cache, the cached data still exists in the cache session. You can use the cache when starting the training script next time.</p>
</li>
<li><p>Stop the cache server.</p>
<p>After using the cache server, you can stop it. This operation will destroy all cache sessions on the current server and release the memory.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ cache_admin --stop
Cache server on port 50052 has been stopped successfully.
</pre></div>
</div>
<p>The preceding command is used to shut down the server with the port number 50052.</p>
<p>If you choose not to shut down the server, the cache sessions on the server will be retained for future use. During the next training, you can create a cache session or reuse the existing cache.</p>
</li>
</ol>
</section>
<section id="cache-sharing">
<h2>Cache Sharing<a class="headerlink" href="#cache-sharing" title="Permalink to this headline"></a></h2>
<p>During the single-node multi-device distributed training, the cache operator allows multiple same training scripts to share the same cache and read and write data from the cache.</p>
<ol class="arabic">
<li><p>Start the cache server.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ cache_admin --start
Cache server startup completed successfully!
The cache server daemon has been created as process id 39337 and listening on port 50052

Recommendation:
Since the server is detached into its own daemon process, monitor the server logs (under /tmp/mindspore/cache/log) for any issues that may happen after startup
</pre></div>
</div>
</li>
<li><p>Create a cache session.</p>
<p>Create the shell script <code class="docutils literal notranslate"><span class="pre">cache.sh</span></code> for starting Python training and run the following command to generate a cache session ID:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1"># This shell script will launch parallel pipelines</span>

<span class="c1"># get path to dataset directory</span>
<span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="nv">$#</span><span class="w"> </span>!<span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="o">]</span>
<span class="k">then</span>
<span class="w">        </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Usage: sh cache.sh DATASET_PATH&quot;</span>
<span class="nb">exit</span><span class="w"> </span><span class="m">1</span>
<span class="k">fi</span>
<span class="nv">dataset_path</span><span class="o">=</span><span class="nv">$1</span>

<span class="c1"># generate a session id that these parallel pipelines can share</span>
<span class="nv">result</span><span class="o">=</span><span class="k">$(</span>cache_admin<span class="w"> </span>-g<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="k">)</span>
<span class="nv">rc</span><span class="o">=</span><span class="nv">$?</span>
<span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="nv">$rc</span><span class="w"> </span>-ne<span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;some error&quot;</span>
<span class="w">    </span><span class="nb">exit</span><span class="w"> </span><span class="m">1</span>
<span class="k">fi</span>

<span class="c1"># grab the session id from the result string</span>
<span class="nv">session_id</span><span class="o">=</span><span class="k">$(</span><span class="nb">echo</span><span class="w"> </span><span class="nv">$result</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>awk<span class="w"> </span><span class="s1">&#39;{print $NF}&#39;</span><span class="k">)</span>
</pre></div>
</div>
</li>
<li><p>Pass the cache session ID to the training script.</p>
<p>Continue to write the shell script and add the following command to pass <code class="docutils literal notranslate"><span class="pre">session_id</span></code> and other parameters when the Python training is started:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># make the session_id available to the python scripts</span>
<span class="nv">num_devices</span><span class="o">=</span><span class="m">4</span>

<span class="k">for</span><span class="w"> </span>p<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="k">$(</span>seq<span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="k">$((</span><span class="si">${</span><span class="nv">num_devices</span><span class="si">}</span><span class="o">-</span><span class="m">1</span><span class="k">)))</span><span class="p">;</span><span class="w"> </span><span class="k">do</span>
<span class="w">    </span>python<span class="w"> </span>my_training_script.py<span class="w"> </span>--num_devices<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$num_devices</span><span class="s2">&quot;</span><span class="w"> </span>--device<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$p</span><span class="s2">&quot;</span><span class="w"> </span>--session_id<span class="w"> </span><span class="nv">$session_id</span><span class="w"> </span>--dataset_path<span class="w"> </span><span class="nv">$dataset_path</span>
<span class="k">done</span>
</pre></div>
</div>
<blockquote>
<div><p>Complete sample code: <a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.6/docs/sample_code/cache/cache.sh">cache.sh</a></p>
</div></blockquote>
</li>
<li><p>Create and apply a cache instance.</p>
<p>CIFAR-10 dataset is used in the following example. Before running the sample, download and store the CIFAR-10 dataset by referring to <a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/en/r1.6/dataset_loading.html#cifar-10-100">Loading Dataset</a>. The directory structure is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>├─cache.sh
├─my_training_script.py
└─cifar-10-batches-bin
    ├── batches.meta.txt
    ├── data_batch_1.bin
    ├── data_batch_2.bin
    ├── data_batch_3.bin
    ├── data_batch_4.bin
    ├── data_batch_5.bin
    ├── readme.html
    └── test_batch.bin
</pre></div>
</div>
<p>Create and write the Python script <code class="docutils literal notranslate"><span class="pre">my_training_script.py</span></code>. Use the following code to receive <code class="docutils literal notranslate"><span class="pre">session_id</span></code> and pass it as a parameter when defining a cache instance.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;Cache Example&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--num_devices&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Device num.&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--device&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Device id.&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--session_id&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Session id.&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--dataset_path&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Dataset path&#39;</span><span class="p">)</span>
<span class="n">args_opt</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

<span class="c1"># apply cache to dataset</span>
<span class="n">test_cache</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">DatasetCache</span><span class="p">(</span><span class="n">session_id</span><span class="o">=</span><span class="n">args_opt</span><span class="o">.</span><span class="n">session_id</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">spilling</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">Cifar10Dataset</span><span class="p">(</span><span class="n">dataset_dir</span><span class="o">=</span><span class="n">args_opt</span><span class="o">.</span><span class="n">dataset_path</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                            <span class="n">num_shards</span><span class="o">=</span><span class="n">args_opt</span><span class="o">.</span><span class="n">num_devices</span><span class="p">,</span> <span class="n">shard_id</span><span class="o">=</span><span class="n">args_opt</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">cache</span><span class="o">=</span><span class="n">test_cache</span><span class="p">)</span>
<span class="n">num_iter</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">():</span>
    <span class="n">num_iter</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Got </span><span class="si">{}</span><span class="s2"> samples on device </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_iter</span><span class="p">,</span> <span class="n">args_opt</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>
</pre></div>
</div>
<blockquote>
<div><p>Complete sample code: <a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.6/docs/sample_code/cache/my_training_script.py">my_training_script.py</a></p>
</div></blockquote>
</li>
<li><p>Execute the training script.</p>
<p>Execute the shell script <code class="docutils literal notranslate"><span class="pre">cache.sh</span></code> to enable distributed training.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ sh cache.sh cifar-10-batches-bin/
Got 4 samples on device 0
Got 4 samples on device 1
Got 4 samples on device 2
Got 4 samples on device 3
</pre></div>
</div>
<p>You can run the <code class="docutils literal notranslate"><span class="pre">cache_admin</span> <span class="pre">--list_sessions</span></code> command to check whether only one group of data exists in the current session. If yes, cache sharing is successful.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ cache_admin --list_sessions
Listing sessions for server on port 50052

     Session    Cache Id  Mem cached Disk cached  Avg cache size  Numa hit
  3392558708   821590605          16         n/a            3227        16
</pre></div>
</div>
</li>
<li><p>Destroy the cache session.</p>
<p>After the training is complete, you can destroy the current cache and release the memory.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ cache_admin --destroy_session 3392558708
Drop session successfully for server on port 50052
</pre></div>
</div>
</li>
<li><p>Stop the cache server.</p>
<p>After using the cache server, you can stop it.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ cache_admin --stop
Cache server on port 50052 has been stopped successfully.
</pre></div>
</div>
</li>
</ol>
</section>
<section id="limitations">
<h2>Limitations<a class="headerlink" href="#limitations" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>Currently, dataset classes such as <code class="docutils literal notranslate"><span class="pre">GraphDataset</span></code>, <code class="docutils literal notranslate"><span class="pre">GeneratorDataset</span></code>, <code class="docutils literal notranslate"><span class="pre">PaddedDataset</span></code>, and <code class="docutils literal notranslate"><span class="pre">NumpySlicesDataset</span></code> do not support cache. <code class="docutils literal notranslate"><span class="pre">GeneratorDataset</span></code>, <code class="docutils literal notranslate"><span class="pre">PaddedDataset</span></code>, and <code class="docutils literal notranslate"><span class="pre">NumpySlicesDataset</span></code> belong to <code class="docutils literal notranslate"><span class="pre">GeneratorOp</span></code>, so their error message is displayed as “There is currently no support for GeneratorOp under cache.”</p></li>
<li><p>Data processed by <code class="docutils literal notranslate"><span class="pre">batch</span></code>, <code class="docutils literal notranslate"><span class="pre">concat</span></code>, <code class="docutils literal notranslate"><span class="pre">filter</span></code>, <code class="docutils literal notranslate"><span class="pre">repeat</span></code>, <code class="docutils literal notranslate"><span class="pre">skip</span></code>, <code class="docutils literal notranslate"><span class="pre">split</span></code>, <code class="docutils literal notranslate"><span class="pre">take</span></code>, and <code class="docutils literal notranslate"><span class="pre">zip</span></code> does not support cache.</p></li>
<li><p>Data processed by random data argumentation operations (such as <code class="docutils literal notranslate"><span class="pre">RandomCrop</span></code>) does not support cache.</p></li>
<li><p>The same cache instance cannot be nested in different locations of the same pipeline.</p></li>
</ul>
</section>
<section id="cache-performance-tuning">
<h2>Cache Performance Tuning<a class="headerlink" href="#cache-performance-tuning" title="Permalink to this headline"></a></h2>
<p>The cache service performance can be significantly improved in following scenarios:</p>
<ul class="simple">
<li><p>Cache the data processed by augmentation, especially when the data processing pipeline contains high complexity operations such as decode. In this scenario, you do not need to perform the data augmentation operation repeatedly on each epoch, which saves a lot of time.</p></li>
<li><p>Use cache services during simple network training and inference. Compared with complex networks, simple networks require less training time. Therefore, the time performance is significantly improved when cache services are used in this scenario.</p></li>
</ul>
<p>However, we may not benefit from cache in the following scenarios:</p>
<ul class="simple">
<li><p>The system memory is insufficient or the cache is not hit, resulting in poor cache service time performance. You can check whether the available system memory is sufficient and set a proper cache size before using the cache.</p></li>
<li><p>Too much cache spilling will deteriorate the time performance. Therefore, try not to spill cache to disks when datasets that support random access (such as <code class="docutils literal notranslate"><span class="pre">ImageFolderDataset</span></code>) are used for data loading.</p></li>
<li><p>Using cache on NLP network such as Bert does not perform. In the NLP scenarios, there are usually no high complexity data augmentation operations like decode.</p></li>
<li><p>There is expectable startup overhead when using cache in non-mappable datasets like <code class="docutils literal notranslate"><span class="pre">TFRecordDataset</span></code>. According to the current design, it is required to cache all rows to the cache server before the first epoch of training. So the first epoch time can be longer than the non-cache case.</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="auto_augmentation.html" class="btn btn-neutral float-left" title="Auto Augmentation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="optimize_data_processing.html" class="btn btn-neutral float-right" title="Optimizing the Data Processing" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>