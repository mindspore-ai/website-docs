<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Custom Operators (Custom based) &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script>
      <link rel="stylesheet" href="_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/training.css" type="text/css" /><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script><script src="_static/jquery.js"></script>
        <script src="_static/js/theme.js"></script><script src="_static/underscore.js"></script><script src="_static/doctools.js"></script><script src="_static/js/training.js"></script><script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Cell" href="cell.html" />
    <link rel="prev" title="Custom Operators (CPU)" href="custom_operator_cpu.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="architecture.html">Overall Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_structure.html">MindSpore API Overview</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="design/technical_white_paper.html">Technical White Paper</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/gradient.html">MindSpore Automatic Differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/distributed_training_design.html">Distributed Training Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/mindir.html">MindSpore IR (MindIR)</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.6/training_visual_design.html">Design of Visualization↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quickstart</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/tutorials/en/r1.6/linear_regression.html">Implementing Simple Linear Function Fitting↗</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/tutorials/en/r1.6/quick_start.html">Implementing an Image Classification Application↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Basic Concepts</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="dtype.html">DataType</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor.html">Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter_introduction.html">Parameter</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="operators.html">Operators</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="operators_usage.html">Operators Usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators_classification.html">Operators Classification</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="custom_operator.html">Custom Operator</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="custom_operator_ascend.html">Custom Operators (Ascend)</a></li>
<li class="toctree-l3"><a class="reference internal" href="custom_operator_gpu.html">Custom Operators (GPU)</a></li>
<li class="toctree-l3"><a class="reference internal" href="custom_operator_cpu.html">Custom Operators (CPU)</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Custom Operators (Custom based)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="#basic-usage">Basic Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="#advanced-usage">Advanced Usage</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="cell.html">Cell</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset_introduction.html">Dataset</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Pipeline</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dataset_sample.html">Quick Start of Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset.html">Loading Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline.html">Processing Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset_advanced.html">Advanced Usage of Pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset_usage.html">Data Iteration</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build the Network</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="build_net.html">Constructing Single Operator Network and Multi-layer Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="initializer.html">Initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter.html">Network Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="control_flow.html">Using the Process Control Statement</a></li>
<li class="toctree-l1"><a class="reference internal" href="indefinite_parameter.html">Parameter Passing</a></li>
<li class="toctree-l1"><a class="reference internal" href="loss.html">Loss Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="grad_operation.html">Gradient Operation</a></li>
<li class="toctree-l1"><a class="reference internal" href="constexpr.html">Construct Constants In the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="hypermap.html">Operation Overloading</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">Optimization Algorithms</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Running</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="context.html">Configuring Running Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="run.html">Running Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_and_load_models.html">Save and Load Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="model.html">Application of Model</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="online_inference.html">Online Inference with Checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="offline_inference.html">Using Offline Model for Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Distributed Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="distributed_training.html">Distributed Parallel Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_advanced.html">Distributed Parallel Advanced Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_example.html">Distributed Parallel Usage Example</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">PyNative</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="debug_in_pynative_mode.html">Debugging in PyNative Mode</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Numpy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="numpy.html">NumPy Interfaces in MindSpore</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Function Debugging</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="read_ir_files.html">Reading IR</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/en/r1.6/debug_in_pynative_mode.html">Debugging in PyNative Mode↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="dump_in_graph_mode.html">Using Dump in the Graph Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_debugging_info.html">Custom Debugging Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="incremental_operator_build.html">Incremental Operator Build</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="enable_mixed_precision.html">Enabling Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_auto_tune.html">AutoTune</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_dataset_autotune.html">Dataset AutoTune for Dataset Pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_dataset_offload.html">Enabling Offload for Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.6/performance_profiling.html">Debugging performance with Profiler↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Features</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="second_order_optimizer.html">Second Order Optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="graph_kernel_fusion.html">Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_quantization_aware_training.html">Quantization Aware Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cv.html">Computer Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp.html">Natural Language Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="hpc.html">High Performance Computing</a></li>
<li class="toctree-l1"><a class="reference internal" href="use_on_the_cloud.html">Using MindSpore on the Cloud</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="operators.html">Operators</a> &raquo;</li>
          <li><a href="custom_operator.html">Custom Operator</a> &raquo;</li>
      <li>Custom Operators (Custom based)</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/custom_operator_custom.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="custom-operators-custom-based">
<h1>Custom Operators (Custom based)<a class="headerlink" href="#custom-operators-custom-based" title="Permalink to this headline"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code> <code class="docutils literal notranslate"><span class="pre">Model</span> <span class="pre">Development</span></code></p>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.6/docs/mindspore/programming_guide/source_en/custom_operator_custom.md"><img alt="View Source On Gitee" src="https://gitee.com/mindspore/docs/raw/r1.6/resource/_static/logo_source_en.png" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>When built-in operators cannot meet requirements during network development, you can call the Python API <a class="reference external" href="https://www.mindspore.cn/docs/api/en/r1.6/api_python/ops/mindspore.ops.Custom.html#mindspore-ops-custom">Custom</a> primitive defined in MindSpore to quickly create different types of custom operators for use.</p>
<p>Traditional methods to add a custom operator need three steps: defining the operator primitive, implementing the operator, and registering the operator information.</p>
<p>The related concepts are as follows:</p>
<ul class="simple">
<li><p>Operator primitive: defines the frontend API prototype of an operator on the network. It is the basic unit for forming a network model and includes the operator name, attribute (optional), input and output names, output shape inference method, and output data type inference method.</p></li>
<li><p>Operator implementation: defines a Python function(Ascend custom operators) or a C++ class(GPU and CPU custom operators), which describes the implementation of the internal computation logic of an operator.</p></li>
<li><p>Operator information: describes basic information about an operator, such as the operator name, supported input and output data types, supported input and output data formats, and attributes. It is the basis for the backend to select and map operators.</p></li>
</ul>
<p>Compared with traditional custom operator creating methods, creating custom operators based on <code class="docutils literal notranslate"><span class="pre">Custom</span></code> primitive has several advantages:</p>
<ul class="simple">
<li><p>Different custom operators use the same <code class="docutils literal notranslate"><span class="pre">Custom</span></code> primitive, there is no need to define a primitive for every operator. The above three parts of work can be implemented in a network script with a unified way, and used as part of the network expression, there is no need to modify and recompile the source codes of MindSpore.</p></li>
<li><p>It unifies the interface and usage for different kinds of custom operators, which is convenient for network developers to flexibly choose which kind of custom operator to use according to their needs.</p></li>
<li><p>Supports defining custom operators with hybrid expression, which can be used across platforms.</p></li>
</ul>
</section>
<section id="basic-usage">
<h2>Basic Usage<a class="headerlink" href="#basic-usage" title="Permalink to this headline"></a></h2>
<p>The supported custom operator defining methods based on the <a class="reference external" href="https://www.mindspore.cn/docs/api/en/r1.6/api_python/ops/mindspore.ops.Custom.html#mindspore-ops-custom">Custom</a> primitive include: akg, tbe, aot and pyfunc.</p>
<p>The difference of these operator defining methods are as follows:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="text-center head"><p>Defining Methods</p></th>
<th class="text-center head"><p>Development Language</p></th>
<th class="text-center head"><p>Compilation Method</p></th>
<th class="head"><p>Supported Platforms</p></th>
<th class="head"><p>Recommended Scenarios</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>akg</p></td>
<td class="text-center"><p>MindSpore AKG DSL</p></td>
<td class="text-center"><p>JIT</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p></td>
<td><p>Ascend/GPU platform general scenarios</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>tbe</p></td>
<td class="text-center"><p>TBE DSL</p></td>
<td class="text-center"><p>JIT</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code></p></td>
<td><p>Ascend platform scenarios</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>aot</p></td>
<td class="text-center"><p>C/C++/CUDA</p></td>
<td class="text-center"><p>AOT</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
<td><p>GPU/CPU platform high-performance scenarios</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>pyfunc</p></td>
<td class="text-center"><p>Python</p></td>
<td class="text-center"><p>JIT</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
<td><p>Fast algorithm verification, need to interact with Python and other scenarios</p></td>
</tr>
</tbody>
</table>
<blockquote>
<div><ul class="simple">
<li><p>The full name of DSL is Domain Specific Language.</p></li>
<li><p>AOT(Ahead Of Time) compiling means the operator implementation needs to be compiled into a dynamic library in advance, and then automatically called by the framework when the network is running. JIT(Just In Time) compiling does not need to compile the operator implementation in advance, the operator implementation will be directly called by the framework during network compilation or runtime.</p></li>
</ul>
</div></blockquote>
<p>Different custom operator defining methods use different development language to implement the operator, but the development process is the same, including operator implementation, operator output shape and data type inference, and operator information registration (optional). You can choose which one to use base on needs. The defining methods of these custom operators will be introduced here, and examples are provided for each method.</p>
<blockquote>
<div><p>More examples can be found in the MindSpore source code <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.6/tests/st/ops/graph_kernel/custom">tests/st/ops/graph_kernel/custom</a>.</p>
</div></blockquote>
<section id="defining-custom-operator-of-akg-type">
<h3>Defining Custom Operator of akg Type<a class="headerlink" href="#defining-custom-operator-of-akg-type" title="Permalink to this headline"></a></h3>
<p>The custom operator of akg type uses the <a class="reference external" href="https://gitee.com/mindspore/akg">MindSpore AKG</a> operator DSL to describe the internal calculation logic of the operator. MindSpore AKG is an operator development and compilation framework based on TVM(Tensor Virtual Machine) and Polyhedral technology, it supports multiple types of operator DSL, such as Hybrid, IR builder and TVM compute.</p>
<p>Operator output shape and data type inference can be realized by defining Python functions to describe the inference logic.</p>
<p>If the operator has attributes or only supports specific input and output data types or data formats, the operator information needs to be registered. For the creation of operator information, please refer to <a class="reference internal" href="#registering-the-operator-information"><span class="std std-doc">Registering the Operator Information</span></a>. If the operator information is not registered, then the operator information will be derived from the inputs of the current operator during operator selection process.</p>
<p>Takes test_custom_akg.py as an example to introduce how to define a custom operator of akg type, where the custom operator implements the function of adding two input tensors.</p>
<p>Here is the content of test_custom_akg.py:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">)</span>

<span class="c1"># Operator implementation, Hybrid DSL</span>
<span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i0</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="k">for</span> <span class="n">i1</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">c</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">c</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="c1"># Define a custom operator of akg type</span>
    <span class="n">op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="n">add</span><span class="p">,</span> <span class="n">out_shape</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">func_type</span><span class="o">=</span><span class="s2">&quot;akg&quot;</span><span class="p">)</span>

    <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x0</span><span class="p">),</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">x1</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<p>The following points need to be explained in this example:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">context.set_context(device_target=&quot;GPU&quot;)</span></code> indicates that the operator runs on the GPU platform. To run on the Ascend platform, please compile an Ascend version of MindSpore and set the value of device_target to “Ascend”.</p></li>
<li><p>Use Python lambda functions to infer the output shape and data type, and pass them to the <code class="docutils literal notranslate"><span class="pre">out_shape</span></code> and <code class="docutils literal notranslate"><span class="pre">out_dtype</span></code> parameters of the <code class="docutils literal notranslate"><span class="pre">Custom</span></code> primitive. In this example, the lambda function indicates that the output shape and data type are the same as the information of the first input tensor.</p></li>
<li><p>The operator information is not registered, so the operator information of the custom operator will be inferred from the inputs.</p></li>
</ul>
<p>Running case:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>test_custom_akg.py
</pre></div>
</div>
<p>Running results:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[2. 2.]
 [4. 4.]]
</pre></div>
</div>
</section>
<section id="defining-custom-operator-of-tbe-type">
<h3>Defining Custom Operator of tbe Type<a class="headerlink" href="#defining-custom-operator-of-tbe-type" title="Permalink to this headline"></a></h3>
<p>The custom operator of tbe type uses the TBE(Tensor Boost Engine) operator DSL to describe the internal calculation logic of the operator. You can refer the <a class="reference external" href="https://support.huaweicloud.com/odevg-A800_3000_3010/atlaste_10_0063.html">TBE document</a> for the implementation details.</p>
<p>Operator output shape and data type inference can be realized by defining Python functions to describe the inference logic.</p>
<p>Operator information needs to be registered. For the creation of operator information, please refer to <a class="reference internal" href="#registering-the-operator-information"><span class="std std-doc">Registering the Operator Information</span></a>.</p>
<p>Takes test_custom_tbe.py as an example to introduce how to define a custom operator of tbe type, where the custom operator implements the function of adding two input tensors.</p>
<p>Here is the content of test_custom_tbe.py:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">DataType</span><span class="p">,</span> <span class="n">CustomRegOp</span><span class="p">,</span> <span class="n">custom_info_register</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;Ascend&quot;</span><span class="p">)</span>

<span class="c1"># Operator implementation, and operator information registration</span>
<span class="nd">@custom_info_register</span><span class="p">(</span><span class="n">CustomRegOp</span><span class="p">()</span> \
                      <span class="o">.</span><span class="n">input</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">)</span> \
                      <span class="o">.</span><span class="n">input</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">)</span> \
                      <span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;output&quot;</span><span class="p">)</span> \
                      <span class="o">.</span><span class="n">dtype_format</span><span class="p">(</span><span class="n">DataType</span><span class="o">.</span><span class="n">F16_Default</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">F16_Default</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">F16_Default</span><span class="p">)</span> \
                      <span class="o">.</span><span class="n">dtype_format</span><span class="p">(</span><span class="n">DataType</span><span class="o">.</span><span class="n">F32_Default</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">F32_Default</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">F32_Default</span><span class="p">)</span> \
                      <span class="o">.</span><span class="n">target</span><span class="p">(</span><span class="s2">&quot;Ascend&quot;</span><span class="p">)</span> \
                      <span class="o">.</span><span class="n">get_op_info</span><span class="p">())</span>
<span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">kernel_name</span><span class="o">=</span><span class="s2">&quot;add&quot;</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">te.lang.cce</span>
    <span class="kn">from</span> <span class="nn">te</span> <span class="kn">import</span> <span class="n">tvm</span>
    <span class="n">data0</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;shape&quot;</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;data0&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;dtype&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>
    <span class="n">data1</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;shape&quot;</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;data1&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">b</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;dtype&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">lang</span><span class="o">.</span><span class="n">cce</span><span class="o">.</span><span class="n">vadd</span><span class="p">(</span><span class="n">data0</span><span class="p">,</span> <span class="n">data1</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">tvm</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">cce</span><span class="p">():</span>
        <span class="n">sch</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">lang</span><span class="o">.</span><span class="n">cce</span><span class="o">.</span><span class="n">auto_schedule</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
    <span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;print_ir&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="n">kernel_name</span><span class="p">,</span> <span class="s2">&quot;tensor_list&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">data0</span><span class="p">,</span> <span class="n">data1</span><span class="p">,</span> <span class="n">res</span><span class="p">]}</span>
    <span class="n">te</span><span class="o">.</span><span class="n">lang</span><span class="o">.</span><span class="n">cce</span><span class="o">.</span><span class="n">cce_build_code</span><span class="p">(</span><span class="n">sch</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="c1"># Define a custom operator of tbe type</span>
    <span class="n">op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="n">add</span><span class="p">,</span> <span class="n">out_shape</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">func_type</span><span class="o">=</span><span class="s2">&quot;tbe&quot;</span><span class="p">)</span>

    <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x0</span><span class="p">),</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">x1</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<p>The following points need to be explained in this example:</p>
<ul class="simple">
<li><p>Use Python lambda functions to infer the output shape and data type, and pass them to the <code class="docutils literal notranslate"><span class="pre">out_shape</span></code> and <code class="docutils literal notranslate"><span class="pre">out_dtype</span></code> parameters of the <code class="docutils literal notranslate"><span class="pre">Custom</span></code> primitive. In this example, the lambda function indicates that the output shape and data type are the same as the information of the first input tensor.</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">CustomRegOp</span></code> to create the the operator information and use <code class="docutils literal notranslate"><span class="pre">custom_info_register</span></code> decorator to register it.</p></li>
</ul>
<p>Running case:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>test_custom_tbe.py
</pre></div>
</div>
<p>Running results:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[2. 2.]
 [4. 4.]]
</pre></div>
</div>
</section>
<section id="defining-custom-operator-of-aot-type">
<h3>Defining Custom Operator of aot Type<a class="headerlink" href="#defining-custom-operator-of-aot-type" title="Permalink to this headline"></a></h3>
<p>The custom operator of aot type adopts the AOT compilation method, which requires network developers to hand-write the source code file of the operator implementation based on a specific interface, and compile the source code file into a dynamic library in advance, and then the framework will automatically call and run the function defined in the dynamic library. In terms of the development language of the operator implementation, the GPU platform supports CUDA, and the CPU platform supports C and C++. The interface specification of the operator implementation in the source file is as follows:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">extern</span><span class="w"> </span><span class="s">&quot;C&quot;</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">func_name</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">nparam</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">**</span><span class="n">params</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="o">*</span><span class="n">ndims</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="o">**</span><span class="n">shapes</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">**</span><span class="n">dtypes</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">stream</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">extra</span><span class="p">);</span>
</pre></div>
</div>
<p>Where, the function name <code class="docutils literal notranslate"><span class="pre">func_name</span></code> can be replaced with any valid function name. The return value is of type int, and 0 means normal exit, non-zero means an exception occurs. The meaning of the parameter list is as follows:</p>
<ul class="simple">
<li><p>nparam (int): The number of inputs and outputs. For example, if an operator has 2 inputs and 1 output, then the value of nparam is 3.</p></li>
<li><p>params (void **): An array of pointers, with each pointer pointing to the input or output data. For example, if an operator has 2 inputs and 1 output, then params[0] points to the first input data, params[1] points to the second input data, params[2] points to the output data.</p></li>
<li><p>ndims (int *): An array of integers, each integer represents for the dimensions of shape of input or output. For example, if params[i] is a tensor with shape [1024, 1024], then ndims[i] is 2.</p></li>
<li><p>shapes (int64_t **): An array of shapes, each element in array represents for the shape of input or output. For example, if params[i] is a tensor with shape [1024, 1024], then shapes[i][0] is 1024, shapes[i][1] is 1024.</p></li>
<li><p>dtypes (const char **): Array of data types, each element in array represents for the data type of input or output. The value of data type can be “float32”, “float16”, “float”, “float64”, “int”, “int8”, “int16”, “int32”, “int64”, “uint”, “uint8”, “uint16”, “uint32”, “uint64”, “bool”.</p></li>
<li><p>stream (void *): Stream pointer, only used in cuda file.</p></li>
<li><p>extra (void *): Used for further extension.</p></li>
</ul>
<p>Operator output shape and data type inference can be realized by defining Python functions to describe the inference logic.</p>
<p>If the operator only supports some specific input and output data types, then the operator information needs to be registered. For the creation of operator information, please refer to <a class="reference internal" href="#registering-the-operator-information"><span class="std std-doc">Registering the Operator Information</span></a>.</p>
<p>The following examples introduces the development process of aot type custom operator on GPU platform and CPU platform, where the custom operator implements the function of adding two input tensors.</p>
<section id="a-gpu-example">
<h4>A GPU Example<a class="headerlink" href="#a-gpu-example" title="Permalink to this headline"></a></h4>
<p>Use the CUDA language to write the source file add.cu for the operator implementation:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#define THREADS 1024</span>
<span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">CustomAddKernel</span><span class="p">(</span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">input1</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">input2</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">output</span><span class="p">,</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">size</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">THREADS</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">idx</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">size</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">output</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input1</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">input2</span><span class="p">[</span><span class="n">idx</span><span class="p">];</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>

<span class="k">extern</span><span class="w"> </span><span class="s">&quot;C&quot;</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">CustomAdd</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">nparam</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">**</span><span class="n">params</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="o">*</span><span class="n">ndims</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="o">**</span><span class="n">shapes</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">**</span><span class="n">dtypes</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">stream</span><span class="p">,</span>
<span class="w">                         </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">extra</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">custream</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">cudaStream_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">stream</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">nparam</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">3</span><span class="p">)</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">input1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">input2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">params</span><span class="p">[</span><span class="mi">2</span><span class="p">];</span>
<span class="w">  </span><span class="kt">size_t</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>

<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">ndims</span><span class="p">[</span><span class="mi">2</span><span class="p">];</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">size</span><span class="w"> </span><span class="o">*=</span><span class="w"> </span><span class="n">shapes</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="n">i</span><span class="p">];</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">THREADS</span><span class="p">;</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">nparam</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">strcmp</span><span class="p">(</span><span class="n">dtypes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="w"> </span><span class="s">&quot;float32&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">CustomAddKernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">n</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">THREADS</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">custream</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">input1</span><span class="p">),</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">input2</span><span class="p">),</span>
<span class="w">                                                   </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">output</span><span class="p">),</span><span class="w"> </span><span class="n">size</span><span class="p">);</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Compile add.cu into a dynamic library add.so:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>nvcc<span class="w"> </span>--shared<span class="w"> </span>-Xcompiler<span class="w"> </span>-fPIC<span class="w"> </span>-o<span class="w"> </span>add.so<span class="w"> </span>add.cu
</pre></div>
</div>
<p>Write the test case test_custom_aot.py:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="c1"># Define a custom operator of aot type</span>
    <span class="n">op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="s2">&quot;./add.so:CustomAdd&quot;</span><span class="p">,</span> <span class="n">out_shape</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">func_type</span><span class="o">=</span><span class="s2">&quot;aot&quot;</span><span class="p">)</span>

    <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x0</span><span class="p">),</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">x1</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<p>The following points need to be explained in this example:</p>
<ul class="simple">
<li><p>In this example, you need to place test_custom_aot.py and add.so in the same directory. If add.so is in another directory, you need to replace the value of the first parameter of <code class="docutils literal notranslate"><span class="pre">Custom</span></code> primitive with the absolute path of add.so.</p></li>
<li><p>Use Python lambda functions to infer the output shape and data type, and pass them to the <code class="docutils literal notranslate"><span class="pre">out_shape</span></code> and <code class="docutils literal notranslate"><span class="pre">out_dtype</span></code> parameters of the <code class="docutils literal notranslate"><span class="pre">Custom</span></code> primitive. In this example, the lambda function indicates that the output shape and data type are the same as the information of the first input tensor.</p></li>
<li><p>The operator information is not registered, so the operator information of the custom operator will be inferred from the inputs.</p></li>
</ul>
<p>Running case:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>test_custom_aot.py
</pre></div>
</div>
<p>Running results:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[2. 2.]
 [4. 4.]]
</pre></div>
</div>
</section>
<section id="a-cpu-example">
<h4>A CPU Example<a class="headerlink" href="#a-cpu-example" title="Permalink to this headline"></a></h4>
<p>Use C/C++ language to write the source file add.cc for the operator implementation:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;string.h&gt;</span>
<span class="k">using</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">decltype</span><span class="p">(</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span>
<span class="k">using</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">decltype</span><span class="p">(</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">long</span><span class="p">));</span>

<span class="k">extern</span><span class="w"> </span><span class="s">&quot;C&quot;</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">CustomAdd</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">nparam</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">**</span><span class="n">params</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="o">*</span><span class="n">ndims</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="o">**</span><span class="n">shapes</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">**</span><span class="n">dtypes</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">stream</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">extra</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">nparam</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">3</span><span class="p">)</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>
<span class="w">  </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">input1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
<span class="w">  </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">input2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span>
<span class="w">  </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="mi">2</span><span class="p">]);</span>
<span class="w">  </span><span class="kt">size_t</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">nparam</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">size</span><span class="w"> </span><span class="o">*=</span><span class="w"> </span><span class="n">shapes</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="n">i</span><span class="p">];</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">nparam</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">strcmp</span><span class="p">(</span><span class="n">dtypes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="w"> </span><span class="s">&quot;float32&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">size</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">output</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">input2</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Compile add.cc into a dynamic library add.so:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>g++<span class="w"> </span>--shared<span class="w"> </span>-fPIC<span class="w"> </span>-o<span class="w"> </span>add.so<span class="w"> </span>add.cc
</pre></div>
</div>
<p>Write the test case test_custom_aot.py:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="c1"># Define a custom operator of aot type</span>
    <span class="n">op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="s2">&quot;./add.so:CustomAdd&quot;</span><span class="p">,</span> <span class="n">out_shape</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">func_type</span><span class="o">=</span><span class="s2">&quot;aot&quot;</span><span class="p">)</span>

    <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x0</span><span class="p">),</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">x1</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<p>The following points need to be explained in this example:</p>
<ul class="simple">
<li><p>In this example, you need to place test_custom_aot.py and add.so in the same directory. If add.so is in another directory, you need to replace the value of the first parameter of <code class="docutils literal notranslate"><span class="pre">Custom</span></code> primitive with the absolute path of add.so.</p></li>
<li><p>Use Python lambda functions to infer the output shape and data type, and pass them to the <code class="docutils literal notranslate"><span class="pre">out_shape</span></code> and <code class="docutils literal notranslate"><span class="pre">out_dtype</span></code> parameters of the <code class="docutils literal notranslate"><span class="pre">Custom</span></code> primitive. In this example, the lambda function indicates that the output shape and data type are the same as the information of the first input tensor.</p></li>
<li><p>The operator information is not registered, so the operator information of the custom operator will be inferred from the inputs.</p></li>
</ul>
<p>Running case:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>test_custom_aot.py
</pre></div>
</div>
<p>Running results:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[2. 2.]
 [4. 4.]]
</pre></div>
</div>
</section>
</section>
<section id="defining-custom-operator-of-pyfunc-type">
<h3>Defining Custom Operator of pyfunc Type<a class="headerlink" href="#defining-custom-operator-of-pyfunc-type" title="Permalink to this headline"></a></h3>
<p>The custom operator of pyfunc type uses native Python syntax to define the operator implementation, which describes the internal calculation logic of the operator. The framework will automatically call this function during the network runtime.</p>
<p>Operator output shape and data type inference can be realized by defining Python functions to describe the inference logic.</p>
<p>If the operator only supports some specific input and output data types, then the operator information needs to be registered. For the creation of operator information, please refer to <a class="reference internal" href="#registering-the-operator-information"><span class="std std-doc">Registering the Operator Information</span></a>.</p>
<p>Takes test_custom_pyfunc.py as an example to introduce how to define a custom operator of pyfunc type, where the custom operator implements the function of adding two input tensors.</p>
<p>Here is the content of test_custom_pyfunc.py:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="c1"># Define a custom operator of pyfunc type</span>
    <span class="n">op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="n">add</span><span class="p">,</span> <span class="n">out_shape</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">func_type</span><span class="o">=</span><span class="s2">&quot;pyfunc&quot;</span><span class="p">)</span>

    <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x0</span><span class="p">),</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">x1</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<p>The following points need to be explained in this example:</p>
<ul class="simple">
<li><p>Use Python lambda functions to infer the output shape and data type, and pass them to the <code class="docutils literal notranslate"><span class="pre">out_shape</span></code> and <code class="docutils literal notranslate"><span class="pre">out_dtype</span></code> parameters of the <code class="docutils literal notranslate"><span class="pre">Custom</span></code> primitive. In this example, the lambda function indicates that the output shape and data type are the same as the information of the first input tensor.</p></li>
<li><p>The operator information is not registered, so the operator information of the custom operator will be inferred from the inputs.</p></li>
</ul>
<p>Running case:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>test_custom_pyfunc.py
</pre></div>
</div>
<p>Running results:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[2. 2.]
 [4. 4.]]
</pre></div>
</div>
</section>
</section>
<section id="advanced-usage">
<h2>Advanced Usage<a class="headerlink" href="#advanced-usage" title="Permalink to this headline"></a></h2>
<section id="registering-the-operator-information">
<h3>Registering the Operator Information<a class="headerlink" href="#registering-the-operator-information" title="Permalink to this headline"></a></h3>
<p>The operator information describes the supported inputs and outputs data type, the supported inputs and outputs format, attributes, and target(platform information) of the operator implementation. It is used to select and map operators at later. The operator information can be defined by using the <a class="reference external" href="https://www.mindspore.cn/docs/api/en/r1.6/api_python/ops/mindspore.ops.CustomRegOp.html#mindspore-ops-customregop">CustomRegOp</a> API, then you can use the <a class="reference external" href="https://www.mindspore.cn/docs/api/en/r1.6/api_python/ops/mindspore.ops.custom_info_register.html#mindspore-ops-custom-info-register">custom_info_register</a> decorator or just pass it to the <code class="docutils literal notranslate"><span class="pre">reg_info</span></code> parameter of <a class="reference external" href="https://www.mindspore.cn/docs/api/en/r1.6/api_python/ops/mindspore.ops.Custom.html#mindspore-ops-custom">Custom</a> primitive to bind the information to the operator implementation. The operator information will be registered to the operator information library on the MindSpore C++ side at last. The <code class="docutils literal notranslate"><span class="pre">reg_info</span></code> parameter takes higher priority than <code class="docutils literal notranslate"><span class="pre">custom_info_register</span></code> decorator.</p>
<p>The target value in operator information can be “Ascend”, “GPU” or “CPU”. Which describes the operator information on a specific target. For the same operator implementation, it may have different supported data types on different targets, so you can use the target value in operator information to differ this. The operator information on a specific target will be registered only once.</p>
<blockquote>
<div><ul class="simple">
<li><p>The numbers and sequences of the input and output information defined in the operator information must be the same as those in the parameters of the operator implementation.</p></li>
<li><p>For the custom operator of akg type, if the operator has attributes, you need to register operator information, The attribute name in the operator information must be consistent with the attribute name used in the operator implementation. For the custom operator of tbe type, you need to register operator information. For the custom operator of aot type, since the operator implementation needs to be compiled into a dynamic library in advance, the decorator will not work, and the operator information can only be passed in through the <code class="docutils literal notranslate"><span class="pre">reg_info</span></code> parameter.</p></li>
<li><p>If the custom operator only supports a specific input and output data type or data format, the operator information needs to be registered so that the data type and data format can be checked when the operator is selected in the backend. For the case where the operator information is not provided, the information will be derived from the inputs of the current operator.</p></li>
</ul>
</div></blockquote>
</section>
<section id="defining-the-bprop-function-for-operators">
<h3>Defining the bprop Function for Operators<a class="headerlink" href="#defining-the-bprop-function-for-operators" title="Permalink to this headline"></a></h3>
<p>If an operator needs to support automatic differentiation, the backpropagation(bprop) function needs to be defined first and then passed to the <code class="docutils literal notranslate"><span class="pre">bprop</span></code> parameter of <code class="docutils literal notranslate"><span class="pre">Custom</span></code> primitive. In the bprop function, you need to describe the backward computation logic that uses the forward input, forward output, and output gradients to obtain the input gradients. The backward computation logic can be composed of built-in operators or custom backward operators.</p>
<p>Note the following points when defining the bprop function:</p>
<ul class="simple">
<li><p>The input parameter sequence of the bprop function is the forward input, forward output, and output gradients. For a multi-output operator, the forward output and output gradients are provided in the form of tuples.</p></li>
<li><p>The return value of the bprop function is tuples consisting of input gradients. The sequence of elements in a tuple is the same as that of the forward input parameters. Even if there is only one input gradient, the return value must be a tuple.</p></li>
</ul>
<p>Take test_grad.py as an example to show the usage of backpropagation function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">Cell</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">)</span>

<span class="c1"># Forward computation of custom operator</span>
<span class="k">def</span> <span class="nf">square</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i0</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">y</span><span class="p">[</span><span class="n">i0</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">i0</span><span class="p">]</span> <span class="o">*</span> <span class="n">y</span><span class="p">[</span><span class="n">i0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">y</span>

<span class="c1"># Backward computation of custom operator</span>
<span class="k">def</span> <span class="nf">square_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i0</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">dx</span><span class="p">[</span><span class="n">i0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i0</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i0</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">dx</span><span class="p">[</span><span class="n">i0</span><span class="p">]</span> <span class="o">=</span> <span class="n">dx</span><span class="p">[</span><span class="n">i0</span><span class="p">]</span> <span class="o">*</span> <span class="n">dout</span><span class="p">[</span><span class="n">i0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">dx</span>

<span class="c1"># Backpropagation function</span>
<span class="k">def</span> <span class="nf">bprop</span><span class="p">():</span>
    <span class="n">op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="n">square_grad</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">func_type</span><span class="o">=</span><span class="s2">&quot;akg&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">custom_bprop</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">dx</span><span class="p">,)</span>

    <span class="k">return</span> <span class="n">custom_bprop</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Define a custom operator of akg type and provide a backpropagation function</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Custom</span><span class="p">(</span><span class="n">square</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">bprop</span><span class="o">=</span><span class="n">bprop</span><span class="p">(),</span> <span class="n">func_type</span><span class="o">=</span><span class="s2">&quot;akg&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">9.0</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">sens</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">GradOperation</span><span class="p">(</span><span class="n">sens_param</span><span class="o">=</span><span class="kc">True</span><span class="p">)(</span><span class="n">Net</span><span class="p">())(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">sens</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">dx</span><span class="p">)</span>
</pre></div>
</div>
<p>The following points need to be explained in this example:</p>
<ul class="simple">
<li><p>The backpropagation function uses a custom operator of akg type, and the operator definition and use need to be separated, that is, the custom operator is defined outside the <code class="docutils literal notranslate"><span class="pre">custom_bprop</span></code> function and used inside the <code class="docutils literal notranslate"><span class="pre">custom_bprop</span></code> function.</p></li>
</ul>
<p>Running case:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>test_grad.py
</pre></div>
</div>
<p>Running results:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[ 2.  8. 18.]
</pre></div>
</div>
<blockquote>
<div><p>More examples can be found in the MindSpore source code <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.6/tests/st/ops/graph_kernel/custom">tests/st/ops/graph_kernel/custom</a>.</p>
</div></blockquote>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="custom_operator_cpu.html" class="btn btn-neutral float-left" title="Custom Operators (CPU)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="cell.html" class="btn btn-neutral float-right" title="Cell" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>