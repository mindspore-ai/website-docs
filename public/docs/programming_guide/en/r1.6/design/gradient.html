<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>MindSpore Automatic Differentiation &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script>
      <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/training.css" type="text/css" /><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script src="../_static/js/training.js"></script><script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script><script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script><script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Distributed Training Design" href="distributed_training_design.html" />
    <link rel="prev" title="Technical White Paper" href="technical_white_paper.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture.html">Overall Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_structure.html">MindSpore API Overview</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Design</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="technical_white_paper.html">Technical White Paper</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">MindSpore Automatic Differentiation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#automatic-differentiation-overview">Automatic Differentiation Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#forward-mode-ad">Forward Mode AD</a></li>
<li class="toctree-l3"><a class="reference internal" href="#reverse-mode-ad">Reverse Mode AD</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#gradoperation">GradOperation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#gradoperation-design">GradOperation Design</a></li>
<li class="toctree-l3"><a class="reference internal" href="#gradoperation-implementation">GradOperation Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#gradoperation-example">GradOperation Example</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#forward-mode-implementation">Forward Mode Implementation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="distributed_training_design.html">Distributed Training Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindir.html">MindSpore IR (MindIR)</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.6/training_visual_design.html">Design of Visualization↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quickstart</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/tutorials/en/r1.6/linear_regression.html">Implementing Simple Linear Function Fitting↗</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/tutorials/en/r1.6/quick_start.html">Implementing an Image Classification Application↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Basic Concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dtype.html">DataType</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tensor.html">Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parameter_introduction.html">Parameter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operators.html">Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cell.html">Cell</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset_introduction.html">Dataset</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Pipeline</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset_sample.html">Quick Start of Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset.html">Loading Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pipeline.html">Processing Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset_advanced.html">Advanced Usage of Pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset_usage.html">Data Iteration</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build the Network</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../build_net.html">Constructing Single Operator Network and Multi-layer Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../initializer.html">Initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parameter.html">Network Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../control_flow.html">Using the Process Control Statement</a></li>
<li class="toctree-l1"><a class="reference internal" href="../indefinite_parameter.html">Parameter Passing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../loss.html">Loss Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../grad_operation.html">Gradient Operation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../constexpr.html">Construct Constants In the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hypermap.html">Operation Overloading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optim.html">Optimization Algorithms</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Running</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../context.html">Configuring Running Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="../run.html">Running Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../save_and_load_models.html">Save and Load Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model.html">Application of Model</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../multi_platform_inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../online_inference.html">Online Inference with Checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../offline_inference.html">Using Offline Model for Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Distributed Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../distributed_training.html">Distributed Parallel Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed_advanced.html">Distributed Parallel Advanced Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed_example.html">Distributed Parallel Usage Example</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">PyNative</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../debug_in_pynative_mode.html">Debugging in PyNative Mode</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Numpy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../numpy.html">NumPy Interfaces in MindSpore</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Function Debugging</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../read_ir_files.html">Reading IR</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/en/r1.6/debug_in_pynative_mode.html">Debugging in PyNative Mode↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dump_in_graph_mode.html">Using Dump in the Graph Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../custom_debugging_info.html">Custom Debugging Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="../incremental_operator_build.html">Incremental Operator Build</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../enable_mixed_precision.html">Enabling Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../enable_auto_tune.html">AutoTune</a></li>
<li class="toctree-l1"><a class="reference internal" href="../enable_dataset_autotune.html">Dataset AutoTune for Dataset Pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../enable_dataset_offload.html">Enabling Offload for Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apply_gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.6/performance_profiling.html">Debugging performance with Profiler↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Features</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../second_order_optimizer.html">Second Order Optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../graph_kernel_fusion.html">Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apply_quantization_aware_training.html">Quantization Aware Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../cv.html">Computer Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nlp.html">Natural Language Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hpc.html">High Performance Computing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use_on_the_cloud.html">Using MindSpore on the Cloud</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>MindSpore Automatic Differentiation</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/design/gradient.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="mindspore-automatic-differentiation">
<h1>MindSpore Automatic Differentiation<a class="headerlink" href="#mindspore-automatic-differentiation" title="Permalink to this headline"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code> <code class="docutils literal notranslate"><span class="pre">Design</span></code> <code class="docutils literal notranslate"><span class="pre">Model</span> <span class="pre">Development</span></code></p>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.6/docs/mindspore/programming_guide/source_zh_cn/design/gradient.md"><img alt="View Source On Gitee" src="https://gitee.com/mindspore/docs/raw/r1.6/resource/_static/logo_source_en.png" /></a></p>
<section id="automatic-differentiation-overview">
<h2>Automatic Differentiation Overview<a class="headerlink" href="#automatic-differentiation-overview" title="Permalink to this headline"></a></h2>
<p>Modern AI algorithm, such as deep learning, uses huge amount of data to train a model with parameters. This training process often uses loss back-propagation to update parameters. Automatic differentiation (AD) is one of the key techniques.</p>
<p>Automatic differentiation is a method between neumerical differentiation and symbolic differentiation. The key concept of AD is to divide the calculation of the computer program into a finite set with basic operations. The gradients of all the basic operations are known. After calculating the gradient of all the basic operations, AD uses chain rule to combine them and gets the final gradient.</p>
<p>The formula of chain rule is: <span class="math notranslate nohighlight">\((f\circ g)^{'}(x)=f^{'}(g(x))g^{'}(x)\)</span></p>
<p>Based on how to connect the gradient of basic components, AD can be divided into forward mode AD and reverse mode AD.</p>
<p>For example, if we define function <span class="math notranslate nohighlight">\(f\)</span>
$<span class="math notranslate nohighlight">\(y=f(x_{1},x_{2})=ln(x_{1})+x_{1}x_{2}-sin(x_{2})\)</span><span class="math notranslate nohighlight">\(and  we want to use forward mode AD to calculate \)</span>\frac{\partial y}{\partial x_{1}}<span class="math notranslate nohighlight">\( when \)</span>x_{1}=2,x_{2}=5$.</p>
<p><img alt="image" src="../_images/forward_ad.png" /></p>
<p>The calculation direction of the origin function is the same as the calculation direction of forward mode AD. The function output and the gradient can be calculated simultaneously.
When we use reverse mode AD:</p>
<p><img alt="image" src="../_images/backward_ad.png" /></p>
<p>The calculation direction of the origin function is opposite to the calculation direction of reverse mode AD. The calculation of the gradient relies on the output of the original function.
MindSpore first developed method GradOperation based on reverse mode AD and then used the GradOperation to develop forward mode AD method Jvp.</p>
<p>In order to explain the differences between forward mode AD and reverse mode AD in further. We define an origin function <span class="math notranslate nohighlight">\(F\)</span> with N inputs and M outputs:
$<span class="math notranslate nohighlight">\( (Y_{1},Y_{2},...,Y_{M})=F(X_{1},X_{2},...,X_{N})\)</span><span class="math notranslate nohighlight">\(
The gradient of function \)</span>F<span class="math notranslate nohighlight">\( is a Jacobian matrix.
\)</span>$
\left[</p>
<div class="amsmath math notranslate nohighlight" id="equation-ba5221c6-d41b-4b79-b41e-21b3eff2a35f">
<span class="eqno">(1)<a class="headerlink" href="#equation-ba5221c6-d41b-4b79-b41e-21b3eff2a35f" title="Permalink to this equation"></a></span>\[\begin{matrix}
   \frac{\partial Y_{1}}{\partial X_{1}}&amp; ... &amp; \frac{\partial Y_{1}}{\partial X_{N}} \\
   ... &amp; ... &amp; ... \\
   \frac{\partial Y_{M}}{\partial X_{1}} &amp; ... &amp; \frac{\partial Y_{M}}{\partial X_{N}}
  \end{matrix}\]</div>
<p>\right]
$$</p>
<section id="forward-mode-ad">
<h3>Forward Mode AD<a class="headerlink" href="#forward-mode-ad" title="Permalink to this headline"></a></h3>
<p>In forward mode AD, the calculation of gradient starts from inputs. So, for each calculation, we can get the gradient of outputs with respect to one input, which is one column of the Jacobian matrix.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
 \left[
 \begin{matrix}
   \frac{\partial Y_{1}}{\partial X_{1}}\\
   ...  \\
   \frac{\partial Y_{M}}{\partial X_{1}}
  \end{matrix}
  \right]
\end{split}\]</div>
<p>In order to get this value, AD divies the program into a series of basic operations. The gradient rules of these basic operations is known. The basic operation can also be represented as a function <span class="math notranslate nohighlight">\(f\)</span> with n inputs and m outputs:</p>
<div class="math notranslate nohighlight">
\[ (y_{1},y_{2},...,y_{m})=f(x_{1},x_{2},...,x_{n})\]</div>
<p>Since we have defined the gradient rule of <span class="math notranslate nohighlight">\(f\)</span>, we know the jacobian matrix of <span class="math notranslate nohighlight">\(f\)</span>. So we can calculate the Jacobian-vector-product (Jvp) and use the chain rule to get the gradient outoput.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
 \left[
 \begin{matrix}
   \frac{\partial y_{1}}{\partial X_{i}}\\
   ...  \\
   \frac{\partial y_{m}}{\partial X_{i}}
  \end{matrix}
  \right]=\left[
 \begin{matrix}
   \frac{\partial y_{1}}{\partial x_{1}}&amp; ... &amp; \frac{\partial y_{1}}{\partial x_{n}} \\
   ... &amp; ... &amp; ... \\
   \frac{\partial y_{m}}{\partial x_{1}} &amp; ... &amp; \frac{\partial y_{M}}{\partial x_{n}}
  \end{matrix}
  \right]\left[
 \begin{matrix}
   \frac{\partial x_{1}}{\partial X_{i}}\\
   ...  \\
   \frac{\partial x_{n}}{\partial X_{i}}
  \end{matrix}
  \right]
\end{split}\]</div>
</section>
<section id="reverse-mode-ad">
<h3>Reverse Mode AD<a class="headerlink" href="#reverse-mode-ad" title="Permalink to this headline"></a></h3>
<p>In reverse mode AD, the calculation of gradient starts from outputs. So, for each calculation, we can get the gradient of one output with respect to inputs, which is one row of the Jacobian matrix.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
 \left[
 \begin{matrix}
   \frac{\partial Y_{1}}{\partial X_{1}}&amp; ... &amp; \frac{\partial Y_{1}}{\partial X_{N}} \\
  \end{matrix}
  \right]
\end{split}\]</div>
<p>In order to get this value, AD divies the program into a series of basic operations. The gradient rules of these basic operations is known. The basic operation can also be represented as a function <span class="math notranslate nohighlight">\(f\)</span> with n inputs and m outputs:</p>
<div class="math notranslate nohighlight">
\[ (y_{1},y_{2},...,y_{m})=f(x_{1},x_{2},...,x_{n})\]</div>
<p>Since we have defined the gradient rule of <span class="math notranslate nohighlight">\(f\)</span>, we know the jacobian matrix of <span class="math notranslate nohighlight">\(f\)</span>. So we can calculate the Vector-Jacobian-product (Vjp) and use the chain rule to get the gradient outoput.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
 \left[
 \begin{matrix}
   \frac{\partial Y_{j}}{\partial x_{1}}&amp; ... &amp; \frac{\partial Y_{j}}{\partial x_{N}} \\
  \end{matrix}
  \right]=\left[
 \begin{matrix}
   \frac{\partial Y_{j}}{\partial y_{1}}&amp; ... &amp; \frac{\partial Y_{j}}{\partial y_{m}} \\
  \end{matrix}
  \right]\left[
 \begin{matrix}
   \frac{\partial y_{1}}{\partial x_{1}}&amp; ... &amp; \frac{\partial y_{1}}{\partial x_{n}} \\
   ... &amp; ... &amp; ... \\
   \frac{\partial y_{m}}{\partial x_{1}} &amp; ... &amp; \frac{\partial y_{m}}{\partial x_{n}}
  \end{matrix}
  \right]
\end{split}\]</div>
</section>
</section>
<section id="gradoperation">
<h2>GradOperation<a class="headerlink" href="#gradoperation" title="Permalink to this headline"></a></h2>
<p>GradOperation uses reverse mode AD, which calcultes gradients from network outputs.</p>
<section id="gradoperation-design">
<h3>GradOperation Design<a class="headerlink" href="#gradoperation-design" title="Permalink to this headline"></a></h3>
<p>Define origin function <span class="math notranslate nohighlight">\(f(g(x, y, z))\)</span> , then:</p>
<div class="math notranslate nohighlight">
\[\frac{df}{dx}=\frac{df}{dg}\frac{dg}{dx}\frac{dx}{dx}+\frac{df}{dg}\frac{dg}{dy}\frac{dy}{dx}+\frac{df}{dg}\frac{dg}{dz}\frac{dz}{dx}\]</div>
<p>The formula of <span class="math notranslate nohighlight">\(\frac{df}{dy}\)</span> and <span class="math notranslate nohighlight">\(\frac{df}{dz}\)</span> is similar to <span class="math notranslate nohighlight">\(\frac{df}{dx}\)</span>.</p>
<p>Based on chain rule, we define gradient function <code class="docutils literal notranslate"><span class="pre">bprop:</span> <span class="pre">dout-&gt;(df,</span> <span class="pre">dinputs)</span></code> for every functions (including operators and graph). Here, <code class="docutils literal notranslate"><span class="pre">df</span></code> means gradients with respect to free variables and <code class="docutils literal notranslate"><span class="pre">dinputs</span></code> is gradients to function inputs. Then we use total derivative rule to accumulate <code class="docutils literal notranslate"><span class="pre">(df,</span> <span class="pre">dinputs)</span></code> to correspond variables.</p>
<p>MindSporeIR has developed the formulas for branching, loops and closures. So if we define the gradient rules correctly, we can get the correct gradient.
Define operator K, backward mode AD can be represented as:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>v = (func, inputs)
F(v): {
    (result, bprop) = K(func)(K(inputs))
    df, dinputs = bprop(dout)
    v.df += df
    v.dinputs += dinputs
}
</pre></div>
</div>
</section>
<section id="gradoperation-implementation">
<h3>GradOperation Implementation<a class="headerlink" href="#gradoperation-implementation" title="Permalink to this headline"></a></h3>
<p>In GradOperation process, the function that needs to calculate gradient will be taken out and used as the input of automatic differentiation module. AD module will map input function to gradient <code class="docutils literal notranslate"><span class="pre">fprop</span></code>. The output gradient has form <code class="docutils literal notranslate"><span class="pre">fprop</span> <span class="pre">=</span> <span class="pre">(forward_result,</span> <span class="pre">bprop)</span></code>. <code class="docutils literal notranslate"><span class="pre">forward_result</span></code> is the output node of the origin function. <code class="docutils literal notranslate"><span class="pre">bprop</span></code> is the gradient function which relies on the closure object of <code class="docutils literal notranslate"><span class="pre">fprop</span></code>. <code class="docutils literal notranslate"><span class="pre">bprop</span></code> has only one input <code class="docutils literal notranslate"><span class="pre">dout</span></code>. <code class="docutils literal notranslate"><span class="pre">inputs</span></code> and <code class="docutils literal notranslate"><span class="pre">outputs</span></code> are the inputs and outputs of <code class="docutils literal notranslate"><span class="pre">fprop</span></code>.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="n">MapObject</span><span class="p">();</span><span class="w"> </span><span class="c1">// Map ValueNode/Parameter/FuncGraph/Primitive object</span>
<span class="w">  </span><span class="n">MapMorphism</span><span class="p">();</span><span class="w"> </span><span class="c1">// Map CNode morphism</span>
<span class="w">  </span><span class="n">res</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">k_graph</span><span class="p">();</span><span class="w"> </span><span class="c1">// res is fprop object</span>
</pre></div>
</div>
<p>When generating gradient function object, we need to do a series of mapping from origin function to gradient function. These mapping will generate gradient function nodes and we will connect these nodes according to reverse mode AD rules. For each subgraph of origin function, we will create an <code class="docutils literal notranslate"><span class="pre">DFunctor</span></code> object. <code class="docutils literal notranslate"><span class="pre">Dfunctor</span></code> will run <code class="docutils literal notranslate"><span class="pre">MapObject</span></code> and <code class="docutils literal notranslate"><span class="pre">MapMorphism</span></code> to do the mapping.</p>
<p><code class="docutils literal notranslate"><span class="pre">MapObject</span></code> maps nodes of origin function to nodes of gradient function, including free variable nodes, parameter nodes and ValueNodes.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">MapFvObject</span><span class="p">();</span><span class="w"> </span><span class="c1">// map free vriabels</span>
<span class="n">MapParamObject</span><span class="p">();</span><span class="w"> </span><span class="c1">// map parameters</span>
<span class="n">MapValueObject</span><span class="p">();</span><span class="w"> </span><span class="c1">// map ValueNodes</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">MapFvObject</span></code> maps free variables, <code class="docutils literal notranslate"><span class="pre">MapParamObject</span></code> maps parameter nodes. <code class="docutils literal notranslate"><span class="pre">MapValueObject</span></code> mainly maps <code class="docutils literal notranslate"><span class="pre">Primitive</span></code> and <code class="docutils literal notranslate"><span class="pre">FuncGraph</span></code> objects. For <code class="docutils literal notranslate"><span class="pre">FuncGraph</span></code>, we need to create another <code class="docutils literal notranslate"><span class="pre">DFunctor</span></code> object and perform the mapping. This is a recursion process. <code class="docutils literal notranslate"><span class="pre">Primitive</span></code> defines the type of the operator. We need to define gradient function for every <code class="docutils literal notranslate"><span class="pre">Primitive</span></code>. MindSpore defines these gradient functions in Python, for example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@bprop_getters</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Sin</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">get_bprop_sin</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Grad definition for `Sin` operation.&quot;&quot;&quot;</span>
    <span class="n">cos</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cos</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">bprop</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="n">dout</span> <span class="o">*</span> <span class="n">cos</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">dx</span><span class="p">,)</span>

    <span class="k">return</span> <span class="n">bprop</span>
</pre></div>
</div>
<p>This code is the gradient function for <code class="docutils literal notranslate"><span class="pre">sin</span></code>. <code class="docutils literal notranslate"><span class="pre">x</span></code> is the input to <code class="docutils literal notranslate"><span class="pre">sin</span></code>, <code class="docutils literal notranslate"><span class="pre">y</span></code> is the output to <code class="docutils literal notranslate"><span class="pre">sin</span></code> and <code class="docutils literal notranslate"><span class="pre">dout</span></code> is the accumulated gradient.</p>
<p>After <code class="docutils literal notranslate"><span class="pre">MapObject</span></code> process, <code class="docutils literal notranslate"><span class="pre">MapMorphism</span></code> maps <code class="docutils literal notranslate"><span class="pre">CNode</span></code> morphism starting from the output of origin function and establishes the connectiontion between AD nodes.</p>
</section>
<section id="gradoperation-example">
<h3>GradOperation Example<a class="headerlink" href="#gradoperation-example" title="Permalink to this headline"></a></h3>
<p>We build a simple network and calculate the gradient according to <code class="docutils literal notranslate"><span class="pre">x</span></code>. The structure of the network is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sin</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Sin</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cos</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Cos</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
<p>Origin network structure is:</p>
<p><img alt="image" src="../_images/origin_net.png" /></p>
<p>After reverse mode AD, the network structure is:</p>
<p><img alt="image" src="../_images/backward_net.png" /></p>
</section>
</section>
<section id="forward-mode-implementation">
<h2>Forward Mode Implementation<a class="headerlink" href="#forward-mode-implementation" title="Permalink to this headline"></a></h2>
<p>Besides GradOperation, Mindspore has developed forward mode automatic differentiation method Jvp (Jacobian-Vector-Product). Compared to reverse mode AD, forward mode AD is more suitable for networks whose input dimension is smaller than output dimension. Mindspore forward mode AD is developed based on reversed mode GradOperation function.</p>
<p><img alt="image" src="../_images/Jvp.png" /></p>
<p>The network in black is the origin function. After the first derivative based on one input x, we get the network in blue. The we compute the gradient of blue network with respect to vector v and we can get the yellow network. This yellow network is the forward mode AD gradient network of black network. Since blue network is a linear network for vector v, there will be no connection between blue network and yellow network. So, all the nodes in blue are dangling nodes. We can use only blue and yellow nodes to calculate the gradient.</p>
<section id="references">
<h3>References<a class="headerlink" href="#references" title="Permalink to this headline"></a></h3>
<p>[1] Baydin, A.G. et al., 2018. Automatic differentiation in machine learning: A survey. arXiv.org. Available at: https://arxiv.org/abs/1502.05767 [Accessed September 1, 2021].</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="technical_white_paper.html" class="btn btn-neutral float-left" title="Technical White Paper" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="distributed_training_design.html" class="btn btn-neutral float-right" title="Distributed Training Design" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>