<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Distributed Parallel Training Example (GPU) &mdash; MindSpore master documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/training.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/js/training.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Saving and Loading Models in Hybrid Parallel Mode" href="save_load_model_hybrid_parallel.html" />
    <link rel="prev" title="Parallel Distributed Training Example (Ascend)" href="distributed_training_ascend.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="architecture.html">Overall Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_structure.html">MindSpore API Overview</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="design/technical_white_paper.html">Technical White Paper</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/gradient.html">MindSpore Automatic Differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/distributed_training_design.html">Distributed Training Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/mindir.html">MindSpore IR (MindIR)</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.6/training_visual_design.html">Design of Visualization↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quickstart</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/tutorials/en/r1.6/linear_regression.html">Implementing Simple Linear Function Fitting↗</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/tutorials/en/r1.6/quick_start.html">Implementing an Image Classification Application↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Basic Concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dtype.html">DataType</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor.html">Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter_introduction.html">Parameter</a></li>
<li class="toctree-l1"><a class="reference internal" href="operators.html">Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="cell.html">Cell</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset_introduction.html">Dataset</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Pipeline</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dataset_sample.html">Quick Start of Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset.html">Loading Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline.html">Processing Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset_advanced.html">Advanced Usage of Pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset_usage.html">Data Iteration</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build the Network</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="build_net.html">Constructing Single Operator Network and Multi-layer Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="initializer.html">Initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter.html">Network Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="control_flow.html">Using the Process Control Statement</a></li>
<li class="toctree-l1"><a class="reference internal" href="indefinite_parameter.html">Parameter Passing</a></li>
<li class="toctree-l1"><a class="reference internal" href="loss.html">Loss Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="grad_operation.html">Gradient Operation</a></li>
<li class="toctree-l1"><a class="reference internal" href="constexpr.html">Construct Constants In the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="hypermap.html">Operation Overloading</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">Optimization Algorithms</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Running</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="context.html">Configuring Running Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="run.html">Running Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_and_load_models.html">Save and Load Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="model.html">Application of Model</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="online_inference.html">Online Inference with Checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="offline_inference.html">Using Offline Model for Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Distributed Training</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="distributed_training.html">Distributed Parallel Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_advanced.html">Distributed Parallel Advanced Features</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="distributed_example.html">Distributed Parallel Usage Example</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="distributed_training_ascend.html">Parallel Distributed Training Example (Ascend)</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Distributed Parallel Training Example (GPU)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#preparation">Preparation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#downloading-the-dataset">Downloading the Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="#configuring-distributed-environment">Configuring Distributed Environment</a></li>
<li class="toctree-l4"><a class="reference internal" href="#calling-the-collective-communication-library">Calling the Collective Communication Library</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#defining-the-network">Defining the Network</a></li>
<li class="toctree-l3"><a class="reference internal" href="#running-the-script">Running the Script</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#single-host-training">Single-host Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="#multi-host-training">Multi-host Training</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="save_load_model_hybrid_parallel.html">Saving and Loading Models in Hybrid Parallel Mode</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">PyNative</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="debug_in_pynative_mode.html">Debugging in PyNative Mode</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Numpy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="numpy.html">NumPy Interfaces in MindSpore</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Function Debugging</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="read_ir_files.html">Reading IR</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/en/r1.6/debug_in_pynative_mode.html">Debugging in PyNative Mode↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="dump_in_graph_mode.html">Using Dump in the Graph Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_debugging_info.html">Custom Debugging Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="incremental_operator_build.html">Incremental Operator Build</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="enable_mixed_precision.html">Enabling Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_auto_tune.html">AutoTune</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_dataset_autotune.html">Dataset AutoTune for Dataset Pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_dataset_offload.html">Enabling Offload for Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.6/performance_profiling.html">Debugging performance with Profiler↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Features</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="second_order_optimizer.html">Second Order Optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="graph_kernel_fusion.html">Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_quantization_aware_training.html">Quantization Aware Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cv.html">Computer Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp.html">Natural Language Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="hpc.html">High Performance Computing</a></li>
<li class="toctree-l1"><a class="reference internal" href="use_on_the_cloud.html">Using MindSpore on the Cloud</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="distributed_example.html">Distributed Parallel Usage Example</a> &raquo;</li>
      <li>Distributed Parallel Training Example (GPU)</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/distributed_training_gpu.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section class="tex2jax_ignore mathjax_ignore" id="distributed-parallel-training-example-gpu">
<h1>Distributed Parallel Training Example (GPU)<a class="headerlink" href="#distributed-parallel-training-example-gpu" title="Permalink to this headline"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">Distributed</span> <span class="pre">Parallel</span></code> <code class="docutils literal notranslate"><span class="pre">Whole</span> <span class="pre">Process</span></code></p>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.6/docs/mindspore/programming_guide/source_en/distributed_training_gpu.md" target="_blank"><img src="https://gitee.com/mindspore/docs/raw/r1.6/resource/_static/logo_source_en.png"></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>This tutorial describes how to train the ResNet-50 network using MindSpore data parallelism and automatic parallelism on the GPU hardware platform.</p>
</section>
<section id="preparation">
<h2>Preparation<a class="headerlink" href="#preparation" title="Permalink to this headline"></a></h2>
<section id="downloading-the-dataset">
<h3>Downloading the Dataset<a class="headerlink" href="#downloading-the-dataset" title="Permalink to this headline"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">CIFAR-10</span></code> dataset is used as an example. The method of downloading and loading the dataset is the same as that for the Ascend 910 AI processor.</p>
<p>The method of downloading and loading the dataset: <a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/en/r1.6/distributed_training_ascend.html">https://www.mindspore.cn/docs/programming_guide/en/r1.6/distributed_training_ascend.html</a></p>
</section>
<section id="configuring-distributed-environment">
<h3>Configuring Distributed Environment<a class="headerlink" href="#configuring-distributed-environment" title="Permalink to this headline"></a></h3>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">OpenMPI-4.0.3</span></code>: multi-process communication library used by MindSpore.</p>
<p>Download the OpenMPI-4.0.3 source code package <code class="docutils literal notranslate"><span class="pre">openmpi-4.0.3.tar.gz</span></code> from <a class="reference external" href="https://www.open-mpi.org/software/ompi/v4.0/">https://www.open-mpi.org/software/ompi/v4.0/</a>.</p>
<p>For details about how to install OpenMPI, see the official tutorial: <a class="reference external" href="https://www.open-mpi.org/faq/?category=building#easy-build">https://www.open-mpi.org/faq/?category=building#easy-build</a>.</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">NCCL-2.7.6</span></code>: Nvidia collective communication library.</p>
<p>Download NCCL-2.7.6 from <a class="reference external" href="https://developer.nvidia.com/nccl/nccl-legacy-downloads">https://developer.nvidia.com/nccl/nccl-legacy-downloads</a>.</p>
<p>For details about how to install NCCL, see the official tutorial: <a class="reference external" href="https://docs.nvidia.com/deeplearning/nccl/install-guide/index.html#debian">https://docs.nvidia.com/deeplearning/nccl/install-guide/index.html#debian</a>.</p>
</li>
<li><p>Password-free login between hosts (required for multi-host training). If multiple hosts are involved in the training, you need to configure password-free login between them. The procedure is as follows:</p>
<ol class="arabic simple">
<li><p>Ensure that the same user is used to log in to each host. (The root user is not recommended.)</p></li>
<li><p>Run the <code class="docutils literal notranslate"><span class="pre">ssh-keygen</span> <span class="pre">-t</span> <span class="pre">rsa</span> <span class="pre">-P</span> <span class="pre">&quot;&quot;</span></code> command to generate a key.</p></li>
<li><p>Run the <code class="docutils literal notranslate"><span class="pre">ssh-copy-id</span> <span class="pre">DEVICE-IP</span></code> command to set the IP address of the host that requires password-free login.</p></li>
<li><p>Run the<code class="docutils literal notranslate"><span class="pre">ssh</span> <span class="pre">DEVICE-IP</span></code> command. If you can log in without entering the password, the configuration is successful.</p></li>
<li><p>Run the preceding command on all hosts to ensure that every two hosts can communicate with each other.</p></li>
</ol>
</li>
</ul>
</section>
<section id="calling-the-collective-communication-library">
<h3>Calling the Collective Communication Library<a class="headerlink" href="#calling-the-collective-communication-library" title="Permalink to this headline"></a></h3>
<p>On the GPU hardware platform, MindSpore parallel distributed training uses NCCL for communication.</p>
<blockquote>
<div><p>On the GPU platform, MindSpore does not support the following operations:</p>
<p><code class="docutils literal notranslate"><span class="pre">get_local_rank</span></code>, <code class="docutils literal notranslate"><span class="pre">get_local_size</span></code>, <code class="docutils literal notranslate"><span class="pre">get_world_rank_from_group_rank</span></code>, <code class="docutils literal notranslate"><span class="pre">get_group_rank_from_world_rank</span></code> and <code class="docutils literal notranslate"><span class="pre">create_group</span></code></p>
</div></blockquote>
<p>The sample code for calling the HCCL is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">)</span>
    <span class="n">init</span><span class="p">(</span><span class="s2">&quot;nccl&quot;</span><span class="p">)</span>
    <span class="o">...</span>
</pre></div>
</div>
<p>In the preceding information,</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mode=context.GRAPH_MODE</span></code>: sets the running mode to graph mode for distributed training. (The PyNative mode does not support parallel running.)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">init(&quot;nccl&quot;)</span></code>: enables NCCL communication and completes the distributed training initialization.</p></li>
</ul>
</section>
</section>
<section id="defining-the-network">
<h2>Defining the Network<a class="headerlink" href="#defining-the-network" title="Permalink to this headline"></a></h2>
<p>On the GPU hardware platform, the network definition is the same as that for the Ascend 910 AI processor.</p>
<p>For details about the definitions of the network, optimizer, and loss function, see <a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/en/r1.6/distributed_training_ascend.html">https://www.mindspore.cn/docs/programming_guide/en/r1.6/distributed_training_ascend.html</a>.</p>
</section>
<section id="running-the-script">
<h2>Running the Script<a class="headerlink" href="#running-the-script" title="Permalink to this headline"></a></h2>
<p>On the GPU hardware platform, MindSpore uses OpenMPI <code class="docutils literal notranslate"><span class="pre">mpirun</span></code> for distributed training.</p>
<section id="single-host-training">
<h3>Single-host Training<a class="headerlink" href="#single-host-training" title="Permalink to this headline"></a></h3>
<p>The following takes the distributed training script for eight devices as an example to describe how to run the script:</p>
<blockquote>
<div><p>Obtain the running script of the example from:</p>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.6/docs/sample_code/distributed_training/run_gpu.sh">https://gitee.com/mindspore/docs/blob/r1.6/docs/sample_code/distributed_training/run_gpu.sh</a></p>
<p>If the script is executed by the root user, the <code class="docutils literal notranslate"><span class="pre">--allow-run-as-root</span></code> parameter must be added to <code class="docutils literal notranslate"><span class="pre">mpirun</span></code>.</p>
</div></blockquote>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;==============================================================================================================&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Please run the script as: &quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;bash run_gpu.sh DATA_PATH&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;For example: bash run_gpu.sh /path/dataset&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;It is better to use the absolute path.&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;==============================================================================================================&quot;</span>
<span class="nv">DATA_PATH</span><span class="o">=</span><span class="nv">$1</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">DATA_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">DATA_PATH</span><span class="si">}</span>

rm<span class="w"> </span>-rf<span class="w"> </span>device
mkdir<span class="w"> </span>device
cp<span class="w"> </span>./resnet50_distributed_training.py<span class="w"> </span>./resnet.py<span class="w"> </span>./device
<span class="nb">cd</span><span class="w"> </span>./device
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;start training&quot;</span>
mpirun<span class="w"> </span>-n<span class="w"> </span><span class="m">8</span><span class="w"> </span>pytest<span class="w"> </span>-s<span class="w"> </span>-v<span class="w"> </span>./resnet50_distributed_training.py<span class="w"> </span>&gt;<span class="w"> </span>train.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span>
</pre></div>
</div>
<p>The script will run in the bachground. The log file is saved in the device directory, we will run 10 epochs and each epochs contain 234 steps, and the loss result is saved in train.log. The output loss values of the grep command are as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>epoch: 1 step: 1, loss is 2.3025854
epoch: 1 step: 1, loss is 2.3025854
epoch: 1 step: 1, loss is 2.3025854
epoch: 1 step: 1, loss is 2.3025854
epoch: 1 step: 1, loss is 2.3025854
epoch: 1 step: 1, loss is 2.3025854
epoch: 1 step: 1, loss is 2.3025854
epoch: 1 step: 1, loss is 2.3025854
</pre></div>
</div>
</section>
<section id="multi-host-training">
<h3>Multi-host Training<a class="headerlink" href="#multi-host-training" title="Permalink to this headline"></a></h3>
<p>If multiple hosts are involved in the training, you need to set the multi-host configuration in the <code class="docutils literal notranslate"><span class="pre">mpirun</span></code> command. You can use the <code class="docutils literal notranslate"><span class="pre">-H</span></code> option in the <code class="docutils literal notranslate"><span class="pre">mpirun</span></code> command. For example, <code class="docutils literal notranslate"><span class="pre">mpirun</span> <span class="pre">-n</span> <span class="pre">16</span> <span class="pre">-H</span> <span class="pre">DEVICE1_IP:8,DEVICE2_IP:8</span> <span class="pre">python</span> <span class="pre">hello.py</span></code> indicates that eight processes are started on the hosts whose IP addresses are DEVICE1_IP and DEVICE2_IP, respectively. Alternatively, you can create a hostfile similar to the following and transfer its path to the <code class="docutils literal notranslate"><span class="pre">--hostfile</span></code> option of <code class="docutils literal notranslate"><span class="pre">mpirun</span></code>. Each line in the hostfile is in the format of <code class="docutils literal notranslate"><span class="pre">[hostname]</span> <span class="pre">slots=[slotnum]</span></code>, where hostname can be an IP address or a host name.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>DEVICE1 slots=8
DEVICE2 slots=8
</pre></div>
</div>
<p>The following is the execution script of the 16-device two-host cluster. The variables <code class="docutils literal notranslate"><span class="pre">DATA_PATH</span></code> and <code class="docutils literal notranslate"><span class="pre">HOSTFILE</span></code> need to be transferred, indicating the dataset path and hostfile path. For details about more mpirun options, see the OpenMPI official website.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

<span class="nv">DATA_PATH</span><span class="o">=</span><span class="nv">$1</span>
<span class="nv">HOSTFILE</span><span class="o">=</span><span class="nv">$2</span>

rm<span class="w"> </span>-rf<span class="w"> </span>device
mkdir<span class="w"> </span>device
cp<span class="w"> </span>./resnet50_distributed_training.py<span class="w"> </span>./resnet.py<span class="w"> </span>./device
<span class="nb">cd</span><span class="w"> </span>./device
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;start training&quot;</span>
mpirun<span class="w"> </span>-n<span class="w"> </span><span class="m">16</span><span class="w"> </span>--hostfile<span class="w"> </span><span class="nv">$HOSTFILE</span><span class="w"> </span>-x<span class="w"> </span><span class="nv">DATA_PATH</span><span class="o">=</span><span class="nv">$DATA_PATH</span><span class="w"> </span>-x<span class="w"> </span>PATH<span class="w"> </span>-mca<span class="w"> </span>pml<span class="w"> </span>ob1<span class="w"> </span>pytest<span class="w"> </span>-s<span class="w"> </span>-v<span class="w"> </span>./resnet50_distributed_training.py<span class="w"> </span>&gt;<span class="w"> </span>train.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span>
</pre></div>
</div>
<p>Run running on GPU, the model parameters can be saved and loaded by referring to <a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/en/r1.6/distributed_training_ascend.html#distributed-training-model-parameters-saving-and-loading">Distributed Training Model Parameters Saving and Loading</a>.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="distributed_training_ascend.html" class="btn btn-neutral float-left" title="Parallel Distributed Training Example (Ascend)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="save_load_model_hybrid_parallel.html" class="btn btn-neutral float-right" title="Saving and Loading Models in Hybrid Parallel Mode" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>