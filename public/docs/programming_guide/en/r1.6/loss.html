

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Loss Function &mdash; MindSpore master documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/training.css" type="text/css" />
   
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script src="_static/js/training.js"></script>
        
        
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Gradient Operation" href="grad_operation.html" />
    <link rel="prev" title="Parameter Passing" href="indefinite_parameter.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Overview</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="architecture.html">Overall Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_structure.html">MindSpore API Overview</a></li>
</ul>
<p class="caption"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="design/technical_white_paper.html">Technical White Paper</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/gradient.html">MindSpore Automatic Differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/distributed_training_design.html">Distributed Training Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/mindir.html">MindSpore IR (MindIR)</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.6/training_visual_design.html">Design of Visualization↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/glossary.html">Glossary</a></li>
</ul>
<p class="caption"><span class="caption-text">Quickstart</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/tutorials/en/r1.6/linear_regression.html">Implementing Simple Linear Function Fitting↗</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/tutorials/en/r1.6/quick_start.html">Implementing an Image Classification Application↗</a></li>
</ul>
<p class="caption"><span class="caption-text">Basic Concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dtype.html">DataType</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor.html">Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter_introduction.html">Parameter</a></li>
<li class="toctree-l1"><a class="reference internal" href="operators.html">Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="cell.html">Cell</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset_introduction.html">Dataset</a></li>
</ul>
<p class="caption"><span class="caption-text">Data Pipeline</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dataset_sample.html">Quick Start of Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset.html">Loading Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline.html">Processing Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset_advanced.html">Advanced Usage of Pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset_usage.html">Data Iteration</a></li>
</ul>
<p class="caption"><span class="caption-text">Build the Network</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="build_net.html">Constructing Single Operator Network and Multi-layer Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="initializer.html">Initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter.html">Network Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="control_flow.html">Using the Process Control Statement</a></li>
<li class="toctree-l1"><a class="reference internal" href="indefinite_parameter.html">Parameter Passing</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Loss Function</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#built-in-loss-functions">Built-in Loss Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#built-in-loss-functions-application-cases">Built-in Loss Functions Application Cases</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#defining-loss-function">Defining Loss Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="#loss-function-and-model-training">Loss Function and Model Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#defining-dataset-and-network">Defining Dataset and Network</a></li>
<li class="toctree-l3"><a class="reference internal" href="#training-model">Training Model</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#multilabel-loss-function-and-model-training">Multilabel Loss Function and Model Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#defining-multilabel-dataset">Defining Multilabel Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="#defining-multilabel-loss-function">Defining Multilabel Loss Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="#training-multilabel-model">Training Multilabel Model</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="grad_operation.html">Gradient Operation</a></li>
<li class="toctree-l1"><a class="reference internal" href="constexpr.html">Construct Constants In the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="hypermap.html">Operation Overloading</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">Optimization Algorithms</a></li>
</ul>
<p class="caption"><span class="caption-text">Model Running</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="context.html">Configuring Running Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="run.html">Running Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_and_load_models.html">Save and Load Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="model.html">Application of Model</a></li>
</ul>
<p class="caption"><span class="caption-text">Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="online_inference.html">Online Inference with Checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="offline_inference.html">Using Offline Model for Inference</a></li>
</ul>
<p class="caption"><span class="caption-text">Distributed Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="distributed_training.html">Distributed Parallel Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_advanced.html">Distributed Parallel Advanced Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_example.html">Distributed Parallel Usage Example</a></li>
</ul>
<p class="caption"><span class="caption-text">PyNative</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="debug_in_pynative_mode.html">Debugging in PyNative Mode</a></li>
</ul>
<p class="caption"><span class="caption-text">Numpy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="numpy.html">NumPy Interfaces in MindSpore</a></li>
</ul>
<p class="caption"><span class="caption-text">Function Debugging</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="read_ir_files.html">Reading IR</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/en/r1.6/debug_in_pynative_mode.html">Debugging in PyNative Mode↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="dump_in_graph_mode.html">Using Dump in the Graph Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_debugging_info.html">Custom Debugging Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="incremental_operator_build.html">Incremental Operator Build</a></li>
</ul>
<p class="caption"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="enable_mixed_precision.html">Enabling Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_auto_tune.html">AutoTune</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_dataset_autotune.html">Dataset AutoTune for Dataset Pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_dataset_offload.html">Enabling Offload for Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.6/performance_profiling.html">Debugging performance with Profiler↗</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced Features</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="second_order_optimizer.html">Second Order Optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="graph_kernel_fusion.html">Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_quantization_aware_training.html">Quantization Aware Training</a></li>
</ul>
<p class="caption"><span class="caption-text">Application</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cv.html">Computer Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp.html">Natural Language Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="hpc.html">High Performance Computing</a></li>
<li class="toctree-l1"><a class="reference internal" href="use_on_the_cloud.html">Using MindSpore on the Cloud</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Loss Function</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/loss.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="loss-function">
<h1>Loss Function<a class="headerlink" href="#loss-function" title="Permalink to this headline">¶</a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code> <code class="docutils literal notranslate"><span class="pre">Model</span> <span class="pre">Development</span></code></p>
<p>Translator: <a class="reference external" href="https://gitee.com/Misaka19998">Misaka19998</a></p>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.6/docs/mindspore/programming_guide/source_en/loss.md" target="_blank"><img src="https://gitee.com/mindspore/docs/raw/r1.6/resource/_static/logo_source_en.png"></a></p>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>Loss function, also known as object function, is used for measuring the difference between predicted and true value. In deep learning, training a model is a process of decrease the loss value by iteration. So it is important to choose a loss function while training a model. A better loss function can efficiently increase model’s performance.</p>
<p>MindSpore provides many general loss functions for users. However, they are not suitable for all the situations. Users need to define their own loss functions in some cases. So this course will introduce how to define loss functions.</p>
<p>Currently, MindSpore supports the following loss functions: <code class="docutils literal notranslate"><span class="pre">L1Loss</span></code>, <code class="docutils literal notranslate"><span class="pre">MSELoss</span></code>, <code class="docutils literal notranslate"><span class="pre">SmoothL1Loss</span></code>, <code class="docutils literal notranslate"><span class="pre">SoftmaxCrossEntropyWithLogits</span></code>, <code class="docutils literal notranslate"><span class="pre">SampledSoftmaxLoss</span></code>, <code class="docutils literal notranslate"><span class="pre">BCELoss</span></code>, and <code class="docutils literal notranslate"><span class="pre">CosineEmbeddingLoss</span></code>.</p>
<p>All loss functions of MindSpore are implemented by subclasses of <code class="docutils literal notranslate"><span class="pre">Cell</span></code>. Therefore, customized loss functions are also supported. For details about how to build a loss function, see “Building a Customized Network.”</p>
<div class="section" id="built-in-loss-functions">
<h3>Built-in Loss Functions<a class="headerlink" href="#built-in-loss-functions" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p>L1Loss</p>
<p>Computes the absolute value error of two input data for the regression model. The default value of <code class="docutils literal notranslate"><span class="pre">reduction</span></code> is mean. If the value of <code class="docutils literal notranslate"><span class="pre">reduction</span></code> is sum, the loss accumulation result is returned. If the value of <code class="docutils literal notranslate"><span class="pre">reduction</span></code> is none, the result of each loss is returned.</p>
</li>
<li><p>MSELoss</p>
<p>Computes the square error of two input data for the regression model. The <code class="docutils literal notranslate"><span class="pre">reduction</span></code> parameter is the same as the <code class="docutils literal notranslate"><span class="pre">L1Loss</span></code> parameter.</p>
</li>
<li><p>SmoothL1Loss</p>
<p><code class="docutils literal notranslate"><span class="pre">SmoothL1Loss</span></code> is the smooth L1 loss function, which is used for the regression model. The default value of the <code class="docutils literal notranslate"><span class="pre">beta</span></code> threshold is 1.</p>
</li>
<li><p>SoftmaxCrossEntropyWithLogits</p>
<p>Cross entropy loss function, which is used to classify models. If the tag data is not encoded in the one-hot mode, set <code class="docutils literal notranslate"><span class="pre">sparse</span></code> to True. The default value of <code class="docutils literal notranslate"><span class="pre">reduction</span></code> is none. The meaning of this parameter is the same as that of <code class="docutils literal notranslate"><span class="pre">L1Loss</span></code>.</p>
</li>
<li><p>CosineEmbeddingLoss</p>
<p><code class="docutils literal notranslate"><span class="pre">CosineEmbeddingLoss</span></code> is used to measure the similarity between two inputs and is used for classification models. The default value of <code class="docutils literal notranslate"><span class="pre">margin</span></code> is 0.0. The <code class="docutils literal notranslate"><span class="pre">reduction</span></code> parameter is the same as the <code class="docutils literal notranslate"><span class="pre">L1Loss</span></code> parameter.</p>
</li>
<li><p>BCELoss</p>
<p>Binary cross entropy loss is used for binary classification. <code class="docutils literal notranslate"><span class="pre">weight</span></code> is a rescaling weight applied to the loss of each batch element. The default value of <code class="docutils literal notranslate"><span class="pre">weight</span></code> is None, which means the weight values are all 1. The default value of <code class="docutils literal notranslate"><span class="pre">reduction</span></code> parameter is none. The <code class="docutils literal notranslate"><span class="pre">reduction</span></code> parameter is the same as the <code class="docutils literal notranslate"><span class="pre">L1Loss</span></code> parameter.</p>
</li>
<li><p>SampledSoftmaxLoss</p>
<p>Sampled softmax loss function, which is used for classification model when the number of class is large. <code class="docutils literal notranslate"><span class="pre">num_sampled</span></code> is the number of classes to randomly sample. <code class="docutils literal notranslate"><span class="pre">num_class</span></code> is the number of possible classes. <code class="docutils literal notranslate"><span class="pre">num_true</span></code> is the number of target classes per training example. <code class="docutils literal notranslate"><span class="pre">sampled_values</span></code> is the sampled candidate. The default value of <code class="docutils literal notranslate"><span class="pre">sampled_values</span></code> is None, which means UniformCandidateSampler is applied. <code class="docutils literal notranslate"><span class="pre">remove_accidental_hits</span></code> is the switch of whether to remove “accidental hits”. The default value of <code class="docutils literal notranslate"><span class="pre">remove_accidental_hits</span></code> is True. <code class="docutils literal notranslate"><span class="pre">seed</span></code> is the random seed for candidate sampling with the default value of 0. The default value of reduction parameter is none. The <code class="docutils literal notranslate"><span class="pre">reduction</span></code> parameter is the same as the L1Loss parameter.</p>
</li>
</ul>
</div>
<div class="section" id="built-in-loss-functions-application-cases">
<h3>Built-in Loss Functions Application Cases<a class="headerlink" href="#built-in-loss-functions-application-cases" title="Permalink to this headline">¶</a></h3>
<p>All loss functions of MindSpore are stored in mindspore.nn. The usage method is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">L1Loss</span><span class="p">()</span>
<span class="n">input_data</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">target_data</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">loss</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">target_data</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>1.5
</pre></div>
</div>
<p>In this case, two pieces of tensor data are built. The <code class="docutils literal notranslate"><span class="pre">nn.L1Loss</span></code> API is used to define the loss, <code class="docutils literal notranslate"><span class="pre">input_data</span></code> and <code class="docutils literal notranslate"><span class="pre">target_data</span></code> are transferred to the loss, and the L1Loss computation is performed. The result is 1.5. If loss is set to nn.L1Loss(reduction=’sum’), the result is 9.0. If loss is set to nn.L1Loss(reduction=’none’), the result is [[1. 0. 2.] [1. 2. 3.]].</p>
</div>
</div>
<div class="section" id="defining-loss-function">
<h2>Defining Loss Function<a class="headerlink" href="#defining-loss-function" title="Permalink to this headline">¶</a></h2>
<p>Cell is the basic network module of MindSpore, and can be used to construct the network and define loss functions. The way to define a loss function is the same as defining a network. The difference is that its execution logic is used to calculate the error between the output of the forward network and the true value.</p>
<p>Taking a MindSpore loss function, L1 Loss, as an example. The way to define the loss function is as follow:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>

<span class="k">class</span> <span class="nc">L1Loss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">L1Loss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">abs</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Abs</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduce_mean</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ReduceMean</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">base</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">base</span> <span class="o">-</span> <span class="n">target</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>The needed operator will be instantiated in <code class="docutils literal notranslate"><span class="pre">__init__</span></code>method and used in <code class="docutils literal notranslate"><span class="pre">construct</span></code>. Then an L1Loss function is defined.</p>
<p>With a series of given predicted and true value, users can call the loss function to get the difference of them, as follow:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">L1Loss</span><span class="p">()</span>
<span class="n">input_data</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">target_data</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">target_data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<p>Taking <code class="docutils literal notranslate"><span class="pre">Ascend</span></code> backup as an example, the output is as follow:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>0.03333334
</pre></div>
</div>
<p>When the loss function is defined, the base class <code class="docutils literal notranslate"><span class="pre">Loss</span></code> of the loss function can also be inherited. <code class="docutils literal notranslate"><span class="pre">Loss</span></code> provides the <code class="docutils literal notranslate"><span class="pre">get_loss</span></code> method, which is used to sum or average the loss values and output a scalar. The definition of L1Loss using <code class="docutils literal notranslate"><span class="pre">Loss</span></code> as the base class is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">LossBase</span>

<span class="k">class</span> <span class="nc">L1Loss</span><span class="p">(</span><span class="n">LossBase</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">L1Loss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">abs</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Abs</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">base</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">base</span> <span class="o">-</span> <span class="n">target</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_loss</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>Firstly, we use <code class="docutils literal notranslate"><span class="pre">Loss</span></code> as the base class of L1Loss, and then add a parameter <code class="docutils literal notranslate"><span class="pre">reduction</span></code> to <code class="docutils literal notranslate"><span class="pre">__init__</span></code>, and then pass to base class by <code class="docutils literal notranslate"><span class="pre">super</span></code>. Finally we call <code class="docutils literal notranslate"><span class="pre">get_loss</span></code> method in <code class="docutils literal notranslate"><span class="pre">construct</span></code>. <code class="docutils literal notranslate"><span class="pre">reduction</span></code> has three legal parameters, <code class="docutils literal notranslate"><span class="pre">mean</span></code>, <code class="docutils literal notranslate"><span class="pre">sum</span></code> and <code class="docutils literal notranslate"><span class="pre">none</span></code>, which represent average, sum and original value.</p>
</div>
<div class="section" id="loss-function-and-model-training">
<h2>Loss Function and Model Training<a class="headerlink" href="#loss-function-and-model-training" title="Permalink to this headline">¶</a></h2>
<p>Now we train model by the defined L1Loss.</p>
<div class="section" id="defining-dataset-and-network">
<h3>Defining Dataset and Network<a class="headerlink" href="#defining-dataset-and-network" title="Permalink to this headline">¶</a></h3>
<p>Taking the simple linear function fitting as an example. The dataset and network structure is defined as follows:</p>
<blockquote>
<div><p>For a detailed introduction of linear fitting, please refer to the tutorial <a class="reference external" href="https://www.mindspore.cn/tutorials/en/r1.6/linear_regression.html">Implementing Simple Linear Function Fitting</a>.</p>
</div></blockquote>
<ol>
<li><p>Defining the Dataset</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dataset</span> <span class="k">as</span> <span class="n">ds</span>

<span class="k">def</span> <span class="nf">get_data</span><span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="n">w</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">3.0</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mf">10.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">)</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">w</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="n">noise</span>
        <span class="k">yield</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">y</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">create_dataset</span><span class="p">(</span><span class="n">num_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">):</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">GeneratorDataset</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">get_data</span><span class="p">(</span><span class="n">num_data</span><span class="p">)),</span> <span class="n">column_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span><span class="p">])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dataset</span>
</pre></div>
</div>
</li>
<li><p>Defining the Network</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore.common.initializer</span> <span class="kn">import</span> <span class="n">Normal</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="k">class</span> <span class="nc">LinearNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LinearNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Normal</span><span class="p">(</span><span class="mf">0.02</span><span class="p">),</span> <span class="n">Normal</span><span class="p">(</span><span class="mf">0.02</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="training-model">
<h3>Training Model<a class="headerlink" href="#training-model" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">Model</span></code> is a MindSpore high level API which is for training, evaluating and inferring a model. After creating a dataset and defining <code class="docutils literal notranslate"><span class="pre">Model</span></code>, we can train the model by API <code class="docutils literal notranslate"><span class="pre">train</span></code>. Then we will train the model by <code class="docutils literal notranslate"><span class="pre">Model</span></code>, and use the defined  <code class="docutils literal notranslate"><span class="pre">L1Loss</span></code> as loss function.</p>
<ol>
<li><p>Defining forward network, loss function and optimizer</p>
<p>We will use the defined <code class="docutils literal notranslate"><span class="pre">LinearNet</span></code>  and <code class="docutils literal notranslate"><span class="pre">L1Loss</span></code> as forward network and loss function, and choose MindSpore’s <code class="docutils literal notranslate"><span class="pre">Momemtum</span></code> as optimizer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># define network</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">LinearNet</span><span class="p">()</span>
<span class="c1"># define loss function</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">L1Loss</span><span class="p">()</span>
<span class="c1"># define optimizer</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.005</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Defining <code class="docutils literal notranslate"><span class="pre">Model</span></code></p>
<p>When defining <code class="docutils literal notranslate"><span class="pre">Model</span></code>, it specifies the forward network, loss function and optimizer. The <code class="docutils literal notranslate"><span class="pre">Model</span></code> will associate them internally to form a training network.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Model</span>

<span class="c1"># define Model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">opt</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Creating dataset, and calling <code class="docutils literal notranslate"><span class="pre">train</span></code> to train the model</p>
<p>When calling the train interface, you must specify the number of iterations <code class="docutils literal notranslate"><span class="pre">epoch</span></code> and the training dataset <code class="docutils literal notranslate"><span class="pre">train_dataset</span></code>. We set <code class="docutils literal notranslate"><span class="pre">epoch</span></code> to 1, and use the dataset created by <code class="docutils literal notranslate"><span class="pre">create_dataset</span></code> as the training set. <code class="docutils literal notranslate"><span class="pre">callbacks</span></code> is an optional parameter of the <code class="docutils literal notranslate"><span class="pre">train</span></code> interface. <code class="docutils literal notranslate"><span class="pre">LossMonitor</span></code> can be used in <code class="docutils literal notranslate"><span class="pre">callbacks</span></code> to monitor the change of the loss function value during the training process. <code class="docutils literal notranslate"><span class="pre">dataset_sink_mode</span></code> is also an optional parameter, here is set to False, which means to use non-sink mode for training.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore.train.callback</span> <span class="kn">import</span> <span class="n">LossMonitor</span>

<span class="c1"># create dataset</span>
<span class="n">ds_train</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">num_data</span><span class="o">=</span><span class="mi">160</span><span class="p">)</span>
<span class="c1"># training</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">epoch</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">train_dataset</span><span class="o">=</span><span class="n">ds_train</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">LossMonitor</span><span class="p">()],</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
<p>The complete code is as follows:</p>
<blockquote>
<div><p>In the following example, the parameter initialization uses random values, and the output results in specific execution may be different from the results of local execution; if you need to stabilize the output of a fixed value, you can set a fixed random seed. For the setting method, please refer to <a class="reference external" href="https://www.mindspore.cn/docs/api/en/r1.6/api_python/mindspore/mindspore.set_seed.html">mindspore.set_seed()</a>.</p>
</div></blockquote>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dataset</span> <span class="k">as</span> <span class="n">ds</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">LossBase</span>
<span class="kn">from</span> <span class="nn">mindspore.common.initializer</span> <span class="kn">import</span> <span class="n">Normal</span>
<span class="kn">from</span> <span class="nn">mindspore.train.callback</span> <span class="kn">import</span> <span class="n">LossMonitor</span>

<span class="k">class</span> <span class="nc">LinearNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LinearNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Normal</span><span class="p">(</span><span class="mf">0.02</span><span class="p">),</span> <span class="n">Normal</span><span class="p">(</span><span class="mf">0.02</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">L1Loss</span><span class="p">(</span><span class="n">LossBase</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">L1Loss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">abs</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Abs</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">base</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">base</span> <span class="o">-</span> <span class="n">target</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_loss</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_data</span><span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="n">w</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">3.0</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mf">10.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">)</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">w</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="n">noise</span>
        <span class="k">yield</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">y</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">create_dataset</span><span class="p">(</span><span class="n">num_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">):</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">GeneratorDataset</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">get_data</span><span class="p">(</span><span class="n">num_data</span><span class="p">)),</span> <span class="n">column_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span><span class="p">])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dataset</span>

<span class="c1"># define network</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">LinearNet</span><span class="p">()</span>
<span class="c1"># define loss functhon</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">L1Loss</span><span class="p">()</span>
<span class="c1"># define optimizer</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.005</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="c1"># define Model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">opt</span><span class="p">)</span>
<span class="c1"># create dataset</span>
<span class="n">ds_train</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">num_data</span><span class="o">=</span><span class="mi">160</span><span class="p">)</span>
<span class="c1"># training</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">epoch</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">train_dataset</span><span class="o">=</span><span class="n">ds_train</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">LossMonitor</span><span class="p">()],</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>The output is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>epoch: 1 step: 1, loss is 8.328788
epoch: 1 step: 2, loss is 8.594973
epoch: 1 step: 3, loss is 13.299595
epoch: 1 step: 4, loss is 9.04059
epoch: 1 step: 5, loss is 8.991402
epoch: 1 step: 6, loss is 6.5928526
epoch: 1 step: 7, loss is 8.239887
epoch: 1 step: 8, loss is 7.3984795
epoch: 1 step: 9, loss is 7.33724
epoch: 1 step: 10, loss is 4.3588376
</pre></div>
</div>
</div>
</div>
<div class="section" id="multilabel-loss-function-and-model-training">
<h2>Multilabel Loss Function and Model Training<a class="headerlink" href="#multilabel-loss-function-and-model-training" title="Permalink to this headline">¶</a></h2>
<p>In the last chapter, we defined a simple loss function <code class="docutils literal notranslate"><span class="pre">L1Loss</span></code>. Writing other loss functions is similar to <code class="docutils literal notranslate"><span class="pre">L1Loss</span></code>. However, some deep learning datasets are complex, such as the object detection network Faster R-CNN’s dataset, which has several labels rather than simple data or label. The definition and usage of loss function is different in this situation.</p>
<p>Faster R-CNN’s structure is too complex to detailed describe here. This chapter will expand the linear function fitting by creating a multilabel dataset. Then we will introduce how to define loss function and train by <code class="docutils literal notranslate"><span class="pre">Model</span></code>.</p>
<div class="section" id="defining-multilabel-dataset">
<h3>Defining Multilabel Dataset<a class="headerlink" href="#defining-multilabel-dataset" title="Permalink to this headline">¶</a></h3>
<p>Firstly we define the dataset and make a slight modification to it:</p>
<ol class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">get_multilabel_data</span></code> will output two labels,<code class="docutils literal notranslate"><span class="pre">y1</span></code> and <code class="docutils literal notranslate"><span class="pre">y2</span></code>.</p></li>
<li><p>The parameters of <code class="docutils literal notranslate"><span class="pre">column_names</span></code> of <code class="docutils literal notranslate"><span class="pre">GeneratorDataset</span></code> are [‘data’, ‘label1’, ‘label2’]</p></li>
</ol>
<p>Then <code class="docutils literal notranslate"><span class="pre">create_multilabel_dataset</span></code> will create dataset which has one <code class="docutils literal notranslate"><span class="pre">data</span></code>, and two labels <code class="docutils literal notranslate"><span class="pre">label1</span></code> and <code class="docutils literal notranslate"><span class="pre">label2</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dataset</span> <span class="k">as</span> <span class="n">ds</span>

<span class="k">def</span> <span class="nf">get_multilabel_data</span><span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="n">w</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">3.0</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mf">10.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">)</span>
        <span class="n">noise1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">noise2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">y1</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">w</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="n">noise1</span>
        <span class="n">y2</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">w</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="n">noise2</span>
        <span class="k">yield</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">y1</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">y2</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">create_multilabel_dataset</span><span class="p">(</span><span class="n">num_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">):</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">GeneratorDataset</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">get_multilabel_data</span><span class="p">(</span><span class="n">num_data</span><span class="p">)),</span> <span class="n">column_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="s1">&#39;label1&#39;</span><span class="p">,</span> <span class="s1">&#39;label2&#39;</span><span class="p">])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dataset</span>
</pre></div>
</div>
</div>
<div class="section" id="defining-multilabel-loss-function">
<h3>Defining Multilabel Loss Function<a class="headerlink" href="#defining-multilabel-loss-function" title="Permalink to this headline">¶</a></h3>
<p>We will define a loss function <code class="docutils literal notranslate"><span class="pre">L1LossForMultiLabel</span></code> according to defined multilabel dataset. The inputs of loss function’s <code class="docutils literal notranslate"><span class="pre">construct</span></code> are predicted value <code class="docutils literal notranslate"><span class="pre">base</span></code>, and true value <code class="docutils literal notranslate"><span class="pre">target1</span></code> and <code class="docutils literal notranslate"><span class="pre">target2</span></code>. We will calculate the error between predict value and <code class="docutils literal notranslate"><span class="pre">target1</span></code>, <code class="docutils literal notranslate"><span class="pre">target2</span></code> respectively, and take the average of two values as final loss. The code is as follow:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">LossBase</span>

<span class="k">class</span> <span class="nc">L1LossForMultiLabel</span><span class="p">(</span><span class="n">LossBase</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">L1LossForMultiLabel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">abs</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Abs</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">base</span><span class="p">,</span> <span class="n">target1</span><span class="p">,</span> <span class="n">target2</span><span class="p">):</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">base</span> <span class="o">-</span> <span class="n">target1</span><span class="p">)</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">base</span> <span class="o">-</span> <span class="n">target2</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_loss</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_loss</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>
</pre></div>
</div>
</div>
<div class="section" id="training-multilabel-model">
<h3>Training Multilabel Model<a class="headerlink" href="#training-multilabel-model" title="Permalink to this headline">¶</a></h3>
<p>Model will internally link the forward network, loss function and optimizer. Forward network is connected to loss function by <code class="docutils literal notranslate"><span class="pre">nn.WithLossCell</span></code>, and forward network is connected to loss function by<code class="docutils literal notranslate"><span class="pre">nn.WithLossCell</span></code> as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="k">class</span> <span class="nc">WithLossCell</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">backbone</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">WithLossCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">auto_prefix</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_backbone</span> <span class="o">=</span> <span class="n">backbone</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_loss_fn</span> <span class="o">=</span> <span class="n">loss_fn</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backbone</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
</pre></div>
</div>
<p>It should be noted that the default <code class="docutils literal notranslate"><span class="pre">nn.WithLossCell</span></code> of normal <code class="docutils literal notranslate"><span class="pre">Model</span></code> only has two inputs <code class="docutils literal notranslate"><span class="pre">data</span></code> and <code class="docutils literal notranslate"><span class="pre">label</span></code> , which is not suitable for multilabel case. Users need to connect the forward network and loss function as follows if they want to train by <code class="docutils literal notranslate"><span class="pre">Model</span></code>.</p>
<ol>
<li><p>Defining the suitable <code class="docutils literal notranslate"><span class="pre">CustomWithLossCell</span></code> in this case</p>
<p>We can copy the definition of <code class="docutils literal notranslate"><span class="pre">nn.WithLossCell</span></code> by changing the input of the <code class="docutils literal notranslate"><span class="pre">construct</span></code> to three parameters, that is, passing data to <code class="docutils literal notranslate"><span class="pre">backend</span></code>, and predicted and true value to<code class="docutils literal notranslate"><span class="pre">loss_fn</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="k">class</span> <span class="nc">CustomWithLossCell</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">backbone</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CustomWithLossCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">auto_prefix</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_backbone</span> <span class="o">=</span> <span class="n">backbone</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_loss_fn</span> <span class="o">=</span> <span class="n">loss_fn</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">label1</span><span class="p">,</span> <span class="n">label2</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backbone</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label1</span><span class="p">,</span> <span class="n">label2</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Connecting the forward network and loss function by <code class="docutils literal notranslate"><span class="pre">CustomWithLossCell</span></code></p>
<p>We use the forward network <code class="docutils literal notranslate"><span class="pre">LinearNet</span></code> defined in last chapter, and loss function <code class="docutils literal notranslate"><span class="pre">L1LossForMultiLabel</span></code>. Then connecting them by <code class="docutils literal notranslate"><span class="pre">CustomWithLossCell</span></code> as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">LinearNet</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">L1LossForMultiLabel</span><span class="p">()</span>
<span class="n">loss_net</span> <span class="o">=</span> <span class="n">CustomWithLossCell</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">loss_net</span></code> contains the logic of forward network and loss function.</p>
</li>
<li><p>Defining Model and Training</p>
<p>The <code class="docutils literal notranslate"><span class="pre">network</span></code> of <code class="docutils literal notranslate"><span class="pre">Model</span></code> is set to <code class="docutils literal notranslate"><span class="pre">loss_net</span></code>. <code class="docutils literal notranslate"><span class="pre">loss_fn</span></code> is not appointed, while the optimizer is still <code class="docutils literal notranslate"><span class="pre">Momentum</span></code>. As the user do not appoint <code class="docutils literal notranslate"><span class="pre">loss_fn</span></code>, <code class="docutils literal notranslate"><span class="pre">Model</span></code> will know that <code class="docutils literal notranslate"><span class="pre">network</span></code> has its own loss function logit. And it will not encapsulate forward network and loss function by <code class="docutils literal notranslate"><span class="pre">nn.WithLossCell</span></code>.</p>
<p>Creating multilabel dataset by <code class="docutils literal notranslate"><span class="pre">create_multilabel_dataset</span></code> and training:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore.train.callback</span> <span class="kn">import</span> <span class="n">LossMonitor</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Model</span>

<span class="n">opt</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.005</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">network</span><span class="o">=</span><span class="n">loss_net</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">)</span>
<span class="n">ds_train</span> <span class="o">=</span> <span class="n">create_multilabel_dataset</span><span class="p">(</span><span class="n">num_data</span><span class="o">=</span><span class="mi">160</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">epoch</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">train_dataset</span><span class="o">=</span><span class="n">ds_train</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">LossMonitor</span><span class="p">()],</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
<p>The complete code is as follows:</p>
<blockquote>
<div><p>In the following example, the parameter initialization uses random values, and the output results in specific execution may be different from the results of local execution; if you need to stabilize the output of a fixed value, you can set a fixed random seed. For the setting method, please refer to <a class="reference external" href="https://www.mindspore.cn/docs/api/en/r1.6/api_python/mindspore/mindspore.set_seed.html">mindspore.set_seed()</a>.</p>
</div></blockquote>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dataset</span> <span class="k">as</span> <span class="n">ds</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">LossBase</span>
<span class="kn">from</span> <span class="nn">mindspore.common.initializer</span> <span class="kn">import</span> <span class="n">Normal</span>
<span class="kn">from</span> <span class="nn">mindspore.train.callback</span> <span class="kn">import</span> <span class="n">LossMonitor</span>

<span class="k">class</span> <span class="nc">LinearNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LinearNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Normal</span><span class="p">(</span><span class="mf">0.02</span><span class="p">),</span> <span class="n">Normal</span><span class="p">(</span><span class="mf">0.02</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">L1LossForMultiLabel</span><span class="p">(</span><span class="n">LossBase</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">L1LossForMultiLabel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">abs</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Abs</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">base</span><span class="p">,</span> <span class="n">target1</span><span class="p">,</span> <span class="n">target2</span><span class="p">):</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">base</span> <span class="o">-</span> <span class="n">target1</span><span class="p">)</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">base</span> <span class="o">-</span> <span class="n">target2</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_loss</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_loss</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>

<span class="k">class</span> <span class="nc">CustomWithLossCell</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">backbone</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CustomWithLossCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">auto_prefix</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_backbone</span> <span class="o">=</span> <span class="n">backbone</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_loss_fn</span> <span class="o">=</span> <span class="n">loss_fn</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">label1</span><span class="p">,</span> <span class="n">label2</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backbone</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label1</span><span class="p">,</span> <span class="n">label2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_multilabel_data</span><span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="n">w</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">3.0</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mf">10.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">)</span>
        <span class="n">noise1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">noise2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">y1</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">w</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="n">noise1</span>
        <span class="n">y2</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">w</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="n">noise2</span>
        <span class="k">yield</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">y1</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">y2</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">create_multilabel_dataset</span><span class="p">(</span><span class="n">num_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">):</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">GeneratorDataset</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">get_multilabel_data</span><span class="p">(</span><span class="n">num_data</span><span class="p">)),</span> <span class="n">column_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="s1">&#39;label1&#39;</span><span class="p">,</span> <span class="s1">&#39;label2&#39;</span><span class="p">])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dataset</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">LinearNet</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">L1LossForMultiLabel</span><span class="p">()</span>
<span class="c1"># build loss network</span>
<span class="n">loss_net</span> <span class="o">=</span> <span class="n">CustomWithLossCell</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>

<span class="n">opt</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.005</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">network</span><span class="o">=</span><span class="n">loss_net</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">)</span>
<span class="n">ds_train</span> <span class="o">=</span> <span class="n">create_multilabel_dataset</span><span class="p">(</span><span class="n">num_data</span><span class="o">=</span><span class="mi">160</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">epoch</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">train_dataset</span><span class="o">=</span><span class="n">ds_train</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">LossMonitor</span><span class="p">()],</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>The output is as follow:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>epoch: 1 step: 1, loss is 11.039986
epoch: 1 step: 2, loss is 7.7847576
epoch: 1 step: 3, loss is 9.236277
epoch: 1 step: 4, loss is 8.3316345
epoch: 1 step: 5, loss is 6.957058
epoch: 1 step: 6, loss is 9.231144
epoch: 1 step: 7, loss is 9.1072
epoch: 1 step: 8, loss is 6.7703295
epoch: 1 step: 9, loss is 6.363703
epoch: 1 step: 10, loss is 5.014839
</pre></div>
</div>
<p>This chapter explains how to define loss function and train by <code class="docutils literal notranslate"><span class="pre">Model</span></code> in multilabel case. In some other cases, we can train the model by similar ways.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="grad_operation.html" class="btn btn-neutral float-right" title="Gradient Operation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="indefinite_parameter.html" class="btn btn-neutral float-left" title="Parameter Passing" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>