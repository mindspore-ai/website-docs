

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Inference on a GPU &mdash; MindSpore master documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/training.css" type="text/css" />
   
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script src="_static/js/training.js"></script>
        
        
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Inference on a CPU" href="multi_platform_inference_cpu.html" />
    <link rel="prev" title="Inference on the Ascend 310 AI Processor" href="multi_platform_inference_ascend_310_air.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Overview</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="architecture.html">Overall Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_structure.html">MindSpore API Overview</a></li>
</ul>
<p class="caption"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="design/technical_white_paper.html">Technical White Paper</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/gradient.html">MindSpore Automatic Differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/distributed_training_design.html">Distributed Training Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/mindir.html">MindSpore IR (MindIR)</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.6/training_visual_design.html">Design of Visualization↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/glossary.html">Glossary</a></li>
</ul>
<p class="caption"><span class="caption-text">Quickstart</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/tutorials/en/r1.6/linear_regression.html">Implementing Simple Linear Function Fitting↗</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/tutorials/en/r1.6/quick_start.html">Implementing an Image Classification Application↗</a></li>
</ul>
<p class="caption"><span class="caption-text">Basic Concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dtype.html">DataType</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor.html">Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter_introduction.html">Parameter</a></li>
<li class="toctree-l1"><a class="reference internal" href="operators.html">Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="cell.html">Cell</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset_introduction.html">Dataset</a></li>
</ul>
<p class="caption"><span class="caption-text">Data Pipeline</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dataset_sample.html">Quick Start of Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset.html">Loading Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline.html">Processing Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset_advanced.html">Advanced Usage of Pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset_usage.html">Data Iteration</a></li>
</ul>
<p class="caption"><span class="caption-text">Build the Network</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="build_net.html">Constructing Single Operator Network and Multi-layer Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="initializer.html">Initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter.html">Network Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="control_flow.html">Using the Process Control Statement</a></li>
<li class="toctree-l1"><a class="reference internal" href="indefinite_parameter.html">Parameter Passing</a></li>
<li class="toctree-l1"><a class="reference internal" href="loss.html">Loss Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="grad_operation.html">Gradient Operation</a></li>
<li class="toctree-l1"><a class="reference internal" href="constexpr.html">Construct Constants In the Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="hypermap.html">Operation Overloading</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">Optimization Algorithms</a></li>
</ul>
<p class="caption"><span class="caption-text">Model Running</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="context.html">Configuring Running Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="run.html">Running Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_and_load_models.html">Save and Load Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="model.html">Application of Model</a></li>
</ul>
<p class="caption"><span class="caption-text">Inference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference.html">Inference Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="online_inference.html">Online Inference with Checkpoint</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="offline_inference.html">Using Offline Model for Inference</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="multi_platform_inference_ascend_910.html">Inference on the Ascend 910 AI processor</a></li>
<li class="toctree-l2"><a class="reference internal" href="multi_platform_inference_ascend_310.html">Inference on Ascend 310</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Inference on a GPU</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#use-c-interface-to-load-a-mindir-file-for-inferencing">Use C++ Interface to Load a MindIR File for Inferencing</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#inference-directory-structure">Inference Directory Structure</a></li>
<li class="toctree-l4"><a class="reference internal" href="#inference-code">Inference Code</a></li>
<li class="toctree-l4"><a class="reference internal" href="#introduce-to-building-script">Introduce to Building Script</a></li>
<li class="toctree-l4"><a class="reference internal" href="#building-inference-code">Building Inference Code</a></li>
<li class="toctree-l4"><a class="reference internal" href="#performing-inference-and-viewing-the-result">Performing Inference and Viewing the Result</a></li>
<li class="toctree-l4"><a class="reference internal" href="#notices">Notices</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#inference-using-an-onnx-file">Inference Using an ONNX File</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="multi_platform_inference_cpu.html">Inference on a CPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="apply_post_training_quantization.html">Applying Post Training Quantization</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Distributed Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="distributed_training.html">Distributed Parallel Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_advanced.html">Distributed Parallel Advanced Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_example.html">Distributed Parallel Usage Example</a></li>
</ul>
<p class="caption"><span class="caption-text">PyNative</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="debug_in_pynative_mode.html">Debugging in PyNative Mode</a></li>
</ul>
<p class="caption"><span class="caption-text">Numpy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="numpy.html">NumPy Interfaces in MindSpore</a></li>
</ul>
<p class="caption"><span class="caption-text">Function Debugging</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="read_ir_files.html">Reading IR</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/en/r1.6/debug_in_pynative_mode.html">Debugging in PyNative Mode↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="dump_in_graph_mode.html">Using Dump in the Graph Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_debugging_info.html">Custom Debugging Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="incremental_operator_build.html">Incremental Operator Build</a></li>
</ul>
<p class="caption"><span class="caption-text">Performance Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="enable_mixed_precision.html">Enabling Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_auto_tune.html">AutoTune</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_dataset_autotune.html">Dataset AutoTune for Dataset Pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_dataset_offload.html">Enabling Offload for Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_gradient_accumulation.html">Gradient Accumulation Algorithm</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/r1.6/performance_profiling.html">Debugging performance with Profiler↗</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced Features</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="second_order_optimizer.html">Second Order Optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="graph_kernel_fusion.html">Graph Kernel Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_quantization_aware_training.html">Quantization Aware Training</a></li>
</ul>
<p class="caption"><span class="caption-text">Application</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cv.html">Computer Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp.html">Natural Language Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="hpc.html">High Performance Computing</a></li>
<li class="toctree-l1"><a class="reference internal" href="use_on_the_cloud.html">Using MindSpore on the Cloud</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="offline_inference.html">Using Offline Model for Inference</a> &raquo;</li>
        
      <li>Inference on a GPU</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/multi_platform_inference_gpu.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="inference-on-a-gpu">
<h1>Inference on a GPU<a class="headerlink" href="#inference-on-a-gpu" title="Permalink to this headline">¶</a></h1>
<p><code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">Inference</span> <span class="pre">Application</span></code></p>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.6/docs/mindspore/programming_guide/source_en/multi_platform_inference_gpu.md" target="_blank"><img src="https://gitee.com/mindspore/docs/raw/r1.6/resource/_static/logo_source_en.png"></a></p>
<div class="section" id="use-c-interface-to-load-a-mindir-file-for-inferencing">
<h2>Use C++ Interface to Load a MindIR File for Inferencing<a class="headerlink" href="#use-c-interface-to-load-a-mindir-file-for-inferencing" title="Permalink to this headline">¶</a></h2>
<div class="section" id="inference-directory-structure">
<h3>Inference Directory Structure<a class="headerlink" href="#inference-directory-structure" title="Permalink to this headline">¶</a></h3>
<p>Create a directory to store the inference code project, for example, <code class="docutils literal notranslate"><span class="pre">/home/mindspore_sample/gpu_resnet50_inference_sample</span></code>. You can download the <a class="reference external" href="https://gitee.com/mindspore/docs/tree/r1.6/docs/sample_code/gpu_resnet50_inference_sample">sample code</a> from the official website. The <code class="docutils literal notranslate"><span class="pre">model</span></code> directory is used to store the exported <code class="docutils literal notranslate"><span class="pre">MindIR</span></code> <a class="reference external" href="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/sample_resources/ascend310_resnet50_preprocess_sample/resnet50_imagenet.mindir">model file</a>. The directory structure of the inference code project is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>└─gpu_resnet50_inference_sample
    ├── build.sh                          // Build script
    ├── CMakeLists.txt                    // CMake script
    ├── README.md                         // Usage description
    ├── src
    │   └── main.cc                       // Main function
    └── model
        └── resnet50_imagenet.mindir      // MindIR model file
</pre></div>
</div>
</div>
<div class="section" id="inference-code">
<h3>Inference Code<a class="headerlink" href="#inference-code" title="Permalink to this headline">¶</a></h3>
<p>Namespaces that reference <code class="docutils literal notranslate"><span class="pre">mindspore</span></code>.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">using</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="p">;</span><span class="w"></span>
<span class="k">using</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Serialization</span><span class="p">;</span><span class="w"></span>
<span class="k">using</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="p">;</span><span class="w"></span>
<span class="k">using</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Status</span><span class="p">;</span><span class="w"></span>
<span class="k">using</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">ModelType</span><span class="p">;</span><span class="w"></span>
<span class="k">using</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">GraphCell</span><span class="p">;</span><span class="w"></span>
<span class="k">using</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">;</span><span class="w"></span>
<span class="k">using</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="p">;</span><span class="w"></span>
</pre></div>
</div>
<p>Initialize the environment, specify the hardware platform used for inference, and set DeviceID.</p>
<p>Set the hardware to GPU, set DeviceID to 0 and Precision Mode to “fp16”. The code example is as follows:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">gpu_device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">GPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span><span class="w"></span>
<span class="n">gpu_device_info</span><span class="o">-&gt;</span><span class="n">SetDeviceID</span><span class="p">(</span><span class="n">device_id</span><span class="p">);</span><span class="w"></span>
<span class="n">gpu_device_info</span><span class="o">-&gt;</span><span class="n">SetPrecisionMode</span><span class="p">(</span><span class="s">&quot;fp16&quot;</span><span class="p">);</span><span class="w"></span>
<span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">().</span><span class="n">push_back</span><span class="p">(</span><span class="n">gpu_device_info</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>Load the model file.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Load the MindIR model.</span>
<span class="n">mindspore</span><span class="o">::</span><span class="n">Graph</span><span class="w"> </span><span class="n">graph</span><span class="p">;</span><span class="w"></span>
<span class="n">Serialization</span><span class="o">::</span><span class="n">Load</span><span class="p">(</span><span class="n">mindir_path</span><span class="p">,</span><span class="w"> </span><span class="n">ModelType</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">graph</span><span class="p">);</span><span class="w"></span>
<span class="c1">// Build a model using a graph.</span>
<span class="n">ms</span><span class="o">::</span><span class="n">Model</span><span class="w"> </span><span class="n">model</span><span class="p">;</span><span class="w"></span>
<span class="n">model</span><span class="p">.</span><span class="n">Build</span><span class="p">(</span><span class="n">ms</span><span class="o">::</span><span class="n">GraphCell</span><span class="p">(</span><span class="n">graph</span><span class="p">),</span><span class="w"> </span><span class="n">context</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>Obtain the input information required by the model.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">ms</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">model_inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span><span class="w"></span>
</pre></div>
</div>
<p>Start inference.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Create an output vector.</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">ms</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outputs</span><span class="p">;</span><span class="w"></span>
<span class="c1">// Create an input vector.</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">ms</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">inputs</span><span class="p">;</span><span class="w"></span>
<span class="n">inputs</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">model_inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">Name</span><span class="p">(),</span><span class="w"> </span><span class="n">model_inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">DataType</span><span class="p">(),</span><span class="w"> </span><span class="n">model_inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">Shape</span><span class="p">(),</span><span class="w"></span>
<span class="w">                    </span><span class="n">image</span><span class="p">.</span><span class="n">Data</span><span class="p">().</span><span class="n">get</span><span class="p">(),</span><span class="w"> </span><span class="n">image</span><span class="p">.</span><span class="n">DataSize</span><span class="p">());</span><span class="w"></span>
<span class="c1">// Call the Predict function of the model for inference.</span>
<span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="n">Predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
</div>
<div class="section" id="introduce-to-building-script">
<h3>Introduce to Building Script<a class="headerlink" href="#introduce-to-building-script" title="Permalink to this headline">¶</a></h3>
<p>Add the header file search path for the compiler:</p>
<div class="highlight-cmake notranslate"><div class="highlight"><pre><span></span><span class="nb">option</span><span class="p">(</span><span class="s">MINDSPORE_PATH</span> <span class="s2">&quot;mindspore install path&quot;</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="nb">include_directories</span><span class="p">(</span><span class="o">${</span><span class="nv">MINDSPORE_PATH</span><span class="o">}</span><span class="p">)</span>
<span class="nb">include_directories</span><span class="p">(</span><span class="o">${</span><span class="nv">MINDSPORE_PATH</span><span class="o">}</span><span class="s">/include</span><span class="p">)</span>
</pre></div>
</div>
<p>Search for the required dynamic library in MindSpore.</p>
<div class="highlight-cmake notranslate"><div class="highlight"><pre><span></span><span class="nb">find_library</span><span class="p">(</span><span class="s">MS_LIB</span> <span class="s">libmindspore.so</span> <span class="o">${</span><span class="nv">MINDSPORE_PATH</span><span class="o">}</span><span class="s">/lib</span><span class="p">)</span>
</pre></div>
</div>
<p>Use the specified source file to generate the target executable file and link the target file to the MindSpore library.</p>
<div class="highlight-cmake notranslate"><div class="highlight"><pre><span></span><span class="nb">add_executable</span><span class="p">(</span><span class="s">main</span> <span class="s">src/main.cc</span><span class="p">)</span>
<span class="nb">target_link_libraries</span><span class="p">(</span><span class="s">main</span> <span class="o">${</span><span class="nv">MS_LIB</span><span class="o">}</span><span class="p">)</span>
</pre></div>
</div>
<blockquote>
<div><p>For details, see
<a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.6/docs/sample_code/gpu_resnet50_inference_sample/CMakeLists.txt">https://gitee.com/mindspore/docs/blob/r1.6/docs/sample_code/gpu_resnet50_inference_sample/CMakeLists.txt</a></p>
</div></blockquote>
</div>
<div class="section" id="building-inference-code">
<h3>Building Inference Code<a class="headerlink" href="#building-inference-code" title="Permalink to this headline">¶</a></h3>
<p>Go to the project directory <code class="docutils literal notranslate"><span class="pre">gpu_resnet50_inference_sample</span></code> and modify the <code class="docutils literal notranslate"><span class="pre">pip3</span></code> in the <code class="docutils literal notranslate"><span class="pre">build.sh</span></code> based on the actual situation. And then execute the building script.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash build.sh
</pre></div>
</div>
<p>After building, the executable <code class="docutils literal notranslate"><span class="pre">main</span></code> file is generated in <code class="docutils literal notranslate"><span class="pre">gpu_resnet50_inference_sample/out</span></code>.</p>
</div>
<div class="section" id="performing-inference-and-viewing-the-result">
<h3>Performing Inference and Viewing the Result<a class="headerlink" href="#performing-inference-and-viewing-the-result" title="Permalink to this headline">¶</a></h3>
<p>After completing the preceding operations, you can learn how to perform inference.</p>
<p>Log in to the GPU environment, and create the <code class="docutils literal notranslate"><span class="pre">model</span></code> directory to store the <code class="docutils literal notranslate"><span class="pre">resnet50_imagenet.mindir</span></code> file, for example, <code class="docutils literal notranslate"><span class="pre">/home/mindspore_sample/gpu_resnet50_inference_sample/model</span></code>.</p>
<p>Set the environment variable base on the actual situation, where the <code class="docutils literal notranslate"><span class="pre">TensorRT</span></code> is an optional configuration item. It is recommended to add <code class="docutils literal notranslate"><span class="pre">TensorRT</span></code> path to <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code> to improve mode inference performance.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">LD_PRELOAD</span><span class="o">=</span>/home/miniconda3/lib/libpython37m.so
<span class="nb">export</span> <span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>/usr/local/TensorRT-7.2.2.3/lib/:<span class="nv">$LD_LIBRARY_PATH</span>
</pre></div>
</div>
<p>Then, perform inference for 1000 times after 10 times warmup.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> out/
./main ../model/resnet50_imagenet.mindir <span class="m">1000</span> <span class="m">10</span>
</pre></div>
</div>
<p>In this example, we print the inference delay for per step and average step.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Start to load model..
Load model successuflly
Start to warmup..
Warmup finished
Start to infer..
step 0 cost 1.54004ms
step 1 cost 1.5271ms
... ...
step 998 cost 1.30688ms
step 999 cost 1.30493ms
infer finished.
=================Average inference time: 1.35195 ms
</pre></div>
</div>
</div>
<div class="section" id="notices">
<h3>Notices<a class="headerlink" href="#notices" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>During the training process, some networks set operator precision to FP16 artificially. For example, the <a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.6/official/nlp/bert/src/bert_model.py">Bert mode</a> set the <code class="docutils literal notranslate"><span class="pre">Dense</span></code> and <code class="docutils literal notranslate"><span class="pre">LayerNorm</span></code> to FP16:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">BertOutput</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">in_channels</span><span class="p">,</span>
                 <span class="n">out_channels</span><span class="p">,</span>
                 <span class="n">initializer_range</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span>
                 <span class="n">dropout_prob</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">compute_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BertOutput</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Set the nn.Dense to fp16.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span>
                              <span class="n">weight_init</span><span class="o">=</span><span class="n">TruncatedNormal</span><span class="p">(</span><span class="n">initializer_range</span><span class="p">))</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">compute_type</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">dropout_prob</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_prob</span> <span class="o">=</span> <span class="n">dropout_prob</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span>
        <span class="c1"># Set the nn.LayerNorm to fp16.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">((</span><span class="n">out_channels</span><span class="p">,))</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">compute_type</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cast</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>
        <span class="o">...</span> <span class="o">...</span>
</pre></div>
</div>
<p>It is recommended that export the MindIR model with fp32 precision mode before deploying inference. If you want to further improve the inference performance, you can set <code class="docutils literal notranslate"><span class="pre">precision_mode</span> <span class="pre">is</span></code> to “fp16”.</p>
<ul class="simple">
<li><p>Some inference scripts may introduce some unique network structures in the training process. For example, the model requires the image label, which are transmitted to the network output directly. It is suggested to delete this part of operators and then export MindIR model to improve inference performance.</p></li>
</ul>
</div>
</div>
<div class="section" id="inference-using-an-onnx-file">
<h2>Inference Using an ONNX File<a class="headerlink" href="#inference-using-an-onnx-file" title="Permalink to this headline">¶</a></h2>
<ol class="simple">
<li><p>Generate a model in ONNX format on the training platform. For details, see <a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/en/r1.6/save_model.html#export-onnx-model">Export ONNX Model</a>.</p></li>
<li><p>Perform inference on a GPU by referring to the runtime or SDK document. For example, use TensorRT to perform inference on the NVIDIA GPU. For details, see <a class="reference external" href="https://github.com/onnx/onnx-tensorrt">TensorRT backend for ONNX</a>.</p></li>
</ol>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="multi_platform_inference_cpu.html" class="btn btn-neutral float-right" title="Inference on a CPU" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="multi_platform_inference_ascend_310_air.html" class="btn btn-neutral float-left" title="Inference on the Ascend 310 AI Processor" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>