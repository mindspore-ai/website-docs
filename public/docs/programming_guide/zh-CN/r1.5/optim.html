<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>优化器 &mdash; MindSpore master documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="构建训练与评估网络" href="train_and_eval.html" />
    <link rel="prev" title="运算重载" href="hypermap.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">整体介绍</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="architecture.html">MindSpore总体架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_structure.html">MindSpore API概述</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">设计介绍</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="design/technical_white_paper.html">技术白皮书</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/all_scenarios_architecture.html">全场景统一架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/gradient.html">函数式可微分编程</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/dynamic_graph_and_static_graph.html">动态图和静态图</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/distributed_training_design.html">分布式训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/heterogeneous_training.html">异构并行训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/mindir.html">MindSpore IR（MindIR）</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/data_engine.html">高性能数据处理引擎</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/zh-CN/r1.5/training_visual_design.html">可视化调试调优↗</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindarmour/docs/zh-CN/r1.5/design.html">安全可信↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="design/glossary.html">术语</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">快速入门</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/tutorials/zh-CN/r1.5/linear_regression.html">实现简单线性函数拟合↗</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/tutorials/zh-CN/r1.5/quick_start.html">实现一个图片分类应用↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">基本概念</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dtype.html">DataType</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor.html">Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter_introduction.html">Parameter</a></li>
<li class="toctree-l1"><a class="reference internal" href="operators.html">算子</a></li>
<li class="toctree-l1"><a class="reference internal" href="cell.html">Cell</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset_introduction.html">Dataset</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">数据加载和处理</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dataset_sample.html">快速入门数据加载和处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset.html">数据集加载</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline.html">数据处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset_advanced.html">数据处理高级用法</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset_usage.html">数据迭代</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">网络构建</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="build_net.html">构建单算子网络和多层网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="initializer.html">Initializer初始化器</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameter.html">网络参数</a></li>
<li class="toctree-l1"><a class="reference internal" href="control_flow.html">使用流程控制语句</a></li>
<li class="toctree-l1"><a class="reference internal" href="indefinite_parameter.html">参数传递</a></li>
<li class="toctree-l1"><a class="reference internal" href="constexpr.html">网络内构造常量</a></li>
<li class="toctree-l1"><a class="reference internal" href="loss.html">损失函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="grad_operation.html">求导</a></li>
<li class="toctree-l1"><a class="reference internal" href="hypermap.html">运算重载</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">优化器</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id2">概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id3">权重配置</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#cell">使用Cell的网络权重获取函数</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id4">自定义筛选</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id5">学习率</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id6">固定学习率</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id7">动态学习率</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id8">预生成学习率列表</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id9">定义学习率计算图</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id10">权重衰减</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id11">参数分组</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id12">混合精度</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="train_and_eval.html">构建训练与评估网络</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">模型运行</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="context.html">配置运行信息</a></li>
<li class="toctree-l1"><a class="reference internal" href="run.html">运行方式</a></li>
<li class="toctree-l1"><a class="reference internal" href="ms_function.html">ms_function动静结合</a></li>
<li class="toctree-l1"><a class="reference internal" href="save_and_load_models.html">模型保存与加载</a></li>
<li class="toctree-l1"><a class="reference internal" href="model.html">Model接口应用</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">推理</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="multi_platform_inference.html">推理模型总览</a></li>
<li class="toctree-l1"><a class="reference internal" href="online_inference.html">加载Checkpoint在线推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="offline_inference.html">使用离线模型推理</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">分布式并行</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="distributed_training.html">分布式并行总览</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_advanced.html">分布式并行高级特性</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_example.html">分布式并行使用样例</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">PyNative</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="debug_in_pynative_mode.html">PyNative模式应用</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Numpy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="numpy.html">MindSpore NumPy函数</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">高级特性</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="second_order_optimizer.html">二阶优化</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_quantization_aware_training.html">应用感知量化训练</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">功能调试</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="read_ir_files.html">如何查看IR文件</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/zh-CN/r1.5/debug_in_pynative_mode.html">使用PyNative模式调试↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="dump_in_graph_mode.html">使用Dump功能在Graph模式调试</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_debugging_info.html">自定义调试信息</a></li>
<li class="toctree-l1"><a class="reference internal" href="incremental_operator_build.html">算子增量编译</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">精度调优</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/zh-CN/r1.5/accuracy_problem_preliminary_location.html">精度问题初步定位指南↗</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/zh-CN/r1.5/accuracy_optimization.html">精度问题详细定位和调优指南↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">性能优化</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="enable_mixed_precision.html">使能混合精度</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_graph_kernel_fusion.html">使能图算融合</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable_auto_tune.html">使能算子调优工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="apply_gradient_accumulation.html">应用梯度累积算法</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/zh-CN/r1.5/performance_profiling.html">使用Profiler调试性能↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">应用实践</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cv.html">机器视觉</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp.html">自然语言处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="hpc.html">高性能计算</a></li>
<li class="toctree-l1"><a class="reference internal" href="use_on_the_cloud.html">在云上使用MindSpore</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>优化器</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/optim.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="id1">
<h1>优化器<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code> <code class="docutils literal notranslate"><span class="pre">模型开发</span></code></p>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.5/docs/mindspore/programming_guide/source_zh_cn/optim.md" target="_blank"><img src="https://gitee.com/mindspore/docs/raw/r1.5/resource/_static/logo_source.png"></a></p>
<section id="id2">
<h2>概述<a class="headerlink" href="#id2" title="Permalink to this headline"></a></h2>
<p>优化器在模型训练过程中，用于计算和更新网络参数，合适的优化器可以有效减少训练时间，提高最终模型性能。最基本的优化器是梯度下降（SGD），在此基础上，很多其他的优化器进行了改进，以实现目标函数能更快速更有效地收敛到全局最优点。</p>
<p><code class="docutils literal notranslate"><span class="pre">mindspore.nn.optim</span></code>是MindSpore框架中实现各种优化算法的模块，包含常用的优化器、学习率等，接口具备较好的通用性，可以将以后更新、更复杂的方法集成到模块里。<code class="docutils literal notranslate"><span class="pre">mindspore.nn.optim</span></code>为模型提供常用的优化器，如<code class="docutils literal notranslate"><span class="pre">mindspore.nn.SGD</span></code>、<code class="docutils literal notranslate"><span class="pre">mindspore.nn.Adam</span></code>、<code class="docutils literal notranslate"><span class="pre">mindspore.nn.Ftrl</span></code>、<code class="docutils literal notranslate"><span class="pre">mindspore.nn.LazyAdam</span></code>、<code class="docutils literal notranslate"><span class="pre">mindspore.nn.Momentum</span></code>、<code class="docutils literal notranslate"><span class="pre">mindspore.nn.RMSProp</span></code>、<code class="docutils literal notranslate"><span class="pre">mindspore.nn.LARS</span></code>、<code class="docutils literal notranslate"><span class="pre">mindspore.nn.ProximalAadagrad</span></code>和<code class="docutils literal notranslate"><span class="pre">mindspore.nn.Lamb</span></code>等。同时<code class="docutils literal notranslate"><span class="pre">mindspore.nn</span></code>提供了动态学习率的模块，分为<code class="docutils literal notranslate"><span class="pre">dynamic_lr</span></code>和<code class="docutils literal notranslate"><span class="pre">learning_rate_schedule</span></code>，学习率的灵活设置可以有效支撑目标函数的收敛和模型的训练。</p>
<p>使用<code class="docutils literal notranslate"><span class="pre">mindspore.nn.optim</span></code>时，我们需要构建一个Optimizer实例。这个实例能够保持当前参数状态并基于计算得到的梯度进行参数更新。为了构建一个Optimizer，要指定需要优化的网络权重（必须是Parameter实例）的iterable，然后设置Optimizer的参数选项，比如学习率，权重衰减等。</p>
<p>以下内容分别从权重学习率、权重衰减、参数分组、混合精度等方面的配置分别进行详细介绍。</p>
</section>
<section id="id3">
<h2>权重配置<a class="headerlink" href="#id3" title="Permalink to this headline"></a></h2>
<p>在构建Optimizer实例时，通过<code class="docutils literal notranslate"><span class="pre">params</span></code>配置模型网络中要训练和更新的权重。<code class="docutils literal notranslate"><span class="pre">params</span></code>必须配置，常见的配置方法有以下两种。</p>
<section id="cell">
<h3>使用Cell的网络权重获取函数<a class="headerlink" href="#cell" title="Permalink to this headline"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">Parameter</span></code>类中包含了一个<code class="docutils literal notranslate"><span class="pre">requires_grad</span></code>的布尔型的类属性，表征了模型网络中的权重是否需要梯度来进行更新（详情可参考：<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.5/mindspore/common/parameter.py">https://gitee.com/mindspore/mindspore/blob/r1.5/mindspore/common/parameter.py</a> ）。其中大部分权重的<code class="docutils literal notranslate"><span class="pre">requires_grad</span></code>的默认值都为True；少数默认为False，例如BatchNormalize中的<code class="docutils literal notranslate"><span class="pre">moving_mean</span></code>和<code class="docutils literal notranslate"><span class="pre">moving_variance</span></code>。用户可以根据需要，自行对<code class="docutils literal notranslate"><span class="pre">requires_grad</span></code>的值进行修改。</p>
<p>MindSpore提供了<code class="docutils literal notranslate"><span class="pre">get_parameters</span></code>方法来获取模型网络中所有权重，该方法返回了<code class="docutils literal notranslate"><span class="pre">Parameter</span></code>类型的网络权重；<code class="docutils literal notranslate"><span class="pre">trainable_params</span></code>方法本质是一个filter，过滤了<code class="docutils literal notranslate"><span class="pre">requires</span> <span class="pre">grad=True</span></code>的<code class="docutils literal notranslate"><span class="pre">Parameter</span></code>。用户在构建优化器时，可以通过配置<code class="docutils literal notranslate"><span class="pre">params</span></code>为<code class="docutils literal notranslate"><span class="pre">net.trainable_params()</span></code>来指定需要优化和更新的权重。</p>
<p>代码样例如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">Model</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span>  <span class="n">Parameter</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">param</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">())</span>
</pre></div>
</div>
</section>
<section id="id4">
<h3>自定义筛选<a class="headerlink" href="#id4" title="Permalink to this headline"></a></h3>
<p>用户也可以设定筛选条件，在使用<code class="docutils literal notranslate"><span class="pre">get_parameters</span></code>获取到网络全部参数后，通过限定参数名字等方法，自定义filter来决定哪些参数需要更新。例如下面的例子，训练过程中将只对非卷积参数进行更新：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="n">params_all</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()</span>
<span class="n">no_conv_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">params_all</span><span class="p">))</span>
<span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">no_conv_params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="id5">
<h2>学习率<a class="headerlink" href="#id5" title="Permalink to this headline"></a></h2>
<p>学习率作为机器学习及深度学习中常见的超参，对目标函数能否收敛到局部最小值及何时收敛到最小值有重要作用。学习率过大容易导致目标函数波动较大，难以收敛到最优值，太小则会导致收敛过程耗时长，除了基本的固定值设置，很多动态学习率的设置方法也在深度网络的训练中取得了显著的效果。</p>
<section id="id6">
<h3>固定学习率<a class="headerlink" href="#id6" title="Permalink to this headline"></a></h3>
<p>使用固定学习率时，优化器传入的<code class="docutils literal notranslate"><span class="pre">learning_rate</span></code>为浮点类型或标量Tensor。</p>
<p>以Momentum为例，固定学习率为0.01，用法如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optim</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id7">
<h3>动态学习率<a class="headerlink" href="#id7" title="Permalink to this headline"></a></h3>
<p>模块提供了动态学习率的两种不同的实现方式，<code class="docutils literal notranslate"><span class="pre">dynamic_lr</span></code>和<code class="docutils literal notranslate"><span class="pre">learning_rate_schedule</span></code>：</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">dynamic_lr</span></code>: 预生成长度为<code class="docutils literal notranslate"><span class="pre">total_step</span></code>的学习率列表，将列表传入优化器中使用， 训练过程中， 第i步使用第i个学习率的值作为当前step的学习率，其中，<code class="docutils literal notranslate"><span class="pre">total_step</span></code>的设置值不能小于训练的总步数；</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">learning_rate_schedule</span></code>: 优化器学习率指定一个LearningRateSchedule的Cell实例，学习率会和训练网络一起组成计算图，在执行过程中，根据step计算出当前学习率。</p></li>
</ul>
<section id="id8">
<h4>预生成学习率列表<a class="headerlink" href="#id8" title="Permalink to this headline"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">mindspore.nn.dynamic_lr</span></code>模块有以下几个类，分别使用不同的数学计算方法对学习率进行计算：</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">piecewise_constant_lr</span></code>类：基于得到分段不变的学习速率。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">exponential_decay_lr</span></code>类：基于指数衰减函数计算学习率。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">natural_exp_decay_lr</span></code>类：基于自然指数衰减函数计算学习率。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">inverse_decay_lr</span></code>类：基于反时间衰减函数计算学习速率。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cosine_decay_lr</span></code>类：基于余弦衰减函数计算学习率。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">polynomial_decay_lr</span></code>类：基于多项式衰减函数计算学习率。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">warmup_lr</span></code>类：提高学习率。</p></li>
</ul>
<p>它们属于<code class="docutils literal notranslate"><span class="pre">dynamic_lr</span></code>的不同实现方式。</p>
<p>以<code class="docutils literal notranslate"><span class="pre">piecewise_constant_lr</span></code>为例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="n">milestone</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
<span class="n">learning_rates</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">]</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">dynamic_lr</span><span class="o">.</span><span class="n">piecewise_constant_lr</span><span class="p">(</span><span class="n">milestone</span><span class="p">,</span> <span class="n">learning_rates</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>
</pre></div>
</div>
<p>输出结果如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[0.1, 0.1, 0.05, 0.05, 0.05, 0.01, 0.01, 0.01, 0.01, 0.01]
</pre></div>
</div>
</section>
<section id="id9">
<h4>定义学习率计算图<a class="headerlink" href="#id9" title="Permalink to this headline"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">mindspore.nn.learning_rate_schedule</span></code>模块下有以下几个：<code class="docutils literal notranslate"><span class="pre">ExponentialDecayLR</span></code>类、<code class="docutils literal notranslate"><span class="pre">NaturalExpDecayLR</span></code>类、<code class="docutils literal notranslate"><span class="pre">InverseDecayLR</span></code>类、<code class="docutils literal notranslate"><span class="pre">CosineDecayLR</span></code>类、<code class="docutils literal notranslate"><span class="pre">PolynomialDecayLR</span></code>类和<code class="docutils literal notranslate"><span class="pre">WarmUpLR</span></code>类。它们都属于<code class="docutils literal notranslate"><span class="pre">learning_rate_schedule</span></code>，只是实现方式不同，各自含义如下：</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">ExponentialDecayLR</span></code>类：基于指数衰减函数计算学习率。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NaturalExpDecayLR</span></code>类：基于自然指数衰减函数计算学习率。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">InverseDecayLR</span></code>类：基于反时间衰减函数计算学习速率。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">CosineDecayLR</span></code>类：基于余弦衰减函数计算学习率。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">PolynomialDecayLR</span></code>类：基于多项式衰减函数计算学习率。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">WarmUpLR</span></code>类：提高学习率。</p></li>
</ul>
<p>它们属于<code class="docutils literal notranslate"><span class="pre">learning_rate_schedule</span></code>的不同实现方式。</p>
<p>例如<code class="docutils literal notranslate"><span class="pre">ExponentialDecayLR</span></code>类代码样例如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>

<span class="n">polynomial_decay_lr</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">learning_rate_schedule</span><span class="o">.</span><span class="n">PolynomialDecayLR</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                                   <span class="n">end_learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
                                   <span class="n">decay_steps</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                                   <span class="n">power</span><span class="o">=</span><span class="mf">0.5</span> <span class="p">)</span>

<span class="n">global_step</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">polynomial_decay_lr</span><span class="p">(</span><span class="n">global_step</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</pre></div>
</div>
<p>输出结果如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>0.0736396
</pre></div>
</div>
</section>
</section>
</section>
<section id="id10">
<h2>权重衰减<a class="headerlink" href="#id10" title="Permalink to this headline"></a></h2>
<p>一般情况下，weight_decay取值范围为[0, 1)，实现对（BatchNorm以外的）参数使用权重衰减的策略，以避免模型过拟合问题；weight_decay的默认值为0.0，此时不使用权重衰减策略。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id11">
<h2>参数分组<a class="headerlink" href="#id11" title="Permalink to this headline"></a></h2>
<p>优化器也支持为不同参数单独设置选项，此时不直接传入变量，而是传入一个字典的列表，每个字典定义一个参数组别的设置值，key可以为“params”，“lr”，“weight_decay”，”grad_centralizaiton”，value为对应的设定值。<code class="docutils literal notranslate"><span class="pre">params</span></code>必须配置，其余参数可以选择配置，未配置的参数项，将采用定义优化器时设置的参数值。</p>
<p>分组时，学习率可以使用固定学习率，也可以使用dynamic_lr和learningrate_schedule动态学习率。</p>
<blockquote>
<div><p>当前MindSpore除个别优化器外（例如AdaFactor，FTRL），均支持对学习率进行分组，详情参考各优化器的说明。</p>
</div></blockquote>
<p>例如下面的例子:</p>
<ul class="simple">
<li><p>conv_params组别的参数，使用固定学习率0.01， <code class="docutils literal notranslate"><span class="pre">weight_decay</span></code>为字典传入的数值0.01；</p></li>
<li><p>no_conv_params组别使用<code class="docutils literal notranslate"><span class="pre">learning_rate_schedule</span></code>的动态学习率<code class="docutils literal notranslate"><span class="pre">PolynomialDecayLR</span></code>， <code class="docutils literal notranslate"><span class="pre">weight_decay</span></code>使用优化器配置的值0.0；</p></li>
<li><p>group_params还提供了<code class="docutils literal notranslate"><span class="pre">order_params</span></code>配置项；一般情况下<code class="docutils literal notranslate"><span class="pre">order_params</span></code>无需配置。group_params根据分组情况会改变parameter和梯度的计算顺序。如果使用自动并行策略，并通过<code class="docutils literal notranslate"><span class="pre">set_all_reduce_fusion_split_indices</span></code>配置了梯度更新的切分点，group_params引起的顺序变化会影响梯度广播的并行效果，此时可以通过<code class="docutils literal notranslate"><span class="pre">order_params</span></code>指定预期的参数顺序，例如指定为<code class="docutils literal notranslate"><span class="pre">net.trainable_params()</span></code>，使参数顺序与网络中定义权重的原始顺序保持一致。</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>

<span class="c1"># Use parameter groups and set different values</span>
<span class="n">conv_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()))</span>
<span class="n">no_conv_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()))</span>

<span class="n">fix_lr</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">polynomial_decay_lr</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">learning_rate_schedule</span><span class="o">.</span><span class="n">PolynomialDecayLR</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                                   <span class="n">end_learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
                                   <span class="n">decay_steps</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                                   <span class="n">power</span><span class="o">=</span><span class="mf">0.5</span> <span class="p">)</span>

<span class="n">group_params</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">conv_params</span><span class="p">,</span> <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="n">fix_lr</span><span class="p">},</span>
                <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">no_conv_params</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="n">polynomial_decay_lr</span><span class="p">},</span>
                <span class="p">{</span><span class="s1">&#39;order_params&#39;</span><span class="p">:</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()}]</span>

<span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">group_params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id12">
<h2>混合精度<a class="headerlink" href="#id12" title="Permalink to this headline"></a></h2>
<p>深度神经网络存在使用混合精度训练的场景，这种方法通过混合使用单精度和半精度数据格式来加速网络训练，同时保持了单精度训练所能达到的网络精度。混合精度训练能够加速计算过程，减少内存使用和存取，并使得在特定的硬件上可以训练更大的模型或batch size。</p>
<p>在混合精度训练过程中，会使用float16类型来替代float32类型存储数据，但由于float16类型数据比float32类型数据范围小很多，所以当某些参数（例如梯度）在训练过程中变得很小时，就会发生数据下溢。为避免半精度float16类型数据下溢，MindSpore提供了<code class="docutils literal notranslate"><span class="pre">FixedLossScaleManager</span></code>和<code class="docutils literal notranslate"><span class="pre">DynamicLossScaleManager</span></code>方法。其主要思想是计算loss时，将loss扩大一定的倍数，由于链式法则的存在，梯度也会相应扩大，然后在优化器更新权重时再缩小相应的倍数，从而避免了数据下溢的情况又不影响计算结果。</p>
<p>一般情况下优化器不需要与<code class="docutils literal notranslate"><span class="pre">LossScale</span></code>功能配合使用，但使用<code class="docutils literal notranslate"><span class="pre">FixedLossScaleManager</span></code>，并且<code class="docutils literal notranslate"><span class="pre">drop_overflow_update</span></code>为False时，优化器需设置<code class="docutils literal notranslate"><span class="pre">loss_scale</span></code>的值，且<code class="docutils literal notranslate"><span class="pre">loss_scale</span></code>值与<code class="docutils literal notranslate"><span class="pre">FixedLossScaleManager</span></code>的相同，具体用法详见：<a class="reference external" href="https://www.mindspore.cn/docs/programming_guide/zh-CN/r1.5/lossscale.html">https://www.mindspore.cn/docs/programming_guide/zh-CN/r1.5/lossscale.html</a>。</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="hypermap.html" class="btn btn-neutral float-left" title="运算重载" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="train_and_eval.html" class="btn btn-neutral float-right" title="构建训练与评估网络" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>