<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore.context &mdash; MindSpore master 文档</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script><script src="../../_static/jquery.js"></script>
        <script src="../../_static/js/theme.js"></script><script src="../../_static/underscore.js"></script><script src="../../_static/doctools.js"></script><script src="../../_static/js/mermaid-9.3.0.js"></script><script src="../../_static/translations.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="索引" href="../../genindex.html" />
    <link rel="search" title="搜索" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">设计</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../design/overview.html">MindSpore设计概览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/programming_paradigm.html">函数式和对象式融合编程范式</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/dynamic_graph_and_static_graph.html">动静态图结合</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/distributed_training_design.html">分布式并行原生</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/data_engine.html">高性能数据处理引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/all_scenarios.html">全场景统一架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/graph_fusion_engine.html">图算融合加速引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/pluggable_device.html">三方硬件对接</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/glossary.html">术语</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">模型库</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../note/official_models.html">官方模型库</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.experimental.html">mindspore.experimental</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API映射</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../note/api_mapping/pytorch_api_mapping.html">PyTorch与MindSpore API映射表</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">迁移指南</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/overview.html">迁移指南概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/enveriment_preparation.html">环境准备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/analysis_and_preparation.html">模型分析与准备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/model_development/model_development.html">网络搭建对比</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/debug_and_tune.html">调试调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/sample_code.html">网络迁移调试实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/faq.html">常见问题</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">语法支持</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../note/static_graph_syntax_support.html">静态图语法支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/static_graph_syntax/operators.html">静态图语法-运算符</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/static_graph_syntax/statements.html">静态图语法-Python语句</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/static_graph_syntax/python_builtin_functions.html">静态图语法-Python内置函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/index_support.html">Tensor索引支持</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">环境变量</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../note/env_var_list.html">环境变量</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../faq/installation.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/data_processing.html">数据处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/implement_problem.html">执行问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/network_compilation.html">网络编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/operators_compile.html">算子编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/performance_tuning.html">性能调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/precision_tuning.html">精度调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/distributed_parallel.html">分布式并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/inference.html">推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/feature_advice.html">特性咨询</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../index.html">模块代码</a> &raquo;</li>
      <li>mindspore.context</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>mindspore.context 源代码</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2020-2023 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">The context of mindspore, used to configure the current execution environment,</span>
<span class="sd">includes the execution mode, execution backend and other feature switches.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span>

<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">threading</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">namedtuple</span>
<span class="kn">from</span> <span class="nn">types</span> <span class="kn">import</span> <span class="n">FunctionType</span>

<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">log</span> <span class="k">as</span> <span class="n">logger</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">MSContext</span><span class="p">,</span> <span class="n">ms_ctx_param</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">_checkparam</span> <span class="k">as</span> <span class="n">Validator</span>
<span class="kn">from</span> <span class="nn">mindspore._checkparam</span> <span class="kn">import</span> <span class="n">args_type_check</span>
<span class="kn">from</span> <span class="nn">mindspore.parallel._auto_parallel_context</span> <span class="kn">import</span> <span class="n">_set_auto_parallel_context</span><span class="p">,</span> <span class="n">_get_auto_parallel_context</span><span class="p">,</span> \
    <span class="n">_reset_auto_parallel_context</span>
<span class="kn">from</span> <span class="nn">mindspore.parallel._ps_context</span> <span class="kn">import</span> <span class="n">_set_ps_context</span><span class="p">,</span> <span class="n">_get_ps_context</span><span class="p">,</span> <span class="n">_reset_ps_context</span><span class="p">,</span> \
    <span class="n">_need_reset_device_target_for_ps</span>
<span class="kn">from</span> <span class="nn">mindspore.parallel._offload_context</span> <span class="kn">import</span> <span class="n">_set_offload_context</span><span class="p">,</span> <span class="n">_get_offload_context</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;GRAPH_MODE&#39;</span><span class="p">,</span> <span class="s1">&#39;PYNATIVE_MODE&#39;</span><span class="p">,</span> <span class="s1">&#39;STRICT&#39;</span><span class="p">,</span> <span class="s1">&#39;COMPATIBLE&#39;</span><span class="p">,</span> <span class="s1">&#39;LAX&#39;</span><span class="p">,</span> <span class="s1">&#39;set_context&#39;</span><span class="p">,</span> <span class="s1">&#39;get_context&#39;</span><span class="p">,</span>
           <span class="s1">&#39;set_auto_parallel_context&#39;</span><span class="p">,</span> <span class="s1">&#39;get_auto_parallel_context&#39;</span><span class="p">,</span> <span class="s1">&#39;reset_auto_parallel_context&#39;</span><span class="p">,</span> <span class="s1">&#39;ParallelMode&#39;</span><span class="p">,</span>
           <span class="s1">&#39;set_ps_context&#39;</span><span class="p">,</span> <span class="s1">&#39;get_ps_context&#39;</span><span class="p">,</span> <span class="s1">&#39;reset_ps_context&#39;</span><span class="p">,</span> <span class="s1">&#39;set_offload_context&#39;</span><span class="p">,</span> <span class="s1">&#39;get_offload_context&#39;</span><span class="p">]</span>

<span class="n">GRAPH_MODE</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">PYNATIVE_MODE</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">_DEVICE_APP_MEMORY_SIZE</span> <span class="o">=</span> <span class="mi">31</span>  <span class="c1"># The max memory size of graph plus variable.</span>
<span class="n">_RE_PATTERN</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;[1-9][0-9]*(\.)?[0-9]*GB|0\.[0-9]*GB&#39;</span>
<span class="n">K_CONTEXT</span> <span class="o">=</span> <span class="kc">None</span>

<span class="c1"># Enumerate for the property &#39;jit_syntax_level&#39;.</span>
<span class="n">STRICT</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">COMPATIBLE</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">LAX</span> <span class="o">=</span> <span class="mi">2</span>


<span class="k">def</span> <span class="nf">_make_directory</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Make directory.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">path</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">or</span> <span class="n">path</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;&quot;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.set_context&#39;, the &#39;save_graphs_path&#39; or the &#39;print_file_path&#39; is invalid &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;type, it should be Non-empty string, but got &#39;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">&#39;.&quot;</span><span class="p">)</span>

    <span class="n">path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">realpath</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;The absolute path is </span><span class="si">%r</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;The directory(</span><span class="si">%s</span><span class="s2">) doesn&#39;t exist, will create it&quot;</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">FileExistsError</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;The directory(</span><span class="si">%s</span><span class="s2">) already exist.&quot;</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">PermissionError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">critical</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;No write permission on the directory &#39;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">&#39;&#39;, error = </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="fm">__str__</span><span class="p">()</span> <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">No write permission on the directory &#39;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">&#39;.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">path</span>


<span class="k">def</span> <span class="nf">_get_print_file_name</span><span class="p">(</span><span class="n">file_name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Add timestamp suffix to file name. Rename the file name:  file_name + &quot;.&quot; + time(seconds).&quot;&quot;&quot;</span>
    <span class="n">time_second</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()))</span>
    <span class="n">file_name</span> <span class="o">=</span> <span class="n">file_name</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span> <span class="o">+</span> <span class="n">time_second</span>
    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">file_name</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;print_file_path&#39; </span><span class="si">{}</span><span class="s2"> already exists, &quot;</span>
                         <span class="s2">&quot;please check it&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">file_name</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">file_name</span>


<span class="k">class</span> <span class="nc">_ThreadLocalInfo</span><span class="p">(</span><span class="n">threading</span><span class="o">.</span><span class="n">local</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Thread local Info used for store thread local attributes.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">_ThreadLocalInfo</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reserve_class_name_in_scope</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">debug_runtime</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">reserve_class_name_in_scope</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get whether to save the network class name in the scope.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reserve_class_name_in_scope</span>

    <span class="nd">@reserve_class_name_in_scope</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">reserve_class_name_in_scope</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reserve_class_name_in_scope</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Set whether to save the network class name in the scope.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reserve_class_name_in_scope</span> <span class="o">=</span> <span class="n">reserve_class_name_in_scope</span>


<span class="n">_ContextRecord</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span>
    <span class="s2">&quot;_ContextRecord&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;is_pynative_mode&quot;</span><span class="p">,</span> <span class="s2">&quot;switch_context_fn&quot;</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">_ContextSwitchInfo</span><span class="p">(</span><span class="n">threading</span><span class="o">.</span><span class="n">local</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Record of context switch information.</span>

<span class="sd">    Args:</span>
<span class="sd">        is_pynative (bool): Whether to adopt the PyNative mode.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_pynative</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">_ContextSwitchInfo</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context_stack</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">is_pynative</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">push</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">push</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_pynative</span><span class="p">,</span> <span class="n">switch_context_fn</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Push a context switch record onto the stack.</span>

<span class="sd">        Args:</span>
<span class="sd">            is_pynative (bool): Whether context switch to PyNative mode.</span>
<span class="sd">            switch_context_fn (Function): A callable that executes the context switch.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">switch_context_fn</span><span class="p">,</span> <span class="n">FunctionType</span><span class="p">):</span>
            <span class="n">switch_context_fn</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context_stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">_ContextRecord</span><span class="p">(</span><span class="n">is_pynative</span><span class="p">,</span> <span class="n">switch_context_fn</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">pop</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context_stack</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">_Context</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    _Context is the environment in which operations are executed</span>

<span class="sd">    Note:</span>
<span class="sd">        Create a context through instantiating Context object is not recommended.</span>
<span class="sd">        should use context() to get the context since Context is a singleton.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_instance</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">_instance_lock</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Lock</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_instance</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_instance_lock</span><span class="o">.</span><span class="n">acquire</span><span class="p">()</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_instance</span> <span class="o">=</span> <span class="nb">object</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_instance_lock</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_instance</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_info</span> <span class="o">=</span> <span class="n">_ThreadLocalInfo</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_context_switches</span> <span class="o">=</span> <span class="n">_ContextSwitchInfo</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span> <span class="o">=</span> <span class="n">MSContext</span><span class="o">.</span><span class="n">get_instance</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_support_binary</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="fm">__getattribute__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr</span><span class="p">):</span>
        <span class="n">value</span> <span class="o">=</span> <span class="nb">object</span><span class="o">.</span><span class="fm">__getattribute__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attr</span> <span class="o">==</span> <span class="s2">&quot;_context_handle&quot;</span> <span class="ow">and</span> <span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Get </span><span class="si">{}</span><span class="s2"> failed, please check whether &#39;env_config_path&#39; is correct.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">attr</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">get_param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="o">.</span><span class="n">get_param</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_mode</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get current mode.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">mode</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_mode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Switch between Graph mode and PyNative mode.</span>

<span class="sd">        Args:</span>
<span class="sd">            mode (int): GRAPH_MODE or PYNATIVE_MODE.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">PYNATIVE_MODE</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_debug_runtime</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">set_backend_policy</span><span class="p">(</span><span class="s2">&quot;vm&quot;</span><span class="p">)</span>
            <span class="n">parallel_mode</span> <span class="o">=</span> <span class="n">_get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;parallel_mode&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">parallel_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">DATA_PARALLEL</span><span class="p">,</span> <span class="n">ParallelMode</span><span class="o">.</span><span class="n">STAND_ALONE</span><span class="p">,</span> <span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Got </span><span class="si">{</span><span class="n">parallel_mode</span><span class="si">}</span><span class="s2">, when the user enabled SEMI_AUTO_PARALELL, &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;pynative mode dose not support, you should set either &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;context.set_auto_parallel_context(parallel_mode=&#39;data_parallel&#39;), &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;context.set_auto_parallel_context(parallel_mode=&#39;stand_alone&#39;) &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;or context.set_auto_parallel_context(parallel_mode=&#39;auto_parallel&#39;).&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_context_switches</span><span class="o">.</span><span class="n">push</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">GRAPH_MODE</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_debug_runtime</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">set_backend_policy</span><span class="p">(</span><span class="s2">&quot;ge&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_context_switches</span><span class="o">.</span><span class="n">push</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;mode&#39; should be context.GRAPH_MODE (0) &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;or context.PYNATIVE_MODE (1), but got </span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">mode</span><span class="p">,</span> <span class="n">mode</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_jit_syntax_level</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">level</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;&quot;Set the JIT syntax level for graph compiling&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">level</span> <span class="o">!=</span> <span class="n">STRICT</span> <span class="ow">and</span> <span class="n">level</span> <span class="o">!=</span> <span class="n">COMPATIBLE</span> <span class="ow">and</span> <span class="n">level</span> <span class="o">!=</span> <span class="n">LAX</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.set_jit_syntax_level&#39;, the argument &#39;level&#39; should be context.STRICT &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;or context.LAX, but got </span><span class="si">{</span><span class="n">level</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">jit_syntax_level</span><span class="p">,</span> <span class="n">level</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_memory_optimize_level</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memory_optimize_level</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The memory optimize level, support &quot;O0&quot;, &quot;O1&quot;.</span>

<span class="sd">        Args:</span>
<span class="sd">            target (str): &quot;O0&quot;, &quot;O1&quot;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">memory_optimize_levels</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;O0&quot;</span><span class="p">,</span> <span class="s2">&quot;O1&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">memory_optimize_level</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">memory_optimize_levels</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;memory_optimize_level&#39; must be one of &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">memory_optimize_levels</span><span class="si">}</span><span class="s2">, but got </span><span class="si">{</span><span class="n">memory_optimize_level</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">memory_optimize_level</span> <span class="o">==</span> <span class="s2">&quot;O0&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">memory_optimize_level</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">memory_optimize_level</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_memory_offload</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memory_offload</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Enable memory offload or not, support &quot;ON&quot;, &quot;OFF&quot;.</span>

<span class="sd">        Args:</span>
<span class="sd">            memory_offload (str): &quot;ON&quot;, &quot;OFF&quot;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">memory_offload_options</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;ON&quot;</span><span class="p">,</span> <span class="s2">&quot;OFF&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">memory_offload</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">memory_offload_options</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;memory_offload&#39; must be one of &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">memory_offload_options</span><span class="si">}</span><span class="s2">, but got </span><span class="si">{</span><span class="n">memory_offload</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">memory_offload</span> <span class="o">==</span> <span class="s2">&quot;ON&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">memory_offload</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">memory_offload</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_deterministic</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">deterministic</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Enable model run in deterministic, and support the values &quot;ON&quot; and &quot;OFF&quot;.</span>

<span class="sd">        Args:</span>
<span class="sd">            deterministic (str): &quot;ON&quot;, &quot;OFF&quot;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">deterministic_options</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;ON&quot;</span><span class="p">,</span> <span class="s2">&quot;OFF&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">deterministic</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">deterministic_options</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;deterministic&#39; must be one of &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">deterministic_options</span><span class="si">}</span><span class="s2">, but got </span><span class="si">{</span><span class="n">deterministic</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">deterministic</span><span class="p">,</span> <span class="n">deterministic</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_ascend_config</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ascend_config</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Enable ascend config.</span>

<span class="sd">        Args:</span>
<span class="sd">            ascend_config (dict):</span>
<span class="sd">                - precision_mode (str): &quot;force_fp16&quot;, &quot;allow_fp32_to_fp16&quot;, &quot;allow_mix_precision&quot;,</span>
<span class="sd">                            &quot;must_keep_origin_dtype&quot;, &quot;force_fp32&quot;, &quot;allow_fp32_to_bf16&quot;,</span>
<span class="sd">                            &quot;allow_mix_precision_fp16&quot; and &quot;allow_mix_precision_bf16&quot;.</span>
<span class="sd">                - jit_compile (bool): ``False`` and ``True``.</span>
<span class="sd">                - atomic_clean_policy (int): ``0`` and ``1``. Default: ``1`` .</span>
<span class="sd">                - op_precision_mode (str): config file path.</span>
<span class="sd">                - parallel_speed_up_json_path(Union[str, None]): The path to the parallel speed up json file.</span>
<span class="sd">                  If its value is None or &#39;&#39;, it does not take effect. Default None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">ascend_cfg_modes</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;precision_mode&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;force_fp16&quot;</span><span class="p">,</span> <span class="s2">&quot;allow_fp32_to_fp16&quot;</span><span class="p">,</span> <span class="s2">&quot;allow_mix_precision&quot;</span><span class="p">,</span> <span class="s2">&quot;must_keep_origin_dtype&quot;</span><span class="p">,</span>
                               <span class="s2">&quot;force_fp32&quot;</span><span class="p">,</span> <span class="s2">&quot;allow_fp32_to_bf16&quot;</span><span class="p">,</span> <span class="s2">&quot;allow_mix_precision_fp16&quot;</span><span class="p">,</span>
                               <span class="s2">&quot;allow_mix_precision_bf16&quot;</span><span class="p">],</span>
            <span class="s1">&#39;jit_compile&#39;</span><span class="p">:</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">],</span>
            <span class="s1">&#39;atomic_clean_policy&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
            <span class="s1">&#39;matmul_allow_hf32&#39;</span><span class="p">:</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">],</span>
            <span class="s1">&#39;conv_allow_hf32&#39;</span><span class="p">:</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">],</span>
            <span class="s1">&#39;op_precision_mode&#39;</span><span class="p">:</span> <span class="p">(</span><span class="nb">str</span><span class="p">,),</span>
            <span class="s1">&#39;parallel_speed_up_json_path&#39;</span><span class="p">:</span> <span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="p">}</span>
        <span class="n">ascend_cfg_setters</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;precision_mode&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_ascend_config_setter</span><span class="p">(</span><span class="s1">&#39;precision_mode&#39;</span><span class="p">),</span>
            <span class="s1">&#39;jit_compile&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_ascend_config_setter</span><span class="p">(</span><span class="s1">&#39;jit_compile&#39;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="s2">&quot;1&quot;</span> <span class="k">if</span> <span class="n">v</span> <span class="k">else</span> <span class="s2">&quot;0&quot;</span><span class="p">),</span>
            <span class="s1">&#39;atomic_clean_policy&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_ascend_config_setter</span><span class="p">(</span><span class="s1">&#39;atomic_clean_policy&#39;</span><span class="p">,</span> <span class="nb">str</span><span class="p">),</span>
            <span class="s1">&#39;matmul_allow_hf32&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_ascend_config_setter</span><span class="p">(</span><span class="s1">&#39;matmul_allow_hf32&#39;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="s2">&quot;1&quot;</span> <span class="k">if</span> <span class="n">v</span> <span class="k">else</span> <span class="s2">&quot;0&quot;</span><span class="p">),</span>
            <span class="s1">&#39;conv_allow_hf32&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_ascend_config_setter</span><span class="p">(</span><span class="s1">&#39;conv_allow_hf32&#39;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="s2">&quot;1&quot;</span> <span class="k">if</span> <span class="n">v</span> <span class="k">else</span> <span class="s2">&quot;0&quot;</span><span class="p">),</span>
            <span class="s1">&#39;op_precision_mode&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_op_precision_mode</span><span class="p">,</span>
            <span class="s1">&#39;parallel_speed_up_json_path&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_speedup_config_path</span>
        <span class="p">}</span>
        <span class="n">ascend_cfg_set</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">ascend_cfg_modes</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="k">for</span> <span class="n">ascend_key</span><span class="p">,</span> <span class="n">ascend_value</span> <span class="ow">in</span> <span class="n">ascend_config</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">ascend_key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ascend_cfg_set</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.set_context&#39;, the key of argument &#39;ascend_config&#39; must be one of &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">ascend_cfg_set</span><span class="si">}</span><span class="s2">, but got </span><span class="si">{</span><span class="n">ascend_key</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
            <span class="n">supported_modes</span> <span class="o">=</span> <span class="n">ascend_cfg_modes</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">ascend_key</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">supported_modes</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="n">ascend_value</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">supported_modes</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;ascend_config&#39;, the value of argument </span><span class="si">{</span><span class="n">ascend_key</span><span class="si">}</span><span class="s2"> must be one of &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">supported_modes</span><span class="si">}</span><span class="s2">, but got </span><span class="si">{</span><span class="n">ascend_value</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">supported_modes</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ascend_value</span><span class="p">,</span> <span class="n">supported_modes</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;ascend_config&#39;, the type of argument </span><span class="si">{</span><span class="n">ascend_key</span><span class="si">}</span><span class="s2"> must be one of &quot;</span>
                                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">supported_modes</span><span class="si">}</span><span class="s2">, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">ascend_value</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
            <span class="n">cfg_setter</span> <span class="o">=</span> <span class="n">ascend_cfg_setters</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">ascend_key</span><span class="p">)</span>
            <span class="n">cfg_setter</span><span class="p">(</span><span class="n">ascend_value</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_gpu_config</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gpu_config</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Enable gpu config.</span>

<span class="sd">        Args:</span>
<span class="sd">            gpu_config (dict):</span>

<span class="sd">                - conv_fprop_algo (str): &quot;normal&quot;, &quot;performance&quot; or user specifies conv forward algorithm directly.</span>
<span class="sd">                - conv_dgrad_algo (str): &quot;normal&quot;, &quot;performance&quot; or user specifies conv data grad algorithm directly.</span>
<span class="sd">                - conv_wgrad_algo (str): &quot;normal&quot;, &quot;performance&quot; or user specifies conv weight grad algorithm directly.</span>
<span class="sd">                - conv_allow_tf32 (bool): ``False`` and ``True``.</span>
<span class="sd">                - matmul_allow_tf32 (bool): ``False`` and ``True``.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">gpu_cfgs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;conv_fprop_algo&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;normal&quot;</span><span class="p">,</span> <span class="s2">&quot;performance&quot;</span><span class="p">,</span> <span class="s2">&quot;implicit_gemm&quot;</span><span class="p">,</span> <span class="s2">&quot;precomp_gemm&quot;</span><span class="p">,</span> <span class="s2">&quot;gemm&quot;</span><span class="p">,</span> <span class="s2">&quot;direct&quot;</span><span class="p">,</span>
                                        <span class="s2">&quot;fft&quot;</span><span class="p">,</span> <span class="s2">&quot;fft_tiling&quot;</span><span class="p">,</span> <span class="s2">&quot;winograd&quot;</span><span class="p">,</span> <span class="s2">&quot;winograd_nonfused&quot;</span><span class="p">],</span>
                    <span class="s1">&#39;conv_dgrad_algo&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;normal&quot;</span><span class="p">,</span> <span class="s2">&quot;performance&quot;</span><span class="p">,</span> <span class="s2">&quot;algo_0&quot;</span><span class="p">,</span> <span class="s2">&quot;algo_1&quot;</span><span class="p">,</span> <span class="s2">&quot;fft&quot;</span><span class="p">,</span> <span class="s2">&quot;fft_tiling&quot;</span><span class="p">,</span> <span class="s2">&quot;winograd&quot;</span><span class="p">,</span>
                                        <span class="s2">&quot;winograd_nonfused&quot;</span><span class="p">],</span>
                    <span class="s1">&#39;conv_wgrad_algo&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;normal&quot;</span><span class="p">,</span> <span class="s2">&quot;performance&quot;</span><span class="p">,</span> <span class="s2">&quot;algo_0&quot;</span><span class="p">,</span> <span class="s2">&quot;algo_1&quot;</span><span class="p">,</span> <span class="s2">&quot;fft&quot;</span><span class="p">,</span> <span class="s2">&quot;algo_3&quot;</span><span class="p">,</span> <span class="s2">&quot;fft_tiling&quot;</span><span class="p">,</span>
                                        <span class="s2">&quot;winograd_nonfused&quot;</span><span class="p">],</span>
                    <span class="s1">&#39;conv_allow_tf32&#39;</span><span class="p">:</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">],</span>
                    <span class="s1">&#39;matmul_allow_tf32&#39;</span><span class="p">:</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">]}</span>
        <span class="k">for</span> <span class="n">gpu_key</span> <span class="ow">in</span> <span class="n">gpu_config</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">gpu_key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">gpu_cfgs</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.set_context&#39;, the key of argument &#39;gpu_config&#39; must be one of &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">gpu_cfgs</span><span class="si">}</span><span class="s2">, but got </span><span class="si">{</span><span class="n">gpu_key</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
            <span class="n">supported_value</span> <span class="o">=</span> <span class="n">gpu_cfgs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">gpu_key</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">gpu_config</span><span class="p">[</span><span class="n">gpu_key</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">supported_value</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;gpu_config&#39;, the value of argument </span><span class="si">{</span><span class="n">gpu_key</span><span class="si">}</span><span class="s2"> must be one of &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">supported_value</span><span class="si">}</span><span class="s2">, but got </span><span class="si">{</span><span class="n">gpu_config</span><span class="p">[</span><span class="n">gpu_key</span><span class="p">]</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">gpu_key</span> <span class="o">==</span> <span class="s1">&#39;conv_fprop_algo&#39;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">conv_fprop_algo</span><span class="p">,</span> <span class="n">gpu_config</span><span class="p">[</span><span class="n">gpu_key</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">gpu_key</span> <span class="o">==</span> <span class="s1">&#39;conv_dgrad_algo&#39;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">conv_dgrad_algo</span><span class="p">,</span> <span class="n">gpu_config</span><span class="p">[</span><span class="n">gpu_key</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">gpu_key</span> <span class="o">==</span> <span class="s1">&#39;conv_wgrad_algo&#39;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">conv_wgrad_algo</span><span class="p">,</span> <span class="n">gpu_config</span><span class="p">[</span><span class="n">gpu_key</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">gpu_key</span> <span class="o">==</span> <span class="s1">&#39;conv_allow_tf32&#39;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">conv_allow_tf32</span><span class="p">,</span> <span class="n">gpu_config</span><span class="p">[</span><span class="n">gpu_key</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">gpu_key</span> <span class="o">==</span> <span class="s1">&#39;matmul_allow_tf32&#39;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">matmul_allow_tf32</span><span class="p">,</span> <span class="n">gpu_config</span><span class="p">[</span><span class="n">gpu_key</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">set_backend_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">policy</span><span class="p">):</span>
        <span class="n">success</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="o">.</span><span class="n">set_backend_policy</span><span class="p">(</span><span class="n">policy</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">success</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Backend policy must be one of values in [&#39;ge&#39;, &#39;vm&#39;, &#39;ms&#39;]. &quot;</span>
                               <span class="s2">&quot;But got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">policy</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">set_save_graphs_path</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">save_graphs_path</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">save_graphs_path</span><span class="p">,</span> <span class="n">_make_directory</span><span class="p">(</span><span class="n">save_graphs_path</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">set_device_target</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The target device to run, support &quot;Ascend&quot;, &quot;GPU&quot;, and &quot;CPU&quot;.</span>

<span class="sd">        Args:</span>
<span class="sd">            target (str): &quot;Ascend&quot;, &quot;GPU&quot;, and &quot;CPU&quot;.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">valid_targets</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;CPU&quot;</span><span class="p">,</span> <span class="s2">&quot;GPU&quot;</span><span class="p">,</span> <span class="s2">&quot;Ascend&quot;</span><span class="p">,</span> <span class="s2">&quot;Davinci&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">target</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">valid_targets</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;device_target&#39; must be one of &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">valid_targets</span><span class="si">}</span><span class="s2">, but got </span><span class="si">{</span><span class="n">target</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">target</span> <span class="o">==</span> <span class="s2">&quot;Davinci&quot;</span><span class="p">:</span>
            <span class="n">target</span> <span class="o">=</span> <span class="s2">&quot;Ascend&quot;</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;The device &#39;Davinci&#39; is deprecated and will be removed in the next version. &quot;</span>
                           <span class="s2">&quot;For &#39;context.set_context&#39;, please set the argument &#39;device_target&#39; &quot;</span>
                           <span class="s2">&quot;to &#39;CPU&#39;, &#39;GPU&#39; or &#39;Ascend&#39;,if you set it to &#39;Davinci&#39;, it will be automatically &quot;</span>
                           <span class="s2">&quot;changed to &#39;Ascend&#39;.&quot;</span><span class="p">)</span>
        <span class="c1"># If in Parameter Server mode, Ascend card should not be used by server and scheduler.</span>
        <span class="k">if</span> <span class="n">_need_reset_device_target_for_ps</span><span class="p">(</span><span class="n">target</span><span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Reset device target to CPU when set_device_target.&quot;</span><span class="p">)</span>
            <span class="n">target</span> <span class="o">=</span> <span class="s2">&quot;CPU&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">device_target</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_debug_runtime</span> <span class="ow">and</span> <span class="n">target</span> <span class="o">==</span> <span class="s2">&quot;CPU&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_backend_policy</span><span class="p">(</span><span class="s2">&quot;vm&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_aoe_tune_mode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tune_mode</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set aoe tune mode, support &quot;online&quot; and &quot;offline&quot;.</span>

<span class="sd">        Args:</span>
<span class="sd">            tune_mode (str): &quot;online&quot; and &quot;offline&quot;.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">candidate</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;online&quot;</span><span class="p">,</span> <span class="s2">&quot;offline&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">tune_mode</span> <span class="ow">in</span> <span class="n">candidate</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">aoe_tune_mode</span><span class="p">,</span> <span class="n">tune_mode</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;aoe_tune_mode&#39; must be in &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;[&#39;online&#39;, &#39;offline&#39;], but got </span><span class="si">{</span><span class="n">tune_mode</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_aoe_config</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">aoe_config</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Enable aoe config.</span>

<span class="sd">        Args:</span>
<span class="sd">            aoe_config (dict):</span>
<span class="sd">                - job_type (str): ``&quot;1&quot;``, ``&quot;2&quot;``. Default: ``&quot;2&quot;`` .</span>
<span class="sd">                  - ``&quot;1&quot;``: subgraph tuning.</span>
<span class="sd">                  - ``&quot;2&quot;``: operator tuning.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">aoe_cfgs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;job_type&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;2&quot;</span><span class="p">]}</span>
        <span class="k">for</span> <span class="n">aoe_config_key</span> <span class="ow">in</span> <span class="n">aoe_config</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">aoe_config_key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">aoe_cfgs</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.set_context&#39;, the key of argument &#39;aoe_config&#39; must be one of &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">aoe_cfgs</span><span class="si">}</span><span class="s2">, but got </span><span class="si">{</span><span class="n">aoe_config_key</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
            <span class="n">supported_value</span> <span class="o">=</span> <span class="n">aoe_cfgs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">aoe_config_key</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">aoe_config</span><span class="p">[</span><span class="n">aoe_config_key</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">supported_value</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;aoe_config&#39;, the value of argument </span><span class="si">{</span><span class="n">aoe_config_key</span><span class="si">}</span><span class="s2"> must be one of &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">supported_value</span><span class="si">}</span><span class="s2">, but got </span><span class="si">{</span><span class="n">aoe_config</span><span class="p">[</span><span class="n">aoe_config_key</span><span class="p">]</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">aoe_config_key</span> <span class="o">==</span> <span class="s1">&#39;job_type&#39;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">aoe_job_type</span><span class="p">,</span> <span class="n">aoe_config</span><span class="p">[</span><span class="n">aoe_config_key</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">set_device_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device_id</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">device_id</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">device_id</span> <span class="o">&gt;</span> <span class="mi">4095</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;device_id&#39; must be in range [0, 4095], &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">device_id</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">device_id</span><span class="p">,</span> <span class="n">device_id</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_max_call_depth</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_call_depth</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">max_call_depth</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;max_call_depth&#39; must be greater than 0, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">max_call_depth</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">max_call_depth</span><span class="p">,</span> <span class="n">max_call_depth</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_profiling_options</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">option</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">option</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;profiling_option&#39; must be string, &quot;</span>
                            <span class="s2">&quot;but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">option</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">profiling_options</span><span class="p">,</span> <span class="n">option</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_variable_memory_max_size</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">variable_memory_max_size</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;set values of variable_memory_max_size and graph_memory_max_size&quot;&quot;&quot;</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;For &#39;context.set_context&#39;, the parameter &#39;variable_memory_max_size&#39; is deprecated, &quot;</span>
                       <span class="s2">&quot;and will be removed in a future &quot;</span>
                       <span class="s2">&quot;version. Please use parameter &#39;max_device_memory&#39; instead.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">Validator</span><span class="o">.</span><span class="n">check_str_by_regular</span><span class="p">(</span><span class="n">variable_memory_max_size</span><span class="p">,</span> <span class="n">_RE_PATTERN</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;variable_memory_max_size&#39; should be in correct&quot;</span>
                             <span class="s2">&quot; format! It must be a string ending with &#39;GB&#39;, in addition to that, it must contain &quot;</span>
                             <span class="s2">&quot;only numbers or decimal points, such as </span><span class="se">\&quot;</span><span class="s2">5GB</span><span class="se">\&quot;</span><span class="s2"> or </span><span class="se">\&quot;</span><span class="s2">3.5GB</span><span class="se">\&quot;</span><span class="s2">, but got </span><span class="si">{}</span><span class="s2">GB.&quot;</span>
                             <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">variable_memory_max_size</span><span class="p">))</span>
        <span class="k">if</span> <span class="nb">float</span><span class="p">(</span><span class="n">variable_memory_max_size</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span> <span class="o">&gt;</span> <span class="n">_DEVICE_APP_MEMORY_SIZE</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;variable_memory_max_size&#39; should not be &quot;</span>
                             <span class="s2">&quot;greater than 31GB, but got </span><span class="si">{}</span><span class="s2">GB.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">variable_memory_max_size</span><span class="p">))</span>
        <span class="n">variable_memory_max_size_</span> <span class="o">=</span> <span class="n">variable_memory_max_size</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot; * 1024 * 1024 * 1024&quot;</span>
        <span class="n">graph_memory_max_size</span> <span class="o">=</span> <span class="n">_DEVICE_APP_MEMORY_SIZE</span> <span class="o">-</span> <span class="nb">int</span><span class="p">(</span><span class="n">variable_memory_max_size</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>
        <span class="n">graph_memory_max_size_</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">graph_memory_max_size</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot; * 1024 * 1024 * 1024&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">variable_memory_max_size</span><span class="p">,</span> <span class="n">variable_memory_max_size_</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">_graph_memory_max_size</span><span class="p">,</span> <span class="n">graph_memory_max_size_</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_max_device_memory</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_device_memory</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">Validator</span><span class="o">.</span><span class="n">check_str_by_regular</span><span class="p">(</span><span class="n">max_device_memory</span><span class="p">,</span> <span class="n">_RE_PATTERN</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;max_device_memory&#39; should be in correct &quot;</span>
                             <span class="s2">&quot; format! It must be a string ending with &#39;GB&#39;, in addition to that, it must contain &quot;</span>
                             <span class="s2">&quot;only numbers or decimal points, such as </span><span class="se">\&quot;</span><span class="s2">5GB</span><span class="se">\&quot;</span><span class="s2"> or </span><span class="se">\&quot;</span><span class="s2">3.5GB</span><span class="se">\&quot;</span><span class="s2">, but got </span><span class="si">{}</span><span class="s2">.&quot;</span>
                             <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">max_device_memory</span><span class="p">))</span>
        <span class="n">max_device_memory_value</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">max_device_memory</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">max_device_memory_value</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;max_device_memory&#39; should not be </span><span class="se">\&quot;</span><span class="s2">0GB</span><span class="se">\&quot;</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">max_device_memory</span><span class="p">,</span> <span class="n">max_device_memory_value</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_mempool_block_size</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mempool_block_size</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Set the block size of memory pool.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">_get_mode</span><span class="p">()</span> <span class="o">==</span> <span class="n">GRAPH_MODE</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Graph mode doesn&#39;t support to set parameter &#39;mempool_block_size&#39; of context currently, &quot;</span>
                           <span class="s2">&quot;you can use context.set_context to set pynative mode.&quot;</span><span class="p">)</span>
            <span class="k">return</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">Validator</span><span class="o">.</span><span class="n">check_str_by_regular</span><span class="p">(</span><span class="n">mempool_block_size</span><span class="p">,</span> <span class="n">_RE_PATTERN</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;mempool_block_size&#39; should be in &quot;</span>
                             <span class="s2">&quot;correct format! Such as </span><span class="se">\&quot;</span><span class="s2">10GB</span><span class="se">\&quot;</span><span class="s2">, &quot;</span>
                             <span class="s2">&quot;but got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mempool_block_size</span><span class="p">))</span>
        <span class="n">mempool_block_size_value</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">mempool_block_size</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">mempool_block_size_value</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;context.set_context&#39;,  the argument &#39;mempool_block_size&#39; should be &quot;</span>
                             <span class="s2">&quot;greater or equal to </span><span class="se">\&quot;</span><span class="s2">1GB</span><span class="se">\&quot;</span><span class="s2">, &quot;</span>
                             <span class="s2">&quot;but got </span><span class="si">{}</span><span class="s2">GB&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">mempool_block_size</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">])))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">mempool_block_size</span><span class="p">,</span> <span class="n">mempool_block_size_value</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_print_file_path</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">file_path</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Add timestamp suffix to file name. Sets print file path.&quot;&quot;&quot;</span>
        <span class="n">print_file_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">realpath</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">print_file_path</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">IOError</span><span class="p">(</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;print_file_path&#39; should be file path, &quot;</span>
                          <span class="s2">&quot;but got directory </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">file_path</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">print_file_path</span><span class="p">):</span>
            <span class="n">_path</span><span class="p">,</span> <span class="n">_file_name</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">print_file_path</span><span class="p">)</span>
            <span class="n">path</span> <span class="o">=</span> <span class="n">_make_directory</span><span class="p">(</span><span class="n">_path</span><span class="p">)</span>
            <span class="n">file_name</span> <span class="o">=</span> <span class="n">_get_print_file_name</span><span class="p">(</span><span class="n">_file_name</span><span class="p">)</span>
            <span class="n">full_file_name</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">file_name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">full_file_name</span> <span class="o">=</span> <span class="n">print_file_path</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">print_file_path</span><span class="p">,</span> <span class="n">full_file_name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_env_config_path</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env_config_path</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check and set env_config_path.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="o">.</span><span class="n">enable_dump_ir</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;env_config_path&#39; is not supported, please &quot;</span>
                             <span class="s2">&quot;enable ENABLE_DUMP_IR with &#39;-D on&#39; and recompile source firstly.&quot;</span><span class="p">)</span>
        <span class="n">env_config_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">realpath</span><span class="p">(</span><span class="n">env_config_path</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">env_config_path</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;context.set_context&#39;, the &#39;env_config_path&#39; file </span><span class="si">%r</span><span class="s2"> is not exists, &quot;</span>
                             <span class="s2">&quot;please check whether &#39;env_config_path&#39; is correct.&quot;</span> <span class="o">%</span> <span class="n">env_config_path</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">env_config_path</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
        <span class="k">except</span> <span class="p">(</span><span class="ne">TypeError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">)</span> <span class="k">as</span> <span class="n">exo</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">exo</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">For &#39;context.set_context&#39;, open or load the &#39;env_config_path&#39; file </span><span class="si">{}</span><span class="s2"> &quot;</span>
                                        <span class="s2">&quot;failed, please check whether &#39;env_config_path&#39; is json file and correct, &quot;</span>
                                        <span class="s2">&quot;or may not have permission to read it.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">env_config_path</span><span class="p">))</span> <span class="kn">from</span> <span class="nn">exo</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">env_config_path</span><span class="p">,</span> <span class="n">env_config_path</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_runtime_num_threads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">runtime_num_threads</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check and set runtime_num_threads.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">runtime_num_threads</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The num of thread must bigger than or equal to 0.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">runtime_num_threads</span><span class="p">,</span> <span class="n">runtime_num_threads</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_op_timeout</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op_timeout</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Set the maximum duration of executing an operator in seconds.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">op_timeout</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The num of op exe timeout must bigger than or equal to 0.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">op_timeout</span><span class="p">,</span> <span class="n">op_timeout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_inter_op_parallel_num</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inter_op_parallel_num</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check and set inter_op_parallel_num.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">inter_op_parallel_num</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The num of parallel thread must bigger than or equal to 0.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">inter_op_parallel_num</span><span class="p">,</span> <span class="n">inter_op_parallel_num</span><span class="p">)</span>

    <span class="n">setters</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;mode&#39;</span><span class="p">:</span> <span class="n">set_mode</span><span class="p">,</span>
        <span class="s1">&#39;save_graphs_path&#39;</span><span class="p">:</span> <span class="n">set_save_graphs_path</span><span class="p">,</span>
        <span class="s1">&#39;device_target&#39;</span><span class="p">:</span> <span class="n">set_device_target</span><span class="p">,</span>
        <span class="s1">&#39;aoe_tune_mode&#39;</span><span class="p">:</span> <span class="n">set_aoe_tune_mode</span><span class="p">,</span>
        <span class="s1">&#39;device_id&#39;</span><span class="p">:</span> <span class="n">set_device_id</span><span class="p">,</span>
        <span class="s1">&#39;max_call_depth&#39;</span><span class="p">:</span> <span class="n">set_max_call_depth</span><span class="p">,</span>
        <span class="s1">&#39;profiling_options&#39;</span><span class="p">:</span> <span class="n">set_profiling_options</span><span class="p">,</span>
        <span class="s1">&#39;variable_memory_max_size&#39;</span><span class="p">:</span> <span class="n">set_variable_memory_max_size</span><span class="p">,</span>
        <span class="s1">&#39;max_device_memory&#39;</span><span class="p">:</span> <span class="n">set_max_device_memory</span><span class="p">,</span>
        <span class="s1">&#39;mempool_block_size&#39;</span><span class="p">:</span> <span class="n">set_mempool_block_size</span><span class="p">,</span>
        <span class="s1">&#39;print_file_path&#39;</span><span class="p">:</span> <span class="n">set_print_file_path</span><span class="p">,</span>
        <span class="s1">&#39;env_config_path&#39;</span><span class="p">:</span> <span class="n">set_env_config_path</span><span class="p">,</span>
        <span class="s1">&#39;inter_op_parallel_num&#39;</span><span class="p">:</span> <span class="n">set_inter_op_parallel_num</span><span class="p">,</span>
        <span class="s1">&#39;runtime_num_threads&#39;</span><span class="p">:</span> <span class="n">set_runtime_num_threads</span><span class="p">,</span>
        <span class="s1">&#39;memory_optimize_level&#39;</span><span class="p">:</span> <span class="n">set_memory_optimize_level</span><span class="p">,</span>
        <span class="s1">&#39;op_timeout&#39;</span><span class="p">:</span> <span class="n">set_op_timeout</span><span class="p">,</span>
        <span class="s1">&#39;memory_offload&#39;</span><span class="p">:</span> <span class="n">set_memory_offload</span><span class="p">,</span>
        <span class="s1">&#39;deterministic&#39;</span><span class="p">:</span> <span class="n">set_deterministic</span><span class="p">,</span>
        <span class="s1">&#39;ascend_config&#39;</span><span class="p">:</span> <span class="n">set_ascend_config</span><span class="p">,</span>
        <span class="s1">&#39;jit_syntax_level&#39;</span><span class="p">:</span> <span class="n">set_jit_syntax_level</span><span class="p">,</span>
        <span class="s1">&#39;gpu_config&#39;</span><span class="p">:</span> <span class="n">set_gpu_config</span><span class="p">,</span>
        <span class="s1">&#39;aoe_config&#39;</span><span class="p">:</span> <span class="n">set_aoe_config</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">reserve_class_name_in_scope</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get whether to save the network class name in the scope.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_info</span><span class="o">.</span><span class="n">reserve_class_name_in_scope</span>

    <span class="nd">@reserve_class_name_in_scope</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">reserve_class_name_in_scope</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reserve_class_name_in_scope</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Set whether to save the network class name in the scope.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">reserve_class_name_in_scope</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;context.set_context&#39;, the type of the property &#39;reserve_class_name_in_scope&#39; must &quot;</span>
                             <span class="s2">&quot;be bool, but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">reserve_class_name_in_scope</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_info</span><span class="o">.</span><span class="n">reserve_class_name_in_scope</span> <span class="o">=</span> <span class="n">reserve_class_name_in_scope</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">enable_ge</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="o">.</span><span class="n">get_backend_policy</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;ge&#39;</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">enable_debug_runtime</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_info</span><span class="o">.</span><span class="n">debug_runtime</span>

    <span class="nd">@enable_debug_runtime</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">enable_debug_runtime</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enable</span><span class="p">):</span>
        <span class="n">thread_info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_info</span>
        <span class="n">thread_info</span><span class="o">.</span><span class="n">debug_runtime</span> <span class="o">=</span> <span class="n">enable</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">support_binary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Whether support run .pyc or .so in graph mode.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_support_binary</span>

    <span class="nd">@support_binary</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">support_binary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">support</span><span class="p">:</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">support</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The attribute &#39;support_binary&#39; should be a bool, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">support</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_support_binary</span> <span class="o">=</span> <span class="n">support</span>

    <span class="k">def</span> <span class="nf">_get_ascend_config_setter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ascend_key</span><span class="p">,</span> <span class="n">trans_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">_config_setter</span><span class="p">(</span><span class="n">ascend_value</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">__members__</span><span class="p">[</span><span class="n">ascend_key</span><span class="p">],</span> <span class="n">trans_fn</span><span class="p">(</span><span class="n">ascend_value</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">trans_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">trans_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span>
        <span class="k">return</span> <span class="n">_config_setter</span>

    <span class="k">def</span> <span class="nf">_set_op_precision_mode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ascend_value</span><span class="p">):</span>
        <span class="n">op_precision_path</span> <span class="o">=</span> <span class="n">ascend_value</span>
        <span class="n">real_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">realpath</span><span class="p">(</span><span class="n">op_precision_path</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">real_path</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;ascend_config&#39;, the &#39;op_precision_mode&#39; is invalid path, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;got &#39;</span><span class="si">{</span><span class="n">op_precision_path</span><span class="si">}</span><span class="s2">&#39;.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">op_precision_mode</span><span class="p">,</span> <span class="n">ascend_value</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_set_speedup_config_path</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">speedup_config_path</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;&quot;Check and set speedup config for auto parallel.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">speedup_config_path</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">speedup_config_path</span> <span class="o">==</span> <span class="s2">&quot;&quot;</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">speedup_config_real_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">speedup_config_path</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">speedup_config_real_path</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;ascend_config&#39;, the path to parallel_speed_up_json: &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">speedup_config_real_path</span><span class="si">}</span><span class="s2"> does not exist, please check whether the &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;&#39;parallel_speed_up_json_path&#39; is correct.&quot;</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">valid_option</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;recompute_comm_overlap&quot;</span><span class="p">:</span> <span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">recompute_comm_overlap</span><span class="p">,</span>
                            <span class="s2">&quot;matmul_grad_comm_overlap&quot;</span><span class="p">:</span> <span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">matmul_grad_comm_overlap</span><span class="p">,</span>
                            <span class="s2">&quot;enable_task_opt&quot;</span><span class="p">:</span> <span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">enable_task_opt</span><span class="p">,</span>
                            <span class="s2">&quot;enable_grad_comm_opt&quot;</span><span class="p">:</span> <span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">enable_grad_comm_opt</span><span class="p">,</span>
                            <span class="s2">&quot;interleaved_matmul_comm&quot;</span><span class="p">:</span> <span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">interleaved_matmul_comm</span><span class="p">,</span>
                            <span class="s2">&quot;interleaved_layernorm_comm&quot;</span><span class="p">:</span> <span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">interleaved_layernorm_comm</span><span class="p">}</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">speedup_config_real_path</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">speedup_config</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">speedup_config</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;key </span><span class="si">{}</span><span class="s2"> is not a str&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>
                    <span class="k">if</span> <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">valid_option</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;key </span><span class="si">{}</span><span class="s2"> should be one of </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">valid_option</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
                        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;value </span><span class="si">{}</span><span class="s2"> is not a bool&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">v</span><span class="p">))</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">valid_option</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">k</span><span class="p">),</span> <span class="n">v</span><span class="p">)</span>
        <span class="k">except</span> <span class="p">(</span><span class="ne">TypeError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">)</span> <span class="k">as</span> <span class="n">exo</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">exo</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">For &#39;context.set_context&#39;, &quot;</span>
                                        <span class="s2">&quot;open or load the &#39;speedup_config_path&#39; file </span><span class="si">{}</span><span class="s2"> &quot;</span>
                                        <span class="s2">&quot;failed, please check whether &#39;speedup_config_path&#39; is json file and correct, &quot;</span>
                                        <span class="s2">&quot;or may not have permission to read it.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">speedup_config_real_path</span><span class="p">))</span> \
                                        <span class="kn">from</span> <span class="nn">exo</span>


<span class="k">def</span> <span class="nf">_context</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get the global _context, if context is not created, create a new one.</span>

<span class="sd">    Returns:</span>
<span class="sd">        _Context, the global context in PyNative mode.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">K_CONTEXT</span>
    <span class="k">if</span> <span class="n">K_CONTEXT</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">default_backend</span> <span class="o">=</span> <span class="s1">&#39;debug&#39;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">default_config</span>
            <span class="n">default_backend</span> <span class="o">=</span> <span class="n">default_config</span><span class="o">.</span><span class="n">__backend__</span>
        <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;import default config fail&quot;</span><span class="p">)</span>
        <span class="n">K_CONTEXT</span> <span class="o">=</span> <span class="n">_Context</span><span class="p">()</span>
        <span class="n">K_CONTEXT</span><span class="o">.</span><span class="n">enable_debug_runtime</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">default_backend</span> <span class="o">==</span> <span class="s1">&#39;debug&#39;</span><span class="p">:</span>
            <span class="n">K_CONTEXT</span><span class="o">.</span><span class="n">enable_debug_runtime</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">default_backend</span> <span class="o">=</span> <span class="s1">&#39;vm&#39;</span>
            <span class="n">K_CONTEXT</span><span class="o">.</span><span class="n">set_backend_policy</span><span class="p">(</span><span class="n">default_backend</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">K_CONTEXT</span>


<div class="viewcode-block" id="set_auto_parallel_context"><a class="viewcode-back" href="../../api_python/mindspore/mindspore.set_auto_parallel_context.html#mindspore.set_auto_parallel_context">[文档]</a><span class="nd">@args_type_check</span><span class="p">(</span><span class="n">device_num</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">global_rank</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">gradients_mean</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">gradient_fp32_sync</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">parallel_mode</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
                 <span class="n">auto_parallel_search_mode</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">search_mode</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">parameter_broadcast</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">strategy_ckpt_load_file</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
                 <span class="n">strategy_ckpt_save_file</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">full_batch</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">enable_parallel_optimizer</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">enable_alltoall</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span>
                 <span class="n">all_reduce_fusion_config</span><span class="o">=</span><span class="nb">list</span><span class="p">,</span> <span class="n">pipeline_stages</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">pipeline_segments</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
                 <span class="n">parallel_optimizer_config</span><span class="o">=</span><span class="nb">dict</span><span class="p">,</span>
                 <span class="n">comm_fusion</span><span class="o">=</span><span class="nb">dict</span><span class="p">,</span> <span class="n">strategy_ckpt_config</span><span class="o">=</span><span class="nb">dict</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">set_auto_parallel_context</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Set auto parallel context, only data parallel supported on CPU.</span>

<span class="sd">    Note:</span>
<span class="sd">        Attribute name is required for setting attributes.</span>
<span class="sd">        If a program has tasks on different parallel modes, before setting a new parallel mode for the</span>
<span class="sd">        next task, interface :func:`mindspore.reset_auto_parallel_context` should be called to reset</span>
<span class="sd">        the configuration.</span>
<span class="sd">        Setting or changing parallel modes must be called before creating any Initializer, otherwise,</span>
<span class="sd">        it may have RuntimeError when compiling the network.</span>

<span class="sd">    Some configurations are parallel mode specific, see the below table for details:</span>

<span class="sd">    ===========================  ===========================</span>
<span class="sd">    Common                       AUTO_PARALLEL</span>
<span class="sd">    ===========================  ===========================</span>
<span class="sd">    device_num                   gradient_fp32_sync</span>
<span class="sd">    global_rank                  loss_repeated_mean</span>
<span class="sd">    gradients_mean               search_mode</span>
<span class="sd">    parallel_mode                parameter_broadcast</span>
<span class="sd">    all_reduce_fusion_config     strategy_ckpt_load_file</span>
<span class="sd">    enable_parallel_optimizer    strategy_ckpt_save_file</span>
<span class="sd">    parallel_optimizer_config    dataset_strategy</span>
<span class="sd">    enable_alltoall              pipeline_stages</span>
<span class="sd">               \                 auto_parallel_search_mode</span>
<span class="sd">               \                 comm_fusion</span>
<span class="sd">               \                 strategy_ckpt_config</span>
<span class="sd">    ===========================  ===========================</span>

<span class="sd">    Args:</span>
<span class="sd">        device_num (int): Available device number, the value must be in [1, 4096]. Default: ``1`` .</span>
<span class="sd">        global_rank (int): Global rank id, the value must be in [0, 4095]. Default: ``0`` .</span>
<span class="sd">        gradients_mean (bool): Whether to perform mean operator after allreduce of gradients.</span>
<span class="sd">                     &quot;stand_alone&quot; do not support gradients_mean. Default: ``False`` .</span>
<span class="sd">        gradient_fp32_sync (bool): Run allreduce of gradients in fp32. &quot;stand_alone&quot;, &quot;data_parallel&quot;</span>
<span class="sd">                     and &quot;hybrid_parallel&quot; do not support gradient_fp32_sync. Default: ``True`` .</span>
<span class="sd">        parallel_mode (str): There are five kinds of parallel modes, ``&quot;stand_alone&quot;`` , ``&quot;data_parallel&quot;`` ,</span>
<span class="sd">                     ``&quot;hybrid_parallel&quot;`` , ``&quot;semi_auto_parallel&quot;`` and ``&quot;auto_parallel&quot;`` . Note the pynative mode</span>
<span class="sd">                     only supports the ``&quot;stand_alone&quot;`` and ``&quot;data_parallel&quot;`` mode. Default: ``&quot;stand_alone&quot;`` .</span>

<span class="sd">                     - stand_alone: Only one processor is working.</span>

<span class="sd">                     - data_parallel: Distributes the data across different processors.</span>

<span class="sd">                     - hybrid_parallel: Achieves data parallelism and model parallelism manually.</span>

<span class="sd">                     - semi_auto_parallel: Achieves data and model parallelism by setting parallel strategies.</span>

<span class="sd">                     - auto_parallel: Achieving parallelism automatically.</span>
<span class="sd">        search_mode (str): There are three kinds of shard strategy search modes: ``&quot;recursive_programming&quot;`` ,</span>
<span class="sd">                     ``&quot;dynamic_programming&quot;`` and ``&quot;sharding_propagation&quot;`` . Default: ``&quot;recursive_programming&quot;`` .</span>

<span class="sd">                     - recursive_programming: Recursive programming search mode. In order to obtain optimal performance,</span>
<span class="sd">                       it is recommended that users set the batch size to be greater than or equal to the product of</span>
<span class="sd">                       the number of devices and the number of multi-copy parallelism.</span>

<span class="sd">                     - dynamic_programming: Dynamic programming search mode.</span>

<span class="sd">                     - sharding_propagation: Propagate shardings from configured ops to non-configured ops.</span>
<span class="sd">        auto_parallel_search_mode (str): This is the old version of &#39;search_mode&#39;. Here, remaining this attribute is</span>
<span class="sd">                     for forward compatibility, and this attribute will be deleted in a future MindSpore version.</span>
<span class="sd">        parameter_broadcast (bool): Whether to broadcast parameters before training. Before training, in order to have</span>
<span class="sd">                     the same network initialization parameter values for all devices, broadcast the parameters</span>
<span class="sd">                     on device 0 to other devices. Parameter broadcasting in different parallel modes is different,</span>
<span class="sd">                     ``data_parallel`` mode, all parameters are broadcast except for the parameter whose attribute</span>
<span class="sd">                     layerwise_parallel is ``True`` . ``Hybrid_parallel`` , ``semi_auto_parallel``  and</span>
<span class="sd">                     ``auto_parallel mode`` , the segmented parameters do not participate in broadcasting.</span>
<span class="sd">                     Default: ``False`` .</span>
<span class="sd">        strategy_ckpt_load_file (str): The path to load parallel strategy checkpoint. The parameter is not to be</span>
<span class="sd">                       recommended currently, it is better using &#39;strategy_ckpt_config&#39; to replace it. Default: ``&#39;&#39;``</span>
<span class="sd">        strategy_ckpt_save_file (str): The path to save parallel strategy checkpoint. The parameter is not to be</span>
<span class="sd">                       recommended currently, it is better using &#39;strategy_ckpt_config&#39; to replace it. Default: ``&#39;&#39;``</span>
<span class="sd">        full_batch (bool): If you load whole batch datasets in ``auto_parallel`` mode, this parameter</span>
<span class="sd">                       should be set as ``True`` . Default: ``False`` . The interface is not to be recommended</span>
<span class="sd">                       currently, it is better using &#39;dataset_strategy&#39; to replace it.</span>
<span class="sd">        dataset_strategy (Union[str, tuple]): Dataset sharding strategy. Default: ``&quot;data_parallel&quot;`` .</span>
<span class="sd">                       dataset_strategy=&quot;data_parallel&quot; is equal to full_batch=False, dataset_strategy=&quot;full_batch&quot; is</span>
<span class="sd">                       equal to full_batch=True. For execution mode is &#39;GRAPH_MODE&#39; and dataset load into net by model</span>
<span class="sd">                       parallel strategy likes ds_stra ((1, 8), (1, 8)), it requires using</span>
<span class="sd">                       set_auto_parallel_context(dataset_strategy=ds_stra).</span>
<span class="sd">        enable_parallel_optimizer (bool): This is a developing feature, which shards the weight update computation for</span>
<span class="sd">                       data parallel training in the benefit of time and memory saving. Currently, auto and semi auto</span>
<span class="sd">                       parallel mode support all optimizers in both Ascend and GPU. Data parallel mode only supports</span>
<span class="sd">                       `Lamb` and `AdamWeightDecay` in Ascend . Default: ``False`` .</span>
<span class="sd">        enable_alltoall (bool): A switch that allows AllToAll operators to be generated during communication. If its</span>
<span class="sd">                        value is ``False`` , there will be a combination of operators such as AllGather, Split and</span>
<span class="sd">                        Concat instead of AllToAll. Default: ``False`` .</span>
<span class="sd">        all_reduce_fusion_config (list): Set allreduce fusion strategy by parameters indices. Only support ReduceOp.SUM</span>
<span class="sd">                       and HCCL_WORLD_GROUP/NCCL_WORLD_GROUP. No Default, if it is not set, the fusion is closed.</span>
<span class="sd">        pipeline_stages (int): Set the stage information for pipeline parallel. This indicates how the devices are</span>
<span class="sd">                        distributed alone in the pipeline. The total devices will be divided into &#39;pipeline_stags&#39;</span>
<span class="sd">                        stages.</span>
<span class="sd">                        Default: ``1`` .</span>
<span class="sd">        parallel_optimizer_config (dict): A dict contains the keys and values for setting the parallel optimizer</span>
<span class="sd">                        configure. The configure provides more detailed behavior control about parallel training</span>
<span class="sd">                        when parallel optimizer is enabled. The configure will be effective when we use</span>
<span class="sd">                        mindspore.set_auto_parallel_context(enable_parallel_optimizer=True).</span>
<span class="sd">                        It supports the following keys.</span>

<span class="sd">                        - gradient_accumulation_shard(bool): If ``true`` , the accumulation gradient parameters will be</span>
<span class="sd">                          sharded across the data parallel devices. This will</span>
<span class="sd">                          introduce additional communication(ReduceScatter) at</span>
<span class="sd">                          each step when accumulate the gradients, but saves a</span>
<span class="sd">                          lot of device memories, thus can make model be trained</span>
<span class="sd">                          with larger batch size. This configure is effective only</span>
<span class="sd">                          when the model runs on pipeline training or gradient</span>
<span class="sd">                          accumulation with data parallel. Default ``False`` .</span>

<span class="sd">                        - parallel_optimizer_threshold(int): Set the threshold of parallel optimizer. When parallel</span>
<span class="sd">                          optimizer is enabled, parameters with size smaller than this threshold will not be sharded</span>
<span class="sd">                          across the devices. Parameter size = shape[0] \* ... \* shape[n] \* size(dtype). Non-negative.</span>
<span class="sd">                          Unit: KB. Default: ``64`` .</span>

<span class="sd">                        - optimizer_weight_shard_size(int): Set the optimizer weight shard group size, if you want to</span>
<span class="sd">                          specific the maximum group size across devices when the parallel optimizer is enabled.</span>
<span class="sd">                          The numerical range can be (0, device_num]. If pipeline parallel is enabled, the numerical</span>
<span class="sd">                          range is (0, device_num/stage]. If the size of data parallel communication domain</span>
<span class="sd">                          of the parameter cannot be divided by `optimizer_weight_shard_size`, then the specified</span>
<span class="sd">                          communication group size will not take effect. Default value is ``-1`` , which means the</span>
<span class="sd">                          optimizer weight shard group size will be the size of data parallel group of each parameter.</span>

<span class="sd">        comm_fusion (dict): A dict contains the types and configurations for setting the communication fusion. each</span>
<span class="sd">                        communication fusion config has two keys: &quot;mode&quot; and &quot;config&quot;.</span>
<span class="sd">                        It supports following communication fusion types and configurations:</span>

<span class="sd">                        - openstate: Whether turn on the communication fusion or not. If `openstate` is ``True`` ,</span>
<span class="sd">                          turn on the communication fusion, otherwise, turn off the communication fusion.</span>
<span class="sd">                          Default: ``True`` .</span>

<span class="sd">                        - allreduce: If communication fusion type is `allreduce`. The `mode` contains: `auto`, `size`</span>
<span class="sd">                          and `index`. In `auto` mode, AllReduce fusion is configured by gradients size and the default</span>
<span class="sd">                          fusion threshold is `64` MB. In &#39;size&#39; mode, AllReduce fusion is configured by gradients size</span>
<span class="sd">                          manually, and the fusion threshold must be larger than `0` MB. In `index` mode, it is same as</span>
<span class="sd">                          `all_reduce_fusion_config`.</span>

<span class="sd">                        - allgather: If communication fusion type is `allgather`. The `mode` contains: `auto`, `size`.</span>
<span class="sd">                          In `auto` mode, AllGather fusion is configured by gradients size, and the default fusion</span>
<span class="sd">                          threshold is `64` MB. In &#39;size&#39; mode, AllGather fusion is configured by gradients size</span>
<span class="sd">                          manually, and the fusion threshold must be larger than `0` MB.</span>

<span class="sd">                        - reducescatter: If communication fusion type is `reducescatter`. The `mode` contains: `auto`</span>
<span class="sd">                          and `size`. Config is same as `allgather`.</span>

<span class="sd">        strategy_ckpt_config (dict): A dict contains the configurations for setting the parallel strategy file. This</span>
<span class="sd">                        interface contains the functions of parameter `strategy_ckpt_load_file` and</span>
<span class="sd">                        `strategy_ckpt_save_file`, it is recommonded to use this parameter to replace those two</span>
<span class="sd">                        parameters.</span>
<span class="sd">                        It contains following configurations:</span>

<span class="sd">                        - load_file (str): The path to load parallel strategy checkpoint. If the file name extension is</span>
<span class="sd">                          `.json`, the file is loaded in JSON format. Otherwise, the file is loaded in ProtoBuf</span>
<span class="sd">                          format.</span>
<span class="sd">                          Default: &#39;&#39;</span>

<span class="sd">                        - save_file (str): The path to save parallel strategy checkpoint. If the file name extension is</span>
<span class="sd">                          `.json`, the file is saved in JSON format. Otherwise, the file is saved in ProtoBuf format.</span>
<span class="sd">                          Default: &#39;&#39;</span>

<span class="sd">                        - only_trainable_params (bool): Only save/load the strategy information for trainable parameter.</span>
<span class="sd">                          Default: ``True`` .</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If input key is not attribute in auto parallel context.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; ms.set_auto_parallel_context(device_num=8)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_auto_parallel_context(global_rank=0)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_auto_parallel_context(gradients_mean=True)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_auto_parallel_context(gradient_fp32_sync=False)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_auto_parallel_context(parallel_mode=&quot;auto_parallel&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_auto_parallel_context(search_mode=&quot;dynamic_programming&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_auto_parallel_context(auto_parallel_search_mode=&quot;dynamic_programming&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_auto_parallel_context(parameter_broadcast=False)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_auto_parallel_context(strategy_ckpt_load_file=&quot;./strategy_stage1.ckpt&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_auto_parallel_context(strategy_ckpt_save_file=&quot;./strategy_stage1.ckpt&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_auto_parallel_context(dataset_strategy=((1, 8), (1, 8)))</span>
<span class="sd">        &gt;&gt;&gt; ms.set_auto_parallel_context(enable_parallel_optimizer=False)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_auto_parallel_context(enable_alltoall=False)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_auto_parallel_context(all_reduce_fusion_config=[8, 160])</span>
<span class="sd">        &gt;&gt;&gt; ms.set_auto_parallel_context(pipeline_stages=2)</span>
<span class="sd">        &gt;&gt;&gt; parallel_config = {&quot;gradient_accumulation_shard&quot;: True, &quot;parallel_optimizer_threshold&quot;: 24,</span>
<span class="sd">        ...                    &quot;optimizer_weight_shard_size&quot;: 2}</span>
<span class="sd">        &gt;&gt;&gt; ms.set_auto_parallel_context(parallel_optimizer_config=parallel_config, enable_parallel_optimizer=True)</span>
<span class="sd">        &gt;&gt;&gt; config = {&quot;allreduce&quot;: {&quot;mode&quot;: &quot;size&quot;, &quot;config&quot;: 32}, &quot;allgather&quot;: {&quot;mode&quot;: &quot;size&quot;, &quot;config&quot;: 32}}</span>
<span class="sd">        &gt;&gt;&gt; ms.set_auto_parallel_context(comm_fusion=config)</span>
<span class="sd">        &gt;&gt;&gt; stra_ckpt_dict = {&quot;load_file&quot;: &quot;./stra0.ckpt&quot;, &quot;save_file&quot;: &quot;./stra1.ckpt&quot;, &quot;only_trainable_params&quot;: False}</span>
<span class="sd">        &gt;&gt;&gt; ms.set_auto_parallel_context(strategy_ckpt_config=stra_ckpt_dict)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_set_auto_parallel_context</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>


<div class="viewcode-block" id="get_auto_parallel_context"><a class="viewcode-back" href="../../api_python/mindspore/mindspore.get_auto_parallel_context.html#mindspore.get_auto_parallel_context">[文档]</a><span class="k">def</span> <span class="nf">get_auto_parallel_context</span><span class="p">(</span><span class="n">attr_key</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get auto parallel context attribute value according to the key.</span>

<span class="sd">    Args:</span>
<span class="sd">        attr_key (str): The key of the attribute.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Returns attribute value according to the key.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If input key is not attribute in auto parallel context.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; parallel_mode = ms.get_auto_parallel_context(&quot;parallel_mode&quot;)</span>
<span class="sd">        &gt;&gt;&gt; dataset_strategy = ms.get_auto_parallel_context(&quot;dataset_strategy&quot;)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_get_auto_parallel_context</span><span class="p">(</span><span class="n">attr_key</span><span class="p">)</span></div>


<div class="viewcode-block" id="reset_auto_parallel_context"><a class="viewcode-back" href="../../api_python/mindspore/mindspore.reset_auto_parallel_context.html#mindspore.reset_auto_parallel_context">[文档]</a><span class="k">def</span> <span class="nf">reset_auto_parallel_context</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reset auto parallel context attributes to the default values.</span>

<span class="sd">    - device_num: 1.</span>
<span class="sd">    - global_rank: 0.</span>
<span class="sd">    - gradients_mean: False.</span>
<span class="sd">    - gradient_fp32_sync: True.</span>
<span class="sd">    - parallel_mode: &#39;stand_alone&#39;.</span>
<span class="sd">    - search_mode: &#39;recursive_programming&#39;.</span>
<span class="sd">    - auto_parallel_search_mode: &#39;recursive_programming&#39;.</span>
<span class="sd">    - parameter_broadcast: False.</span>
<span class="sd">    - strategy_ckpt_load_file: &#39;&#39;.</span>
<span class="sd">    - strategy_ckpt_save_file: &#39;&#39;.</span>
<span class="sd">    - full_batch: False.</span>
<span class="sd">    - enable_parallel_optimizer: False.</span>
<span class="sd">    - enable_alltoall: False.</span>
<span class="sd">    - pipeline_stages: 1.</span>
<span class="sd">    - fusion_threshold: 64.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; ms.reset_auto_parallel_context()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_reset_auto_parallel_context</span><span class="p">()</span></div>


<div class="viewcode-block" id="set_offload_context"><a class="viewcode-back" href="../../api_python/mindspore/mindspore.set_offload_context.html#mindspore.set_offload_context">[文档]</a><span class="nd">@args_type_check</span><span class="p">(</span><span class="n">offload_config</span><span class="o">=</span><span class="nb">dict</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">set_offload_context</span><span class="p">(</span><span class="n">offload_config</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configure heterogeneous training detailed parameters to adjust the offload strategy.</span>

<span class="sd">    Note:</span>
<span class="sd">        The offload configuration is only used if the memory offload feature is enabled</span>
<span class="sd">        via mindspore.set_context(memory_offload=&quot;ON&quot;).</span>

<span class="sd">    Args:</span>
<span class="sd">        offload_config (dict): A dict contains the keys and values for setting the offload context</span>
<span class="sd">            configure.It supports the following keys.</span>

<span class="sd">            - offload_path (str):  The path of offload, relative paths are supported. Default: ``&quot;./offload&quot;``.</span>
<span class="sd">            - offload_cpu_size (str):  The cpu memory size for offload. The format is &quot;xxGB&quot;.</span>
<span class="sd">            - offload_disk_size (str): The disk size for offload. The format is &quot;xxGB&quot;</span>
<span class="sd">            - hbm_ratio (float): The ratio that can be used based on the maximum device memory.</span>
<span class="sd">              The range is (0,1], Default: ``1.0``.</span>
<span class="sd">            - cpu_ratio (float): The ratio that can be used based on the maximum host memory.</span>
<span class="sd">              The range is (0,1], Default: ``1.0``.</span>
<span class="sd">            - enable_pinned_mem (bool): The flag of whether enabling Pinned Memory. Default: ``True``.</span>
<span class="sd">            - enable_aio (bool): The flag of whether enabling aio. Default: ``True``.</span>
<span class="sd">            - aio_block_size (str): The size of aio block. The format is &quot;xxGB&quot;.</span>
<span class="sd">            - aio_queue_depth (int): The depth of aio queue.</span>
<span class="sd">            - offload_param (str):  The param for offload destination, cpu or disk, Default: ``&quot;&quot;``.</span>
<span class="sd">            - offload_checkpoint (str):  The checkpoint for offload destination, only valid if recompute is turned on,</span>
<span class="sd">              cpu or disk, Default: ``&quot;&quot;``.</span>
<span class="sd">            - auto_offload (bool): The flag of whether auto offload. Default: ``True``.</span>
<span class="sd">            - host_mem_block_size (str): The memory block size of host memory pool. The format is &quot;xxGB&quot;</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If input key is not attribute in auto parallel context.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import context</span>
<span class="sd">        &gt;&gt;&gt; context.set_offload_context(offload_config={&quot;offload_param&quot;:&quot;cpu&quot;})</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_set_offload_context</span><span class="p">(</span><span class="n">offload_config</span><span class="p">)</span></div>


<div class="viewcode-block" id="get_offload_context"><a class="viewcode-back" href="../../api_python/mindspore/mindspore.get_offload_context.html#mindspore.get_offload_context">[文档]</a><span class="k">def</span> <span class="nf">get_offload_context</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets the offload configuration parameters. Configure through interface mindspore.set_offload_context().</span>
<span class="sd">    If the user is not set, the default configuration is obtained.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Dict, heterogeneous training offload detailed configuration parameters.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import context</span>
<span class="sd">        &gt;&gt;&gt; offload_config = context.get_offload_context()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_get_offload_context</span><span class="p">()</span></div>


<span class="k">def</span> <span class="nf">_check_target_specific_cfgs</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">arg_key</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Checking whether a config is suitable for a specified device&quot;&quot;&quot;</span>
    <span class="n">device_cfgs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;enable_graph_kernel&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Ascend&#39;</span><span class="p">,</span> <span class="s1">&#39;GPU&#39;</span><span class="p">,</span> <span class="s1">&#39;CPU&#39;</span><span class="p">],</span>
        <span class="s1">&#39;graph_kernel_flags&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Ascend&#39;</span><span class="p">,</span> <span class="s1">&#39;GPU&#39;</span><span class="p">,</span> <span class="s1">&#39;CPU&#39;</span><span class="p">],</span>
        <span class="s1">&#39;enable_reduce_precision&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Ascend&#39;</span><span class="p">],</span>
        <span class="s1">&#39;print_file_path&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Ascend&#39;</span><span class="p">],</span>
        <span class="s1">&#39;variable_memory_max_size&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Ascend&#39;</span><span class="p">],</span>
        <span class="s1">&#39;max_device_memory&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Ascend&#39;</span><span class="p">,</span> <span class="s1">&#39;GPU&#39;</span><span class="p">],</span>
        <span class="s1">&#39;mempool_block_size&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;GPU&#39;</span><span class="p">,</span> <span class="s1">&#39;Ascend&#39;</span><span class="p">],</span>
        <span class="s1">&#39;disable_format_transform&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;GPU&#39;</span><span class="p">],</span>
        <span class="s1">&#39;ascend_config&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Ascend&#39;</span><span class="p">],</span>
        <span class="s1">&#39;gpu_config&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;GPU&#39;</span><span class="p">],</span>
    <span class="p">}</span>
    <span class="c1"># configs not in map device_cfgs are supposed to be suitable for all devices</span>
    <span class="k">if</span> <span class="n">arg_key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">device_cfgs</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="n">supported_devices</span> <span class="o">=</span> <span class="n">device_cfgs</span><span class="p">[</span><span class="n">arg_key</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">device</span> <span class="ow">in</span> <span class="n">supported_devices</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.set_context&#39;, when set the argument &#39;</span><span class="si">{</span><span class="n">arg_key</span><span class="si">}</span><span class="s2">&#39;, &quot;</span>
                   <span class="sa">f</span><span class="s2">&quot;the argument &#39;device_target&#39; only supports devices in &#39;</span><span class="si">{</span><span class="n">supported_devices</span><span class="si">}</span><span class="s2">&#39;, &quot;</span>
                   <span class="sa">f</span><span class="s2">&quot;but got &#39;</span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&#39;, ignore it.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">False</span>


<div class="viewcode-block" id="set_context"><a class="viewcode-back" href="../../api_python/mindspore/mindspore.set_context.html#mindspore.set_context">[文档]</a><span class="nd">@args_type_check</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">precompile_only</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">device_id</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">save_graphs</span><span class="o">=</span><span class="p">(</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">int</span><span class="p">),</span>
                 <span class="n">save_graphs_path</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">enable_dump</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">aoe_tune_mode</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">aoe_config</span><span class="o">=</span><span class="nb">dict</span><span class="p">,</span>
                 <span class="n">save_dump_path</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">enable_reduce_precision</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">variable_memory_max_size</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
                 <span class="n">enable_auto_mixed_precision</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">inter_op_parallel_num</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
                 <span class="n">enable_graph_kernel</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">reserve_class_name_in_scope</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">check_bprop</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span>
                 <span class="n">max_device_memory</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">print_file_path</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">max_call_depth</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">env_config_path</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
                 <span class="n">graph_kernel_flags</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">save_compile_cache</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">runtime_num_threads</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">load_compile_cache</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span>
                 <span class="n">grad_for_scalar</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">pynative_synchronize</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">mempool_block_size</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">disable_format_transform</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span>
                 <span class="n">op_timeout</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">ascend_config</span><span class="o">=</span><span class="nb">dict</span><span class="p">,</span> <span class="n">jit_syntax_level</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
                 <span class="n">jit_enable_inplace_ops</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">gpu_config</span><span class="o">=</span><span class="nb">dict</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">set_context</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Set context for running environment.</span>

<span class="sd">    Context should be configured before running your program. If there is no configuration,</span>
<span class="sd">    it will be automatically set according to the device target by default.</span>

<span class="sd">    Note:</span>
<span class="sd">        Attribute name is required for setting attributes.</span>
<span class="sd">        The mode is not recommended to be changed after net was initialized because the implementations of some</span>
<span class="sd">        operations are different in graph mode and pynative mode. Default: ``PYNATIVE_MODE`` .</span>

<span class="sd">    Some configurations are device specific, see the below table for details:</span>

<span class="sd">    +-------------------------+------------------------------+----------------------------+</span>
<span class="sd">    | Function Classification |   Configuration Parameters   |   Hardware Platform Support|</span>
<span class="sd">    +=========================+==============================+============================+</span>
<span class="sd">    | System Configuration    |   device_id                  |   CPU/GPU/Ascend           |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |   device_target              |   CPU/GPU/Ascend           |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  max_device_memory           |  GPU/Ascend                |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  variable_memory_max_size    |  Ascend                    |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  mempool_block_size          |  GPU/Ascend                |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  op_timeout                  |  Ascend                    |</span>
<span class="sd">    +-------------------------+------------------------------+----------------------------+</span>
<span class="sd">    | Debug Configuration     |  save_graphs                 |  CPU/GPU/Ascend            |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  save_graphs_path            |  CPU/GPU/Ascend            |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  enable_dump                 |  Ascend                    |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  save_dump_path              |  Ascend                    |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  deterministic               |  Ascend                    |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  print_file_path             |  Ascend                    |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  env_config_path             |  CPU/GPU/Ascend            |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  precompile_only             |  CPU/GPU/Ascend            |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  reserve_class_name_in_scope |  CPU/GPU/Ascend            |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  pynative_synchronize        |  CPU/GPU/Ascend            |</span>
<span class="sd">    +-------------------------+------------------------------+----------------------------+</span>
<span class="sd">    | Executive Control       |   mode                       |   CPU/GPU/Ascend           |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  enable_graph_kernel         |  Ascend/GPU                |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  graph_kernel_flags          |  Ascend/GPU                |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  enable_reduce_precision     |  Ascend                    |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  aoe_tune_mode               |  Ascend                    |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  aoe_config                  |  Ascend                    |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  check_bprop                 |  CPU/GPU/Ascend            |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  max_call_depth              |  CPU/GPU/Ascend            |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  grad_for_scalar             |  CPU/GPU/Ascend            |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  enable_compile_cache        |  CPU/GPU/Ascend            |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  inter_op_parallel_num       |  CPU/GPU/Ascend            |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  runtime_num_threads         |  CPU/GPU/Ascend            |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  compile_cache_path          |  CPU/GPU/Ascend            |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  disable_format_transform    |  GPU                       |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  support_binary              |  CPU/GPU/Ascend            |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  memory_optimize_level       |  CPU/GPU/Ascend            |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  memory_offload              |  GPU/Ascend                |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  ascend_config               |  Ascend                    |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  jit_syntax_level            |  CPU/GPU/Ascend            |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  gpu_config                  |  GPU                       |</span>
<span class="sd">    +-------------------------+------------------------------+----------------------------+</span>

<span class="sd">    Args:</span>
<span class="sd">        device_id (int): ID of the target device, the value must be in [0, device_num_per_host-1],</span>
<span class="sd">            while device_num_per_host should be no more than 4096. Default: ``0`` .</span>
<span class="sd">        device_target (str): The target device to run, support &quot;Ascend&quot;, &quot;GPU&quot;, and &quot;CPU&quot;.</span>
<span class="sd">            If device target is not set, the version of MindSpore package is used.</span>
<span class="sd">        max_device_memory (str): Set the maximum memory available for devices. The format is &quot;xxGB&quot;.</span>
<span class="sd">            Default: ``&quot; 1024GB&quot;`` . The actual used memory size is the minimum of the available memory of the device</span>
<span class="sd">            and max_device_memory. &#39;max_device_memory&#39; should be set before the program runs.</span>
<span class="sd">        variable_memory_max_size (str): This parameter is deprecated, and will be removed in a future version.</span>
<span class="sd">            Please use parameter &#39;max_device_memory&#39; instead.</span>
<span class="sd">        mempool_block_size (str): Set the size of the memory pool block in PyNative mode for devices.</span>
<span class="sd">            The format is &quot;xxGB&quot;. Default: ``&quot;1GB&quot;`` . Minimum size is &quot;1G&quot;. The actual used memory block size is the</span>
<span class="sd">            minimum of the available memory of the device and mempool_block_size.</span>
<span class="sd">        op_timeout (int): Set the maximum duration of executing an operator in seconds.</span>
<span class="sd">            If the execution time exceeds this value, system will terminate the task. 0 means endless wait.</span>
<span class="sd">            Default: ``1900`` .</span>
<span class="sd">        save_graphs (bool or int): Whether to save intermediate compilation graphs. Default: ``0`` .</span>
<span class="sd">            Available values are:</span>

<span class="sd">            - False or 0: disable saving of intermediate compilation graphs.</span>
<span class="sd">            - 1: some intermediate files will be generated during graph compilation.</span>
<span class="sd">            - True or 2: Generate more ir files related to backend process.</span>
<span class="sd">            - 3: Generate visualization computing graphs and detailed frontend ir graphs.</span>

<span class="sd">            When the `save_graphs` attribute is set as ``True`` , ``1`` , ``2`` or ``3`` , attribute of</span>
<span class="sd">            `save_graphs_path` is used to set the intermediate compilation graph storage path. By default, the graphs</span>
<span class="sd">            are saved in the current directory.</span>
<span class="sd">        save_graphs_path (str): Path to save graphs. Default: &quot;.&quot;.</span>
<span class="sd">            If the specified directory does not exist, the system will automatically create the directory.</span>
<span class="sd">            During distributed training, graphs will be saved to the directory of</span>
<span class="sd">            `save_graphs_path/rank_${rank_id}/`. `rank_id` is the ID of the current device in the cluster.</span>
<span class="sd">        deterministic (str): Whether to enable op run in deterministic mode. The value must be in the</span>
<span class="sd">            range of [&#39;ON&#39;, &#39;OFF&#39;], and the default value is ``&#39;OFF&#39;`` .</span>

<span class="sd">            - &quot;ON&quot;: Enable operator deterministic running mode.</span>
<span class="sd">            - &quot;OFF&quot;: Disable operator deterministic running mode.</span>

<span class="sd">            When deterministic mode is on, model ops will be deterministic in Ascend. This means that if op run</span>
<span class="sd">            multiple times with the same inputs on the same hardware, it will have the exact same outputs each time.</span>
<span class="sd">            This is useful for debugging models.</span>
<span class="sd">        enable_dump (bool): This parameters is deprecated, and will be deleted in the next version.</span>
<span class="sd">        save_dump_path (str): This parameters is deprecated, and will be deleted in the next version.</span>
<span class="sd">        print_file_path (str): The path of saving print data. If this parameter is set, print data is saved to</span>
<span class="sd">            a file by default, and print_file_path is not set, the screen will be displayed.</span>
<span class="sd">            If the saved file already exists, the timestamp suffix will be added to the file. Saving data to a file</span>
<span class="sd">            solves the problem of data loss in screen printing when a large amount of data is generated.</span>
<span class="sd">            If it is not set, an error will be reported: prompt to set the upper absolute path.</span>
<span class="sd">        env_config_path (str): Config path for DFX.</span>
<span class="sd">            Through mindspore.set_context(env_config_path=&quot;./mindspore_config.json&quot;)</span>

<span class="sd">            configure RDR:</span>

<span class="sd">            - enable: controls whether the RDR is enabled to collect the key data during training and</span>
<span class="sd">              save key data in the fault scenario. When set to ``true`` , the RDR will be turned on.</span>
<span class="sd">              When set to ``false`` , the RDR will be turned off.</span>
<span class="sd">            - mode: sets the mode of RDR on exporting data. When set to ``1`` , the RDR only exports data</span>
<span class="sd">              in the fault scenario. When set to ``2`` , the RDR exports data in the fault scenario and the</span>
<span class="sd">              normal end scenario. Default: ``1`` .</span>
<span class="sd">            - path: sets the path where RDR saves data. The current path must be absolute.</span>

<span class="sd">            Memory reuse:</span>

<span class="sd">            - mem_Reuse: controls whether the memory reuse function is turned on. When set to ``True`` ,</span>
<span class="sd">              the memory reuse function is turned on. When set to ``False`` , the memory reuse function is turned off.</span>

<span class="sd">        precompile_only (bool): Whether to only precompile the network. Default: ``False`` .</span>
<span class="sd">            If set to ``True`` , the network will only be compiled, not executed.</span>
<span class="sd">        reserve_class_name_in_scope (bool) : Whether to save the network class name in the scope. Default: ``True`` .</span>
<span class="sd">            Each node has a scope. A scope of a subnode is the name of its parent node. If reserve_class_name_in_scope</span>
<span class="sd">            is set to ``True`` , the class name will be saved after keyword &#39;net-&#39; in the scope.</span>
<span class="sd">            For example:</span>

<span class="sd">            Default/net-Net1/net-Net2 (reserve_class_name_in_scope=True)</span>

<span class="sd">            Default/net/net (reserve_class_name_in_scope=False)</span>

<span class="sd">        pynative_synchronize (bool): Whether to enable synchronous execution of the device in PyNative mode.</span>
<span class="sd">            Default: ``False`` . When the value is set to ``False`` , the operator is executed asynchronously on the</span>
<span class="sd">            device. When an error occurs in the execution of the operator, the specific error script code location</span>
<span class="sd">            cannot be located, when the value is set to ``True`` , the operator is executed synchronously on the</span>
<span class="sd">            device. It will reduce the execution performance of the program. At this time, when an error occurs in the</span>
<span class="sd">            execution of the operator, the location of the error script code can be located according to the call stack</span>
<span class="sd">            of the error.</span>
<span class="sd">        mode (int): Running in GRAPH_MODE(0) or PYNATIVE_MODE(1).</span>
<span class="sd">            Both modes support all backends. Default: ``PYNATIVE_MODE`` .</span>
<span class="sd">        enable_graph_kernel (bool): Whether to enable graph kernel fusion to optimize network execution performance.</span>
<span class="sd">            Default: ``False`` .</span>
<span class="sd">            Indicates whether to enable image-computing convergence to optimize network execution performance.</span>
<span class="sd">            If enable_graph_kernel is set to ``True`` , acceleration can be enabled.</span>
<span class="sd">            For details of graph kernel fusion, please check</span>
<span class="sd">            `Enabling Graph Kernel Fusion</span>
<span class="sd">            &lt;https://www.mindspore.cn/tutorials/experts/en/master/optimize/graph_fusion_engine.html&gt;`_.</span>
<span class="sd">        graph_kernel_flags (str):</span>
<span class="sd">            Optimization options of graph kernel fusion, and the priority is higher when it conflicts</span>
<span class="sd">            with enable_graph_kernel. Only for experienced users.</span>
<span class="sd">            For example,</span>

<span class="sd">            .. code-block::</span>

<span class="sd">                mindspore.set_context(graph_kernel_flags=&quot;--opt_level=2 --dump_as_text&quot;)</span>

<span class="sd">            Some general options:</span>

<span class="sd">            - opt_level: Set the optimization level.</span>
<span class="sd">              Default: ``2`` . Graph kernel fusion can be enabled equivalently by setting opt_level greater than 0.</span>
<span class="sd">              Available values are:</span>

<span class="sd">              - 0: disables graph kernel fusion;</span>
<span class="sd">              - 1: enables the basic fusion of operators;</span>
<span class="sd">              - 2: includes all optimizations of level 1,</span>
<span class="sd">                and turns on more optimizations such as CSE, arithmetic simplification and so on;</span>
<span class="sd">              - 3: includes all optimizations of level 2, and turns on more optimizations such as SitchingFusion,</span>
<span class="sd">                ParallelFusion and so on. Optimizations of this level are radical and unstable in some scenarios.</span>
<span class="sd">                Be caution when using this level.</span>

<span class="sd">            - dump_as_text: dumps detail info as text files. Default: ``False`` .</span>

<span class="sd">        enable_reduce_precision (bool): Whether to enable precision reduction.</span>
<span class="sd">            If the operator does not support the user-specified precision, the precision will</span>
<span class="sd">            be changed automatically. Default: ``True`` .</span>
<span class="sd">        aoe_tune_mode (str): AOE tuning mode setting, which is not set by default.</span>
<span class="sd">            When set to ``&quot;online&quot;`` , the tuning in online function is turned on.</span>
<span class="sd">            When set to ``&quot;offline&quot;`` , ge graph will be save for offline tuning.</span>
<span class="sd">        aoe_config (dict): Set the parameters specific to Ascend Optimization Engine. It is not set by default.</span>

<span class="sd">            - job_type (str): Mode type setting, default value is ``&quot;2&quot;``.</span>

<span class="sd">              - ``&quot;1&quot;``: subgraph tuning;</span>
<span class="sd">              - ``&quot;2&quot;``: operator tuning.</span>

<span class="sd">        check_bprop (bool): Whether to check back propagation nodes. The checking ensures that the shape and dtype</span>
<span class="sd">            of back propagation node outputs is the same as input parameters. Default: ``False`` .</span>
<span class="sd">        max_call_depth (int): Specify the maximum depth of function call. Must be positive integer. Default: ``1000`` .</span>
<span class="sd">            The max_call_depth parameter needs to be set when the nested call is too deep or the number</span>
<span class="sd">            of subgraphs is too large. If max_call_depth is set larger than before, the system max stack depth should be</span>
<span class="sd">            set larger too, otherwise a `core dumped` exception may be raised because of system stack overflow.</span>
<span class="sd">        grad_for_scalar (bool):  Whether to get gradient for scalar. Default: ``False`` .</span>
<span class="sd">            When grad_for_scalar is set to ``True`` , the function&#39;s scalar input can be derived.</span>
<span class="sd">            The default value is ``False`` . Because the back-end does not support scaling operations currently,</span>
<span class="sd">            this interface only supports simple operations that can be deduced by the front-end.</span>
<span class="sd">        enable_compile_cache (bool): Whether to save or load the cache of the graph compiled by front-end.</span>
<span class="sd">            After enable_compile_cache is set to ``True`` , during the first execution, a hardware-independent</span>
<span class="sd">            compilation cache is generated and exported to a MINDIR file. When the network is executed again,</span>
<span class="sd">            if enable_compile_cache is still set to ``True`` and the network scripts are not changed,</span>
<span class="sd">            the compile cache is loaded. Note that only limited automatic detection for the changes of</span>
<span class="sd">            python scripts is supported by now, which means that there is a correctness risk. Default: ``False`` .</span>
<span class="sd">            This is an experimental prototype that is subject to change and/or deletion.</span>
<span class="sd">        compile_cache_path (str): Path to save the compile cache. Default: &quot;.&quot;.</span>
<span class="sd">            If the specified directory does not exist, the system will automatically create the directory.</span>
<span class="sd">            The cache will be saved to the directory of `compile_cache_path/rank_${rank_id}/`. The `rank_id` is</span>
<span class="sd">            the ID of the current device in the cluster.</span>
<span class="sd">        inter_op_parallel_num(int): The thread number of op parallel at the same time. Default value is ``0`` ,</span>
<span class="sd">            which means use the default num.</span>
<span class="sd">        runtime_num_threads(int): The thread pool number of cpu kernel used in runtime,</span>
<span class="sd">            which must bigger than or equal to 0. Default value is ``30`` , if you run many processes at</span>
<span class="sd">            the same time, you should set the value smaller to avoid thread contention.</span>
<span class="sd">        disable_format_transform (bool): Whether to disable the automatic format transform function from NCHW to NHWC.</span>
<span class="sd">            When the network training performance of fp16 is worse than fp32, `disable_format_transform` can be set to</span>
<span class="sd">            ``True`` to try to improve training performance. Default: ``False`` .</span>
<span class="sd">        support_binary (bool): Whether to support run .pyc or .so in graph mode. If want to support run .so or .pyc</span>
<span class="sd">            in graph mode, coulde set &#39;support_binary&#39; to be ``True`` , and run once .py file. It would save the source</span>
<span class="sd">            of the interfaces would be compiled by MindSpore to the interfaces definition .py file that should be</span>
<span class="sd">            guaranteed to be writable. Then compile the .py file to the .pyc or .so file, and could run in Graph mode.</span>
<span class="sd">        memory_optimize_level (str): The memory optimize level.</span>
<span class="sd">            Default: O0. The value must be in [&#39;O0&#39;, &#39;O1&#39;].</span>

<span class="sd">            - O0: priority performance option, disable SOMAS (Safe Optimized Memory Allocation Solver).</span>
<span class="sd">            - O1: priority memory option, enable SOMAS.</span>
<span class="sd">        memory_offload (str): Whether to enable the memory offload function. When it is enabled, the idle data will be</span>
<span class="sd">            temporarily copied to the host side in the case of insufficient device memory. The value must be in the</span>
<span class="sd">            range of [&#39;ON&#39;, &#39;OFF&#39;], and the default value is ``&#39;OFF&#39;`` .</span>

<span class="sd">            - ON: Enable the memory Offload function. On Ascend hardware platform, this parameter does not take effect</span>
<span class="sd">              when the environment variable &quot;GRAPH_OP_RUN=1&quot; is not set; This parameter does not take effect when</span>
<span class="sd">              memory_optimize_level is set &#39;O1&#39;.</span>
<span class="sd">            - OFF: Turn off the memory Offload function.</span>
<span class="sd">        ascend_config (dict): Set the parameters specific to Ascend hardware platform. It is not set by default.</span>
<span class="sd">            The default value of `precision_mode`, `jit_compile` and</span>
<span class="sd">            `atomic_clean_policy` are experimental parameters, may change in the future.</span>

<span class="sd">            - precision_mode (str): Mixed precision mode setting, and the default value of inference network</span>
<span class="sd">              is ``force_fp16`` . The value range is as follows:</span>

<span class="sd">              - force_fp16: When the operator supports both float16 and float32, select float16 directly.</span>
<span class="sd">              - allow_fp32_to_fp16: When the operator does not support the float32 data type, directly reduce</span>
<span class="sd">                the precision of float16.</span>
<span class="sd">              - allow_mix_precision: Automatic mixing precision, facing the whole network operator, according</span>
<span class="sd">                to the built-in optimization strategy, automatically reduces the precision of some operators</span>
<span class="sd">                to float16 or bfloat16.</span>
<span class="sd">              - must_keep_origin_dtype: Keep the accuracy of the original drawing.</span>
<span class="sd">              - force_fp32: When the input of the matrix calculation operator is float16 and the output supports</span>
<span class="sd">                float16 and float32, output is forced to float32.</span>
<span class="sd">              - allow_fp32_to_bf16: When the operator does not support the float32 data type, directly reduce</span>
<span class="sd">                the precision of bfloat16.</span>
<span class="sd">              - allow_mix_precision_fp16: Automatic mixing precision, facing the whole network operator, automatically</span>
<span class="sd">                reduces the precision of some operators to float16 according to the built-in optimization strategy.</span>
<span class="sd">              - allow_mix_precision_bf16: Automatic mixing precision, facing the whole network operator, according to</span>
<span class="sd">                the built-in optimization strategy, automatically reduces the precision of some operators to bfloat16.</span>

<span class="sd">            - jit_compile (bool): Whether to select online compilation. the default value is based on CANN.</span>
<span class="sd">            - atomic_clean_policy (int): The policy for cleaning memory occupied by atomic operators in the network.</span>
<span class="sd">              Default: ``1`` .</span>

<span class="sd">              - 0: The memory occupied by all atomic operators in the network is cleaned centrally.</span>
<span class="sd">              - 1: Memory is not cleaned centrally and each atomic operator in the network is cleaned separately.</span>
<span class="sd">                When the memory of the network exceeds the limit, you may try this cleaning policy, but it may cause</span>
<span class="sd">                performance loss.</span>
<span class="sd">            - matmul_allow_hf32 (bool): Whether to convert FP32 to HF32 for Matmul operators. Default value: ``False``.</span>
<span class="sd">              This is an experimental prototype that is subject to change and/or deletion.</span>
<span class="sd">              For detailed information, please refer to `Ascend community &lt;https://www.hiascend.com/&gt;`_ .</span>
<span class="sd">            - conv_allow_hf32 (bool): Whether to convert FP32 to HF32 for Conv operators. Default value: ``True``.</span>
<span class="sd">              This is an experimental prototype that is subject to change and/or deletion.</span>
<span class="sd">              For detailed information, please refer to `Ascend community &lt;https://www.hiascend.com/&gt;`_ .</span>
<span class="sd">            - op_precision_mode (str): Path to config file of op precision mode. For detailed information, please refer</span>
<span class="sd">              to `Ascend community &lt;https://www.hiascend.com/&gt;`_ .</span>
<span class="sd">            - parallel_speed_up_json_path(Union[str, None]): The path to the parallel speed up json file, configuration</span>
<span class="sd">              can refer to `parallel_speed_up.json</span>
<span class="sd">              &lt;https://gitee.com/mindspore/mindspore/blob/master/config/parallel_speed_up.json&gt;`_ .</span>
<span class="sd">              If its value is None or &#39;&#39;, it does not take effect. Default None.</span>

<span class="sd">              - recompute_comm_overlap (bool): Enable overlap between recompute ops and communication ops if True.</span>
<span class="sd">                Default: False.</span>
<span class="sd">              - matmul_grad_comm_overlap (bool): Enable overlap between grad ops and communication ops if True.</span>
<span class="sd">                Default: False.</span>
<span class="sd">              - enable_task_opt (bool): Enable the optimization of the number of tasks for each communication if True.</span>
<span class="sd">                Default: False.</span>
<span class="sd">              - interleaved_matmul_comm (bool): Enable interleaved optimization of Matmul-Comm if True. Default: False.</span>
<span class="sd">              - interleaved_layernorm_comm (bool): Enable interleaved optimization of LayerNorm-Comm if True.</span>
<span class="sd">                Default: False.</span>

<span class="sd">        jit_syntax_level (int): Set JIT syntax level for graph compiling, triggered by GRAPH_MODE and @jit decorator.</span>
<span class="sd">            The value must be ``STRICT`` or ``LAX`` . Default: ``LAX`` . All levels support all backends.</span>

<span class="sd">            - ``STRICT`` : Only basic syntax is supported, and execution performance is optimal. Can be used for MindIR</span>
<span class="sd">              load and export.</span>
<span class="sd">            - ``LAX`` : Compatible with all Python syntax as much as possible. However, execution performance may be</span>
<span class="sd">              affected and not optimal. Cannot be used for MindIR load and export due to some syntax that may not be</span>
<span class="sd">              able to be exported.</span>

<span class="sd">        gpu_config (dict): Set the parameters specific to gpu hardware platform. It is not set by default.</span>
<span class="sd">            Currently, only setting `conv_fprop_algo` and `conv_dgrad_algo` and `conv_wgrad_algo` and `conv_allow_tf32`</span>
<span class="sd">            and `matmul_allow_tf32` are supported on GPU hardware platform.</span>

<span class="sd">            - conv_fprop_algo (str): Specifies convolution forward algorithm and the default value is &#39;normal&#39;,</span>
<span class="sd">              The value range is as follows:</span>

<span class="sd">              - normal: Use the heuristic search algorithm.</span>
<span class="sd">              - performance: Use the trial search algorithm.</span>
<span class="sd">              - implicit_gemm: This algorithm expresses the convolution as a matrix product without actually explicitly</span>
<span class="sd">                forming the matrix that holds the input tensor data.</span>
<span class="sd">              - implicit_precomp_gemm: This algorithm expresses convolution as a matrix product without actually</span>
<span class="sd">                explicitly forming the matrix that holds the input tensor data, but still needs some memory workspace to</span>
<span class="sd">                precompute some indices in order to facilitate the implicit construction of the matrix that holds the</span>
<span class="sd">                input tensor data.</span>
<span class="sd">              - gemm: This algorithm expresses the convolution as an explicit matrix product. A significant memory</span>
<span class="sd">                workspace is needed to store the matrix that holds the input tensor data.</span>
<span class="sd">              - direct: This algorithm expresses the convolution as a direct convolution (for example, without</span>
<span class="sd">                implicitly or explicitly doing a matrix multiplication).</span>
<span class="sd">              - fft: This algorithm uses the Fast-Fourier Transform approach to compute the convolution. A significant</span>
<span class="sd">                memory workspace is needed to store intermediate results.</span>
<span class="sd">              - fft_tiling: This algorithm uses the Fast-Fourier Transform approach but splits the inputs into tiles.</span>
<span class="sd">                A significant memory workspace is needed to store intermediate results but less than fft algorithm for</span>
<span class="sd">                large size images.</span>
<span class="sd">              - winograd: This algorithm uses the Winograd Transform approach to compute the convolution. A reasonably</span>
<span class="sd">                sized workspace is needed to store intermediate results.</span>
<span class="sd">              - winograd_nonfused: This algorithm uses the Winograd Transform approach to compute the convolution. A</span>
<span class="sd">                significant workspace may be needed to store intermediate results.</span>
<span class="sd">            - conv_dgrad_algo (str): Specifies convolution data grad algorithm and the default value is &#39;normal&#39;,</span>
<span class="sd">              The value range is as follows:</span>

<span class="sd">              - normal: Use the heuristic search algorithm.</span>
<span class="sd">              - performance: Use the trial search algorithm.</span>
<span class="sd">              - algo_0: This algorithm expresses the convolution as a sum of matrix products without actually explicitly</span>
<span class="sd">                forming the matrix that holds the input tensor data. The sum is done using the atomic add operation,</span>
<span class="sd">                thus the results are non-deterministic.</span>
<span class="sd">              - algo_1: This algorithm expresses the convolution as a matrix product without actually explicitly forming</span>
<span class="sd">                the matrix that holds the input tensor data. The results are deterministic.</span>
<span class="sd">              - fft: This algorithm uses a Fast-Fourier Transform approach to compute the convolution. A significant</span>
<span class="sd">                memory workspace is needed to store intermediate results. The results are deterministic.</span>
<span class="sd">              - fft_tiling: This algorithm uses the Fast-Fourier Transform approach but splits the inputs into tiles.</span>
<span class="sd">                A significant memory workspace is needed to store intermediate results but less than fft for large size</span>
<span class="sd">                images. The results are deterministic.</span>
<span class="sd">              - winograd: This algorithm uses the Winograd Transform approach to compute the convolution. A reasonably</span>
<span class="sd">                sized workspace is needed to store intermediate results. The results are deterministic.</span>
<span class="sd">              - winograd_nonfused: This algorithm uses the Winograd Transform approach to compute the convolution.</span>
<span class="sd">                A significant workspace may be needed to store intermediate results. The results are deterministic.</span>
<span class="sd">            - conv_wgrad_algo (str): Specifies convolution filter grad algorithm and the default value is &#39;normal&#39;,</span>
<span class="sd">              The value range is as follows:</span>

<span class="sd">              - normal: Use the heuristic search algorithm.</span>
<span class="sd">              - performance: Use the trial search algorithm.</span>
<span class="sd">              - algo_0: This algorithm expresses the convolution as a sum of matrix products without actually explicitly</span>
<span class="sd">                forming the matrix that holds the input tensor data. The sum is done using the atomic add operation,</span>
<span class="sd">                thus the results are non-deterministic.</span>
<span class="sd">              - algo_1: This algorithm expresses the convolution as a matrix product without actually explicitly forming</span>
<span class="sd">                the matrix that holds the input tensor data. The results are deterministic.</span>
<span class="sd">              - fft: This algorithm uses a Fast-Fourier Transform approach to compute the convolution. A significant</span>
<span class="sd">                memory workspace is needed to store intermediate results. The results are deterministic.</span>
<span class="sd">              - algo_3: This algorithm is similar to algo_0 but uses some small workspace to precompute some indices.</span>
<span class="sd">                The results are also non-deterministic.</span>
<span class="sd">              - winograd_nonfused: This algorithm uses the Winograd Transform approach to compute the convolution.</span>
<span class="sd">                A significant workspace may be needed to store intermediate results. The results are deterministic.</span>
<span class="sd">              - fft_tiling: This algorithm uses the Fast-Fourier Transform approach but splits the inputs into tiles.</span>
<span class="sd">                A significant memory workspace is needed to store intermediate results but less than fft for large size</span>
<span class="sd">                images. The results are deterministic.</span>
<span class="sd">            - conv_allow_tf32 (bool): The flag below controls to allow Tensor core TF32 computation on CUDNN and the</span>
<span class="sd">              default value is ``True``.</span>
<span class="sd">            - matmul_allow_tf32 (bool): The flag below controls to allow Tensor core TF32 computation on CUBLAS and the</span>
<span class="sd">              default value is ``False``.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If input key is not an attribute in context.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(mode=ms.PYNATIVE_MODE)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(precompile_only=True)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(device_target=&quot;Ascend&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(device_id=0)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(save_graphs=True, save_graphs_path=&quot;./model.ms&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(enable_reduce_precision=True)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(enable_graph_kernel=True)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(graph_kernel_flags=&quot;--opt_level=2 --dump_as_text&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(reserve_class_name_in_scope=True)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(variable_memory_max_size=&quot;6GB&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(aoe_tune_mode=&quot;online&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(aoe_config={&quot;job_type&quot;: &quot;2&quot;})</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(check_bprop=True)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(max_device_memory=&quot;3.5GB&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(mempool_block_size=&quot;1GB&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(print_file_path=&quot;print.pb&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(max_call_depth=80)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(env_config_path=&quot;./env_config.json&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(grad_for_scalar=True)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(enable_compile_cache=True, compile_cache_path=&quot;./cache.ms&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(pynative_synchronize=True)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(runtime_num_threads=10)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(inter_op_parallel_num=4)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(disable_format_transform=True)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(memory_optimize_level=&#39;O0&#39;)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(memory_offload=&#39;ON&#39;)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(deterministic=&#39;ON&#39;)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(ascend_config={&quot;precision_mode&quot;: &quot;force_fp16&quot;, &quot;jit_compile&quot;: True,</span>
<span class="sd">        ...                &quot;atomic_clean_policy&quot;: 1, &quot;op_precision_mode&quot;: &quot;./op_precision_config_file&quot;})</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(jit_syntax_level=ms.STRICT)</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(gpu_config={&quot;conv_fprop_algo&quot;: &quot;performance&quot;, &quot;conv_allow_tf32&quot;: True,</span>
<span class="sd">        ...                &quot;matmul_allow_tf32&quot;: True})</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="p">()</span>
    <span class="c1"># set device target first</span>
    <span class="k">if</span> <span class="s1">&#39;device_target&#39;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">set_device_target</span><span class="p">(</span><span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;device_target&#39;</span><span class="p">])</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">get_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">device_target</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;enable_sparse&#39;</span><span class="p">,</span> <span class="s1">&#39;auto_tune_mode&#39;</span><span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.set_context&#39;, &#39;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&#39; parameter is deprecated, &quot;</span>
                           <span class="s2">&quot;and will be removed in the next version.&quot;</span><span class="p">)</span>
            <span class="k">continue</span>
        <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;enable_auto_mixed_precision&#39;</span><span class="p">,</span> <span class="s1">&#39;enable_dump&#39;</span><span class="p">,</span> <span class="s1">&#39;save_dump_path&#39;</span><span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.set_context&#39;, &#39;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&#39; parameter is deprecated. &quot;</span>
                           <span class="s2">&quot;For details, please see the interface parameter API comments&quot;</span><span class="p">)</span>
            <span class="k">continue</span>
        <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;precision_mode&#39;</span><span class="p">,</span> <span class="s1">&#39;jit_compile&#39;</span><span class="p">,</span> <span class="s1">&#39;atomic_clean_policy&#39;</span><span class="p">,</span> <span class="s1">&#39;matmul_allow_hf32&#39;</span><span class="p">,</span> <span class="s1">&#39;conv_allow_hf32&#39;</span><span class="p">,</span>
                   <span class="s1">&#39;op_precision_mode&#39;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Please set &#39;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&#39; through parameter ascend_config&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="s1">&#39;save_graphs&#39;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
                <span class="n">value</span> <span class="o">=</span> <span class="mi">2</span>
            <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
                <span class="n">value</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">if</span> <span class="n">value</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;value for save_graphs should be 0-3 but got &#39;</span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="s1">&#39;jit_syntax_level&#39;</span> <span class="ow">and</span> <span class="n">value</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">STRICT</span><span class="p">,</span> <span class="n">COMPATIBLE</span><span class="p">,</span> <span class="n">LAX</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;jit_syntax_level&#39;, the value should be context.STRICT&quot;</span>
                             <span class="sa">f</span><span class="s2">&quot; or context.LAX, but got </span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_target_specific_cfgs</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
            <span class="k">continue</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
            <span class="k">continue</span>
        <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">ctx</span><span class="o">.</span><span class="n">setters</span><span class="p">:</span>
            <span class="n">ctx</span><span class="o">.</span><span class="n">setters</span><span class="p">[</span><span class="n">key</span><span class="p">](</span><span class="n">ctx</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
            <span class="k">continue</span>
        <span class="c1"># enum variables beginning with &#39;_&#39; are for internal use</span>
        <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">__members__</span> <span class="ow">and</span> <span class="n">key</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="s1">&#39;_&#39;</span><span class="p">:</span>
            <span class="n">ctx</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">__members__</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="n">value</span><span class="p">)</span>
            <span class="k">continue</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.set_context&#39;, the keyword argument </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2"> is not recognized! For detailed &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;usage of &#39;set_context&#39;, please refer to the Mindspore official website.&quot;</span><span class="p">)</span></div>


<div class="viewcode-block" id="get_context"><a class="viewcode-back" href="../../api_python/mindspore/mindspore.get_context.html#mindspore.get_context">[文档]</a><span class="k">def</span> <span class="nf">get_context</span><span class="p">(</span><span class="n">attr_key</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get context attribute value according to the input key.</span>
<span class="sd">    If some attributes are not set, they will be automatically obtained.</span>

<span class="sd">    Args:</span>
<span class="sd">        attr_key (str): The key of the attribute.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Object, The value of given attribute key.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If input key is not an attribute in context.</span>
<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; ms.get_context(&quot;device_target&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ms.get_context(&quot;device_id&quot;)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="p">()</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">get_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">device_target</span><span class="p">)</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">_check_target_specific_cfgs</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">attr_key</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">attr_key</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">attr_key</span><span class="p">)</span>
    <span class="c1"># enum variables beginning with &#39;_&#39; are for internal use</span>
    <span class="k">if</span> <span class="n">attr_key</span> <span class="ow">in</span> <span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">__members__</span> <span class="ow">and</span> <span class="n">attr_key</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="s1">&#39;_&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">ctx</span><span class="o">.</span><span class="n">get_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">__members__</span><span class="p">[</span><span class="n">attr_key</span><span class="p">])</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.get_context&#39;, the argument </span><span class="si">{</span><span class="n">attr_key</span><span class="si">}</span><span class="s2"> is not recognized! For detailed &quot;</span>
                     <span class="sa">f</span><span class="s2">&quot;usage of &#39;get_context&#39;, please refer to the Mindspore official website.&quot;</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_get_mode</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get execution mode. Only for internal using.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Object: The Value of execution mode.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">ctx</span><span class="o">.</span><span class="n">get_mode</span><span class="p">()</span>


<div class="viewcode-block" id="ParallelMode"><a class="viewcode-back" href="../../api_python/mindspore/mindspore.ParallelMode.html#mindspore.ParallelMode">[文档]</a><span class="k">class</span> <span class="nc">ParallelMode</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Parallel mode options.</span>

<span class="sd">    There are five kinds of parallel modes, ``STAND_ALONE``, ``DATA_PARALLEL``,</span>
<span class="sd">    ``HYBRID_PARALLEL``, ``SEMI_AUTO_PARALLEL`` and ``AUTO_PARALLEL``. Default: ``STAND_ALONE``.</span>

<span class="sd">    - ``STAND_ALONE``: Only one processor is working.</span>
<span class="sd">    - ``DATA_PARALLEL``: Distributes the data across different processors.</span>
<span class="sd">    - ``HYBRID_PARALLEL``: Achieves data parallelism and model parallelism manually.</span>
<span class="sd">    - ``SEMI_AUTO_PARALLEL``: Achieves data parallelism and model parallelism by setting parallel strategies.</span>
<span class="sd">    - ``AUTO_PARALLEL``: Achieves parallelism automatically.</span>

<span class="sd">    ``MODE_LIST``: The list of all supported parallel modes.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">STAND_ALONE</span> <span class="o">=</span> <span class="s2">&quot;stand_alone&quot;</span>
    <span class="n">DATA_PARALLEL</span> <span class="o">=</span> <span class="s2">&quot;data_parallel&quot;</span>
    <span class="n">HYBRID_PARALLEL</span> <span class="o">=</span> <span class="s2">&quot;hybrid_parallel&quot;</span>
    <span class="n">SEMI_AUTO_PARALLEL</span> <span class="o">=</span> <span class="s2">&quot;semi_auto_parallel&quot;</span>
    <span class="n">AUTO_PARALLEL</span> <span class="o">=</span> <span class="s2">&quot;auto_parallel&quot;</span>
    <span class="n">MODE_LIST</span> <span class="o">=</span> <span class="p">[</span><span class="n">STAND_ALONE</span><span class="p">,</span> <span class="n">DATA_PARALLEL</span><span class="p">,</span> <span class="n">HYBRID_PARALLEL</span><span class="p">,</span> <span class="n">SEMI_AUTO_PARALLEL</span><span class="p">,</span> <span class="n">AUTO_PARALLEL</span><span class="p">]</span></div>


<div class="viewcode-block" id="set_ps_context"><a class="viewcode-back" href="../../api_python/mindspore/mindspore.set_ps_context.html#mindspore.set_ps_context">[文档]</a><span class="nd">@args_type_check</span><span class="p">(</span><span class="n">enable_ps</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">set_ps_context</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Set parameter server training mode context.</span>

<span class="sd">    Note:</span>
<span class="sd">        Parameter server mode is only supported in graph mode.</span>
<span class="sd">        Some other environment variables should also be set for parameter server training mode.</span>
<span class="sd">        These environment variables are listed below:</span>

<span class="sd">        - MS_SERVER_NUM: Server number</span>
<span class="sd">        - MS_WORKER_NUM: Worker number</span>
<span class="sd">        - MS_SCHED_HOST: Scheduler IP address</span>
<span class="sd">        - MS_SCHED_PORT: Scheduler port</span>
<span class="sd">        - MS_ROLE: The role of this process:</span>

<span class="sd">          - MS_SCHED: represents the scheduler,</span>
<span class="sd">          - MS_WORKER: represents the worker,</span>
<span class="sd">          - MS_PSERVER/MS_SERVER: represents the Server</span>

<span class="sd">    Args:</span>
<span class="sd">        enable_ps (bool): Whether to enable parameter server training mode.</span>
<span class="sd">                          Only after enable_ps is set True, the environment variables will be effective.</span>
<span class="sd">                          Default: ``False`` .</span>
<span class="sd">        config_file_path (string): Configuration file path used by recovery, parameter server training mode only</span>
<span class="sd">                                   supports Server disaster recovery currently. Default: ``&#39;&#39;`` .</span>
<span class="sd">        scheduler_manage_port (int): Scheduler manage port used to scale out/in. Default: ``11202`` .</span>
<span class="sd">        enable_ssl (bool): Set PS SSL mode enabled or disabled. Default: ``False`` .</span>
<span class="sd">        client_password (str): Password to decrypt the secret key stored in the client certificate. Default: ``&#39;&#39;`` .</span>
<span class="sd">        server_password (str): Password to decrypt the secret key stored in the server certificate. Default: ``&#39;&#39;`` .</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If input key is not the attribute in parameter server training mode context.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; ms.set_ps_context(enable_ps=True, enable_ssl=True, client_password=&#39;123456&#39;, server_password=&#39;123456&#39;)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_set_ps_context</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>


<div class="viewcode-block" id="get_ps_context"><a class="viewcode-back" href="../../api_python/mindspore/mindspore.get_ps_context.html#mindspore.get_ps_context">[文档]</a><span class="k">def</span> <span class="nf">get_ps_context</span><span class="p">(</span><span class="n">attr_key</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get parameter server training mode context attribute value according to the key.</span>

<span class="sd">    Args:</span>
<span class="sd">        attr_key (str): The key of the attribute:</span>

<span class="sd">            - enable_ps (bool): Whether to enable parameter server training mode. Default: ``False`` .</span>
<span class="sd">            - config_file_path (string): Configuration file path used by recovery, parameter server training mode only</span>
<span class="sd">              supports Server disaster recovery currently. Default: ``&#39;&#39;`` .</span>
<span class="sd">            - scheduler_manage_port (int): Scheduler manage port used to scale out/in. Default: ``11202`` .</span>
<span class="sd">            - enable_ssl (bool): Set PS SSL mode enabled or disabled. Default: ``False`` .</span>
<span class="sd">            - client_password (str): Password to decrypt the secret key stored in the client certificate.</span>
<span class="sd">              Default: ``&#39;&#39;`` .</span>
<span class="sd">            - server_password (str): Password to decrypt the secret key stored in the server certificate.</span>
<span class="sd">              Default: ``&#39;&#39;`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Returns attribute value according to the key.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If input key is not attribute in auto parallel context.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; ms.get_ps_context(&quot;enable_ps&quot;)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_get_ps_context</span><span class="p">(</span><span class="n">attr_key</span><span class="p">)</span></div>


<div class="viewcode-block" id="reset_ps_context"><a class="viewcode-back" href="../../api_python/mindspore/mindspore.reset_ps_context.html#mindspore.reset_ps_context">[文档]</a><span class="k">def</span> <span class="nf">reset_ps_context</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reset parameter server training mode context attributes to the default values:</span>

<span class="sd">    - enable_ps: False.</span>

<span class="sd">    Meaning of each field and its default value refer to :func:`mindspore.set_ps_context`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; ms.reset_ps_context()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_reset_ps_context</span><span class="p">()</span></div>


<span class="n">_hccl_connect_timeout</span> <span class="o">=</span> <span class="s1">&#39;600&#39;</span>


<span class="k">def</span> <span class="nf">_init_parallel_env</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Set hccl connect timeout.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="s1">&#39;HCCL_CONNECT_TIMEOUT&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">:</span>
        <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;HCCL_CONNECT_TIMEOUT&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">_hccl_connect_timeout</span>


<span class="n">_init_parallel_env</span><span class="p">()</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 MindSpore.</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用了 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a>开发.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>