<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore.ops.operations._inner_ops &mdash; MindSpore master 文档</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script>
      <link rel="stylesheet" href="../../../../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/css/training.css" type="text/css" /><link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script><script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/js/theme.js"></script><script src="../../../../_static/underscore.js"></script><script src="../../../../_static/doctools.js"></script><script src="../../../../_static/js/mermaid-9.3.0.js"></script><script src="../../../../_static/js/training.js"></script><script src="../../../../_static/translations.js"></script><script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script><script src="https://unpkg.com/mermaid@10.2.0/dist/mermaid.min.js"></script><script>mermaid.initialize({startOnLoad:true});</script>
    <link rel="index" title="索引" href="../../../../genindex.html" />
    <link rel="search" title="搜索" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">设计</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/overview.html">MindSpore设计概览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/programming_paradigm.html">函数式和对象式融合编程范式</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/dynamic_graph_and_static_graph.html">动静态图结合</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/distributed_training_design.html">分布式并行原生</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/data_engine.html">高性能数据处理引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/all_scenarios.html">全场景统一架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/graph_fusion_engine.html">图算融合加速引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/pluggable_device.html">三方硬件对接</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/glossary.html">术语</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">模型库</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/official_models.html">官方模型库</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.experimental.html">mindspore.experimental</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API映射</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/api_mapping/pytorch_api_mapping.html">PyTorch与MindSpore API映射表</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">迁移指南</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/overview.html">迁移指南概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/enveriment_preparation.html">环境准备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/analysis_and_preparation.html">模型分析与准备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/model_development/model_development.html">网络搭建对比</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/debug_and_tune.html">调试调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/sample_code.html">网络迁移调试实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/faq.html">常见问题</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">语法支持</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/static_graph_syntax_support.html">静态图语法支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/static_graph_syntax/operators.html">静态图语法-运算符</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/static_graph_syntax/statements.html">静态图语法-Python语句</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/static_graph_syntax/python_builtin_functions.html">静态图语法-Python内置函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/index_support.html">Tensor索引支持</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">环境变量</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/env_var_list.html">环境变量</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/installation.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/data_processing.html">数据处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/implement_problem.html">执行问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/network_compilation.html">网络编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/operators_compile.html">算子编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/usage_migrate_3rd.html">第三方框架迁移使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/performance_tuning.html">性能调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/precision_tuning.html">精度调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/distributed_parallel.html">分布式并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/inference.html">推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/feature_advice.html">特性咨询</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../../index.html">模块代码</a> &raquo;</li>
      <li>mindspore.ops.operations._inner_ops</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>mindspore.ops.operations._inner_ops 源代码</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2020-2022 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>

<span class="sd">&quot;&quot;&quot;Inner operators.&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">types</span> <span class="kn">import</span> <span class="n">FunctionType</span><span class="p">,</span> <span class="n">MethodType</span>
<span class="kn">from</span> <span class="nn">collections.abc</span> <span class="kn">import</span> <span class="n">Iterable</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">mindspore.common</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">mindspore.common._stub_tensor</span> <span class="kn">import</span> <span class="n">StubTensor</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">composite</span> <span class="k">as</span> <span class="n">C</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations.array_ops</span> <span class="kn">import</span> <span class="n">Cast</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations._scalar_ops</span> <span class="kn">import</span> <span class="n">bit_or</span><span class="p">,</span> <span class="n">bit_and</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations.comm_ops</span> <span class="kn">import</span> <span class="n">ReduceOp</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">signature</span> <span class="k">as</span> <span class="n">sig</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations.math_ops</span> <span class="kn">import</span> <span class="n">_infer_shape_reduce</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.primitive</span> <span class="kn">import</span> <span class="n">PrimitiveWithCheck</span><span class="p">,</span> <span class="n">PrimitiveWithInfer</span><span class="p">,</span> <span class="n">prim_attr_register</span><span class="p">,</span> <span class="n">Primitive</span><span class="p">,</span>\
    <span class="n">_run_op</span><span class="p">,</span> <span class="n">_check_contains_variable</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">Tensor</span> <span class="k">as</span> <span class="n">Tensor_</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">typing</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">_checkparam</span> <span class="k">as</span> <span class="n">validator</span>
<span class="kn">from</span> <span class="nn">mindspore.common</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="kn">from</span> <span class="nn">mindspore.common.parameter</span> <span class="kn">import</span> <span class="n">Parameter</span>
<span class="kn">from</span> <span class="nn">mindspore.communication.management</span> <span class="kn">import</span> <span class="n">GlobalComm</span><span class="p">,</span> <span class="n">get_rank</span>
<span class="kn">from</span> <span class="nn">mindspore.common.api</span> <span class="kn">import</span> <span class="n">_pynative_executor</span>
<span class="kn">from</span> <span class="nn">mindspore.common._register_for_adapter</span> <span class="kn">import</span> <span class="n">ms_adapter_registry</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span>

<span class="c1"># Bit operation</span>
<span class="n">bit_and</span> <span class="o">=</span> <span class="n">bit_and</span><span class="p">()</span>
<span class="n">bit_or</span> <span class="o">=</span> <span class="n">bit_or</span><span class="p">()</span>
<span class="n">bit_xor</span> <span class="o">=</span> <span class="n">Primitive</span><span class="p">(</span><span class="s2">&quot;bit_xor&quot;</span><span class="p">)</span>
<span class="n">bit_left_shift</span> <span class="o">=</span> <span class="n">Primitive</span><span class="p">(</span><span class="s2">&quot;bit_left_shift&quot;</span><span class="p">)</span>
<span class="n">bit_right_shift</span> <span class="o">=</span> <span class="n">Primitive</span><span class="p">(</span><span class="s2">&quot;bit_right_shift&quot;</span><span class="p">)</span>
<span class="c1"># String operation</span>
<span class="n">string_lt</span> <span class="o">=</span> <span class="n">Primitive</span><span class="p">(</span><span class="s2">&quot;string_lt&quot;</span><span class="p">)</span>
<span class="n">string_gt</span> <span class="o">=</span> <span class="n">Primitive</span><span class="p">(</span><span class="s2">&quot;string_gt&quot;</span><span class="p">)</span>
<span class="n">string_le</span> <span class="o">=</span> <span class="n">Primitive</span><span class="p">(</span><span class="s2">&quot;string_le&quot;</span><span class="p">)</span>
<span class="n">string_ge</span> <span class="o">=</span> <span class="n">Primitive</span><span class="p">(</span><span class="s2">&quot;string_ge&quot;</span><span class="p">)</span>
<span class="n">string_not</span> <span class="o">=</span> <span class="n">Primitive</span><span class="p">(</span><span class="s2">&quot;string_not&quot;</span><span class="p">)</span>
<span class="n">string_in</span> <span class="o">=</span> <span class="n">Primitive</span><span class="p">(</span><span class="s2">&quot;string_in&quot;</span><span class="p">)</span>
<span class="n">string_mul</span> <span class="o">=</span> <span class="n">Primitive</span><span class="p">(</span><span class="s2">&quot;string_mul&quot;</span><span class="p">)</span>
<span class="n">string_getitem</span> <span class="o">=</span> <span class="n">Primitive</span><span class="p">(</span><span class="s2">&quot;string_getitem&quot;</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ExtractImagePatches</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extracts patches from images.</span>
<span class="sd">    The input tensor must be a 4-D tensor and the data format is NCHW.</span>

<span class="sd">    Args:</span>
<span class="sd">        ksizes (Union[tuple[int], list[int]]): The size of sliding window, must be a tuple or a list of integers,</span>
<span class="sd">            and the format is [1, 1, ksize_row, ksize_col].</span>
<span class="sd">        strides (Union[tuple[int], list[int]]): Distance between the centers of the two consecutive patches,</span>
<span class="sd">            must be a tuple or list of int, and the format is [1, 1, stride_row, stride_col].</span>
<span class="sd">        rates (Union[tuple[int], list[int]]): In each extracted patch, the gap between the corresponding dimension</span>
<span class="sd">            pixel positions, must be a tuple or a list of integers, and the format is [1, 1, rate_row, rate_col].</span>
<span class="sd">        padding (str): The type of padding algorithm, is a string whose value is &quot;same&quot; or &quot;valid&quot;,</span>
<span class="sd">            not case sensitive. Default: &quot;valid&quot;.</span>

<span class="sd">            - same: Means that the patch can take the part beyond the original image, and this part is filled with 0.</span>

<span class="sd">            - valid: Means that the taken patch area must be completely covered in the original image.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - A 4-D tensor whose shape is :math:`(in\_batch, in\_depth, in\_row, in\_col)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, a 4-D tensor whose data type is same as &#39;input_x&#39;, and the shape</span>
<span class="sd">        is :math:`(out\_batch, out\_depth, out\_row, out\_col)`,where the out_batch is the same as the in_batch</span>
<span class="sd">        and</span>

<span class="sd">        .. math::</span>
<span class="sd">            out_depth=ksize\_row * ksize\_col * in\_depth</span>

<span class="sd">        and</span>
<span class="sd">        if &#39;padding&#39; is &quot;valid&quot;:</span>

<span class="sd">        .. math::</span>
<span class="sd">            out\_row=floor((in\_row - (ksize\_row + (ksize\_row - 1) * (rate\_row - 1))) / stride\_row) + 1</span>
<span class="sd">            out\_col=floor((in\_col - (ksize\_col + (ksize\_col - 1) * (rate\_col - 1))) / stride\_col) + 1</span>

<span class="sd">        if &#39;padding&#39; is &quot;same&quot;:</span>

<span class="sd">        .. math::</span>
<span class="sd">            out\_row=floor((in\_row - 1) / stride\_row) + 1</span>
<span class="sd">            out\_col=floor((in\_col - 1) / stride\_col) + 1</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ksizes</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">rates</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;init&quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">_check_tuple_or_list</span><span class="p">(</span><span class="n">arg_name</span><span class="p">,</span> <span class="n">arg_val</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">arg_name</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">,</span> <span class="n">arg_val</span><span class="p">,</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">arg_val</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">4</span> <span class="ow">or</span> <span class="n">arg_val</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">arg_val</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For </span><span class="se">\&#39;</span><span class="si">{</span><span class="n">prim_name</span><span class="si">}</span><span class="se">\&#39;</span><span class="s2"> the format of </span><span class="si">{</span><span class="n">arg_name</span><span class="si">}</span><span class="s2">s must be [1, </span><span class="si">{</span><span class="n">arg_name</span><span class="si">}</span><span class="s2">_row, &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">arg_name</span><span class="si">}</span><span class="s2">_col, 1], but got </span><span class="si">{</span><span class="n">arg_val</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg_val</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg_val</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="n">arg_val</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">arg_val</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="n">prim_name</span><span class="si">}</span><span class="s2">&#39; the </span><span class="si">{</span><span class="n">arg_name</span><span class="si">}</span><span class="s2">_row and </span><span class="si">{</span><span class="n">arg_name</span><span class="si">}</span><span class="s2">_col in </span><span class="si">{</span><span class="n">arg_name</span><span class="si">}</span><span class="s2">s must be &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;an positive integer number, but got </span><span class="si">{</span><span class="n">arg_name</span><span class="si">}</span><span class="s2">_row is </span><span class="si">{</span><span class="n">arg_val</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2">, &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">arg_name</span><span class="si">}</span><span class="s2">_col is </span><span class="si">{</span><span class="n">arg_val</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">_check_tuple_or_list</span><span class="p">(</span><span class="s2">&quot;ksize&quot;</span><span class="p">,</span> <span class="n">ksizes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">_check_tuple_or_list</span><span class="p">(</span><span class="s2">&quot;stride&quot;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">_check_tuple_or_list</span><span class="p">(</span><span class="s2">&quot;rate&quot;</span><span class="p">,</span> <span class="n">rates</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;padding&#39;</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">padding</span><span class="o">.</span><span class="n">upper</span><span class="p">(),</span> <span class="p">[</span><span class="s1">&#39;VALID&#39;</span><span class="p">,</span> <span class="s1">&#39;SAME&#39;</span><span class="p">],</span> <span class="s1">&#39;padding&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;padding&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Quant</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the quantized value of input_x.</span>

<span class="sd">    If `sqrt_mode` is False:</span>

<span class="sd">    .. math::</span>
<span class="sd">        y = round(scale * x + offset)</span>

<span class="sd">    If `sqrt_mode` is True:</span>

<span class="sd">    .. math::</span>
<span class="sd">        y = round(scale * x * scale + offset)</span>

<span class="sd">    Note:</span>
<span class="sd">        This operation only support Ascend 310 inference environment.</span>

<span class="sd">    Args:</span>
<span class="sd">        scale (float) : Specifies the scaling ratio.</span>
<span class="sd">        offset (float): Specifies the offset.</span>
<span class="sd">        sqrt_mode (bool) : Specifies whether to perform square root on `scale`. Default: ``False``.</span>
<span class="sd">        round_mode (str): Specifies the way to round. Must be one of [&quot;Round&quot;, &quot;Floor&quot;, &quot;Ceil&quot;, &quot;Trunc&quot;].</span>
<span class="sd">          Default: &quot;Round&quot;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) : Input tensor. Its data type must be mindspore.float16 or mindspore.float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - Tensor: The quantized output tensor of type mindspore.int8.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor([100.0, 150.0], mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; quant = ops.Quant(80.0, 0.0, False, &quot;Round&quot;)</span>
<span class="sd">        &gt;&gt;&gt; y = quant(input_x)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="n">sqrt_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">round_mode</span><span class="o">=</span><span class="s2">&quot;Round&quot;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;scale&quot;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">offset</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;offset&quot;</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sqrt_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;sqrt_mode&quot;</span><span class="p">,</span> <span class="n">sqrt_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">round_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">round_mode</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;Round&quot;</span><span class="p">,</span> <span class="s2">&quot;Floor&quot;</span><span class="p">,</span> <span class="s2">&quot;Ceil&quot;</span><span class="p">,</span> <span class="s2">&quot;Trunc&quot;</span><span class="p">],</span>
                                                 <span class="s2">&quot;round_mode&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_type</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;input_x&quot;</span><span class="p">,</span> <span class="n">x_type</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_name</span><span class="p">(</span><span class="s2">&quot;input_x&quot;</span><span class="p">,</span> <span class="n">x_type</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int8</span>


<span class="k">class</span> <span class="nc">Lamb</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    LAMB optimizer algorithm.</span>

<span class="sd">    The Lamb optimizer is proposed in `Large Batch Optimization for Deep Learning: Training BERT in 76 minutes</span>
<span class="sd">    &lt;https://arxiv.org/abs/1904.00962&gt;`_.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Tensor) - Weights to be updated. The shape is :math:`(N, *)` where :math:`*` means,</span>
<span class="sd">          any number of additional dimensions. The data type can be float16 or float32.</span>
<span class="sd">        - **m** (Tensor) - The 1st moment vector in the updating formula,</span>
<span class="sd">          the shape and data type value should be the same as `var`.</span>
<span class="sd">        - **v** (Tensor) - the 2nd moment vector in the updating formula,</span>
<span class="sd">          the shape and data type value should be the same as `var`. Mean square gradients with the same type as `var`.</span>
<span class="sd">        - **lr** (float) - :math:`l` in the updating formula. The paper suggested value is :math:`10^{-8}`,</span>
<span class="sd">          the data type value should be the same as `var`.</span>
<span class="sd">        - **beta1** (float) - The exponential decay rate for the 1st moment estimations,</span>
<span class="sd">          the data type value should be the same as `var`. The paper suggested value is :math:`0.9`</span>
<span class="sd">        - **beta2** (float) - The exponential decay rate for the 2nd moment estimations,</span>
<span class="sd">          the data type value should be the same as `var`. The paper suggested value is :math:`0.999`</span>
<span class="sd">        - **epsilon** (float) - Term added to the denominator to improve numerical stability.</span>
<span class="sd">        - **decay** (float) - The weight decay value, must be a scalar tensor with float data type.</span>
<span class="sd">          Default: 0.0.</span>
<span class="sd">        - **global_step** (Tensor) - Tensor to record current global step.</span>
<span class="sd">        - **gradient** (Tensor) - Gradient, has the same shape and data type as `var`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend````GPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Lamb.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">,</span> <span class="n">v_shape</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">,</span> <span class="n">beta1_shape</span><span class="p">,</span> <span class="n">beta2_shape</span><span class="p">,</span>
                    <span class="n">epsilon_shape</span><span class="p">,</span> <span class="n">decay_shape</span><span class="p">,</span> <span class="n">global_step_shape</span><span class="p">,</span> <span class="n">gradient_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s2">&quot;m_shape&quot;</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s2">&quot;v_shape&quot;</span><span class="p">,</span> <span class="n">v_shape</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s2">&quot;gradient_shape&quot;</span><span class="p">,</span> <span class="n">gradient_shape</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="n">v_dtype</span><span class="p">,</span> <span class="n">lr_dtype</span><span class="p">,</span> <span class="n">beta1_dtype</span><span class="p">,</span> <span class="n">beta2_dtype</span><span class="p">,</span>
                    <span class="n">epsilon_dtype</span><span class="p">,</span> <span class="n">decay_dtype</span><span class="p">,</span> <span class="n">global_step_dtype</span><span class="p">,</span> <span class="n">gradient_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;var&quot;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s2">&quot;m&quot;</span><span class="p">:</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">:</span> <span class="n">v_dtype</span><span class="p">,</span> <span class="s2">&quot;grad&quot;</span><span class="p">:</span> <span class="n">gradient_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">lr_dtype</span><span class="p">,</span> <span class="s2">&quot;decay&quot;</span><span class="p">:</span> <span class="n">decay_dtype</span><span class="p">,</span> <span class="s2">&quot;beta1&quot;</span><span class="p">:</span> <span class="n">beta1_dtype</span><span class="p">,</span> <span class="s2">&quot;beta2&quot;</span><span class="p">:</span> <span class="n">beta2_dtype</span><span class="p">,</span>
                <span class="s2">&quot;epsilon&quot;</span><span class="p">:</span> <span class="n">epsilon_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_dtype</span>


<span class="k">class</span> <span class="nc">Dequant</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the dequantized value of input_x.</span>
<span class="sd">    This operation will do ReLU to the dequantized value if `relu_flag` is True.</span>

<span class="sd">    If `sqrt_mode` is False:</span>

<span class="sd">    .. math::</span>
<span class="sd">        y = x * deq\_scale</span>

<span class="sd">    If `sqrt_mode` is True:</span>

<span class="sd">    .. math::</span>
<span class="sd">        y = x * deq\_scale * deq\_scale</span>

<span class="sd">    Note:</span>
<span class="sd">        This operation only support Ascend 310 inference environment.</span>

<span class="sd">    Args:</span>
<span class="sd">        sqrt_mode (bool) : Specifies whether to perform square root on `scale`. Default: ``False``.</span>
<span class="sd">        relu_flag (bool): Specifies whether to perform ReLU. Default: ``False``.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) : Input tensor. Must be mindspore.int32.</span>
<span class="sd">        - **deq_scale** (Tensor) : Specifies the scaling ratio.</span>
<span class="sd">          Data type must be mindspore.float16 or mindspore.uint64</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - Tensor: The quantized output tensor of type mindspore.float16.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor([100.0, 150.0], mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; dequant = ops.Dequant(False, False)</span>
<span class="sd">        &gt;&gt;&gt; y = dequant(input_x)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sqrt_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">relu_flag</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sqrt_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;sqrt_mode&quot;</span><span class="p">,</span> <span class="n">sqrt_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu_flag</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;relu_flag&quot;</span><span class="p">,</span> <span class="n">relu_flag</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">deq_scale_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_type</span><span class="p">,</span> <span class="n">deq_scale_type</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">x_type</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_name</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">x_type</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_name</span><span class="p">(</span><span class="s2">&quot;deq_scale&quot;</span><span class="p">,</span> <span class="n">deq_scale_type</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint64</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span>


<span class="k">class</span> <span class="nc">MatrixDiag</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a batched diagonal tensor with a given batched diagonal values.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - A tensor which to be element-wise multi by `assist`. It can be one of the following data</span>
<span class="sd">          types: float32, float16, int32, int8, and uint8.</span>
<span class="sd">        - **assist** (Tensor) - A eye tensor of the same type as `x`. It&#39;s rank must be greater than or equal to 2 and</span>
<span class="sd">          it&#39;s last dimension must be equal to the second to last dimension.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same type and shape as input `assist`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, -1]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; assist = Tensor(np.arange(-12, 0).reshape(3, 2, 2), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; matrix_diag = ops.MatrixDiag()</span>
<span class="sd">        &gt;&gt;&gt; result = matrix_diag(x, assist)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [[[-12.   11.]</span>
<span class="sd">          [-10.    9.]]</span>
<span class="sd">         [[ -8.    7.]</span>
<span class="sd">          [ -6.    5.]]</span>
<span class="sd">         [[ -4.    3.]</span>
<span class="sd">          [ -2.    1.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize MatrixDiag&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">assist_dtype</span><span class="p">):</span>
        <span class="n">valid_type</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint8</span><span class="p">]</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="s2">&quot;assist&quot;</span><span class="p">:</span> <span class="n">assist_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">assist_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">assist_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s2">&quot;assist rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;rank of x&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                        <span class="s1">&#39;rank of assist&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">assist_shape</span><span class="p">),</span> <span class="n">validator</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;assist</span><span class="se">\&#39;</span><span class="s1">s penultimate dimension&#39;</span><span class="p">,</span> <span class="n">assist_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="s1">&#39;assist</span><span class="se">\&#39;</span><span class="s1">s last dimension&#39;</span><span class="p">,</span>
                        <span class="n">assist_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">r_end_dim</span> <span class="o">=</span> <span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span>
        <span class="n">r_idx</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="k">while</span> <span class="n">r_idx</span> <span class="o">&gt;=</span> <span class="n">r_end_dim</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">x_shape</span><span class="p">[</span><span class="n">r_idx</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;reverse x dim </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">r_idx</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="n">r_idx</span><span class="p">],</span> <span class="s2">&quot;reverse assist dim </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span>
                                <span class="n">assist_shape</span><span class="p">[</span><span class="n">r_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span> <span class="n">assist_shape</span><span class="p">[</span><span class="n">r_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">r_idx</span> <span class="o">=</span> <span class="n">r_idx</span> <span class="o">-</span> <span class="mi">1</span>

        <span class="k">return</span> <span class="n">assist_shape</span>


<span class="k">class</span> <span class="nc">MatrixDiagPart</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the batched diagonal part of a batched tensor.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The batched tensor. It can be one of the following data types:</span>
<span class="sd">          float32, float16, int32, int8, uint8.</span>
<span class="sd">        - **assist** (Tensor) - A eye tensor of the same type as `x`. With shape same as `x`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, data type same as input `x`. The shape must be x.shape[:-2] + [min(x.shape[-2:])].</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[[-1, 0], [0, 1]], [[-1, 0], [0, 1]], [[-1, 0], [0, 1]]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; assist = Tensor(np.arange(-12, 0).reshape(3, 2, 2), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; matrix_diag_part = ops.MatrixDiagPart()</span>
<span class="sd">        &gt;&gt;&gt; result = matrix_diag_part(x, assist)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [[12., -9.], [8., -5.], [4., -1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize MatrixDiagPart&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">assist_dtype</span><span class="p">):</span>
        <span class="n">valid_type</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint8</span><span class="p">]</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="s2">&quot;assist&quot;</span><span class="p">:</span> <span class="n">assist_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">assist_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s2">&quot;x rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;x shape&quot;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="s2">&quot;assist shape&quot;</span><span class="p">,</span> <span class="n">assist_shape</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">assist_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">assist_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">out_shape</span> <span class="o">=</span> <span class="n">assist_shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">out_shape</span> <span class="o">=</span> <span class="n">assist_shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">assist_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:]</span>
        <span class="k">return</span> <span class="n">out_shape</span>


<span class="k">class</span> <span class="nc">Send</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Send tensors from src_rank to the specified dest_rank.</span>

<span class="sd">    Note:</span>
<span class="sd">        Send and Receive must be used in combination and have same sr_tag.</span>
<span class="sd">        Send must be used between servers.</span>

<span class="sd">    Args:</span>
<span class="sd">        sr_tag (int): A required integer identifying the send/recv message tag. The message will</span>
<span class="sd">                      will be received by the Receive op with the same &quot;sr_tag&quot;.</span>
<span class="sd">        dest_rank (int): A required integer identifying the destination rank.</span>
<span class="sd">        group (str): The communication group to work on. Default: &quot;hccl_world_group/nccl_world_group&quot;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.communication import init</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; init()</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;         super(Net, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;         self.depend = ops.Depend()</span>
<span class="sd">        &gt;&gt;&gt;         self.send = ops.Send(st_tag=0, dest_rank=8, group=&quot;hccl_world_group&quot;)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt;     def construct(self, x):</span>
<span class="sd">        &gt;&gt;&gt;         out = self.depend(x, self.send(x))</span>
<span class="sd">        &gt;&gt;&gt;         return out</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; input_ = Tensor(np.ones([2, 8]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; output = net(input_)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sr_tag</span><span class="p">,</span> <span class="n">dest_rank</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">GlobalComm</span><span class="o">.</span><span class="n">WORLD_COMM_GROUP</span><span class="p">,</span> <span class="n">group_back</span><span class="o">=</span><span class="n">GlobalComm</span><span class="o">.</span><span class="n">WORLD_COMM_GROUP</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="n">dest_rank</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sr_tag</span> <span class="o">=</span> <span class="n">sr_tag</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="o">=</span> <span class="n">group</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;no_eliminate&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;shape&quot;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_dtype</span>


<span class="k">class</span> <span class="nc">Receive</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Receive tensors from src_rank.</span>

<span class="sd">    Note:</span>
<span class="sd">        Send and Receive must be used in combination and have same sr_tag.</span>
<span class="sd">        Receive must be used between servers.</span>

<span class="sd">    Args:</span>
<span class="sd">        sr_tag (int): A required integer identifying the send/recv message tag. The message will</span>
<span class="sd">                      will be send by the Send op with the same &quot;sr_tag&quot;.</span>
<span class="sd">        src_rank (int): A required integer identifying the source rank.</span>
<span class="sd">        shape (list[int]): A required list identifying the shape of the tensor to be received.</span>
<span class="sd">        dtype (Type): A required Type identifying the type of the tensor to be received. The supported types:</span>
<span class="sd">                       int8, int16, int32, float16, float32.</span>
<span class="sd">        group (str, optional): The communication group to work on.</span>
<span class="sd">            Default: &quot;hccl_world_group&quot; on Ascend, &quot;nccl_world_group&quot; on GPU.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.communication import init</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; init()</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;         super(Net, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;         self.recv = ops.Receive(st_tag=0, src_rank=0, shape=[2, 8], dtype=np.float32,</span>
<span class="sd">        &gt;&gt;&gt;                               group=&quot;hccl_world_group&quot;)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt;     def construct(self):</span>
<span class="sd">        &gt;&gt;&gt;         out = self.recv()</span>
<span class="sd">        &gt;&gt;&gt;         return out</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; output = net()</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sr_tag</span><span class="p">,</span> <span class="n">src_rank</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">GlobalComm</span><span class="o">.</span><span class="n">WORLD_COMM_GROUP</span><span class="p">,</span>
                 <span class="n">group_back</span><span class="o">=</span><span class="n">GlobalComm</span><span class="o">.</span><span class="n">WORLD_COMM_GROUP</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="n">src_rank</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tag</span> <span class="o">=</span> <span class="n">sr_tag</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="o">=</span> <span class="n">group</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;no_eliminate&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="n">valid_type</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint8</span><span class="p">]</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_attr_dict</span><span class="p">()[</span><span class="s1">&#39;shape&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_attr_dict</span><span class="p">()[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]</span>


<span class="k">class</span> <span class="nc">Reduce</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces tensor across the processes in the specified communication group.</span>

<span class="sd">    Note:</span>
<span class="sd">        Only process with destination rank receives the reduced output.</span>
<span class="sd">        Other processes only get a tensor with shape [1], which has no mathematical meaning.</span>

<span class="sd">    Args:</span>
<span class="sd">        dest_rank (int): Specifies the rank of the process that receives the reduced output.</span>
<span class="sd">        op (str, optional): Specifies an operation used for element-wise reductions, like sum, prod, max, and min.</span>
<span class="sd">                On the CPU, only &#39;sum&#39; is supported. Default: ``ReduceOp.SUM`` .</span>
<span class="sd">        group (str, optional): The communication group to work on.</span>
<span class="sd">            Default: &quot;hccl_world_group&quot; on Ascend, &quot;nccl_world_group&quot; on GPU.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.communication import init</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; # Launch 4 processes.</span>
<span class="sd">        &gt;&gt;&gt; init()</span>
<span class="sd">        &gt;&gt;&gt; class ReduceNet(nn.Cell):</span>
<span class="sd">        &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;         super(Net, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;         self.reduce = ops.Reduce(dest_rank=1)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt;     def construct(self, x):</span>
<span class="sd">        &gt;&gt;&gt;         out = self.reduce(x)</span>
<span class="sd">        &gt;&gt;&gt;         return out</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.ones([2, 8]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; net = ReduceNet()</span>
<span class="sd">        &gt;&gt;&gt; output = net(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        Process with rank 1: [[4. 4. 4. 4. 4. 4. 4. 4.]</span>
<span class="sd">                             [4. 4. 4. 4. 4. 4. 4. 4.]],</span>
<span class="sd">        Other proesses: [0.].</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dest_rank</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">GlobalComm</span><span class="o">.</span><span class="n">WORLD_COMM_GROUP</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dest_rank</span> <span class="o">=</span> <span class="n">dest_rank</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">op</span> <span class="o">=</span> <span class="n">op</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="o">=</span> <span class="n">group</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="c1"># The process with dest_rank returns the reduced output.</span>
        <span class="c1"># Other processes only gets a tensor with shape [1], which has no mathematical meaning.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dest_rank</span> <span class="o">==</span> <span class="n">get_rank</span><span class="p">():</span>
            <span class="k">return</span> <span class="n">x_shape</span>
        <span class="k">return</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_dtype</span>


<span class="k">class</span> <span class="nc">Barrier</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Synchronizes all processes in the specified group.</span>

<span class="sd">    Note:</span>
<span class="sd">        After calling this collective operator,</span>
<span class="sd">        this process will be blocked until all other processes in the group call this operator.</span>

<span class="sd">    Args:</span>
<span class="sd">        group (str, optional): The communication group to work on.</span>
<span class="sd">            Default: &quot;hccl_world_group&quot; on Ascend, &quot;nccl_world_group&quot; on GPU.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.communication import init</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; # Launch 4 processes.</span>
<span class="sd">        &gt;&gt;&gt; init()</span>
<span class="sd">        &gt;&gt;&gt; class BarrierNet(nn.Cell):</span>
<span class="sd">        &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;         super(Net, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;         self.barrier = ops.Barrier()</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt;     def construct(self):</span>
<span class="sd">        &gt;&gt;&gt;         self.barrier()</span>
<span class="sd">        &gt;&gt;&gt; net = BarrierNet()</span>
<span class="sd">        &gt;&gt;&gt; net()</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">GlobalComm</span><span class="o">.</span><span class="n">WORLD_COMM_GROUP</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="o">=</span> <span class="n">group</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;side_effect_mem&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span>


<span class="k">class</span> <span class="nc">MatrixSetDiag</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Modifies the batched diagonal part of a batched tensor.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The batched tensor. Rank k+1, where k &gt;= 1. It can be one of the following data types:</span>
<span class="sd">          float32, float16, int32, int8, uint8.</span>
<span class="sd">        - **diagonal** (Tensor) - The diagonal values. Must have the same type as input `x`. Rank k, where k &gt;= 1.</span>
<span class="sd">        - **assist** (Tensor) - A eye tensor of the same type as `x`. With shape same as `x`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, data type same as input `x`. The shape same as `x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[[-1, 0], [0, 1]], [[-1, 0], [0, 1]], [[-1, 0], [0, 1]]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; diagonal = Tensor([[-1., 2.], [-1., 1.], [-1., 1.]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; matrix_set_diag = ops.MatrixSetDiag()</span>
<span class="sd">        &gt;&gt;&gt; result = matrix_set_diag(x, diagonal)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [[[-1, 0], [0, 2]], [[-1, 0], [0, 1]], [[-1, 0], [0, 1]]]</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize MatrixSetDiag&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">diagonal_dtype</span><span class="p">,</span> <span class="n">assist_dtype</span><span class="p">):</span>
        <span class="n">valid_type</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint8</span><span class="p">]</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="s2">&quot;diagonal&quot;</span><span class="p">:</span> <span class="n">diagonal_dtype</span><span class="p">,</span> <span class="s2">&quot;assist&quot;</span><span class="p">:</span> <span class="n">assist_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">diagonal_shape</span><span class="p">,</span> <span class="n">assist_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s2">&quot;x rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;x shape&quot;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="s2">&quot;assist shape&quot;</span><span class="p">,</span> <span class="n">assist_shape</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">x_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">x_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;diagonal shape&quot;</span><span class="p">,</span> <span class="n">diagonal_shape</span><span class="p">,</span> <span class="s2">&quot;x shape excluding the last dimension&quot;</span><span class="p">,</span>
                            <span class="n">x_shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;diagonal shape&quot;</span><span class="p">,</span> <span class="n">diagonal_shape</span><span class="p">,</span> <span class="s2">&quot;x shape excluding the second last dimension&quot;</span><span class="p">,</span>
                            <span class="n">x_shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">x_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:],</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">assist_shape</span>


<span class="k">class</span> <span class="nc">ConfusionMulGrad</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    `output0` is the dot product result of input0 and input1.</span>

<span class="sd">    `output1` is the dot product result of input0 and input1, then apply the reducesum operation on it.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (Union[int, tuple[int], list[int]]): The dimensions to reduce.</span>
<span class="sd">            Default:(), reduce all dimensions. Only constant value is allowed.</span>
<span class="sd">        keep_dims (bool):</span>

<span class="sd">            - If true, keep these reduced dimensions and the length as 1.</span>
<span class="sd">            - If false, don&#39;t keep these dimensions. Default:False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_0** (Tensor) - The input Tensor.</span>
<span class="sd">        - **input_1** (Tensor) - The input Tensor.</span>
<span class="sd">        - **input_2** (Tensor) - The input Tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **output_0** (Tensor) - The same shape as `input0`.</span>
<span class="sd">        - **output_1** (Tensor)</span>

<span class="sd">            - If axis is (), and keep_dims is false, the output is a 0-D array representing</span>
<span class="sd">              the sum of all elements in the input array.</span>
<span class="sd">            - If axis is int, set as 2, and keep_dims is false,</span>
<span class="sd">              the shape of output is :math:`(x_1,x_3,...,x_R)`.</span>
<span class="sd">            - If axis is tuple(int), set as (2,3), and keep_dims is false,</span>
<span class="sd">              the shape of output is :math:`(x_1,x_4,...x_R)`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; confusion_mul_grad = ops.ConfusionMulGrad()</span>
<span class="sd">        &gt;&gt;&gt; input_0 = Tensor(np.random.randint(-2, 2, (2, 3)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_1 = Tensor(np.random.randint(0, 4, (2, 3)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_2 = Tensor(np.random.randint(-4, 0, (2, 3)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output_0, output_1 = confusion_mul_grad(input_0, input_1, input_2)</span>
<span class="sd">        output_0:</span>
<span class="sd">            [[ 3.   1.   0.]</span>
<span class="sd">             [-6.   2.  -2.]]</span>
<span class="sd">        output_1:</span>
<span class="sd">            -3.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(),</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;input0&quot;</span><span class="p">,</span> <span class="s2">&quot;input1&quot;</span><span class="p">,</span> <span class="s2">&quot;input2&quot;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;output0&quot;</span><span class="p">,</span> <span class="s2">&quot;output1&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">axis_</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_dims_</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input0_shape</span><span class="p">,</span> <span class="n">input1_shape</span><span class="p">,</span> <span class="n">input2_shape</span><span class="p">):</span>
        <span class="n">outshape0</span> <span class="o">=</span> <span class="n">input0_shape</span>
        <span class="n">outshape1</span> <span class="o">=</span> <span class="n">_infer_shape_reduce</span><span class="p">(</span><span class="n">input1_shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keep_dims_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">outshape0</span><span class="p">,</span> <span class="n">outshape1</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input0_dtype</span><span class="p">,</span> <span class="n">input1_dtype</span><span class="p">,</span> <span class="n">input2_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;input0_dtype&quot;</span><span class="p">,</span> <span class="n">input0_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;input1_dtype&quot;</span><span class="p">,</span> <span class="n">input1_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;input2_dtype&quot;</span><span class="p">,</span> <span class="n">input2_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input0_dtype</span><span class="p">,</span> <span class="n">input1_dtype</span>


<span class="k">class</span> <span class="nc">ConvertToDynamic</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This op is used for dynamic rank testing. Its inferred shape will be unknown</span>
<span class="sd">    during compile time, so that its output will appear to be dynamically ranked.</span>
<span class="sd">    The input will not be altered in any way. Put this operator before the operator</span>
<span class="sd">    being tested for dynamic rank support.</span>

<span class="sd">    Args:</span>
<span class="sd">        is_dynamic_rank (bool): If true, convert to dynamic rank.</span>
<span class="sd">                                If false, convert to dynamic shape. Default: ``False``.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - The tensor used for testing.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **output** (Tensor) - Same shape, type and value as `input`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">          &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">          &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">          &gt;&gt;&gt; from mindspore.ops.operations import _inner_ops as inner</span>
<span class="sd">          &gt;&gt;&gt; from mindspore.ops import operations as P</span>
<span class="sd">          &gt;&gt;&gt; class TestDynamicNet(nn.Cell):</span>
<span class="sd">          &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">          &gt;&gt;&gt;         super(TestDynamicNet, self).__init__()</span>
<span class="sd">          &gt;&gt;&gt;         self.convert_to_dynamic = inner.ConvertToDynamic()</span>
<span class="sd">          &gt;&gt;&gt;         # suppose we are testing Reshape op</span>
<span class="sd">          &gt;&gt;&gt;         self.reshape = P.Reshape()</span>
<span class="sd">          &gt;&gt;&gt;</span>
<span class="sd">          &gt;&gt;&gt;     def construct(self, input, new_shape):</span>
<span class="sd">          &gt;&gt;&gt;         dynamic_input = self.convert_to_dynamic(input)</span>
<span class="sd">          &gt;&gt;&gt;         reshaped_input = self.reshape(dynamic_input, new_shape)</span>
<span class="sd">          &gt;&gt;&gt;</span>
<span class="sd">          &gt;&gt;&gt; ms.set_context(mode=ms.GRAPH_MODE, device_target=&quot;CPU&quot;)</span>
<span class="sd">          &gt;&gt;&gt; input = Tensor(np.array([0, 1, 2, 3])</span>
<span class="sd">          &gt;&gt;&gt; new_shape = (2, 2)</span>
<span class="sd">          &gt;&gt;&gt; net = TestDynamicNet()</span>
<span class="sd">          &gt;&gt;&gt; output = net(input, new_shape)</span>
<span class="sd">          &gt;&gt;&gt; print(output)</span>
<span class="sd">          [[0, 1], [2, 3]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_dynamic_rank</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;is_dynamic_rank&#39;</span><span class="p">,</span> <span class="n">is_dynamic_rank</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;output&quot;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">check_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;input_shape rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">),</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GT</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;input_dtype&quot;</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">GpuConvertToDynamicShape</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This op is used for dynamic shape testing. Its inferred shape will be unknown</span>
<span class="sd">    during compile time, so that its output will appear to be dynamically shaped.</span>
<span class="sd">    The input will not be altered in any way. Put this operator before the operator</span>
<span class="sd">    being tested for dynamic shape support.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - The tensor used for testing.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **output** (Tensor) - Same shape, type and value as `input`.</span>

<span class="sd">    Examples:</span>
<span class="sd">          &gt;&gt;&gt; # make a model, since dynamic shape operators must be in GRAPH_MODE</span>
<span class="sd">          &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">          &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">          &gt;&gt;&gt; from mindspore.ops.operations import _inner_ops as inner</span>
<span class="sd">          &gt;&gt;&gt; from mindspore.ops import operations as P</span>
<span class="sd">          &gt;&gt;&gt; class TestDynamicShapeReshapeNet(nn.Cell):</span>
<span class="sd">          &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">          &gt;&gt;&gt;         super(TestDynamicShapeReshapeNet, self).__init__()</span>
<span class="sd">          &gt;&gt;&gt;         self.convert_to_dynamic_shape = inner.GpuConvertToDynamicShape()</span>
<span class="sd">          &gt;&gt;&gt;         # suppose we are testing Reshape op</span>
<span class="sd">          &gt;&gt;&gt;         self.reshape = P.Reshape()</span>
<span class="sd">          &gt;&gt;&gt;</span>
<span class="sd">          &gt;&gt;&gt;     def construct(self, input, new_shape):</span>
<span class="sd">          &gt;&gt;&gt;         dynamic_shape_input = self.convert_to_dynamic_shape(input)</span>
<span class="sd">          &gt;&gt;&gt;         reshaped_input = self.reshape(input, new_shape)</span>
<span class="sd">          &gt;&gt;&gt;</span>
<span class="sd">          &gt;&gt;&gt; ms.set_context(mode=ms.GRAPH_MODE, device_target=&quot;GPU&quot;)</span>
<span class="sd">          &gt;&gt;&gt; input = Tensor(np.array([0, 1, 2, 3])</span>
<span class="sd">          &gt;&gt;&gt; new_shape = (2, 2)</span>
<span class="sd">          &gt;&gt;&gt; net = TestDynamicShapeReshapeNet()</span>
<span class="sd">          &gt;&gt;&gt; output = net(input, new_shape)</span>
<span class="sd">          &gt;&gt;&gt; print(output)</span>
<span class="sd">          [[0, 1], [2, 3]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;output&quot;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">check_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;input_shape rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">),</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GT</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;input_dtype&quot;</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ErrorOnDynamicShapeInput</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This op is used for dynamic shape testing. The only purpose of this operator is</span>
<span class="sd">    that it will throw a value error if the input is dynamically shaped.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - The tensor used for testing.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **output** (Tensor) - Same shape, type and value as `input`.</span>

<span class="sd">    Examples:</span>
<span class="sd">          &gt;&gt;&gt; # make a model, since dynamic shape operators must be in GRAPH_MODE</span>
<span class="sd">          &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">          &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">          &gt;&gt;&gt; from mindspore.ops.operations import _inner_ops as inner</span>
<span class="sd">          &gt;&gt;&gt; from mindspore.ops import operations as P</span>
<span class="sd">          &gt;&gt;&gt; class AssertDynamicShapeNet(nn.Cell):</span>
<span class="sd">          &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">          &gt;&gt;&gt;         super(AssertDynamicShapeNet, self).__init__()</span>
<span class="sd">          &gt;&gt;&gt;         self.convert_to_dynamic_shape = inner.GpuConvertToDynamicShape()</span>
<span class="sd">          &gt;&gt;&gt;         self.error_on_dynamic_shape_input = inner.ErrorOnDynamicShapeInput()</span>
<span class="sd">          &gt;&gt;&gt;</span>
<span class="sd">          &gt;&gt;&gt;     def construct(self, input, new_shape):</span>
<span class="sd">          &gt;&gt;&gt;         dynamic_shape_input = self.convert_to_dynamic_shape(input)</span>
<span class="sd">          &gt;&gt;&gt;         self.error_on_dynamic_shape_input(dynamic_shape_input)</span>
<span class="sd">          &gt;&gt;&gt;</span>
<span class="sd">          &gt;&gt;&gt; ms.set_context(mode=ms.GRAPH_MODE, device_target=&quot;GPU&quot;)</span>
<span class="sd">          &gt;&gt;&gt; input = Tensor(np.array([0])</span>
<span class="sd">          &gt;&gt;&gt; net = TestDynamicShapeReshapeNet()</span>
<span class="sd">          &gt;&gt;&gt; output = net(input, new_shape)</span>
<span class="sd">          ValueError: Input is dynamically shaped.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;output&quot;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="n">shape</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">dim</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Input is dynamically shaped.&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">input_shape</span>

    <span class="k">def</span> <span class="nf">infer_type</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Infer the dtype of input for ErrorOnDynamicShapeInput.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;input_dtype&quot;</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_dtype</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_tensor</span>


<span class="k">class</span> <span class="nc">SequenceMask</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a mask tensor representing the first N positions of each cell.</span>

<span class="sd">    If lengths has shape [d_1, d_2, ..., d_n], then the resulting tensor mask has type and shape</span>
<span class="sd">    [d_1, d_2, ..., d_n, maxlen], with mask[i_1, i_2, ..., i_n, j] = (j &lt; lengths[i_1, i_2, ..., i_n])</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **lengths** (Tensor) - Tensor to calculate the mask for. All values in this tensor should be</span>
<span class="sd">          less than or equal to `maxlen`. Values greater than `maxlen` will be treated as `maxlen`.</span>
<span class="sd">          Must be type int32 or int64.</span>

<span class="sd">        - **maxlen** (int) - size of the last dimension of returned tensor. Must be positive and same</span>
<span class="sd">          type as elements in `lengths`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        One mask tensor of shape lengths.shape + (maxlen,).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 3], [2, 0]]))</span>
<span class="sd">        &gt;&gt;&gt; sequence_mask = ops.SequenceMask()</span>
<span class="sd">        &gt;&gt;&gt; output = sequence_mask(x, 3)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[True False False]</span>
<span class="sd">          [True True True]]</span>
<span class="sd">         [[True True False]</span>
<span class="sd">          [False False False]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;lengths&quot;</span><span class="p">,</span> <span class="s2">&quot;maxlen&quot;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;mask&quot;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">check_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lengths_shape</span><span class="p">,</span> <span class="n">maxlen_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;lengths_shape&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">lengths_shape</span><span class="p">),</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GT</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;maxlen_shape&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">maxlen_shape</span><span class="p">),</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lengths_dtype</span><span class="p">,</span> <span class="n">maxlen_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;lengths_dtype&quot;</span><span class="p">,</span> <span class="n">lengths_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;maxlen&quot;</span><span class="p">,</span> <span class="n">maxlen_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">SyncBatchNorm</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sync Batch Normalization for input data and updated parameters.</span>

<span class="sd">    Sync Batch Normalization is cross device synchronized Batch Normalization. Batch Normalization is</span>
<span class="sd">    widely used in convolutional neural networks. This operation applies Batch Normalization over input</span>
<span class="sd">    to avoid internal covariate shift as described in the paper `Batch Normalization: Accelerating</span>
<span class="sd">    Deep Network Training by Reducing Internal Covariate Shift &lt;https://arxiv.org/abs/1502.03167&gt;`_.</span>
<span class="sd">    It rescales and recenters the features using a mini-batch of data and the learned parameters which</span>
<span class="sd">    can be described in the following formula,</span>

<span class="sd">    .. math::</span>
<span class="sd">        y = \frac{x - mean}{\sqrt{variance + \epsilon}} * \gamma + \beta</span>

<span class="sd">    where :math:`\gamma` is scale, :math:`\beta` is bias, :math:`\epsilon` is epsilon.</span>

<span class="sd">    Args:</span>
<span class="sd">        epsilon (float): A small value added for numerical stability. Default: 1e-5.</span>
<span class="sd">        momentum (float): The hyper parameter to compute moving average for running_mean and running_var</span>
<span class="sd">            (e.g. :math:`new\_running\_mean = (1 - momentum) * running\_mean + momentum * current\_mean`).</span>
<span class="sd">            Momentum value must be [0, 1]. Default: 0.1.</span>
<span class="sd">        group (str): The communication group to work on. Default: &quot;sync_bn_group0&quot;.</span>
<span class="sd">        device_num (int): The number of devices in each group. Default: 2.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor of shape :math:`(N, C)`, with float16 or float32 data type.</span>
<span class="sd">        - **scale** (Tensor) - Tensor of shape :math:`(C,)`, with float16 or float32 data type.</span>
<span class="sd">        - **bias** (Tensor) - Tensor of shape :math:`(C,)`, has the same data type with `scale`.</span>
<span class="sd">        - **mean** (Tensor) - Tensor of shape :math:`(C,)`, with float16 or float32 data type.</span>
<span class="sd">        - **variance** (Tensor) - Tensor of shape :math:`(C,)`, has the same data type with `mean`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 5 Tensor, the normalized inputs and the updated parameters.</span>

<span class="sd">        - **output_x** (Tensor) - The same type and shape as the input_x. The shape is :math:`(N, C)`.</span>
<span class="sd">        - **updated_scale** (Tensor) - Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **updated_bias** (Tensor) - Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **updated_moving_mean** (Tensor) - Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **updated_moving_variance** (Tensor) - Tensor of shape :math:`(C,)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # This example should be run with multiple processes.</span>
<span class="sd">        &gt;&gt;&gt; # Please refer to nn.SyncBatchNorm for direct use.</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones([2, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; scale = Tensor(np.ones([2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; bias = Tensor(np.ones([2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; mean = Tensor(np.ones([2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; variance = Tensor(np.ones([2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; sync_batch_norm = ops._inner_ops.SyncBatchNorm()</span>
<span class="sd">        &gt;&gt;&gt; output = sync_batch_norm(input_x, scale, bias, mean, variance)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 1.00000000e+00, 1.00000000e+00],</span>
<span class="sd">         [ 1.00000000e+00, 1.00000000e+00]]), Tensor(shape=[2], dtype=Float32, value=</span>
<span class="sd">         [ 1.00000000e+00, 1.00000000e+00]), Tensor(shape=[2], dtype=Float32, value=</span>
<span class="sd">         [ 1.00000000e+00, 1.00000000e+00]), Tensor(shape=[2], dtype=Float32, value=</span>
<span class="sd">         [ 1.00000000e+00, 1.00000000e+00]), Tensor(shape=[2], dtype=Float32, value=</span>
<span class="sd">         [ 1.00000000e+00, 1.00000000e+00]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="s2">&quot;sync_bn_group0&quot;</span><span class="p">,</span> <span class="n">device_num</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">epsilon</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">INC_RIGHT</span><span class="p">,</span> <span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">momentum</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_isinstance</span><span class="p">(</span><span class="s2">&quot;group&quot;</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">device_num</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s2">&quot;device_num&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="s1">&#39;offset&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;variance&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;batch_mean&#39;</span><span class="p">,</span> <span class="s1">&#39;batch_variance&#39;</span><span class="p">,</span> <span class="s1">&#39;reserve_space_1&#39;</span><span class="p">,</span> <span class="s1">&#39;reserve_space_2&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;format&#39;</span><span class="p">,</span> <span class="s1">&#39;NCHW&#39;</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Centralization</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes centralization. y = x - mean(x, axis).</span>

<span class="sd">    Note:</span>
<span class="sd">        The dimension index starts at 0 and must be in the range `[-input.ndim, input.ndim)`.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor. The data type mast be float16 or float32.</span>
<span class="sd">        - **axis** (Union[Int, Tuple(Int), List(Int)]) - The dimensions to reduce. Default: (), reduce all dimensions.</span>
<span class="sd">          Only constant value is allowed. Must be in the range [-rank(input_x), rank(input_x)).</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is not one of the following types: int, list, tuple, NoneType.</span>
<span class="sd">        TypeError: If `axis` has non-Int elements.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; mindspore.set_seed(1)</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.random.randn(2, 2).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; centralization = ops.Centralization()</span>
<span class="sd">        &gt;&gt;&gt; output = centralization(input_x, -1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 1.1180509 -1.1180508]</span>
<span class="sd">         [ 0.2723984 -0.2723984]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input_x&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="p">())</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Centralization&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="s1">&#39;axis&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
        <span class="n">x_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">])</span>
        <span class="n">x_dtype</span> <span class="o">=</span> <span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]</span>
        <span class="n">axis_v</span> <span class="o">=</span> <span class="n">axis</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span>

        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;input_x&#39;</span><span class="p">:</span> <span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">axis_v</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">, axis must be const.&quot;</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">axis_v</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis_v</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="n">axis_v</span><span class="p">,</span> <span class="o">-</span><span class="n">rank</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span> <span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">axis</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">one_axis</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axis_v</span><span class="p">):</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;axis[</span><span class="si">%d</span><span class="s1">]&#39;</span> <span class="o">%</span> <span class="n">index</span><span class="p">,</span> <span class="n">one_axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="n">x_shape</span><span class="p">,</span>
               <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">,</span>
               <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span>
        <span class="k">return</span> <span class="n">out</span>


<span class="k">class</span> <span class="nc">StackInit</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create a stack that produces tensors in first-in last-out order.</span>

<span class="sd">    After `StackInit`, a tensor can be pushed onto the stack using `StackPush`, and popped</span>
<span class="sd">    at the top of the stack using `StackPop`. Finally, the stack should be destroyed with `StackDestroy`.</span>

<span class="sd">    Args:</span>
<span class="sd">        index (int): The index of the stack. Default: 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 3], [2, 0]]))</span>
<span class="sd">        &gt;&gt;&gt; index = 0</span>
<span class="sd">        &gt;&gt;&gt; stack = ops.StackInit(index)</span>
<span class="sd">        &gt;&gt;&gt; push = ops.StackPush(index)</span>
<span class="sd">        &gt;&gt;&gt; pop = ops.StackPop(index, x.shape, x.dtype)</span>
<span class="sd">        &gt;&gt;&gt; destroy = ops.StackDestroy(index)</span>
<span class="sd">        &gt;&gt;&gt; stack()</span>
<span class="sd">        &gt;&gt;&gt; push(x)</span>
<span class="sd">        &gt;&gt;&gt; y = pop()</span>
<span class="sd">        &gt;&gt;&gt; destroy()</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[1 3]</span>
<span class="sd">         [2 0]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;StackInit&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;index&quot;</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">StackPush</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Push a tensor onto the stack.</span>

<span class="sd">    Before `StackPush`, the stack should be created using `StackInit`.</span>
<span class="sd">    Please refer to the usage in source code of `StackInit`.</span>

<span class="sd">    Args:</span>
<span class="sd">        index (int): The index of the stack. Default: 1.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - A tensor to be pushed onto the stack.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        Please refer to the usage of `StackInit`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;StackPush&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;index&quot;</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[])</span>


<span class="k">class</span> <span class="nc">StackPop</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Pop the tensor at the top of the stack.</span>

<span class="sd">     Before `StackPop`, the stack should be created using `StackInit`.</span>
<span class="sd">     Please refer to the usage in source code of `StackInit`.</span>

<span class="sd">    Args:</span>
<span class="sd">        index (int): The index of the stack. Default: 1.</span>
<span class="sd">        shape (tuple): The shape of the tensor at the top of the stack. Default: (1,).</span>
<span class="sd">        dtype (mindspore.dtype): The type of the tensor at the top of the stack. Default: mindspore.float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **output** (Tensor) - The tensor at the top of the stack.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        Please refer to the usage of `StackInit`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;StackPop&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;index&quot;</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;shape type&#39;</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;dim of shape&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">shape</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">elem</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s1">&#39;shape element&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;type of shape element&#39;</span><span class="p">,</span> <span class="n">elem</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_name</span><span class="p">(</span><span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">,)</span> <span class="o">+</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">)),</span>
                <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
                <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span>


<span class="k">class</span> <span class="nc">StackDestroy</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Destroy the stack.</span>

<span class="sd">     Before `StackDestroy`, the stack should be created using `StackInit`.</span>
<span class="sd">     Please refer to the usage in source code of `StackInit`.</span>

<span class="sd">    Args:</span>
<span class="sd">        index (int): The index of the stack. Default: 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        Please refer to the usage of `StackInit`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;StackDestroy&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;index&quot;</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">DynamicStitch</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Interleave the values from the data tensors into a single tensor.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **indices** (Union[tuple, list]) - A Tuple or list of Tensor objects with the same shape and type.</span>
<span class="sd">        - **data** (Union[tuple, list]) - A Tuple or list of Tensor objects with the same shape and type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor. A stacked Tensor with the same type as `data`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the data types of elements in `data` or `indices` are not the same.</span>
<span class="sd">        ValueError: If the length of `data` or `indices` is not greater than 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor([6], mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([4, 1]), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; x3 = Tensor(np.array([[5, 2], [0, 3]]), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; y1 = Tensor(np.array([[6, 1]]), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; y2 = Tensor(np.array([[41, 42], [11, 12]]), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; y3 = Tensor(np.array([[[51, 52], [21, 22]], [[1, 2], [31, 32]]]), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; stitch = ops.DynamicStitch()</span>
<span class="sd">        &gt;&gt;&gt; output = stitch([x1, x2, x3], [y1, y2, y3])</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 1  2]</span>
<span class="sd">         [11 12]</span>
<span class="sd">         [21 22]</span>
<span class="sd">         [31 32]</span>
<span class="sd">         [41 42]</span>
<span class="sd">         [51 52]</span>
<span class="sd">         [61 62]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize DynamicStitch&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">check_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">,</span> <span class="n">data_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;shape of indices&quot;</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">,</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indices_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s2">&quot;len of indices_shape&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">indices_dim0</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">indices_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">indices_num</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">indices_shape</span><span class="p">)</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;shape of data&quot;</span><span class="p">,</span> <span class="n">data_shape</span><span class="p">,</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s2">&quot;len of data_shape&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">data_dim0</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">data_num</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">indices_shape</span><span class="p">)</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;size of indices&quot;</span><span class="p">,</span> <span class="n">indices_num</span><span class="p">,</span> <span class="s1">&#39;size of data&#39;</span><span class="p">,</span> <span class="n">data_num</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="c1"># shape of `data` must start with shape of `indices`</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">indices_num</span><span class="p">):</span>
            <span class="n">indices_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">indices_shape</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">data_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_shape</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;dim of indices[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">,</span> <span class="n">indices_dim</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;dim of data[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">,</span> <span class="n">data_dim</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">data_shape</span><span class="p">[</span><span class="n">i</span><span class="p">][:</span><span class="n">indices_dim</span><span class="p">]</span> <span class="o">!=</span> <span class="n">data_shape</span><span class="p">[</span><span class="n">i</span><span class="p">][:</span><span class="n">indices_dim</span><span class="p">]:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;data[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">].shape: </span><span class="si">{</span><span class="n">data_shape</span><span class="si">}</span><span class="s2"> does not start with indices[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">].shape: </span><span class="si">{</span><span class="n">data_shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># the last-(data_dim0-indices_dim0)-dim of data shape must end with same shape.</span>
        <span class="n">base_extra</span> <span class="o">=</span> <span class="n">data_dim0</span> <span class="o">-</span> <span class="n">indices_dim0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">data_num</span><span class="p">):</span>
            <span class="n">indices_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">indices_shape</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">data_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_shape</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">extra</span> <span class="o">=</span> <span class="n">data_dim</span> <span class="o">-</span> <span class="n">indices_dim</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;extra dim of data[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">,</span> <span class="n">extra</span><span class="p">,</span>
                            <span class="sa">f</span><span class="s2">&quot;extra dim of data[0]&quot;</span><span class="p">,</span> <span class="n">base_extra</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;data[0].shape[</span><span class="si">{</span><span class="n">indices_dim0</span><span class="si">}</span><span class="s2">:]&quot;</span><span class="p">,</span> <span class="n">data_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">indices_dim0</span><span class="p">:],</span>
                            <span class="sa">f</span><span class="s2">&quot;data[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">].shape[</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">indices_shape</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="si">}</span><span class="s2">:]&quot;</span><span class="p">,</span>
                            <span class="n">data_shape</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">indices_dim</span><span class="p">:],</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">out_shape</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">data_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">indices_dim0</span><span class="p">:]</span>
        <span class="k">return</span> <span class="n">out_shape</span>

    <span class="k">def</span> <span class="nf">check_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices_type</span><span class="p">,</span> <span class="n">data_type</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;indices[0]&quot;</span><span class="p">,</span> <span class="n">indices_type</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;data[0]&quot;</span><span class="p">,</span> <span class="n">data_type</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">indices_num</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">indices_type</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">indices_num</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;indices[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">]&#39;</span><span class="p">,</span> <span class="n">indices_type</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;data[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">]&#39;</span><span class="p">,</span> <span class="n">data_type</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                               <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span> <span class="o">+</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">,),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;type of data[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">,</span> <span class="n">data_type</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="sa">f</span><span class="s2">&quot;type of data[0]&quot;</span><span class="p">,</span>
                            <span class="n">data_type</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">data_type</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>


<span class="k">class</span> <span class="nc">DynamicBroadcastGradientArgs</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Broadcast the two input shapes, return the dimensions that each need to be broadcast.</span>

<span class="sd">    Input shape `s0` and shape `s1` can be broadcast to a common shape if for each dimension pair they are either equal</span>
<span class="sd">    or input is one or the target dimension is -1. In case of -1 in target shape, it will be replaced by the input</span>
<span class="sd">    shape&#39;s value in that dimension.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **s0** (Tensor) - A `1-D` tensor. The data type should be one of the following types: int32, int64,</span>
<span class="sd">          uint32, uint64.</span>
<span class="sd">        - **s1** (Tensor) - A `1-D` tensor with the same type as `s0`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple(Tensor), tuple of 2 tensors, r0 and r1. The first one is the index tensor and the other one is the mask</span>
<span class="sd">        tensor.</span>

<span class="sd">        - **r0** (Tensor) - The output shape is 1-D with the same type as s0.</span>
<span class="sd">        - **r1** (Tensor) - The output shape is 1-D with the same type as s0.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: if the `s0` and `s1` are incompatible, or if a - 1 in the target shape is in an invalid</span>
<span class="sd">                    location.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; shape0 = (4, 2, 1)</span>
<span class="sd">        &gt;&gt;&gt; shape1 = (2, 7)</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops.operations import _inner_ops</span>
<span class="sd">        &gt;&gt;&gt; args = _inner_ops.DynamicBroadcastGradientArgs()</span>
<span class="sd">        &gt;&gt;&gt; r0, r1 = args(Tensor(shape0), Tensor(shape1))</span>
<span class="sd">        &gt;&gt;&gt; print(r0, r1)</span>
<span class="sd">        [2], [0]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Init BroadcastGradientArgs&quot;&quot;&quot;</span>


<span class="k">class</span> <span class="nc">TensorCopySlices</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Copy continues memory.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The target Tensor.</span>
<span class="sd">        - **value** (Tensor) - The tensor to update x.</span>
<span class="sd">        - **begin** (tuple[int]) - A tuple which represents the location where to start. Only</span>
<span class="sd">          constant value is allowed.</span>
<span class="sd">        - **end** (tuple[int]) - A tuple or which represents the maximum location where to end.</span>
<span class="sd">          Only constant value is allowed.</span>
<span class="sd">        - **strides** (tuple[int]) - A tuple which represents the stride is continuously added</span>
<span class="sd">          before reaching the maximum location. Only constant value is allowed.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor), has the same shape and data type of x.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops.operations import _inner_ops</span>
<span class="sd">        &gt;&gt;&gt; copy_slices = _inner_ops.TensorCopySlices()</span>
<span class="sd">        &gt;&gt;&gt; out = copy_slices(Tensor(np.zeros((5, 5))), Tensor(np.ones((2, 5))), (3, 0), (5, 5), (1, 1))</span>
<span class="sd">        &gt;&gt;&gt; print(out)</span>
<span class="sd">            [[1., 1., 1., 1., 1.],</span>
<span class="sd">             [1., 1., 1., 1., 1.],</span>
<span class="sd">             [1., 1., 1., 1., 1.],</span>
<span class="sd">             [0., 0., 0., 0., 0.],</span>
<span class="sd">             [0., 0., 0., 0., 0.]]</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize TensorScatterUpdate&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">,</span> <span class="s1">&#39;begin&#39;</span><span class="p">,</span> <span class="s1">&#39;end&#39;</span><span class="p">,</span> <span class="s1">&#39;strides&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">DSDMatmul</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The definition of the CusSquare primitive.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_w1&#39;</span><span class="p">,</span> <span class="s1">&#39;input_w2&#39;</span><span class="p">,</span> <span class="s1">&#39;input_v&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output_y&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_w1_shape</span><span class="p">,</span> <span class="n">input_w2_shape</span><span class="p">,</span> <span class="n">input_v_shape</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">input_w1_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">head</span> <span class="o">=</span> <span class="n">input_w1_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">v_embedding</span> <span class="o">=</span> <span class="n">input_v_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">//</span> <span class="n">head</span>
        <span class="n">seq_len</span> <span class="o">=</span> <span class="n">input_v_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">//</span> <span class="n">batch_size</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">head</span><span class="p">,</span> <span class="n">v_embedding</span> <span class="o">//</span> <span class="mi">16</span><span class="p">,</span> <span class="n">seq_len</span> <span class="o">//</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_dtype1</span><span class="p">,</span> <span class="n">data_dtype2</span><span class="p">,</span> <span class="n">data_dtype3</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">data_dtype1</span>


<span class="k">class</span> <span class="nc">MatmulDDS</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;MatmulDDS definition&quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bs</span><span class="p">,</span> <span class="n">heads</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;init MatmulDDS&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;q&#39;</span><span class="p">,</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="s1">&#39;local_mask&#39;</span><span class="p">,</span> <span class="s1">&#39;global_mask&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;local_prob&#39;</span><span class="p">,</span> <span class="s1">&#39;global_prob&#39;</span><span class="p">])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">heads</span> <span class="o">=</span> <span class="n">heads</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">local_mask</span><span class="p">,</span> <span class="n">global_mask</span><span class="p">):</span>
        <span class="n">seq_len</span> <span class="o">=</span> <span class="n">local_mask</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">local_mask</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">bs</span> <span class="o">=</span> <span class="n">q</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">q</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">//</span> <span class="n">seq_len</span>
        <span class="n">global_size</span> <span class="o">=</span> <span class="n">seq_len</span> <span class="o">//</span> <span class="mi">4</span>
        <span class="n">size_per_head</span> <span class="o">=</span> <span class="n">q</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">q</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">heads</span>
        <span class="n">heads</span> <span class="o">=</span> <span class="n">q</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">q</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="n">size_per_head</span>
        <span class="n">block_size</span> <span class="o">=</span> <span class="n">local_mask</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">local_mask</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">//</span> <span class="n">bs</span>
        <span class="n">block_num</span> <span class="o">=</span> <span class="n">seq_len</span> <span class="o">//</span> <span class="n">block_size</span>
        <span class="n">l_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">heads</span><span class="p">,</span> <span class="n">block_num</span><span class="p">,</span> <span class="n">block_size</span> <span class="o">//</span> <span class="mi">16</span><span class="p">,</span> <span class="n">block_size</span> <span class="o">//</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
        <span class="n">g_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">heads</span><span class="p">,</span> <span class="n">block_num</span><span class="p">,</span> <span class="n">global_size</span> <span class="o">//</span> <span class="mi">16</span><span class="p">,</span> <span class="n">block_size</span> <span class="o">//</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">l_size</span><span class="p">,</span> <span class="n">g_size</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">local_mask</span><span class="p">,</span> <span class="n">global_mask</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">q</span><span class="p">,</span> <span class="n">q</span>


<span class="k">class</span> <span class="nc">DSDGrad</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The definition of the CusSquare primitive.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;w1_gm&#39;</span><span class="p">,</span> <span class="s1">&#39;w2_gm&#39;</span><span class="p">,</span> <span class="s1">&#39;v_gm&#39;</span><span class="p">,</span> <span class="s1">&#39;a_gm&#39;</span><span class="p">,</span> <span class="s1">&#39;d_a_gm&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;d_w1_gm&#39;</span><span class="p">,</span> <span class="s1">&#39;d_w2_gm&#39;</span><span class="p">,</span> <span class="s1">&#39;d_v_gm&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_w1_shape</span><span class="p">,</span> <span class="n">input_w2_shape</span><span class="p">,</span> <span class="n">input_v_shape</span><span class="p">,</span> <span class="n">input_a_shape</span><span class="p">,</span> <span class="n">input_da_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_w1_shape</span><span class="p">,</span> <span class="n">input_w2_shape</span><span class="p">,</span> <span class="n">input_v_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_dtype1</span><span class="p">,</span> <span class="n">data_dtype2</span><span class="p">,</span> <span class="n">data_dtype3</span><span class="p">,</span> <span class="n">data_dtype4</span><span class="p">,</span> <span class="n">data_dtype5</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">data_dtype1</span><span class="p">,</span> <span class="n">data_dtype1</span><span class="p">,</span> <span class="n">data_dtype1</span>


<span class="k">class</span> <span class="nc">MatmulDDSGrad</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;MatmulDDS definition&quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;init MatmulDDS&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;q&#39;</span><span class="p">,</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="s1">&#39;local_prob&#39;</span><span class="p">,</span> <span class="s1">&#39;global_prob&#39;</span><span class="p">,</span> <span class="s1">&#39;local_prob_grad&#39;</span><span class="p">,</span> <span class="s1">&#39;global_prob_grad&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;dq&#39;</span><span class="p">,</span> <span class="s1">&#39;dk&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">local_prob</span><span class="p">,</span> <span class="n">global_prob</span><span class="p">,</span> <span class="n">local_prob_grad</span><span class="p">,</span> <span class="n">global_prob_grad</span><span class="p">):</span>
        <span class="n">k_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">q</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">q</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">q</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">q</span><span class="p">,</span> <span class="n">k_size</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">local_prob</span><span class="p">,</span> <span class="n">global_prob</span><span class="p">,</span> <span class="n">local_prob_grad</span><span class="p">,</span> <span class="n">global_prob_grad</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span>


<span class="k">class</span> <span class="nc">NonZeroWithValue</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the value of elements that are non-zero (in row-major order - by dimension).</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor), input array of rank &gt;= 2.</span>

<span class="sd">    Outputs:</span>
<span class="sd">         elements that are non-zero.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; op = NonZeroWithValue()</span>
<span class="sd">        &gt;&gt;&gt; data = Tensor(np.array([[1, 0, 0], [0, 0, 1]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; value, index, count = op(data)</span>
<span class="sd">        &gt;&gt;&gt; print(value)</span>
<span class="sd">        [1.0, 1.0]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">transpose</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize NonZeroWithValue&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;transpose&quot;</span><span class="p">,</span> <span class="n">transpose</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">,</span> <span class="s1">&#39;index&#39;</span><span class="p">,</span> <span class="s1">&#39;count&#39;</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">NonZeroWithValueShape</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the value and index of elements that are non-zero (in row-major order - by dimension).</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor), input array of rank &gt;= 2.</span>

<span class="sd">    Outputs:</span>
<span class="sd">         elements that are non-zero.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; non_zero = NonZeroWithValue()</span>
<span class="sd">        &gt;&gt;&gt; op = NonZeroWithValueShape()</span>
<span class="sd">        &gt;&gt;&gt; data = Tensor(np.array([[1, 0, 0], [0, 0, 1]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; value, index, count = non_zero(data)</span>
<span class="sd">        &gt;&gt;&gt; out_value, out_index = op(value, index, count)</span>
<span class="sd">        &gt;&gt;&gt; print(out_index)</span>
<span class="sd">        [[0, 1], [0, 2]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize NonZeroWithValueShape&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">,</span> <span class="s1">&#39;index&#39;</span><span class="p">,</span> <span class="s1">&#39;count&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;out_value&#39;</span><span class="p">,</span> <span class="s1">&#39;out_index&#39;</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">DecodeImage</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns image data that parse from string Tensor.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor), a Tensor of type string. 0-D. The jPEG, GIF, PNG, BMP-encoded image.</span>

<span class="sd">    Outputs:</span>
<span class="sd">         A Tensor of type uint8, uint16, float.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">channels</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span> <span class="n">expand_animations</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">_op_max_shape</span><span class="o">=</span><span class="s2">&quot;8192,8192,3&quot;</span><span class="p">,</span>
                 <span class="n">_op_max_size</span><span class="o">=</span><span class="p">[</span><span class="mi">8000000</span><span class="p">]):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;contents&quot;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;image&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">res_type</span> <span class="o">=</span> <span class="n">dtype</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">res_type</span>


<span class="k">class</span> <span class="nc">SliceGetItem</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        using SliceGetItem to get slice&#39;s attribute of &#39;start&#39; &#39;stop&#39; &#39;step&#39;</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ScatterElements&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;slice&#39;</span><span class="p">,</span> <span class="s1">&#39;attr&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;slice_item&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">slice_value</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">slice_value</span><span class="p">,</span> <span class="nb">slice</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="s2">&quot;Primitive[SliceGetItem] only support to get a slice type element but got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">slice_value</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">value</span> <span class="o">==</span> <span class="s2">&quot;start&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">slice_value</span><span class="o">.</span><span class="n">start</span><span class="p">,</span> <span class="s2">&quot;ndim&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">slice_value</span><span class="o">.</span><span class="n">start</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">slice_value</span><span class="o">.</span><span class="n">start</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">slice_value</span><span class="o">.</span><span class="n">start</span>
        <span class="k">if</span> <span class="n">value</span> <span class="o">==</span> <span class="s2">&quot;stop&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">slice_value</span><span class="o">.</span><span class="n">stop</span><span class="p">,</span> <span class="s2">&quot;ndim&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">slice_value</span><span class="o">.</span><span class="n">stop</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">slice_value</span><span class="o">.</span><span class="n">stop</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">slice_value</span><span class="o">.</span><span class="n">stop</span>
        <span class="k">if</span> <span class="n">value</span> <span class="o">==</span> <span class="s2">&quot;step&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">slice_value</span><span class="o">.</span><span class="n">step</span><span class="p">,</span> <span class="s2">&quot;ndim&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">slice_value</span><span class="o">.</span><span class="n">step</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">slice_value</span><span class="o">.</span><span class="n">step</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">slice_value</span><span class="o">.</span><span class="n">step</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\&#39;</span><span class="s2">slice</span><span class="se">\&#39;</span><span class="s2"> object has no attribute </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">value</span><span class="p">))</span>


<span class="k">class</span> <span class="nc">DynamicBroadcastTo</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Broadcasts input tensor to a given shape.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor. The data type should be one of the following types:</span>
<span class="sd">          float16, float32, int32, int8, uint8.</span>
<span class="sd">          The shape is :math:`(N,*)` where :math:`*` means any number of additional dimensions.</span>
<span class="sd">        - **shape** (Tensor): The target shape to broadcast.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the given `shape` and the same data type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: if the target and input shapes are incompatible.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize DynamicBroadcastTo&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;shape&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>


<div class="viewcode-block" id="Cummin"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Cummin.html#mindspore.ops.Cummin">[文档]</a><span class="k">class</span> <span class="nc">Cummin</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the cumulative minimum of elements and the index.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.cummin` for more detail.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (int): The axis to accumulate the tensor&#39;s value. Must be in the range [-rank(input), rank(input)).</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - The input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        A tuple of 2 Tensors(values, indices), containing the cumulative minimum of elements and the index,</span>
<span class="sd">        The shape of each output tensor is the same as input `input`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; a = Tensor([-0.2284, -0.6628,  0.0975,  0.2680, -1.3298, -0.4220], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; func = ops.Cummin(axis=0)</span>
<span class="sd">        &gt;&gt;&gt; output = func(a)</span>
<span class="sd">        &gt;&gt;&gt; print(output[0])</span>
<span class="sd">        [-0.2284 -0.6628 -0.6628 -0.6628 -1.3298 -1.3298]</span>
<span class="sd">        &gt;&gt;&gt; print(output[1])</span>
<span class="sd">        [0 1 1 1 4 4]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Cummin&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">DynamicResizeNearestNeighbor</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Resizes the input tensor by using the nearest neighbor algorithm.</span>

<span class="sd">    Resizes the input tensor to a given size by using the nearest neighbor algorithm. The nearest</span>
<span class="sd">    neighbor algorithm selects the value of the nearest point and does not consider the</span>
<span class="sd">    values of neighboring points at all, yielding a piecewise-constant interpolant.</span>

<span class="sd">    Note:</span>
<span class="sd">        The operator supports dynamic shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        align_corners (bool): Whether the centers of the 4 corner pixels of the input</span>
<span class="sd">                              and output tensors are aligned. Default: ``False``.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor. The shape of the tensor is :math:`(N, C, H, W)`.</span>
<span class="sd">        - **size** (Union[tuple, list]): The target size. The dimension of size must be 2.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape of the output tensor is  :math:`(N, C, NEW\_H, NEW\_W)`.</span>
<span class="sd">        The data type is the same as the `input_x`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ResizeNearestNeighbor&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;align_corners&quot;</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;image_in&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;image_out&#39;</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">PsROIPooling</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Position Sensitive ROI-Pooling</span>
<span class="sd">    Inputs:</span>
<span class="sd">        - feature(Tensor)</span>
<span class="sd">        - rois(Tensor)</span>

<span class="sd">        - **features** (Tensor) - The input features, whose shape must be :math:`(N, C, H, W)`.</span>
<span class="sd">        - **rois** (Tensor) - The shape is :math:`(rois\_n, 5)`. With data type of float16 or float32.</span>
<span class="sd">          `rois_n` represents the number of RoI. The size of the second dimension must be `5` and the `5` colunms</span>
<span class="sd">          are :math:`(image\_index, top\_left\_x, top\_left\_y, bottom\_right\_x, bottom\_right\_y)`.</span>
<span class="sd">          `image_index` represents the index of image. `top_left_x` and `top_left_y` represent the `x, y`</span>
<span class="sd">          coordinates of the top left corner of corresponding RoI, respectively. `bottom_right_x` and `bottom_right_y`</span>
<span class="sd">          represent the `x, y` coordinates of the bottom right corner of corresponding RoI, respectively.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - out shape(rois_num, out_channel, pool_height, pool_width), the result after pooling.</span>
<span class="sd">        - channel_map shape(rois_num, out_channel, pool_height, pool_width), use for back forward to compute grad</span>
<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops.operations import _inner_ops as inner</span>
<span class="sd">        &gt;&gt;&gt; features = np.random.randn(4, 21 * 7 * 7, 80, 48)</span>
<span class="sd">        &gt;&gt;&gt; features = Tensor.from_numpy(features).astype(mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; rois = Tensor.from_numpy(</span>
<span class="sd">        &gt;&gt;&gt;     np.array([</span>
<span class="sd">        &gt;&gt;&gt;        [0.0000, 150.3563, 200.1320, 579.3563, 602.3452],</span>
<span class="sd">        &gt;&gt;&gt;        [1.0000, 657.1263, 302.8564, 762.4214, 567.9854],</span>
<span class="sd">        &gt;&gt;&gt;        [2.0000, 321.3122, 232.2410, 679.0281, 587.6346],</span>
<span class="sd">        &gt;&gt;&gt;        [3.0000, 664.1630, 387.4919, 778.7322, 562.7321],</span>
<span class="sd">        &gt;&gt;&gt;     ])).astype(mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; psRoIPooling = inner.PsROIPooling(pooled_height=7, pooled_width=7, num_rois=4,</span>
<span class="sd">        &gt;&gt;&gt;                                  spatial_scale=1.0/16, out_dim=21,</span>
<span class="sd">        &gt;&gt;&gt;                                  group_size=7)</span>
<span class="sd">        &gt;&gt;&gt; out, channel_map = psRoIPooling(features, rois)</span>
<span class="sd">        &gt;&gt;&gt; print(out.shape)</span>
<span class="sd">            [4, 21, 7, 7]</span>
<span class="sd">        &gt;&gt;&gt; print(channel_map.shape)</span>
<span class="sd">            [4, 21, 7, 7]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pooled_height</span><span class="p">,</span> <span class="n">pooled_width</span><span class="p">,</span> <span class="n">num_rois</span><span class="p">,</span> <span class="n">spatial_scale</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">group_size</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize PsROIPooling&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;pooled_height&quot;</span><span class="p">,</span> <span class="n">pooled_height</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;pooled_width&quot;</span><span class="p">,</span> <span class="n">pooled_width</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;num_rois&quot;</span><span class="p">,</span> <span class="n">pooled_width</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;spatial_scale&quot;</span><span class="p">,</span> <span class="n">spatial_scale</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;out_dim&quot;</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;group_size&quot;</span><span class="p">,</span> <span class="n">group_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pooled_height</span> <span class="o">=</span> <span class="n">pooled_height</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pooled_width</span> <span class="o">=</span> <span class="n">pooled_width</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_rois</span> <span class="o">=</span> <span class="n">num_rois</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">spatial_scale</span> <span class="o">=</span> <span class="n">spatial_scale</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span> <span class="o">=</span> <span class="n">out_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group_size</span> <span class="o">=</span> <span class="n">group_size</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs_shape</span><span class="p">,</span> <span class="n">rois_shape</span><span class="p">):</span>
        <span class="n">output_shape</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_rois</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pooled_height</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pooled_width</span><span class="p">]</span>
        <span class="n">output_map_shape</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_rois</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pooled_height</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pooled_width</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">output_shape</span><span class="p">,</span> <span class="n">output_map_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs_type</span><span class="p">,</span> <span class="n">rois_type</span><span class="p">):</span>
        <span class="n">map_type</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">TensorType</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">inputs_type</span><span class="p">,</span> <span class="n">map_type</span>


<span class="k">class</span> <span class="nc">ParallelResizeBilinear</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ParallelResizeBilinear ops&quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ori_image_size</span><span class="p">,</span> <span class="n">split_size</span><span class="p">,</span> <span class="n">src_start_w</span><span class="p">,</span> <span class="n">dst_start_w</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ParallelResizeBilinear.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;ori_image_size&quot;</span><span class="p">,</span> <span class="n">ori_image_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;split_size&quot;</span><span class="p">,</span> <span class="n">split_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">split_size</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;len of split_size&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;src_start_w&quot;</span><span class="p">,</span> <span class="n">src_start_w</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;dst_start_w&quot;</span><span class="p">,</span> <span class="n">dst_start_w</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;align_corners&quot;</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ori_image_size</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">ori_image_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">split_size</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">split_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">src_start_w</span> <span class="o">=</span> <span class="n">src_start_w</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dst_start_w</span> <span class="o">=</span> <span class="n">dst_start_w</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">align_corners</span> <span class="o">=</span> <span class="n">align_corners</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">half_pixel_centers</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;ori_image_size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ori_image_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;split_size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;src_start_w&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">src_start_w</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;dst_start_w&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dst_start_w</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;align_corners&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">align_corners</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;half_pixel_centers&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">half_pixel_centers</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
        <span class="n">size_val</span> <span class="o">=</span> <span class="n">size</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>
        <span class="n">x_shape</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]</span>
        <span class="n">x_dtype</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;x_dtype&quot;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">size_val</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;size must be const input&quot;</span><span class="p">)</span>
        <span class="n">output_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>

        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="n">output_shape</span><span class="p">,</span>
                <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">,</span>
                <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span>


<span class="k">class</span> <span class="nc">PartitionedCall</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Pass the input tensors to the subgraph and return the output tensors.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **inputs** (Tuple), the input tensors, which will be passed to subgraph.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - outputs(Tuple), the output tensor returned by subgraph.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">graph</span><span class="p">,</span> <span class="n">executor_type</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PartitionedCall</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;executor_type&quot;</span><span class="p">,</span> <span class="n">executor_type</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">graph</span> <span class="o">=</span> <span class="n">graph</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
        <span class="k">return</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
        <span class="k">return</span> <span class="ne">NotImplementedError</span>


<span class="k">class</span> <span class="nc">CellBackwardHook</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This operator is used to hook input gradient and output gradient of Cell object.</span>

<span class="sd">    Note:</span>
<span class="sd">        This operator is only used in backward hook function of Cell object in pynative mode.</span>

<span class="sd">    Args:</span>
<span class="sd">        cell_id (str): Used to identify which cell obj the hook function registered on. For example, &#39;nn.Add()&#39; is a</span>
<span class="sd">        cell object.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** - The variable to hook.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **output** - Returns `input` directly. `CellBackwardHook` does not affect the forward result.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import GradOperation</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops.operations import _inner_ops as inner</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(mode=ms.PYNATIVE_MODE)</span>
<span class="sd">        &gt;&gt;&gt; def hook_fn(grad):</span>
<span class="sd">        ...     print(grad)</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; hook = inner.CellBackwardHook()</span>
<span class="sd">        &gt;&gt;&gt; hook_fn_key = hook.register_backward_hook(hook_fn)</span>
<span class="sd">        &gt;&gt;&gt; def hook_test(x, y):</span>
<span class="sd">        ...     z = x * y</span>
<span class="sd">        ...     z = hook(z)</span>
<span class="sd">        ...     z = z * y</span>
<span class="sd">        ...     return z</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; grad_all = GradOperation(get_all=True)</span>
<span class="sd">        &gt;&gt;&gt; def backward(x, y):</span>
<span class="sd">        ...     return grad_all(hook_test)(x, y)</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; output = backward(Tensor(1, mindspore.float32), Tensor(2, mindspore.float32))</span>
<span class="sd">        (Tensor(shape=[], dtype=Float32, value= 2),)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[], dtype=Float32, value= 4), Tensor(shape=[], dtype=Float32, value= 4))</span>
<span class="sd">        &gt;&gt;&gt; hook.remove_backward_hook(hook_fn_key)</span>
<span class="sd">        &gt;&gt;&gt; output = backward(Tensor(1, mindspore.float32), Tensor(2, mindspore.float32))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[], dtype=Float32, value= 4), Tensor(shape=[], dtype=Float32, value= 4))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cell_id</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize CellBackwardHook&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CellBackwardHook</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cell_id</span> <span class="o">=</span> <span class="n">cell_id</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;cell_id&quot;</span><span class="p">,</span> <span class="n">cell_id</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_attrs</span><span class="p">[</span><span class="s2">&quot;cell_id&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">cell_id</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="n">args</span><span class="p">,)</span>
        <span class="k">return</span> <span class="n">_run_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs_shape</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">inputs_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">inputs_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs_type</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs_type</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">inputs_type</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">inputs_type</span>

    <span class="k">def</span> <span class="nf">register_backward_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hook_fn</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This function is used to register backward hook function. Note that this function is only supported in pynative</span>
<span class="sd">        mode.</span>

<span class="sd">        Note:</span>
<span class="sd">            The &#39;hook_fn&#39; must be defined as the following code.</span>
<span class="sd">            `cell_id` is the information of registered cell. `grad_input` is the gradient passed to the cell.</span>
<span class="sd">            `grad_output` is the gradient computed and passed to the next cell or primitive, which may be modified by</span>
<span class="sd">            returning a new output gradient.</span>
<span class="sd">            The &#39;hook_fn&#39; should have the following signature:</span>
<span class="sd">            hook_fn(cell_id, grad_input, grad_output) -&gt; New output gradient or none.</span>
<span class="sd">            The &#39;hook_fn&#39; is executed in the python environment.</span>

<span class="sd">        Args:</span>
<span class="sd">            hook_fn (Function): Python function. Backward hook function.</span>

<span class="sd">        Returns:</span>
<span class="sd">            - **key** (int) - The key of &#39;hook_fn&#39;.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If the `hook_fn` is not a function of python.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">hook_fn</span><span class="p">,</span> <span class="p">(</span><span class="n">FunctionType</span><span class="p">,</span> <span class="n">MethodType</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;When using &#39;register_backward_hook(hook_fn)&#39;, the type of &#39;hook_fn&#39; must be python &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;function, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">hook_fn</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_backward_hook_fn</span><span class="p">(</span><span class="n">hook_fn</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">key</span>

    <span class="k">def</span> <span class="nf">remove_backward_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This function is used to remove backward hook function. Note that this operation is only supported in pynative</span>
<span class="sd">        mode.</span>

<span class="sd">        Note:</span>
<span class="sd">            The &#39;key&#39; is the object returned by &#39;register_backward_hook&#39; function of the same CellBackwardHook</span>
<span class="sd">            operator.</span>

<span class="sd">        Args:</span>
<span class="sd">            key (int): The key corresponding to the &#39;hook_fn&#39;.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">remove_backward_hook_fn</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Format</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This operator is used to format a string.</span>

<span class="sd">    Note:</span>
<span class="sd">     Current not supported to using by customer.</span>
<span class="sd">     Only support convert str.format() in user code and it will be converted to be Format</span>
<span class="sd">     operation by ME-Compiler automatically.</span>


<span class="sd">    Inputs:</span>
<span class="sd">     - **input** -</span>
<span class="sd">     string : the string to be formatted.</span>
<span class="sd">     args : the format args.</span>

<span class="sd">    Outputs:</span>
<span class="sd">     - **output** - Returns formatted string.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">     ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;string&#39;</span><span class="p">,</span> <span class="s1">&#39;args&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;string&#39;</span><span class="p">])</span>


    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">str_</span><span class="p">,</span> <span class="o">*</span><span class="n">var</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">check_variable</span><span class="p">(</span><span class="n">str_</span><span class="p">,</span> <span class="n">var</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">_check_contains_variable</span><span class="p">(</span><span class="n">str_</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="n">str_</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]):</span>
                <span class="k">return</span> <span class="kc">True</span>

            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">var</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">_check_contains_variable</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="n">item</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]):</span>
                    <span class="k">return</span> <span class="kc">True</span>
            <span class="k">return</span> <span class="kc">False</span>


        <span class="k">if</span> <span class="n">check_variable</span><span class="p">(</span><span class="n">str_</span><span class="p">,</span> <span class="n">var</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">mstype</span><span class="o">.</span><span class="n">string</span><span class="p">,</span> <span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span>


        <span class="n">str_value</span> <span class="o">=</span> <span class="n">str_</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="n">var_value</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">var</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="s2">&quot;dtype&quot;</span><span class="p">],</span> <span class="n">typing</span><span class="o">.</span><span class="n">Keyword</span><span class="p">):</span>
                <span class="n">kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="s2">&quot;value&quot;</span><span class="p">])</span>
            <span class="n">var_value</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="s2">&quot;value&quot;</span><span class="p">])</span>

        <span class="n">value</span> <span class="o">=</span> <span class="n">str_value</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">*</span><span class="n">var_value</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">mstype</span><span class="o">.</span><span class="n">string</span><span class="p">,</span> <span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="n">value</span><span class="p">}</span>


<span class="k">class</span> <span class="nc">FlattenConcat</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Flatten input tensors and concatenate them into several chunk tensors grouped by data types.</span>

<span class="sd">    Args:</span>
<span class="sd">        fusion_size (int): Maximum memory chunk size in bytes, 0 for unlimited. Default: 0.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **tensors** (tuple[Tensor], list[Tensor]) - The input Tensors to be flattened and concatenated.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        tuple[Tensor], result chunk tensors.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops.operations import _inner_ops as inner</span>
<span class="sd">        &gt;&gt;&gt; t1 = Tensor(np.array([1]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; t2 = Tensor(np.array([2]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; t3 = Tensor(np.array([3]).astype(np.float64))</span>
<span class="sd">        &gt;&gt;&gt; t4 = Tensor(np.array([4]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; t5 = Tensor(np.array([5]).astype(np.float64))</span>
<span class="sd">        &gt;&gt;&gt; chunks = inner.FlattenConcat()([t1, t2, t2, t3, t4, t5])</span>
<span class="sd">        &gt;&gt;&gt; print(chunks[0].asnumpy())</span>
<span class="sd">        &gt;&gt;&gt; print(chunks[1].asnumpy())</span>
<span class="sd">        [1. 2. 4.]</span>
<span class="sd">        [3. 5.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fusion_size</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize FlattenConcat&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">fusion_size</span><span class="p">,</span> <span class="s1">&#39;fusion_size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fusion_size</span> <span class="o">=</span> <span class="n">fusion_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;fusion_size&#39;</span><span class="p">,</span> <span class="n">fusion_size</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">KMeansCentroids</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the segment_sum, segment_count, kmean_total_sum that are clustering results</span>

<span class="sd">    Args:</span>
<span class="sd">        use_actual_distance (bool): A bool value to decide whether do complete calculation of distance.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor(float32)) - Input data used for clustering</span>
<span class="sd">        - **y** (Tensor(float32)) - Initial centroids of clutering</span>
<span class="sd">        - **sum_square_y** (Tensor(float32)) - The result of preprocessing such as square, reduce and transpose of y</span>
<span class="sd">        - **sum_square_x** (Tensor(float32)) - The result of preprocessing such as square and reduce of x</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **segment_sum** (Tensor(float32)) - Clustering result w.r.t. each centroid</span>
<span class="sd">        - **segment_count** (Tensor(float32)) - Clustering count w.r.t. each centroid</span>
<span class="sd">        - **kmean_total_sum** (Tensor(float32)) - The sum of the distances from all vectors to ther nearest centroid</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        &#39;&#39;Ascend&#39;&#39;</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.common.dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as P</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(mode=ms.GRAPH_MODE, device_target=&quot;Ascend&quot;)</span>

<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        &gt;&gt;&gt;    def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;        super(Net, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;        self.reduce_sum = P.ReduceSUm(keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt;        self.square = P.Square()</span>
<span class="sd">        &gt;&gt;&gt;        self.transpose = P.Transpose()</span>
<span class="sd">        &gt;&gt;&gt;        self.k_means_centroids = P.KMeansCentroids(True)</span>

<span class="sd">        &gt;&gt;&gt;    def construct(self, x, y):</span>
<span class="sd">        &gt;&gt;&gt;        p1 = self.reduce_sum(self.square(x), -1)</span>
<span class="sd">        &gt;&gt;&gt;        p2 = self.transpose(self.reduce_sum(self.square(y), -1), (1, 0))</span>
<span class="sd">        &gt;&gt;&gt;        return self.k_means_centroids(x, y, p2, p1)</span>

<span class="sd">        &gt;&gt;&gt; def test_net():</span>
<span class="sd">        &gt;&gt;&gt;    data_type = np.float32</span>
<span class="sd">        &gt;&gt;&gt;    x = Tensor(np.random.uniform(-10, 10, (65536, 128)).astype(data_type))</span>
<span class="sd">        &gt;&gt;&gt;    y = P.Ones()((1048576, 128), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt;    net = Net()</span>
<span class="sd">        &gt;&gt;&gt;    local_sum, local_count, local_avg_distance = net(x, y)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_actual_distance</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;use_actual_distance&#39;</span><span class="p">,</span> <span class="n">use_actual_distance</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;sum_square_y&#39;</span><span class="p">,</span> <span class="s1">&#39;sum_square_x&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;segment_sum&#39;</span><span class="p">,</span> <span class="s1">&#39;segment_count&#39;</span><span class="p">,</span> <span class="s1">&#39;kmean_total_sum&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">,</span> <span class="n">sum_square_y_shape</span><span class="p">,</span> <span class="n">sum_square_x_shape</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;infer shape of primitive&quot;&quot;&quot;</span>
        <span class="n">expected_shape_size</span> <span class="o">=</span> <span class="mi">2</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="n">expected_shape_size</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;dims of x&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_shape</span><span class="p">),</span> <span class="n">expected_shape_size</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;dims of y&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sum_square_y_shape</span><span class="p">),</span> <span class="n">expected_shape_size</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span>
                            <span class="s2">&quot;dims of sum_square_y&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sum_square_x_shape</span><span class="p">),</span> <span class="n">expected_shape_size</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span>
                            <span class="s2">&quot;dims of sum_square_x&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">y_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span>
                            <span class="s2">&quot;the second dim of x and the second dim of y&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">y_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">sum_square_y_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span>
                            <span class="s2">&quot;the first dim of y and the second dim of sum_square_y&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">sum_square_x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span>
                            <span class="s2">&quot;the first dim of x and the first dim of sum_square_x&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">sum_square_y_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">sum_square_x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span>
                            <span class="s2">&quot;the first dim of sum_square_y and the first dim of sum_square_x&quot;</span><span class="p">,</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">sum_square_y_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span>
                            <span class="s2">&quot;the first dim of sum_square_y&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">k</span> <span class="o">=</span> <span class="n">y_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">em_size</span> <span class="o">=</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">em_size</span><span class="p">),</span> <span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ClipByNorm</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Clips tensor values to a maximum :math:`L_2`-norm.</span>

<span class="sd">    Note:</span>
<span class="sd">        The output tensor of this operator remains the same with input tensor if the :math:`L_2`-norm of the input</span>
<span class="sd">        tensor is not greater than the argument `clip_norm`. Otherwise the output tensor will be normalized as:</span>

<span class="sd">        .. math::</span>
<span class="sd">            \text{output}(X) = \frac{\text{clip_norm} * X}{L_2(X)},</span>

<span class="sd">        where :math:`L_2(X)` is the :math:`L_2`-norm of :math:`X`.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (Union[None, int, tuple(int), list(int)]): Compute the `L_2`-norm along the specific dimension.</span>
<span class="sd">                                                       Default: ``None``, all dimensions to calculate.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Tensor of shape N-D. The type must be float16 or float32.</span>
<span class="sd">        - **clip_norm** (Tensor) - A scalar Tensor of shape :math:`()` or :math:`(1)`.</span>
<span class="sd">          Or a Tensor which shape can be broadcast to the shape of `x`. The type must be float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, clipped Tensor with the same shape as the `x`, whose type is float32.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is not one of None, int, tuple(int) and list(int).</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `clip_norm` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops.operations import _inner_ops as inner</span>
<span class="sd">        &gt;&gt;&gt; clip_by_norm = inner.ClipByNorm()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.randint(0, 10, [4, 16]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; clip_norm = Tensor(np.array([100]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = clip_by_norm(x, clip_norm)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (4, 16)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ClipByNorm&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">axis</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">axis_check</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">)</span> <span class="k">else</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">,)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axis_check</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;axis[</span><span class="si">%d</span><span class="s1">]&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_attrs</span><span class="p">[</span><span class="s1">&#39;axis&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;clip_norm&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">clip_norm_shape</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Infer shape for ClipByNorm&quot;&quot;&quot;</span>
        <span class="n">x_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">)</span> <span class="k">else</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">,)</span>
        <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axis</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="o">-</span><span class="n">x_dim</span><span class="p">,</span> <span class="n">x_dim</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span> <span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_type</span><span class="p">,</span> <span class="n">clip_norm_type</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Infer data type for ClipByNorm&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;x_type&quot;</span><span class="p">,</span> <span class="n">x_type</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;clip_norm_type&quot;</span><span class="p">,</span> <span class="n">clip_norm_type</span><span class="p">,</span>
                                           <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span>


<span class="k">class</span> <span class="nc">TopTypeof</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Internal primitive method, to speed up mindspore.ops.typeof.</span>

<span class="sd">        Returns the top type of the input data.</span>

<span class="sd">        In Pynative mode, returns the top type in cache.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prim</span> <span class="o">=</span> <span class="n">Primitive</span><span class="p">(</span><span class="s1">&#39;TopTypeof&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">typeof_cache</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;slice&#39;</span><span class="p">:</span> <span class="n">mstype</span><span class="o">.</span><span class="n">Slice</span><span class="p">(),</span>
            <span class="s1">&#39;list&#39;</span><span class="p">:</span> <span class="n">mstype</span><span class="o">.</span><span class="n">List</span><span class="p">(),</span>
            <span class="s1">&#39;tuple&#39;</span><span class="p">:</span> <span class="n">mstype</span><span class="o">.</span><span class="n">Tuple</span><span class="p">(),</span>
            <span class="s1">&#39;Tensor&#39;</span><span class="p">:</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">,</span>
            <span class="s1">&#39;NoneType&#39;</span><span class="p">:</span> <span class="n">mstype</span><span class="o">.</span><span class="n">NoneType</span><span class="p">(),</span>
            <span class="s1">&#39;int&#39;</span><span class="p">:</span> <span class="n">mstype</span><span class="o">.</span><span class="n">Int</span><span class="p">(),</span>
            <span class="s1">&#39;bool&#39;</span><span class="p">:</span> <span class="n">mstype</span><span class="o">.</span><span class="n">Bool</span><span class="p">(),</span>
            <span class="s1">&#39;ellipsis&#39;</span><span class="p">:</span> <span class="n">mstype</span><span class="o">.</span><span class="n">Ellipsis_</span><span class="p">(),</span>
            <span class="s1">&#39;dict&#39;</span><span class="p">:</span> <span class="n">mstype</span><span class="o">.</span><span class="n">Dict</span><span class="p">()</span>
        <span class="p">}</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">index_type</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="k">if</span> <span class="s1">&#39;Tensor&#39;</span> <span class="ow">in</span> <span class="n">index_type</span><span class="p">:</span>
            <span class="n">index_type</span> <span class="o">=</span> <span class="s1">&#39;Tensor&#39;</span>
        <span class="k">if</span> <span class="n">index_type</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">typeof_cache</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">typeof_cache</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">index_type</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">_pynative_executor</span><span class="o">.</span><span class="n">constant_folding</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prim</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">MixedPrecisionCast</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Internal primitive method, to achieve mindspore.functional.mixed_precision_cast.</span>

<span class="sd">    Note:</span>
<span class="sd">        This internal primitive method used to do mixed precision conversion.</span>
<span class="sd">        Only the input object with float dtype will be cast.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **dtype** (Union[Float16, Float32]) - The data type of the output object.</span>
<span class="sd">        - **input** (Union[Tensor, Tuple, Dictionary, KeywordArg]) - The object to be cast.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Object, its dtype is the same as `dtype` and shape is the same as &#39;input&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops.operations import _inner_ops as inner</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([2, 3], dtype=np.float32))</span>
<span class="sd">        &gt;&gt;&gt; out = inner.MixedPrecisionCast(mstype.float16, x)</span>
<span class="sd">        &gt;&gt;&gt; print(out.dtype)</span>
<span class="sd">        Float16</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize MixedPrecisionCast&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;dst_dtype&#39;</span><span class="p">,</span> <span class="s1">&#39;input_x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cast</span> <span class="o">=</span> <span class="n">Cast</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hyper_map</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">HyperMap</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dst_dtype</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">cast_inner</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">data</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">):</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">dst_dtype</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">data</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">hyper_map</span><span class="p">(</span><span class="n">cast_inner</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">CheckBprop</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks whether the data type and the shape of corresponding elements from tuples x and y are the same.</span>

<span class="sd">    Args:</span>
<span class="sd">        prim_to_check (str): The name of the primitive being checked. Default: &#39;&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (tuple[Tensor]) - The `input_x` contains the outputs of bprop to be checked.</span>
<span class="sd">        - **input_y** (tuple[Tensor]) - The `input_y` contains the inputs of bprop to check against.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple[Tensor], the `input_x`,</span>
<span class="sd">        if data type and shape of corresponding elements from `input_x` and `input_y` are the same.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` or `input_y` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.op = ops.CheckBprop()</span>
<span class="sd">        ...     def construct(self, x, y):</span>
<span class="sd">        ...         return self.op(x, y)</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; input_x = (Tensor(np.array([[2, 2], [2, 2]]), mindspore.float32),)</span>
<span class="sd">        &gt;&gt;&gt; input_y = (Tensor(np.array([[2, 2], [2, 2]]), mindspore.float32),)</span>
<span class="sd">        &gt;&gt;&gt; output = net(input_x, input_y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 2.00000000e+00,  2.00000000e+00],</span>
<span class="sd">         [ 2.00000000e+00,  2.00000000e+00]]),)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prim_to_check</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize CheckBprop&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prim_to_check</span> <span class="o">=</span> <span class="n">prim_to_check</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xshapes</span><span class="p">,</span> <span class="n">yshapes</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;infer shape&quot;&quot;&quot;</span>
        <span class="n">tips</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;user defined method &#39;bprop&#39;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;grads&#39;</span><span class="p">,</span> <span class="n">xshapes</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,),</span> <span class="n">tips</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;params&#39;</span><span class="p">,</span> <span class="n">yshapes</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,),</span> <span class="n">tips</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">len</span><span class="p">(</span><span class="n">xshapes</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">yshapes</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For </span><span class="si">{</span><span class="n">tips</span><span class="si">}</span><span class="s2"> the number of return values(gradients) must be equal to &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;the number of input arguments except &#39;out&#39; and &#39;dout&#39;, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;which is:</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">yshapes</span><span class="p">)</span><span class="si">}</span><span class="s2"> but got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">xshapes</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">shape_equal</span><span class="p">(</span><span class="n">shape1</span><span class="p">,</span> <span class="n">shape2</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape1</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape2</span><span class="p">):</span>
                <span class="k">return</span> <span class="kc">False</span>
            <span class="k">for</span> <span class="n">shape_axis1</span><span class="p">,</span> <span class="n">shape_axis2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">shape1</span><span class="p">,</span> <span class="n">shape2</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">shape_axis1</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">or</span> <span class="n">shape_axis2</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="k">if</span> <span class="n">shape_axis1</span> <span class="o">!=</span> <span class="n">shape_axis2</span><span class="p">:</span>
                    <span class="k">return</span> <span class="kc">False</span>
            <span class="k">return</span> <span class="kc">True</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">xshape</span><span class="p">,</span> <span class="n">yshape</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">xshapes</span><span class="p">,</span> <span class="n">yshapes</span><span class="p">)):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">xshape</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">yshape</span><span class="p">:</span>
                <span class="k">continue</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">shape_equal</span><span class="p">(</span><span class="n">xshape</span><span class="p">,</span> <span class="n">yshape</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For </span><span class="si">{</span><span class="n">tips</span><span class="si">}</span><span class="s2">, the </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">th return value(gradient of the </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">th argument) &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;should have the same shape as the </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">th argument, &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;which is:</span><span class="si">{</span><span class="n">yshape</span><span class="si">}</span><span class="s2">, but got: </span><span class="si">{</span><span class="n">xshape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">xshapes</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xdtypes</span><span class="p">,</span> <span class="n">ydtypes</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;infer dtype&quot;&quot;&quot;</span>
        <span class="n">tips</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;user defined method &#39;bprop&#39;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;grads&#39;</span><span class="p">,</span> <span class="n">xdtypes</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,),</span> <span class="n">tips</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;params&#39;</span><span class="p">,</span> <span class="n">ydtypes</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,),</span> <span class="n">tips</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">len</span><span class="p">(</span><span class="n">xdtypes</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">ydtypes</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For </span><span class="si">{</span><span class="n">tips</span><span class="si">}</span><span class="s2">, the number of return values(gradients) must be equal to &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;the number of input arguments except &#39;out&#39; and &#39;dout&#39;, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;which is:</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">ydtypes</span><span class="p">)</span><span class="si">}</span><span class="s2"> but got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">xdtypes</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">checking_range</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ydtypes</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">checking_range</span><span class="p">):</span>
            <span class="n">xdtype</span> <span class="o">=</span> <span class="n">xdtypes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">ydtype</span> <span class="o">=</span> <span class="n">ydtypes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">xdtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">AnythingType</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ydtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">AnythingType</span><span class="p">):</span>
                <span class="k">continue</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ydtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">FunctionType</span><span class="p">):</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">xdtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">EnvType</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For </span><span class="si">{</span><span class="n">tips</span><span class="si">}</span><span class="s2">, the </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">th return value(gradient of the </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">th argument) type &quot;</span>
                                    <span class="sa">f</span><span class="s2">&quot;should be </span><span class="si">{</span><span class="n">mstype</span><span class="o">.</span><span class="n">EnvType</span><span class="si">}</span><span class="s2">, but got </span><span class="si">{</span><span class="n">xdtype</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">xdtype</span> <span class="o">!=</span> <span class="n">ydtype</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For </span><span class="si">{</span><span class="n">tips</span><span class="si">}</span><span class="s2">, the </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">th return value(gradient of the </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">th argument) &quot;</span>
                                <span class="sa">f</span><span class="s2">&quot;should have the same dtype as the </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">th argument, &quot;</span>
                                <span class="sa">f</span><span class="s2">&quot;which is:</span><span class="si">{</span><span class="n">ydtype</span><span class="si">}</span><span class="s2">, but got: </span><span class="si">{</span><span class="n">xdtype</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">xdtypes</span>


<span class="n">check_bprop</span> <span class="o">=</span> <span class="n">CheckBprop</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">SameTypeShape</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks whether the data type and shape of two tensors are the same.</span>

<span class="sd">    Refer to :func:`mindspore.ops.same_type_shape` for more detail.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[2, 2], [2, 2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.array([[2, 2], [2, 2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.SameTypeShape()(input_x, input_y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[2. 2.]</span>
<span class="sd">         [2. 2.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Same&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;run in PyNative mode&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;x dtype&#39;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="s1">&#39;y dtype&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="ne">TypeError</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;x shape&#39;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s1">&#39;y shape&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;x dtype&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="s1">&#39;y dtype&#39;</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="ne">TypeError</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;x shape&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">],</span> <span class="s1">&#39;y shape&#39;</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">],</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="n">same_type_shape_</span> <span class="o">=</span> <span class="n">SameTypeShape</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_is_subclass_</span><span class="p">(</span><span class="n">type_</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">type_</span><span class="p">,</span> <span class="n">typing</span><span class="o">.</span><span class="n">Type</span><span class="p">):</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="n">typing</span><span class="o">.</span><span class="n">is_subclass</span><span class="p">(</span><span class="n">type_</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">IsSubClass</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks whether this type is a sub-class of another type.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **sub_type** (mindspore.dtype) - The type to be checked. Only constant value is allowed.</span>
<span class="sd">        - **type_** (mindspore.dtype) - The target type. Only constant value is allowed.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        bool, the check result.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `sub_type` or `type_` is not a Type.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; output = ops.IsSubClass()(mindspore.int32,  mindspore.intc)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        True</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sub_type</span><span class="p">,</span> <span class="n">type_</span><span class="p">):</span>
        <span class="n">sub_type_t</span> <span class="o">=</span> <span class="n">sub_type</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>
        <span class="n">type_v</span> <span class="o">=</span> <span class="n">type_</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;sub_type&quot;</span><span class="p">,</span> <span class="n">sub_type_t</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">Type</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;type_&quot;</span><span class="p">,</span> <span class="n">type_v</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">Type</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">value</span> <span class="o">=</span> <span class="n">_is_subclass_</span><span class="p">(</span><span class="n">sub_type_t</span><span class="p">,</span> <span class="n">type_v</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="p">(),</span>
               <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">mstype</span><span class="o">.</span><span class="n">type_type</span><span class="p">,</span>
               <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="n">value</span><span class="p">}</span>
        <span class="k">return</span> <span class="n">out</span>


<span class="n">issubclass_</span> <span class="o">=</span> <span class="n">IsSubClass</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">IsInstance</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks whether an object is an instance of a target type.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **inst** (Any Object) - The instance to be checked. Only constant value is allowed.</span>
<span class="sd">        - **type_** (mindspore.dtype) - The target type. Only constant value is allowed.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        bool, the check result.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `type_` is not a Type.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; inst = 1</span>
<span class="sd">        &gt;&gt;&gt; output = ops.IsInstance()(inst, mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        False</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inst</span><span class="p">,</span> <span class="n">type_</span><span class="p">):</span>
        <span class="n">sub_type_t</span> <span class="o">=</span> <span class="n">inst</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]</span>
        <span class="n">type_v</span> <span class="o">=</span> <span class="n">type_</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;type_&quot;</span><span class="p">,</span> <span class="n">type_v</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">Type</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">type_v</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">list_</span><span class="p">:</span>
            <span class="n">value</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sub_type_t</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">type_v</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tuple_</span><span class="p">:</span>
            <span class="n">value</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sub_type_t</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">_is_subclass_</span><span class="p">(</span><span class="n">sub_type_t</span><span class="p">,</span> <span class="n">type_v</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="p">(),</span>
               <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">mstype</span><span class="o">.</span><span class="n">type_type</span><span class="p">,</span>
               <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="n">value</span><span class="p">}</span>
        <span class="k">return</span> <span class="n">out</span>


<span class="k">class</span> <span class="nc">ConvertToAdapterTensor</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Convert a tensor from MindSpore&#39;s Tensor type to MSAdapter&#39;s Tensor type,</span>
<span class="sd">    where MSAdapter&#39;s Tensor is a subclass of MindSpore&#39;s Tensor.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        A tensor, whose type is MSAdapter&#39;s Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([1, 2 ,3])</span>
<span class="sd">        &gt;&gt;&gt; x = ops.ConvertToAdapterTensor()(x)</span>
<span class="sd">        &gt;&gt;&gt; print(x)</span>
<span class="sd">        [1 2 3]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Run in PyNative mode&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">ms_adapter_registry</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">cast_tensor</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="n">convert_to_adapter_tensor</span> <span class="o">=</span> <span class="n">ConvertToAdapterTensor</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">ConvertToMsTensor</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Convert a tensor from MSAdapter&#39;s Tensor type to MindSpore&#39;s Tensor type,</span>
<span class="sd">    where MSAdapter&#39;s Tensor is a subclass of MindSpore&#39;s Tensor.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        A tensor, whose type is MindSpore&#39;s Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([1, 2 ,3])</span>
<span class="sd">        &gt;&gt;&gt; x = ops.ConvertToMsTensor()(x)</span>
<span class="sd">        &gt;&gt;&gt; print(x)</span>
<span class="sd">        [1 2 3]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Run in PyNative mode&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">StubTensor</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">StubTensor</span><span class="p">(</span><span class="n">stub</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">stub</span><span class="p">,</span> <span class="n">tensor</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="n">convert_to_ms_tensor</span> <span class="o">=</span> <span class="n">ConvertToMsTensor</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">GetGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Use the position id or Parameter object to get the gradient from the output</span>
<span class="sd">        which returned by the :func:`mindspore.ops.grad`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ScatterElements&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span>
            <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;gradients&#39;</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;gradient&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gradients</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;For `get_grad`, the `x` should be an integer or a Parameter, but got </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">hash_id</span> <span class="o">=</span> <span class="n">x</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">):</span>
            <span class="n">hash_id</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span>
        <span class="n">output</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">def</span> <span class="nf">_get_grad</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">identifier</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span> <span class="ow">or</span> <span class="n">identifier</span> <span class="o">!=</span> <span class="n">grads</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                    <span class="k">for</span> <span class="n">gradient</span> <span class="ow">in</span> <span class="n">grads</span><span class="p">:</span>
                        <span class="n">_get_grad</span><span class="p">(</span><span class="n">gradient</span><span class="p">,</span> <span class="n">identifier</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">nonlocal</span> <span class="n">output</span>
                    <span class="n">output</span> <span class="o">=</span> <span class="n">grads</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
                    <span class="k">return</span>

        <span class="n">_get_grad</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">hash_id</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">output</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Can not find the gradient for position or Parameter </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>


<span class="k">class</span> <span class="nc">IsParameter</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Check if input is `Parameter`</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize IsParameter&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="p">[],</span>
                <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">,</span>
                <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">RefType</span><span class="p">)}</span>


<span class="k">class</span> <span class="nc">SiLU</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes SiLU (Sigmoid Linear Unit activation function) of input tensors element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.silu` for more details.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-1, 2, -3, 2, -1]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.SiLU(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-0.269  1.762  -0.1423  1.762  -0.269]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SiLU&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">TileSize</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Tile size for matmul</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize TileSize&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">,</span> <span class="s1">&#39;out_shape&#39;</span><span class="p">,</span> <span class="s1">&#39;ndim&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">out_shape</span><span class="p">,</span> <span class="n">ndim</span><span class="p">):</span>
        <span class="n">size</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">ndim</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">out_shape</span><span class="p">)):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">j</span><span class="p">:</span>
                <span class="n">size</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">j</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">GetitemTensorIndexInfo</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get getitem tensor index info</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_ascend</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize GetitemTensorIndexInfo&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="s1">&#39;index&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;new_index&quot;</span><span class="p">,</span> <span class="s2">&quot;tensor_update_types&quot;</span><span class="p">,</span> <span class="s2">&quot;tensor_update_args&quot;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;is_ascend&#39;</span><span class="p">,</span> <span class="n">is_ascend</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_ascend</span> <span class="o">=</span> <span class="n">is_ascend</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">Tensor_</span><span class="o">.</span><span class="n">getitem_index_info</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_ascend</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">SetitemTensorIndexInfo</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get setitem tensor index info</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_ascend</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize GetitemTensorIndexInfo&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span>
            <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="s1">&#39;index&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;new_index&#39;</span><span class="p">,</span>
                                                        <span class="s1">&#39;v_transfer_types&#39;</span><span class="p">,</span>
                                                        <span class="s1">&#39;v_transfer_args&#39;</span><span class="p">,</span>
                                                        <span class="s1">&#39;tensor_update_types&#39;</span><span class="p">,</span>
                                                        <span class="s1">&#39;tensor_update_args&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;is_ascend&#39;</span><span class="p">,</span> <span class="n">is_ascend</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_ascend</span> <span class="o">=</span> <span class="n">is_ascend</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">Tensor_</span><span class="o">.</span><span class="n">setitem_index_info</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_ascend</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">IsConstant</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Check if the input is constant</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize IsConstant&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="kc">True</span>


<span class="k">class</span> <span class="nc">SelectView</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Select tensor of view</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_tensor&#39;</span><span class="p">,</span> <span class="s1">&#39;input_indices&#39;</span><span class="p">,</span> <span class="s1">&#39;axis&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">CopyWithSlice</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Copy data to discontinuous tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">MoeFFN</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The MoeFFN computation is similar to Feed-Forward Network, it contains matmul + gelu + matmul.</span>

<span class="sd">    Args:</span>
<span class="sd">        activation (string): The activation type, set to &#39;fastgelu&#39; or &#39;gelu&#39;.</span>
<span class="sd">        Only support &#39;fastgelu&#39; for now. Default: &quot;fastgelu&quot;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor with data type of int8, float16.</span>
<span class="sd">          Input tensor of shape :math:`(batch\_size * seq\_length, hidden\_size)`.</span>
<span class="sd">        - **expert_tokens** (Tensor]) - The expert tokens tensor with data type of int64.</span>
<span class="sd">          Expert tokens tensor of shape :math:`(16,)`. For example, `(2, 1, 0, .., 9)`</span>
<span class="sd">          indicate that the 0th expert deals with 2 tokens, the 1th expert deals with 1 tokens,</span>
<span class="sd">          the 2th expert do noting and so on.</span>
<span class="sd">        - **weight1** (Tensor) - The weight1 tensor with data type of float16.</span>
<span class="sd">          Weight1 tensor of shape :math:`(expert\_num, hidden\_size, ffn\_hidden\_size)`.</span>
<span class="sd">        - **bias1** (Tensor) - The bias1 tensor with data type of float16.</span>
<span class="sd">          Bias1 tensor of shape :math:`(expert\_num, ffn\_hidden\_size)`.</span>
<span class="sd">        - **weight2** (Tensor) - The weight2 tensor with data type of float16.</span>
<span class="sd">          Weight2 tensor of shape :math:`(expert\_num, ffn\_hidden\_size, hidden\_size)`.</span>
<span class="sd">        - **bias2** (Tensor) - The bias2 tensor with data type of float16.</span>
<span class="sd">          Bias2 tensor of shape :math:`(expert\_num, hidden\_size)`.</span>
<span class="sd">        - **scale** (Tensor) - The scale tensor with data type of float16. Not enable now.</span>
<span class="sd">        - **offset** (Tensor) - The offset tensor with data type of float16. Not enable now.</span>
<span class="sd">        - **deq_scale1** (Tensor) - The deq_scale1 tensor with data type of float16. Not enable now.</span>
<span class="sd">        - **deq_scale2** (Tensor) - The deq_scale2 tensor with data type of float16. Not enable now.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor of shape :math:`(batch\_size * seq\_length, hidden\_size)`. With data type of float16.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops.operations import _inner_ops</span>
<span class="sd">        &gt;&gt;&gt; b = 4</span>
<span class="sd">        &gt;&gt;&gt; s = 128</span>
<span class="sd">        &gt;&gt;&gt; h = 1024</span>
<span class="sd">        &gt;&gt;&gt; h_f = 4 * h</span>
<span class="sd">        &gt;&gt;&gt; e = 16</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.randn(b * s, h).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; expert_tokens = Tensor(np.random.randn(e).astype(np.int64))</span>
<span class="sd">        &gt;&gt;&gt; w1 = Tensor(np.random.randn(e, h, h_f).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; bias1 = Tensor(np.random.randn(e, h_f).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; w2 = Tensor(np.random.randn(e, h_f, h).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; bias2 = Tensor(np.random.randn(e, h).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; moe_ffn = _inner_ops.MoeFFN(&quot;fastgelu&quot;)</span>
<span class="sd">        &gt;&gt;&gt; output = moe_ffn(x, w1, bias1, w2, bias2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">activation</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize MoeFFN.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="s2">&quot;expert_tokens&quot;</span><span class="p">,</span> <span class="s2">&quot;weight1&quot;</span><span class="p">,</span> <span class="s2">&quot;bias1&quot;</span><span class="p">,</span>
                                        <span class="s2">&quot;weight2&quot;</span><span class="p">,</span> <span class="s2">&quot;bias2&quot;</span><span class="p">,</span> <span class="s2">&quot;scale&quot;</span><span class="p">,</span> <span class="s2">&quot;offset&quot;</span><span class="p">,</span> <span class="s2">&quot;deq_scale1&quot;</span>
                                        <span class="s2">&quot;deq_scale2&quot;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">activation</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 MindSpore.</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用了 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a>开发.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>