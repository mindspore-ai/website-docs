

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>分布式并行 &mdash; MindSpore master 文档</title>
  

  
  <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/training.css" type="text/css" />
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script src="../_static/js/training.js"></script>
        <script src="../_static/translations.js"></script>
        
        
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="推理" href="inference.html" />
    <link rel="prev" title="精度调优" href="precision_tuning.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">设计</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../design/overview.html">MindSpore设计概览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/programming_paradigm.html">编程范式</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/auto_gradient.html">函数式微分编程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/mindir.html">中间表示MindIR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/all_scenarios.html">全场景统一</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/dynamic_graph_and_static_graph.html">动静态图结合</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/pluggable_device.html">三方硬件对接</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/distributed_training_design.html">分布式并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/graph_fusion_engine.html">图算融合加速引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/data_engine.html">高性能数据处理引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/glossary.html">术语</a></li>
</ul>
<p class="caption"><span class="caption-text">规格</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/benchmark.html">基准性能</a></li>
<li class="toctree-l1"><a class="reference external" href="https://gitee.com/mindspore/models/blob/master/README_CN.md#目录">网络支持↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/operator_list.html">API支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/syntax_list.html">语法支持</a></li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/master/api_cpp/mindspore.html">C++ API↗</a></li>
</ul>
<p class="caption"><span class="caption-text">API映射</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/pytorch_api_mapping.html">PyTorch与MindSpore API映射表</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/tensorflow_api_mapping.html">TensorFlow与MindSpore API映射表</a></li>
</ul>
<p class="caption"><span class="caption-text">迁移指南</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/overview.html">迁移指南概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/enveriment_preparation.html">环境准备与资料获取</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/analysis_and_preparation.html">模型分析与准备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/model_development/model_development.html">MindSpore网络搭建</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/debug_and_tune.html">调试调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/sample_code.html">网络迁移调试实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/faq.html">常见问题</a></li>
</ul>
<p class="caption"><span class="caption-text">FAQ</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_processing.html">数据处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="implement_problem.html">执行问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="network_compilation.html">网络编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="operators_compile.html">算子编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="usage_migrate_3rd.html">第三方框架迁移使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance_tuning.html">性能调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="precision_tuning.html">精度调优</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">分布式并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="inference.html">推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="feature_advice.html">特性咨询</a></li>
</ul>
<p class="caption"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>分布式并行</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/faq/distributed_parallel.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="分布式并行">
<h1>分布式并行<a class="headerlink" href="#分布式并行" title="永久链接至标题">¶</a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/master/docs/mindspore/source_zh_cn/faq/distributed_parallel.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source.png"></a></p>
<p><font size=3><strong>Q: 进行HCCL分布式训练出错：<code class="docutils literal notranslate"><span class="pre">Init</span> <span class="pre">plugin</span> <span class="pre">so</span> <span class="pre">failed,</span> <span class="pre">ret</span> <span class="pre">=</span> <span class="pre">1343225860</span></code>？</strong></font></p>
<p>A: 在Ascend进行分布式训练时初始化HCCL失败了，通常由于<code class="docutils literal notranslate"><span class="pre">rank_table.json</span></code>没写对，可以执行此文件<a class="reference external" href="https://gitee.com/mindspore/models/blob/master/utils/hccl_tools/hccl_tools.py">hccl_tools.py</a>生成一个新的<code class="docutils literal notranslate"><span class="pre">rank_table.json</span></code>。或者导入环境变量<code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">ASCEND_SLOG_PRINT_TO_STDOUT=1</span></code>打开HCCL的日志打印，根据日志中的ERROR信息来排查问题。</p>
<br/>
<p><font size=3><strong>Q: MindSpore执行GPU分布式训练报错如下，如何解决:</strong></font></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Loading libgpu_collective.so failed. Many reasons could cause this:
1.libgpu_collective.so is not installed.
2.nccl is not installed or found.
3.mpi is not installed or found
</pre></div>
</div>
<p>A: 此问题为MindSpore动态加载集合通信库失败，可能原因如下:</p>
<ul class="simple">
<li><p>执行环境未安装分布式训练依赖的OpenMPI以及NCCL。</p></li>
<li><p>NCCL版本未更新至<code class="docutils literal notranslate"><span class="pre">v2.7.6</span></code>: MindSpore <code class="docutils literal notranslate"><span class="pre">v1.1.0</span></code>新增GPU P2P通信算子，该特性依赖于NCCL <code class="docutils literal notranslate"><span class="pre">v2.7.6</span></code>，若环境使用的NCCL未升级为此版本，则会引起加载失败错误。</p></li>
</ul>
<br/>
<p><font size=3><strong>Q：GPU分布式训练场景下，若错误设置环境变量CUDA_VISIBLE_DEVICES的个数小于执行的进程数时，可能导致进程阻塞问题。</strong></font></p>
<p>A：此场景下，部分训练进程会提示如下报错：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[ERROR] DEVICE [mindspore/ccsrc/runtime/device/gpu/cuda_driver.cc:245] SetDevice] SetDevice for id:7 failed, ret[101], invalid device ordinal. Please make sure that the &#39;device_id&#39; set in context is in the range:[0, total number of GPU). If the environment variable &#39;CUDA_VISIBLE_DEVICES&#39; is set, the total number of GPU will be the number set in the environment variable &#39;CUDA_VISIBLE_DEVICES&#39;. For example, if export CUDA_VISIBLE_DEVICES=4,5,6, the &#39;device_id&#39; can be 0,1,2 at the moment, &#39;device_id&#39; starts from 0, and &#39;device_id&#39;=0 means using GPU of number 4.
[ERROR] DEVICE [mindspore/ccsrc/runtime/device/gpu/gpu_device_manager.cc:27] InitDevice] Op Error: Failed to set current device id | Error Number: 0
</pre></div>
</div>
<p>其余进程由于GPU资源已分配成功，会正常执行到初始化<code class="docutils literal notranslate"><span class="pre">NCCL</span></code>步骤，日志如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[INFO] DEVICE [mindspore/ccsrc/runtime/hardware/gpu/gpu_device_context.cc:90] Initialize] Start initializing NCCL communicator for device 1
</pre></div>
</div>
<p>此步骤中会调用<code class="docutils literal notranslate"><span class="pre">NCCL</span></code>接口<code class="docutils literal notranslate"><span class="pre">ncclCommInitRank</span></code>，该接口会阻塞，直到所有进程达成一致。因此如果某进程没有调用<code class="docutils literal notranslate"><span class="pre">ncclCommInitRank</span></code>，则会导致进程阻塞。
此问题我们已向<code class="docutils literal notranslate"><span class="pre">NCCL</span></code>社区反馈，社区开发者正在设计解决方案中，目前最新版本还未修复，详见<a class="reference external" href="https://github.com/NVIDIA/nccl/issues/593#issuecomment-965939279">issue链接</a>。
解决方法：手动<code class="docutils literal notranslate"><span class="pre">kill</span></code>训练进程，根据报错日志，设置正确的卡号后，重启训练任务。</p>
<br/>
<p><font size=3><strong>Q：GPU分布式训练场景下，若某进程异常退出，可能导致其余进程阻塞问题。</strong></font></p>
<p>A：此场景下，异常进程由于各种问题退出，其余进程由于GPU资源已分配成功，会正常执行到初始化<code class="docutils literal notranslate"><span class="pre">NCCL</span></code>步骤，日志如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[INFO] DEVICE [mindspore/ccsrc/runtime/hardware/gpu/gpu_device_context.cc:90] Initialize] Start initializing NCCL communicator for device 1
</pre></div>
</div>
<p>此步骤中会调用<code class="docutils literal notranslate"><span class="pre">NCCL</span></code>接口<code class="docutils literal notranslate"><span class="pre">ncclCommInitRank</span></code>，该接口会阻塞，直到所有进程达成一致。因此如果某进程没有调用<code class="docutils literal notranslate"><span class="pre">ncclCommInitRank</span></code>，则会导致进程阻塞。
此问题我们已向<code class="docutils literal notranslate"><span class="pre">NCCL</span></code>社区反馈，社区开发者正在设计解决方案中，目前最新版本还未修复，详见<a class="reference external" href="https://github.com/NVIDIA/nccl/issues/593#issuecomment-965939279">issue链接</a>。
解决方法：手动<code class="docutils literal notranslate"><span class="pre">kill</span></code>训练进程后重启训练任务。</p>
<br/>
<p><font size=3><strong>Q：在执行GPU单机单卡的脚本时，不使用mpirun启动进程时，调用mindspore.communication.init方法可能会报错,导致执行失败，该如何处理？</strong></font></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[CRITICAL] DISTRIBUTED [mindspore/ccsrc/distributed/cluster/cluster_context.cc:130] InitNodeRole] Role name is invalid...
</pre></div>
</div>
<p>A：在用户不使用<code class="docutils literal notranslate"><span class="pre">mpirun</span></code>启动进程，但是依然调用了<code class="docutils literal notranslate"><span class="pre">init()</span></code>方法的情况下，MindSpore要求用户按照<a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/master/parallel/train_gpu.html#%E4%B8%8D%E4%BE%9D%E8%B5%96openmpi%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83">不依赖OpenMPI进行训练</a>配置若干环境变量并进行校验，若没有配置，MindSpore会给出以上报错提示。因此建议只有在执行分布式训练时调用<code class="docutils literal notranslate"><span class="pre">mindspore.communication.init</span></code>，并在不使用<code class="docutils literal notranslate"><span class="pre">mpirun</span></code>的场景下，根据文档配置正确的环境变量以启动分布式训练。</p>
<br/>
<p><font size=3><strong>Q：在通过OpenMPI执行多机多卡训练时，提示由于MPI_Allgather失败。</strong></font></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>pml_ucx.c:175 Error: Failed to receive UCX worker address: Not found (-13)
pml_ucx.c:452 Error: Failed to resolve UCX endpoint for rank X
</pre></div>
</div>
<p>A：此问题是<code class="docutils literal notranslate"><span class="pre">OpenMPI</span></code>在Host侧通信时，无法和对端地址进行通信，一般是机器之间的网卡配置不同导致的，可以通过手动设置网卡名或者子网的方式解决：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mpirun -n process_num --mca btl tcp --mca btl_tcp_if_include eth0 ./run.sh
</pre></div>
</div>
<p>以上指令启动了<code class="docutils literal notranslate"><span class="pre">process_num</span></code>个<code class="docutils literal notranslate"><span class="pre">run.sh</span></code>进程，并且选择Host侧通信方式为<code class="docutils literal notranslate"><span class="pre">tcp</span></code>，网卡选择了<code class="docutils literal notranslate"><span class="pre">eth0</span></code>，这样就能保证在每台机器上使用的网卡相同，进而解决通信异常问题。</p>
<p>还可以选择子网来进行匹配：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mpirun -n process_num --mca btl tcp --mca btl_tcp_if_include 192.168.1.0/24 ./run.sh
</pre></div>
</div>
<p>子网范围需要包括所有机器所用的IP地址。</p>
<br/>
<p><font size=3><strong>Q：在通过OpenMPI执行分布式训练时，单机多卡训练正常，但在多机多卡训练时，某些机器提示GPU device id设置失败。</strong></font></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[ERROR] DEVICE [mindspore/ccsrc/runtime/device/gpu/cuda_driver.cc:245] SetDevice] SetDevice for id:7 failed, ret[101], invalid device ordinal. Please make sure that the &#39;device_id&#39; set in context is in the range:[0, total number of GPU). If the environment variable &#39;CUDA_VISIBLE_DEVICES&#39; is set, the total number of GPU will be the number set in the environment variable &#39;CUDA_VISIBLE_DEVICES&#39;. For example, if export CUDA_VISIBLE_DEVICES=4,5,6, the &#39;device_id&#39; can be 0,1,2 at the moment, &#39;device_id&#39; starts from 0, and &#39;device_id&#39;=0 means using GPU of number 4.
[ERROR] DEVICE [mindspore/ccsrc/runtime/device/gpu/gpu_device_manager.cc:27] InitDevice] Op Error: Failed to set current device id | Error Number: 0
</pre></div>
</div>
<p>A：在多机场景下，各进程卡号需要通过在Host侧<code class="docutils literal notranslate"><span class="pre">AllGather</span></code> <code class="docutils literal notranslate"><span class="pre">HOSTNAME</span></code>后计算得到，如果机器间有使用相同的<code class="docutils literal notranslate"><span class="pre">HOSTNAME</span></code>，则进程卡号会计算出错，导致卡号越界而设置失败。可以在执行脚本中设置每台机器的HOSTNAME为各自的IP地址来解决：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>export HOSTNAME=node_ip_address
</pre></div>
</div>
<br/>
<p><font size=3><strong>Q：在通过OpenMPI执行多机多卡训练时，NCCL报错提示网络不通。</strong></font></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>include/socket.h:403 NCCL WARN Connect to XXX failed: Network is unreachable
</pre></div>
</div>
<p>A：此问题是<code class="docutils literal notranslate"><span class="pre">NCCL</span></code>在Host侧同步进程信息或者初始化通信域时，无法和对端地址进行通信，一般是机器之间的网卡配置不同导致的，可以通过设置<code class="docutils literal notranslate"><span class="pre">NCCL</span></code>环境变量<code class="docutils literal notranslate"><span class="pre">NCCL_SOCKET_IFNAME</span></code>，进行网卡选择：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>export NCCL_SOCKET_IFNAME=eth
</pre></div>
</div>
<p>以上指令设置了<code class="docutils literal notranslate"><span class="pre">NCCL</span></code>在Host侧选择网卡名中带有<code class="docutils literal notranslate"><span class="pre">eth</span></code>的网卡进行通信。</p>
<br/>
<p><font size=3><strong>Q：多机多卡选择特定名称的RDMA网卡(通过NCCL_SOCKET_IFNAME设置)通信后，训练仍然报错：</strong></font></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>misc/ibvwrap.cc:284 NCCL WARN Call to ibv_modify_qp failed with error Invalid argument
...
include/socket.h:403 NCCL WARN Connect to XXX failed: Connection refused
</pre></div>
</div>
<p>A：一般此问题是多机之间RDMA网卡配置存在差异，需要具体情况具体分析。但常见原因是存在某些主机网卡存在IB协议和RoCE协议同时存在的情况，可能出现连接建立失败的情况。解决方案：</p>
<p>需要使用以下指令指定使用的RDMA网卡名为ib开头：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>export NCCL_IB_HCA=mlx
</pre></div>
</div>
<br/>
<p><font size=3><strong>Q：单机多卡训练能够成功，但是扩展脚本到多机多卡后，其他主机提示各类报错：</strong></font></p>
<p>报错内容有多种，下面是几种典型的报错，可能有：
1.已经安装的whl包找不到。
2.IB网卡通信失败。
3.Cuda库加载失败。</p>
<p>A：这些问题，都是由于在<code class="docutils literal notranslate"><span class="pre">mpirun</span></code>启动其他主机时，其他主机的环境变量(包括NCCL的网卡选择配置)没有与本机同步，导致了单机多卡正常执行而多机多卡失败的现象。解决方法是通过mpirun的-x选项，导出特定的环境变量：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mpirun --hostfile /path/to/hostfile -n 64 -x PYTHONPATH -x GLOG_v -x LD_LIBRARY_PATH -x NCCL_SOCKET_IFNAME -x NCCL_IB_HCA -x NCCL_DEBUG=INFO python train.py
</pre></div>
</div>
<p>以上指令导出了在本机已经设置的一些环境变量到其他主机，保证了在执行训练脚本前所有主机环境变量保持一致，达到多机多卡训练目标。</p>
<br/>
<p><font size=3><strong>Q: 在Ascend上通过OpenMPI执行分布式训练时，<code class="docutils literal notranslate"><span class="pre">HcclCommInitRootInfo</span></code>报错:</strong></font></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Ascend collective Error: &quot;HcclCommInitRootInfo failed. | Error Number 2
</pre></div>
</div>
<p>A: OpenMPI启动时，当前版本的hccl下，创建通信域时，相应的卡需要分配大约300M的device内存，因此每张卡所在的通信域的数量越多，则额外需要的内存越多，因此会有内存不足的问题。
可以设置<code class="docutils literal notranslate"><span class="pre">context</span></code>中的<code class="docutils literal notranslate"><span class="pre">variable_memory_max_size</span></code>来减少Ascend进程可用的内存，从而为hccl预留足够的内存创建通信域。</p>
<p><font size=3><strong>Q: 在自动并行下执行分布式网络时，报张量无法被当前策略完整切分的错误如下，该怎么解决？:</strong></font></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>np_tensor can not be split by strategy!
</pre></div>
</div>
<p>A: 该报错表明网络中有对参数配置了切分策略，但是参数的某个维度无法被切分策略整除。可能的问题有两个：1、该参数作为某个算子的输入，脚本中调用了shard接口对该算子设置了非法策略；2、在<code class="docutils literal notranslate"><span class="pre">auto_parallel_context</span></code>中设置了<code class="docutils literal notranslate"><span class="pre">dataset_strategy</span></code>=”data_parallel”或<code class="docutils literal notranslate"><span class="pre">full_batch</span></code>=False时，框架会自动为网络输入设置数据并行策略，如果网络输入含有参数且其形状恰好不能被数据并行策略整除，也会报该错误。目前自动并行下仅支持网络输入为Tensor，需要对脚本进行调整。</p>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="inference.html" class="btn btn-neutral float-right" title="推理" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="precision_tuning.html" class="btn btn-neutral float-left" title="精度调优" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; 版权所有 2022, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>