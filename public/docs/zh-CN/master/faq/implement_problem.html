<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>执行问题 &mdash; MindSpore master 文档</title>
      <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/training.css" type="text/css" /><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/js/training.js"></script>
        <script src="../_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="网络编译" href="network_compilation.html" />
    <link rel="prev" title="数据处理" href="data_processing.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">设计</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../design/overview.html">MindSpore设计概览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/programming_paradigm.html">函数式和对象式融合编程范式</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/all_scenarios.html">全场景统一架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/dynamic_graph_and_static_graph.html">动静态图结合</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/pluggable_device.html">三方硬件对接</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/distributed_training_design.html">原生分布式并行架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/graph_fusion_engine.html">图算融合加速引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/data_engine.html">高性能数据处理引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/glossary.html">术语</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">模型库</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/official_models.html">官方模型库</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.experimental.html">mindspore.experimental</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API映射</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/pytorch_api_mapping.html">PyTorch与MindSpore API映射表</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/tensorflow_api_mapping.html">TensorFlow与MindSpore API映射表</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">迁移指南</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/overview.html">迁移指南概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/enveriment_preparation.html">环境准备与资料获取</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/analysis_and_preparation.html">模型分析与准备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/typical_api_comparision.html">PyTorch对比</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/model_development/model_development.html">MindSpore网络搭建</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/debug_and_tune.html">调试调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/sample_code.html">网络迁移调试实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/migrator_with_tools.html">网络迁移工具应用实践指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/faq.html">常见问题</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">语法支持</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax_support.html">静态图语法支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax/operators.html">静态图语法——运算符</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax/statements.html">静态图语法——Python语句</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax/python_builtin_functions.html">静态图语法——Python内置函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/index_support.html">Tensor索引支持</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">环境变量</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/env_var_list.html">环境变量</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_processing.html">数据处理</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">执行问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="network_compilation.html">网络编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="operators_compile.html">算子编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="usage_migrate_3rd.html">第三方框架迁移使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance_tuning.html">性能调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="precision_tuning.html">精度调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_parallel.html">分布式并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="inference.html">推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="feature_advice.html">特性咨询</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>执行问题</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/faq/implement_problem.md.txt" rel="nofollow"> 查看页面源码</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="执行问题">
<h1>执行问题<a class="headerlink" href="#执行问题" title="永久链接至标题"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/docs/mindspore/source_zh_cn/faq/implement_problem.md"><img alt="查看源文件" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source.png" /></a></p>
<p><font size=3><strong>Q: 请问使用MindSpore如何实现多尺度训练？</strong></font></p>
<p>A: 在多尺度训练过程中，使用不同<code class="docutils literal notranslate"><span class="pre">shape</span></code>调用<code class="docutils literal notranslate"><span class="pre">Cell</span></code>对象的时候，会自动根据不同<code class="docutils literal notranslate"><span class="pre">shape</span></code>编译并调用不同的图，从而实现多尺度的训练。要注意多尺度训练只支持非数据下沉模式，不能支持数据下沉的训练方式。可以参考<a class="reference external" href="https://gitee.com/mindspore/models/tree/master/official/cv/YOLOv3">yolov3</a>的多尺度训练实现。</p>
<br/>
<p><font size=3><strong>Q: 如果MindSpore的<code class="docutils literal notranslate"><span class="pre">requires_grad=False</span></code>的<code class="docutils literal notranslate"><span class="pre">tensor</span></code>转化为<code class="docutils literal notranslate"><span class="pre">numpy</span></code>类型进行处理然后再转化会<code class="docutils literal notranslate"><span class="pre">tensor</span></code>，会对计算图和反向传播有影响吗？</strong></font></p>
<p>A: 在PyNative模式下，如果中间使用<code class="docutils literal notranslate"><span class="pre">numpy</span></code>计算，会导致梯度传递中断，<code class="docutils literal notranslate"><span class="pre">requires_grad=False</span></code>的场景下，如果该<code class="docutils literal notranslate"><span class="pre">tensor</span></code>的反向传播不传给其他参数使用，是没有影响的；如果<code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code>的场景下，是有影响的。</p>
<br/>
<p><font size=3><strong>Q: 请问怎样实现类似<code class="docutils literal notranslate"><span class="pre">torch.nn.functional.linear()</span></code>那样能够对全连接层<code class="docutils literal notranslate"><span class="pre">weight</span></code>、<code class="docutils literal notranslate"><span class="pre">bias</span></code>进行修改，应该如何操作？</strong></font></p>
<p>A: MindSpore与<code class="docutils literal notranslate"><span class="pre">torch.nn.functional.linear()</span></code>功能最接近的接口就是<code class="docutils literal notranslate"><span class="pre">nn.Dense</span></code>了。<code class="docutils literal notranslate"><span class="pre">nn.Dense</span></code>能指定<code class="docutils literal notranslate"><span class="pre">weight</span></code>和<code class="docutils literal notranslate"><span class="pre">bias</span></code>的初始值，后续的变化是由优化器自动更新的。训练过程中，用户不需要主动修改这两个参数的值。</p>
<br/>
<p><font size=3><strong>Q: 使用MindSpore在模型保存后生成的<code class="docutils literal notranslate"><span class="pre">.meta</span></code>文件作用是什么，可以用<code class="docutils literal notranslate"><span class="pre">.meta</span></code>文件导入图结构吗？</strong></font></p>
<p>A: 这里的<code class="docutils literal notranslate"><span class="pre">.meta</span></code>文件是编译好的图结构，但是目前并不支持直接导入这种结构。如果不知道图结构的情况下想要导入网络，还是需要用MindIR格式的文件。</p>
<br/>
<p><font size=3><strong>Q: 请问<code class="docutils literal notranslate"><span class="pre">yolov4-tiny-3l.weights</span></code>模型文件可以直接转换成MindSpore模型吗？</strong></font></p>
<p>A: 不能的，需要把其他框架训练好的参数转换成MindSpore的格式，才能转成MindSpore的模型。</p>
<br/>
<p><font size=3><strong>Q: 使用MindSpore进行<code class="docutils literal notranslate"><span class="pre">model.train</span></code>的时候进行了如下设置，为什么会报错呢？</strong></font></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">LossMonitor</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">LossMonitor</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>A: 因为在已经设置为下沉模式的情况下，就不能再设置为非下沉了，是运行机制上的限制。</p>
<br/>
<p><font size=3><strong>Q: 使用MindSpore训练模型在<code class="docutils literal notranslate"><span class="pre">eval</span></code>阶段，需要注意什么？能够直接加载网络和参数吗？需要在Model中使用优化器吗？</strong></font></p>
<p>A: 在<code class="docutils literal notranslate"><span class="pre">eval</span></code>阶段主要看需要什么，比如图像分类任务<code class="docutils literal notranslate"><span class="pre">eval</span></code>网络的输出是各个类的概率值，与对应标签计算<code class="docutils literal notranslate"><span class="pre">acc</span></code>。
大多数情况是可以直接复用训练的网络和参数的，需要注意的是需要设置推理模式。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">net</span><span class="o">.</span><span class="n">set_train</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>在eval阶段不需要优化器，但是需要使用MindSpore的<code class="docutils literal notranslate"><span class="pre">model.eval</span></code>接口的话需要配置一下<code class="docutils literal notranslate"><span class="pre">loss</span> <span class="pre">function</span></code>，如：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 定义模型</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;top_1_accuracy&#39;</span><span class="p">,</span> <span class="s1">&#39;top_5_accuracy&#39;</span><span class="p">})</span>
<span class="c1"># 评估模型</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<br/>
<p><font size=3><strong>Q: 如何使用SGD里的<code class="docutils literal notranslate"><span class="pre">param_group</span></code>来实现学习率的衰减？</strong></font></p>
<p>A: 如果需要按照<code class="docutils literal notranslate"><span class="pre">epoch</span></code>来变化，可以使用<a class="reference external" href="https://mindspore.cn/docs/zh-CN/master/api_python/mindspore.nn.html#dynamic-lr%E5%87%BD%E6%95%B0">Dynamic LR</a>,把其中的<code class="docutils literal notranslate"><span class="pre">step_per_epoch</span></code>设置成<code class="docutils literal notranslate"><span class="pre">step_size</span></code>，如果需要按照<code class="docutils literal notranslate"><span class="pre">step</span></code>来变化，可以把其中的<code class="docutils literal notranslate"><span class="pre">step_per_epoch</span></code>设置成1，也可以用<a class="reference external" href="https://mindspore.cn/docs/zh-CN/master/api_python/mindspore.nn.html#learningrateschedule%E7%B1%BB">LearningRateSchedule</a>。</p>
<br/>
<p><font size=3><strong>Q: MindSpore如何进行参数（如dropout值）修改？</strong></font></p>
<p>A: 在构造网络的时候可以通过 <code class="docutils literal notranslate"><span class="pre">if</span> <span class="pre">self.training:</span> <span class="pre">x</span> <span class="pre">=</span> <span class="pre">dropput(x)</span></code>，推理时，执行前设置<code class="docutils literal notranslate"><span class="pre">network.set_train(False)</span></code>，就可以不使用dropout，训练时设置为True就可以使用dropout。</p>
<br/>
<p><font size=3><strong>Q: 如何查看模型参数量？</strong></font></p>
<p>A: 可以直接加载CheckPoint统计，可能额外统计了动量和optimizer中的变量，需要过滤下相关变量。
您可以参考如下接口统计网络参数量:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">count_params</span><span class="p">(</span><span class="n">net</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Count number of parameters in the network</span>
<span class="sd">    Args:</span>
<span class="sd">        net (mindspore.nn.Cell): Mindspore network instance</span>
<span class="sd">    Returns:</span>
<span class="sd">        total_params (int): Total number of trainable params</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">total_params</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">():</span>
        <span class="n">total_params</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">total_params</span>
</pre></div>
</div>
<p>具体<a class="reference external" href="https://gitee.com/mindspore/models/blob/master/research/cv/tinynet/src/utils.py">脚本链接</a>。</p>
<br/>
<p><font size=3><strong>Q: 如何在训练过程中监控<code class="docutils literal notranslate"><span class="pre">loss</span></code>在最低的时候并保存训练参数？</strong></font></p>
<p>A: 可以自定义一个<code class="docutils literal notranslate"><span class="pre">Callback</span></code>。参考<code class="docutils literal notranslate"><span class="pre">ModelCheckpoint</span></code>的写法，此外再增加判断<code class="docutils literal notranslate"><span class="pre">loss</span></code>的逻辑:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">EarlyStop</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">control_loss</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EarlyStop</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_control_loss</span> <span class="o">=</span> <span class="n">control_loss</span>

    <span class="k">def</span> <span class="nf">step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">run_context</span><span class="p">):</span>
        <span class="n">cb_params</span> <span class="o">=</span> <span class="n">run_context</span><span class="o">.</span><span class="n">original_args</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">cb_params</span><span class="o">.</span><span class="n">net_outputs</span>
        <span class="k">if</span> <span class="n">loss</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_control_loss</span><span class="p">:</span>
            <span class="c1"># Stop training</span>
            <span class="n">run_context</span><span class="o">.</span><span class="n">_stop_requested</span> <span class="o">=</span> <span class="kc">True</span>

<span class="n">stop_cb</span> <span class="o">=</span> <span class="n">EarlyStop</span><span class="p">(</span><span class="n">control_loss</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">epoch_size</span><span class="p">,</span> <span class="n">ds_train</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">stop_cb</span><span class="p">])</span>
</pre></div>
</div>
<br/>
<p><font size=3><strong>Q: 使用<code class="docutils literal notranslate"><span class="pre">nn.Conv2d</span></code>时，怎样获取期望大小的<code class="docutils literal notranslate"><span class="pre">feature</span> <span class="pre">map</span></code>？</strong></font></p>
<p>A: <code class="docutils literal notranslate"><span class="pre">Conv2d shape</span></code>推导方法可以<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/master/api_python/nn/mindspore.nn.Conv2d.html#mindspore.nn.Conv2d">参考这里</a>，<code class="docutils literal notranslate"><span class="pre">Conv2d</span></code>的<code class="docutils literal notranslate"><span class="pre">pad_mode</span></code>改成<code class="docutils literal notranslate"><span class="pre">same</span></code>，或者可以根据<code class="docutils literal notranslate"><span class="pre">Conv2d shape</span></code>推导公式自行计算<code class="docutils literal notranslate"><span class="pre">pad</span></code>，想要使得<code class="docutils literal notranslate"><span class="pre">shape</span></code>不变，一般pad为<code class="docutils literal notranslate"><span class="pre">(kernel_size-1)//2</span></code>。</p>
<br/>
<p><font size=3><strong>Q: 使用MindSpore可以自定义一个可以返回多个值的loss函数？</strong></font></p>
<p>A: 自定义<code class="docutils literal notranslate"><span class="pre">loss</span> <span class="pre">function</span></code>后还需自定义<code class="docutils literal notranslate"><span class="pre">TrainOneStepCell</span></code>，实现梯度计算时<code class="docutils literal notranslate"><span class="pre">sens</span></code>的个数和<code class="docutils literal notranslate"><span class="pre">network</span></code>的输出个数相同。具体可参考:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">MyLoss</span><span class="p">()</span>
<span class="n">loss_with_net</span> <span class="o">=</span> <span class="n">MyWithLossCell</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">)</span>
<span class="n">train_net</span> <span class="o">=</span> <span class="n">MyTrainOneStepCell</span><span class="p">(</span><span class="n">loss_with_net</span><span class="p">,</span> <span class="n">optim</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="o">=</span><span class="n">train_net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
<br/>
<p><font size=3><strong>Q: MindSpore如何实现早停功能？</strong></font></p>
<p>A：可以使用<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/master/api_python/train/mindspore.train.EarlyStopping.html">EarlyStopping 方法</a>。</p>
<br/>
<p><font size=3><strong>Q: 模型已经训练好，如何将模型的输出结果保存为文本或者<code class="docutils literal notranslate"><span class="pre">npy</span></code>的格式？</strong></font></p>
<p>A: 您好，我们网络的输出为<code class="docutils literal notranslate"><span class="pre">Tensor</span></code>，需要使用<code class="docutils literal notranslate"><span class="pre">asnumpy()</span></code>方法将<code class="docutils literal notranslate"><span class="pre">Tensor</span></code>转换为<code class="docutils literal notranslate"><span class="pre">numpy</span></code>，再进行下一步保存。具体可参考:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">out</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;output.npy&quot;</span><span class="p">,</span> <span class="n">out</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
</pre></div>
</div>
<br/>
<p><font size=3><strong>Q: 缓存服务器异常关闭如何处理？</strong></font></p>
<p>A: 缓存服务器使用过程中，会进行IPC共享内存和socket文件等系统资源的分配。若允许溢出，在磁盘空间还会存在溢出的数据文件。一般情况下，如果通过<code class="docutils literal notranslate"><span class="pre">cache_admin</span> <span class="pre">--stop</span></code>命令正常关闭服务器，这些资源将会被自动清理。</p>
<p>但如果缓存服务器被异常关闭，例如缓存服务进程被杀等，用户需要首先尝试重新启动服务器，若启动失败，则应该依照以下步骤手动清理系统资源:</p>
<ul>
<li><p>删除IPC资源。</p>
<ol class="arabic">
<li><p>检查是否有IPC共享内存残留。</p>
<p>一般情况下，系统会为缓存服务分配4GB的共享内存。通过以下命令可以查看系统中的共享内存块使用情况。</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ ipcs -m
------ Shared Memory Segments --------
key        shmid      owner      perms      bytes      nattch     status
0x61020024 15532037   root       666        4294967296 1
</pre></div>
</div>
<p>其中，<code class="docutils literal notranslate"><span class="pre">shmid</span></code>为共享内存块id，<code class="docutils literal notranslate"><span class="pre">bytes</span></code>为共享内存块的大小，<code class="docutils literal notranslate"><span class="pre">nattch</span></code>为链接到该共享内存块的进程数量。<code class="docutils literal notranslate"><span class="pre">nattch</span></code>不为0表示仍有进程使用该共享内存块。在删除共享内存前，需要停止使用该内存块的所有进程。</p>
</li>
<li><p>删除IPC共享内存。</p>
<p>找到对应的共享内存id，并通过以下命令删除。</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>ipcrm -m {shmid}
</pre></div>
</div>
</li>
</ol>
</li>
<li><p>删除socket文件。</p>
<p>一般情况下，socket文件位于<code class="docutils literal notranslate"><span class="pre">/tmp/mindspore/cache</span></code>。进入文件夹，执行以下命令删除socket文件。</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>rm cache_server_p{port_number}
</pre></div>
</div>
<p>其中<code class="docutils literal notranslate"><span class="pre">port_number</span></code>为用户创建缓存服务器时指定的端口号，默认为50052。</p>
</li>
<li><p>删除溢出到磁盘空间的数据文件。</p>
<p>进入启用缓存服务器时指定的溢出数据路径。通常，默认溢出路径为<code class="docutils literal notranslate"><span class="pre">/tmp/mindspore/cache</span></code>。找到路径下对应的数据文件夹并逐一删除。</p>
</li>
</ul>
<br/>
<p><font size=3><strong>Q: 通过Hub可以使用GPU加载<code class="docutils literal notranslate"><span class="pre">vgg16</span></code>模型以及是否可以做迁移模型吗？</strong></font></p>
<p>A: 请手动修改如下两处参数即可:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 增加**kwargs参数: 如下</span>
<span class="k">def</span> <span class="nf">vgg16</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">phase</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 增加**kwargs参数: 如下</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Vgg</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;16&#39;</span><span class="p">],</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span> <span class="n">batch_norm</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_norm</span><span class="p">,</span> <span class="n">phase</span><span class="o">=</span><span class="n">phase</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>
</div>
<br/>
<p><font size=3><strong>Q: 如何得到VGG模型中间层特征？</strong></font></p>
<p>A: 你好，获取网络中间层的特征，其实跟具体框架没有太大关系了。<code class="docutils literal notranslate"><span class="pre">torchvison</span></code>里定义的<code class="docutils literal notranslate"><span class="pre">vgg</span></code>模型，可以通过<code class="docutils literal notranslate"><span class="pre">features</span></code>字段获取”中间层特征”，<code class="docutils literal notranslate"><span class="pre">torchvison</span></code>的<code class="docutils literal notranslate"><span class="pre">vgg</span></code>源码如下:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">VGG</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">init_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VGG</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">features</span> <span class="o">=</span> <span class="n">features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">((</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
</pre></div>
</div>
<p>在MindSpore的ModelZoo里定义的<code class="docutils literal notranslate"><span class="pre">vgg16</span></code>，可以通过<code class="docutils literal notranslate"><span class="pre">layers</span></code>字段获取，如下:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">network</span> <span class="o">=</span> <span class="n">vgg16</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span>
</pre></div>
</div>
<br/>
<p><font size=3><strong>Q: 使用MindSpore进行模型训练时，<code class="docutils literal notranslate"><span class="pre">CTCLoss</span></code>的输入参数有四个: <code class="docutils literal notranslate"><span class="pre">inputs</span></code>, <code class="docutils literal notranslate"><span class="pre">labels_indices</span></code>, <code class="docutils literal notranslate"><span class="pre">labels_values</span></code>, <code class="docutils literal notranslate"><span class="pre">sequence_length</span></code>，如何使用<code class="docutils literal notranslate"><span class="pre">CTCLoss</span></code>进行训练？</strong></font></p>
<p>A: 定义的<code class="docutils literal notranslate"><span class="pre">model.train</span></code>接口里接收的<code class="docutils literal notranslate"><span class="pre">dataset</span></code>可以是多个数据组成，形如(<code class="docutils literal notranslate"><span class="pre">data1</span></code>, <code class="docutils literal notranslate"><span class="pre">data2</span></code>, <code class="docutils literal notranslate"><span class="pre">data3</span></code>, …)，所以<code class="docutils literal notranslate"><span class="pre">dataset</span></code>是可以包含<code class="docutils literal notranslate"><span class="pre">inputs</span></code>,<code class="docutils literal notranslate"><span class="pre">labels_indices</span></code>,<code class="docutils literal notranslate"><span class="pre">labels_values</span></code>,<code class="docutils literal notranslate"><span class="pre">sequence_length</span></code>的信息的。只需要定义好相应形式的<code class="docutils literal notranslate"><span class="pre">dataset</span></code>，传入<code class="docutils literal notranslate"><span class="pre">model.train</span></code>里就可以。具体的可以了解下相应的<a class="reference external" href="https://www.mindspore.cn/tutorials/zh-CN/master/advanced/dataset.html">数据处理接口</a></p>
<br/>
<p><font size=3><strong>Q: MindSpore有哪些现成的推荐类或生成类网络或模型可用？</strong></font></p>
<p>A: 目前正在开发Wide &amp; Deep、DeepFM、NCF等推荐类模型，NLP领域已经支持Bert_NEZHA，正在开发MASS等模型，用户可根据场景需要改造为生成类网络，可以关注<a class="reference external" href="https://gitee.com/mindspore/models/blob/master/README_CN.md#">MindSpore ModelZoo</a>。</p>
<br/>
<p><font size=3><strong>Q: 如何使用MindSpore拟合<span class="math notranslate nohighlight">\(f(x)=a \times sin(x)+b\)</span>这类函数？</strong></font></p>
<p>A: 以下拟合案例是基于MindSpore线性拟合官方案例改编而成。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># The fitting function is: f(x)=2*sin(x)+3.</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dataset</span> <span class="k">as</span> <span class="n">ds</span>
<span class="kn">from</span> <span class="nn">mindspore.common.initializer</span> <span class="kn">import</span> <span class="n">Normal</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.train</span> <span class="kn">import</span> <span class="n">Model</span><span class="p">,</span> <span class="n">LossMonitor</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_data</span><span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="n">w</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">3.0</span><span class="p">):</span>
    <span class="c1"># f(x)=w * sin(x) + b</span>
    <span class="c1"># f(x)=2 * sin(x) +3</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="n">noise</span>
        <span class="k">yield</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">y</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">create_dataset</span><span class="p">(</span><span class="n">num_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">repeat_size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">GeneratorDataset</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">get_data</span><span class="p">(</span><span class="n">num_data</span><span class="p">)),</span> <span class="n">column_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">,</span><span class="s1">&#39;label&#39;</span><span class="p">])</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">repeat_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">input_data</span>

<span class="k">class</span> <span class="nc">LinearNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LinearNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Normal</span><span class="p">(</span><span class="mf">0.02</span><span class="p">),</span> <span class="n">Normal</span><span class="p">(</span><span class="mf">0.02</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">num_data</span> <span class="o">=</span> <span class="mi">1600</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span>
    <span class="n">repeat_size</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.005</span>
    <span class="n">momentum</span> <span class="o">=</span> <span class="mf">0.9</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">LinearNet</span><span class="p">()</span>
    <span class="n">net_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">net_loss</span><span class="p">,</span> <span class="n">opt</span><span class="p">)</span>

    <span class="n">ds_train</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">num_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">repeat_size</span><span class="o">=</span><span class="n">repeat_size</span><span class="p">)</span>

    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ds_train</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">LossMonitor</span><span class="p">(),</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
<br/>
<p><font size=3><strong>Q: 如何使用MindSpore拟合<span class="math notranslate nohighlight">\(f(x)=ax^2+bx+c\)</span>这类的二次函数？</strong></font></p>
<p>A: 以下代码引用自MindSpore的官方教程的<a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/docs/sample_code/linear_regression.py">代码仓</a></p>
<p>在以下几处修改即可很好的拟合<span class="math notranslate nohighlight">\(f(x)=ax^2+bx+c\)</span>:</p>
<ol class="arabic simple">
<li><p>数据集生成。</p></li>
<li><p>拟合网络。</p></li>
<li><p>优化器。</p></li>
</ol>
<p>修改的详细信息如下，附带解释。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Since the selected optimizer does not support CPU, so the training computing platform is changed to GPU, which requires readers to install the corresponding GPU version of MindSpore.</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;GPU&quot;</span><span class="p">)</span>

<span class="c1"># Assuming that the function to be fitted this time is f(x)=2x^2+3x+4, the data generation function is modified as follows:</span>
<span class="k">def</span> <span class="nf">get_data</span><span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">3.0</span> <span class="p">,</span><span class="n">c</span> <span class="o">=</span> <span class="mi">4</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mf">10.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">)</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="c1"># The y value is generated by the fitting target function ax^2+bx+c.</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="n">a</span> <span class="o">+</span> <span class="n">x</span> <span class="o">*</span> <span class="n">b</span> <span class="o">+</span> <span class="n">c</span> <span class="o">+</span> <span class="n">noise</span>
        <span class="c1"># When a*x^2+b*x+c is fitted, a and b are weight parameters and c is offset parameter bias. The training data corresponding to the two weights are x^2 and x respectively, so the dataset generation mode is changed as follows:</span>
        <span class="k">yield</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x</span><span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">y</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">create_dataset</span><span class="p">(</span><span class="n">num_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">repeat_size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">GeneratorDataset</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">get_data</span><span class="p">(</span><span class="n">num_data</span><span class="p">)),</span> <span class="n">column_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">,</span><span class="s1">&#39;label&#39;</span><span class="p">])</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">repeat_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">input_data</span>

<span class="k">class</span> <span class="nc">LinearNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LinearNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Because the full join function inputs two training parameters, the input value is changed to 2, the first Nomral(0.02) will automatically assign random weights to the input two parameters, and the second Normal is the random bias.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Normal</span><span class="p">(</span><span class="mf">0.02</span><span class="p">),</span> <span class="n">Normal</span><span class="p">(</span><span class="mf">0.02</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">num_data</span> <span class="o">=</span> <span class="mi">1600</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span>
    <span class="n">repeat_size</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.005</span>
    <span class="n">momentum</span> <span class="o">=</span> <span class="mf">0.9</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">LinearNet</span><span class="p">()</span>
    <span class="n">net_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
    <span class="c1"># RMSProp optimalizer with better effect is selected for quadratic function fitting, Currently, Ascend and GPU computing platforms are supported.</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RMSProp</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">net_loss</span><span class="p">,</span> <span class="n">opt</span><span class="p">)</span>

    <span class="n">ds_train</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">num_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">repeat_size</span><span class="o">=</span><span class="n">repeat_size</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ds_train</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">LossMonitor</span><span class="p">(),</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
<br/>
<p><font size=3><strong>Q: <code class="docutils literal notranslate"><span class="pre">mindspore/tests</span></code>下怎样执行单个<code class="docutils literal notranslate"><span class="pre">ut</span></code>用例？</strong></font></p>
<p>A: <code class="docutils literal notranslate"><span class="pre">ut</span></code>用例通常需要基于debug版本的MindSpore包，官网并没有提供。可以基于源码使用<code class="docutils literal notranslate"><span class="pre">sh</span> <span class="pre">build.sh</span></code>编译，然后通过<code class="docutils literal notranslate"><span class="pre">pytest</span></code>指令执行，debug模式编包不依赖后端。编译选项<code class="docutils literal notranslate"><span class="pre">sh</span> <span class="pre">build.sh</span> <span class="pre">-t</span> <span class="pre">on</span></code>，用例执行可以参考<code class="docutils literal notranslate"><span class="pre">tests/runtest.sh</span></code>脚本。</p>
<br/>
<p><font size=3><strong>Q: 在Ascend平台上，执行用例有时候会报错<code class="docutils literal notranslate"><span class="pre">run</span> <span class="pre">task</span> <span class="pre">error</span></code>，如何获取更详细的日志帮助问题定位？</strong></font></p>
<p>A: 使用msnpureport工具设置device侧日志级别，工具位置在: <code class="docutils literal notranslate"><span class="pre">/usr/local/Ascend/latest/driver/tools/msnpureport</span></code>。</p>
<ul class="simple">
<li><p>全局级别:</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>/usr/local/Ascend/latest/driver/tools/msnpureport<span class="w"> </span>-g<span class="w"> </span>info
</pre></div>
</div>
<ul class="simple">
<li><p>模块级别:</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>/usr/local/Ascend/latest/driver/tools/msnpureport<span class="w"> </span>-m<span class="w"> </span>SLOG:error
</pre></div>
</div>
<ul class="simple">
<li><p>Event级别:</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>/usr/local/Ascend/latest/driver/tools/msnpureport<span class="w"> </span>-e<span class="w"> </span>disable/enable
</pre></div>
</div>
<ul class="simple">
<li><p>多device id级别:</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>/usr/local/Ascend/latest/driver/tools/msnpureport<span class="w"> </span>-d<span class="w"> </span><span class="m">1</span><span class="w"> </span>-g<span class="w"> </span>warning
</pre></div>
</div>
<p>假设deviceID的取值范围是[0-7]，<code class="docutils literal notranslate"><span class="pre">device0</span></code>-<code class="docutils literal notranslate"><span class="pre">device3</span></code>和<code class="docutils literal notranslate"><span class="pre">device4</span></code>-<code class="docutils literal notranslate"><span class="pre">device7</span></code>分别在一个os上。其中<code class="docutils literal notranslate"><span class="pre">device0</span></code>-<code class="docutils literal notranslate"><span class="pre">device3</span></code>共用一个日志配置文件；<code class="docutils literal notranslate"><span class="pre">device4</span></code>-<code class="docutils literal notranslate"><span class="pre">device7</span></code>共用一个配置文件。如果修改了<code class="docutils literal notranslate"><span class="pre">device0</span></code>-<code class="docutils literal notranslate"><span class="pre">device3</span></code>中的任意一个日志级别，其他<code class="docutils literal notranslate"><span class="pre">device</span></code>的日志级别也会被修改。如果修改了<code class="docutils literal notranslate"><span class="pre">device4</span></code>-<code class="docutils literal notranslate"><span class="pre">device7</span></code>中的任意一个日志级别，其他device的日志级别也会被修改。</p>
<p><code class="docutils literal notranslate"><span class="pre">Driver</span></code>包安装以后（假设安装路径为/usr/local/HiAI，在Windows环境下，<code class="docutils literal notranslate"><span class="pre">msnpureport.exe</span></code>执行文件在C:\ProgramFiles\Huawei\Ascend\Driver\tools\目录下），假设用户在/home/shihangbo/目录下直接执行命令行，则Device侧日志被导出到当前目录下，并以时间戳命名文件夹进行存放。</p>
<br/>
<p><font size=3><strong>Q: 使用Ascend平台执行训练过程，出现报错: <code class="docutils literal notranslate"><span class="pre">Out</span> <span class="pre">of</span> <span class="pre">Memory!!!</span> <span class="pre">total[3212254720]</span> <span class="pre">(dynamic[0]</span> <span class="pre">memory</span> <span class="pre">poll[524288000])</span> <span class="pre">malloc[32611480064]</span> <span class="pre">failed!</span></code> 如何解决？</strong></font></p>
<p>A: 此问题属于内存占用过多导致的内存不够问题，可能原因有两种:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">batch_size</span></code>的值设置过大。解决办法: 将<code class="docutils literal notranslate"><span class="pre">batch_size</span></code>的值设置减小。</p></li>
<li><p>引入了异常大的<code class="docutils literal notranslate"><span class="pre">Parameter</span></code>，例如单个数据shape为[640,1024,80,81]，数据类型为float32，单个数据大小超过15G，这样差不多大小的两个数据相加时，占用内存超过3*15G，容易造成<code class="docutils literal notranslate"><span class="pre">Out</span> <span class="pre">of</span> <span class="pre">Memory</span></code>。解决办法: 检查参数的<code class="docutils literal notranslate"><span class="pre">shape</span></code>，如果异常过大，减少shape。</p></li>
<li><p>如果以上操作还是未能解决，可以上<a class="reference external" href="https://www.hiascend.com/forum/forum-0106101385921175002-1.html">官方论坛</a>发帖提出问题，将会有专门的技术人员帮助解决。</p></li>
</ul>
<br/>
<p><font size=3><strong>Q: 如何在训练神经网络过程中对计算损失的超参数进行改变？</strong></font></p>
<p>A: 您好，很抱歉暂时还未有这样的功能。目前只能通过训练–&gt;重新定义优化器–&gt;训练，这样的过程寻找较优的超参数。</p>
<br/>
<p><font size=3><strong>Q: 运行应用时报错<code class="docutils literal notranslate"><span class="pre">error</span> <span class="pre">while</span> <span class="pre">loading</span> <span class="pre">shared</span> <span class="pre">libraries:</span> <span class="pre">libge_compiler.so:</span> <span class="pre">cannot</span> <span class="pre">open</span> <span class="pre">shared</span> <span class="pre">object</span> <span class="pre">file:</span> <span class="pre">No</span> <span class="pre">such</span> <span class="pre">file</span> <span class="pre">or</span> <span class="pre">directory</span></code>怎么办？</strong></font></p>
<p>A: 安装MindSpore所依赖的Ascend 310 AI处理器配套软件包时，<code class="docutils literal notranslate"><span class="pre">CANN</span></code>包不能安装<code class="docutils literal notranslate"><span class="pre">nnrt</span></code>版本，而是需要安装功能完整的<code class="docutils literal notranslate"><span class="pre">toolkit</span></code>版本。</p>
<br/>
<p><font size=3><strong>Q: MindSpore代码里面的model_zoo/official/cv/ResNet/train.py中set_ps_context(enable_ps=True)为什么一定要在init之前设置</strong></font></p>
<p>A: MindSpore Ascend模式下，如果先调用init，那么会为所有的进程都分配卡，但是parameter server训练模式下server是不需要分配卡的，那么worker和server就会去使用同一块卡，导致会报错: Ascend kernel runtime initialization failed。</p>
<br/>
<p><font size=3><strong>Q: 在CPU ARM平台上进行resnet50训练，内存持续增长怎么办？</strong></font></p>
<p>A: 在CPU ARM上进行resnet50训练时，部分算子的实现是基于oneDNN库，oneDNN库中是基于libgomp库实现多线程并行，当前libgomp存在多个并行域配置的线程数不同时有内存占用持续增长的问题。可通过全局配置统一的线程数来控制内存的持续增长。再综合性能上的考虑，建议统一配置为物理核数的1/4，比如<code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">OMP_NUM_THREADS=32</span></code>。</p>
<br/>
<p><font size=3><strong>Q: 为什么在Ascend平台执行模型时报错<code class="docutils literal notranslate"><span class="pre">Stream</span> <span class="pre">isn't</span> <span class="pre">enough</span></code>？</strong></font></p>
<p>A: 流表示一个操作队列，同一条流上的任务按序串行执行，不同流之间可以并行执行。网络中的各种操作会生成Task并被分配到流上，以控制任务执行的并发方式。由于Ascend平台对同一条流上的的任务数存在限制，超限的任务会分配新流，且MindSpore框架的多种并行方式也会分配新流，例如通信算子并行，因此当分配流的数目超过Ascend平台的资源限制就会报流超限的错误。参考解决方案：</p>
<ul class="simple">
<li><p>减小网络模型规模</p></li>
<li><p>减少网络中通信算子的使用</p></li>
<li><p>减少网络中的条件控制语句</p></li>
</ul>
<br/>
<p><font size=3><strong>Q: 在Ascend平台上，日志中出现报错“Ascend error occurred, error message:”且跟随了一个错误码，如“E40011”，如何查找出现错误码的原因？</strong></font></p>
<p>A: 当出现“Ascend error occurred, error message:”时，说明昇腾CANN相关模块出现异常，上报了错误日志。</p>
<p>此时错误码后有异常的错误信息。如果需要该异常更详细的可能原因和处理方法，请参考对应昇腾版本文档的《Error Code故障处理》部分，如<a class="reference external" href="https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/60RC1alpha02/troublemanage/troubleshooting/troubleshooting_0006.html">昇腾CANN社区版(6.0.RC1.alpha002) Error Code故障处理</a>。</p>
<br/>
<p><font size=3><strong>Q: 训练nlp类网络，当使用第三方组件gensim时，可能会报错: ValueError，如何解决？</strong></font></p>
<p>A: 以下为报错信息:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt;<span class="w"> </span>import<span class="w"> </span>gensim
Traceback<span class="w"> </span><span class="o">(</span>most<span class="w"> </span>recent<span class="w"> </span>call<span class="w"> </span>last<span class="o">)</span>:
<span class="w">  </span>File<span class="w"> </span><span class="s2">&quot;&lt;stdin&gt;&quot;</span>,<span class="w"> </span>line<span class="w"> </span><span class="m">1</span>,<span class="w"> </span><span class="k">in</span><span class="w"> </span>&lt;module&gt;
<span class="w">  </span>File<span class="w"> </span><span class="s2">&quot;/home/miniconda3/envs/ci39_cj/lib/python3.9/site-packages/gensim/__init__.py&quot;</span>,<span class="w"> </span>line<span class="w"> </span><span class="m">11</span>,<span class="w"> </span><span class="k">in</span><span class="w"> </span>&lt;module&gt;
<span class="w">    </span>from<span class="w"> </span>gensim<span class="w"> </span>import<span class="w"> </span>parsing,<span class="w"> </span>corpora,<span class="w"> </span>matutils,<span class="w"> </span>interfaces,<span class="w"> </span>models,<span class="w"> </span>similarities,<span class="w"> </span>utils<span class="w">  </span><span class="c1"># noqa:F401</span>
<span class="w">  </span>File<span class="w"> </span><span class="s2">&quot;/home/miniconda3/envs/ci39_cj/lib/python3.9/site-packages/gensim/corpora/__init__.py&quot;</span>,<span class="w"> </span>line<span class="w"> </span><span class="m">6</span>,<span class="w"> </span><span class="k">in</span><span class="w"> </span>&lt;module&gt;
<span class="w">    </span>from<span class="w"> </span>.indexedcorpus<span class="w"> </span>import<span class="w"> </span>IndexedCorpus<span class="w">  </span><span class="c1"># noqa:F401 must appear before the other classes</span>
<span class="w">  </span>File<span class="w"> </span><span class="s2">&quot;/home/miniconda3/envs/ci39_cj/lib/python3.9/site-packages/gensim/corpora/indexedcorpus.py&quot;</span>,<span class="w"> </span>line<span class="w"> </span><span class="m">14</span>,<span class="w"> </span><span class="k">in</span><span class="w"> </span>&lt;module&gt;
<span class="w">    </span>from<span class="w"> </span>gensim<span class="w"> </span>import<span class="w"> </span>interfaces,<span class="w"> </span>utils
<span class="w">  </span>File<span class="w"> </span><span class="s2">&quot;/home/miniconda3/envs/ci39_cj/lib/python3.9/site-packages/gensim/interfaces.py&quot;</span>,<span class="w"> </span>line<span class="w"> </span><span class="m">19</span>,<span class="w"> </span><span class="k">in</span><span class="w"> </span>&lt;module&gt;
<span class="w">    </span>from<span class="w"> </span>gensim<span class="w"> </span>import<span class="w"> </span>utils,<span class="w"> </span>matutils
<span class="w">  </span>File<span class="w"> </span><span class="s2">&quot;/home/miniconda3/envs/ci39_cj/lib/python3.9/site-packages/gensim/matutils.py&quot;</span>,<span class="w"> </span>line<span class="w"> </span><span class="m">1024</span>,<span class="w"> </span><span class="k">in</span><span class="w"> </span>&lt;module&gt;
<span class="w">    </span>from<span class="w"> </span>gensim._matutils<span class="w"> </span>import<span class="w"> </span>logsumexp,<span class="w"> </span>mean_absolute_difference,<span class="w"> </span>dirichlet_expectation
<span class="w">  </span>File<span class="w"> </span><span class="s2">&quot;gensim/_matutils.pyx&quot;</span>,<span class="w"> </span>line<span class="w"> </span><span class="m">1</span>,<span class="w"> </span><span class="k">in</span><span class="w"> </span>init<span class="w"> </span>gensim._matutils
ValueError:<span class="w"> </span>numpy.ndarray<span class="w"> </span>size<span class="w"> </span>changed,<span class="w"> </span>may<span class="w"> </span>indicate<span class="w"> </span>binary<span class="w"> </span>incompatibility.<span class="w"> </span>Expected<span class="w"> </span><span class="m">88</span><span class="w"> </span>from<span class="w"> </span>C<span class="w"> </span>header,<span class="w"> </span>got<span class="w"> </span><span class="m">80</span><span class="w"> </span>from<span class="w"> </span>PyObject
</pre></div>
</div>
<p>报错原因请参考<a class="reference external" href="https://github.com/RaRe-Technologies/gensim/issues/3095">gensim</a>官网，或者<a class="reference external" href="https://github.com/numpy/numpy/issues/18709">numpy</a>官网:</p>
<p>解决方案:</p>
<p>方法一: 重新安装numpy及gensim, 执行命令: <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">uninstall</span> <span class="pre">gensim</span> <span class="pre">numpy</span> <span class="pre">-y</span> <span class="pre">&amp;&amp;</span> <span class="pre">pip</span> <span class="pre">install</span> <span class="pre">numpy</span> <span class="pre">gensim</span></code> ；</p>
<p>方法二: 如果还是有问题，请删除wheel安装包的缓存文件，然后执行方法一（wheel安装包缓存目录为: <code class="docutils literal notranslate"><span class="pre">~/.cache/pip/wheels</span></code>）。</p>
<br/>
<p><font size=3><strong>Q：运行文档示例代码的过程中，遇到<code class="docutils literal notranslate"><span class="pre">matplotlib.pyplot.show()</span></code>或<code class="docutils literal notranslate"><span class="pre">plt.show()</span></code>无法执行怎么处理？</strong></font></p>
<p>A: 首先确认是否安装<code class="docutils literal notranslate"><span class="pre">matplotlib</span></code>，如果没有安装，可以在命令行中执行<code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">matplotlib</span></code>进行安装。</p>
<p>其次由于<code class="docutils literal notranslate"><span class="pre">matplotlib.pyplot.show()</span></code>的作用是以图形化方式展示，所以需要运行系统支持图形展示功能，如果系统不能支持图形展示，需要将该
图形展示的命令行注释后再运行，不影响整体代码的运行结果。</p>
<br/>
<p><font size=3><strong>Q: 使用文档中提供的在线运行时，遇到运行失败该如何处理？</strong></font></p>
<p>A: 需要确认有做以下准备工作。</p>
<ul class="simple">
<li><p>首先，需要通过华为云账号登录ModelArts。</p></li>
<li><p>其次，注意教程文档的标签中列举的硬件环境，以及样例代码中配置的硬件环境，是Ascend、GPU还是CPU，由于登录后默认使用的硬件环境是CPU，Ascend环境和GPU环境需要用户手动点击切换。</p></li>
<li><p>最后，确保当前<code class="docutils literal notranslate"><span class="pre">Kernel</span></code>为MindSpore。</p></li>
</ul>
<p>完成上述步骤后，就可以运行文档了。</p>
<p>具体的操作过程可以参考<a class="reference external" href="https://www.hiascend.com/forum/thread-0254122007639293043-1-1.html">基于ModelArts在线体验MindSpore</a>。</p>
<br/>
<p><font size=3><strong>Q: 静态图下使用除法结果未报错，动态图下使用除法结果却报错？</strong></font></p>
<p>A: 在静态图模式下，由于使用的是静态编译，对于算子输出结果的数据类型是在图编译阶段确定的。</p>
<p>例如如下代码在静态图模式下执行，输入数据的类型都为int类型，根据静态图编译，其输出结果也是int类型。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">MyTest</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MyTest</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">/</span> <span class="n">y</span>
<span class="n">x</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">MyTest</span><span class="p">()</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">output</span><span class="p">))</span>
</pre></div>
</div>
<p>输出结果：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>4 &lt;class &#39;int&#39;&gt;
</pre></div>
</div>
<p>修改执行模式，将GRAPH_MODE修改成PYNATIVE_MODE，由于在动态图模式下使用的Python语法执行，Python语法对任意除法输出的类型都是float类型，因此执行结果如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>4.0 &lt;class &#39;float&#39;&gt;
</pre></div>
</div>
<p>因此在后续算子明确需要使用int的场景下，建议使用Python的整除符号<code class="docutils literal notranslate"><span class="pre">//</span></code>。</p>
<br/>
<p><font size=3><strong>Q: 1.8版本首次运行GPU的脚本会卡很久？</strong></font></p>
<p>A: 由于NVCC编译CUDA算子时为了兼容更多GPU架构，先编译成ptx文件，在首次使用时会进行JIT编译成二进制执行文件，因此会产生编译耗时。
而1.8版本相较于前版本增加了许多CUDA算子，导致这部分编译时间增加（不同设备时间不同，如在V100上首次编译时间为5分钟左右）。
该编译会产生缓存文件（以ubuntu系统为例，缓存文件位于 <code class="docutils literal notranslate"><span class="pre">~/.nv/ComputeCache</span></code> 路径下)，在后续执行时会直接加载缓存文件。
因此会产生首次使用时会卡住几分钟，后续使用时间正常的现象。</p>
<p>后续版本会进行预编译优化。</p>
<br/>
<p><font size=3><strong>Q: 算子执行过程中出现报错: <code class="docutils literal notranslate"><span class="pre">MemoryError:</span> <span class="pre">std::bad_alloc</span></code> 如何解决？</strong></font></p>
<p>A: 此问题的原因为：用户未正确配置算子参数，导致算子申请的内存空间超过了系统内存限制，进而系统分配内存失败。下面以算子 mindspore.ops.UniformCandidateSampler 为例进行说明：</p>
<ul class="simple">
<li><p>UniformCandidateSampler使用均匀分布对一组类别进行采样，根据用户设定的参数<code class="docutils literal notranslate"><span class="pre">num_sampled</span></code>，其输出Tensor的shape为<code class="docutils literal notranslate"><span class="pre">(num_sampled,)</span></code>。</p></li>
<li><p>当用户设定的<code class="docutils literal notranslate"><span class="pre">num_sampled=int64.max</span></code>时，其输出Tensor申请的内存空间超过了系统内存限制，并导致<code class="docutils literal notranslate"><span class="pre">bad_alloc</span></code>。</p></li>
</ul>
<p>因此，用户需要适当设置算子参数，以避免此类报错。</p>
<br/>
<p><font size=3><strong>Q: 如何理解报错提示中的”Ascend Error Message”？</strong></font></p>
<p>A: “Ascend Error Message”是MindSpore调用CANN(昇腾异构计算架构)接口时，CANN执行出错后抛出的故障信息，其中包含错误码和错误描述等信息，如下例子：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Traceback (most recent call last):
 File &quot;train.py&quot;, line 292, in &lt;module&gt;
 train_net()
 File  &quot;/home/resnet_csj2/scripts/train_parallel0/src/model_utils/moxing_adapter.py&quot;, line 104, in wrapped_func
 run_func(*args, **kwargs)
 File &quot;train.py&quot;, line 227, in train_net
 set_parameter()
 File &quot;train.py&quot;, line 114, in set_parameter
 init()
 File &quot;/home/miniconda3/envs/ms/lib/python3.7/site-packages/mindspore/communication/management.py&quot;, line 149, in init
 init_hccl()
 RuntimeError: Ascend kernel runtime initialization failed.

 \----------------------------------------------------
 \- Ascend Error Message:
 \----------------------------------------------------
 EJ0001: Failed to initialize the HCCP process. Reason: Maybe the last training process is running. //EJ0001为错误码，之后是错误的描述与原因，本例子的错误原因是多次启动了相同8节点的分布式训练，造成进程冲突
 Solution: Wait for 10s after killing the last training process and try again. //此处打印信息给出了问题的解决方案，此例子建议用户清理进程
 TraceBack (most recent call last): //此处打印的信息是开发用于定位的堆栈信息，一般情况下用户不需关注
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>tsd client wait response fail, device response code[1]. unknown device  error.[FUNC:WaitRsp][FILE:process_mode_manager.cpp][LINE:233]
</pre></div>
</div>
<p>另外在一些情况下，CANN会抛出一些内部错误(Inner Error)，例如：错误码为 “EI9999: Inner Error” 此种情况如果在MindSpore官网或者论坛无法搜索到案例说明，可在社区提单求助。</p>
<br/>
<p><font size=3><strong>Q: 如何控制<code class="docutils literal notranslate"><span class="pre">print</span></code>方法打印出的Tensor值？</strong></font></p>
<p>A: 在PyNative动态图模式下，可以使用numpy原生方法如<code class="docutils literal notranslate"><span class="pre">set_printoptions</span></code>对输出的值进行控制。在Graph静态图模式下，因为<code class="docutils literal notranslate"><span class="pre">print</span></code>方法需要转化成为算子，所以暂时无法对输出的值进行控制。print算子具体用法可<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/master/api_python/ops/mindspore.ops.Print.html">参考</a>。</p>
<br/>
<p><font size=3><strong>Q: <code class="docutils literal notranslate"><span class="pre">Tensor.asnumpy()</span></code>是怎么和Tensor共享内存地址的？</strong></font></p>
<p>A: <code class="docutils literal notranslate"><span class="pre">Tensor.asnumpy()</span></code>会将Tensor本身转换为NumPy的ndarray。这个Tensor和<code class="docutils literal notranslate"><span class="pre">Tensor.asnumpy()</span></code>返回的ndarray共享host侧的内存地址，在host侧，对Tensor本身的修改会反映到相应的ndarray上，反之亦然。需要注意的是，host侧的修改无法自动同步到device侧。如：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>import mindspore as ms
x = ms.Tensor([1, 2, 3]) + ms.Tensor([4, 5, 6])
y = x.asnumpy()

# x 是 device 侧算子计算的结果，而 y 在 host 侧。host 侧对 y 的修改无法自动同步到 device 侧的 x。
y[0] = 11
print(y)

# 打印 x 会触发数据同步，将 x 的数据同步到 y。
print(x)
print(y)
</pre></div>
</div>
<p>运行结果如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[11 7 9]
[5 7 9]
[5 7 9]
</pre></div>
</div>
<br/>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="data_processing.html" class="btn btn-neutral float-left" title="数据处理" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
        <a href="network_compilation.html" class="btn btn-neutral float-right" title="网络编译" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 2022, MindSpore.</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用了 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a>开发.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>