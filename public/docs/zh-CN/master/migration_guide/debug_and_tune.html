<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>调试调优 &mdash; MindSpore master 文档</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script src="../_static/js/mermaid-9.3.0.js"></script><script src="../_static/translations.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="网络迁移调试实例" href="sample_code.html" />
    <link rel="prev" title="训练及推理流程" href="model_development/training_and_evaluation.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">设计</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../design/overview.html">MindSpore设计概览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/programming_paradigm.html">函数式和对象式融合编程范式</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/dynamic_graph_and_static_graph.html">动静态图结合</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/distributed_training_design.html">分布式并行原生</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/data_engine.html">高性能数据处理引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/all_scenarios.html">全场景统一架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/graph_fusion_engine.html">图算融合加速引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/pluggable_device.html">三方硬件对接</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/glossary.html">术语</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">模型库</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/official_models.html">官方模型库</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.experimental.html">mindspore.experimental</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API映射</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/pytorch_api_mapping.html">PyTorch与MindSpore API映射表</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">迁移指南</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">迁移指南概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="enveriment_preparation.html">环境准备</a></li>
<li class="toctree-l1"><a class="reference internal" href="analysis_and_preparation.html">模型分析与准备</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_development/model_development.html">网络搭建对比</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">调试调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="sample_code.html">网络迁移调试实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">常见问题</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">语法支持</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax_support.html">静态图语法支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax/operators.html">静态图语法-运算符</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax/statements.html">静态图语法-Python语句</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax/python_builtin_functions.html">静态图语法-Python内置函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/index_support.html">Tensor索引支持</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">环境变量</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/env_var_list.html">环境变量</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq/installation.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/data_processing.html">数据处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/implement_problem.html">执行问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/network_compilation.html">网络编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/operators_compile.html">算子编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/usage_migrate_3rd.html">第三方框架迁移使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/performance_tuning.html">性能调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/precision_tuning.html">精度调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/distributed_parallel.html">分布式并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/inference.html">推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/feature_advice.html">特性咨询</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>调试调优</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/migration_guide/debug_and_tune.md.txt" rel="nofollow"> 查看页面源码</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section class="tex2jax_ignore mathjax_ignore" id="调试调优">
<h1>调试调优<a class="headerlink" href="#调试调优" title="永久链接至标题"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/docs/mindspore/source_zh_cn/migration_guide/debug_and_tune.md"><img alt="查看源文件" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source.png" /></a></p>
<section id="调优常见问题及解决办法">
<h2>调优常见问题及解决办法<a class="headerlink" href="#调优常见问题及解决办法" title="永久链接至标题"></a></h2>
<ul class="simple">
<li><p>精度调试阶段，需要进行网络逐层对比：</p>
<ul>
<li><p>在API级别，可以通过<a class="reference external" href="https://gitee.com/mindspore/toolkits/tree/master/troubleshooter">TroubleShooter</a>工具的Tensor保存和比较功能，使用二分法逐层保存Tensor与PyTorch进行比较；</p></li>
<li><p>在算子级别，可使用<a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/master/debug/dump.html">Dump</a>将模型训练中的图以及算子的输入输出数据保存到磁盘文件。用于网络迁移复杂问题定位（例如：算子溢出等）。</p></li>
</ul>
</li>
<li><p>性能调试阶段，可通过<a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/zh-CN/master/performance_profiling.html">Profiler</a>将训练过程中的算子耗时等信息记录到文件中，提供框架的host执行、以及算子执行的Profiler分析功能，通过可视化界面供用户查看分析，帮助用户更高效地调试神经网络性能。</p></li>
</ul>
</section>
<section id="mindspore调优功能介绍">
<h2>MindSpore调优功能介绍<a class="headerlink" href="#mindspore调优功能介绍" title="永久链接至标题"></a></h2>
<section id="功能调试">
<h3>功能调试<a class="headerlink" href="#功能调试" title="永久链接至标题"></a></h3>
<p>在网络的迁移过程，建议优先使用PYNATIVE模式进行调试，在PYNATIVE模式下可以进行debug，日志打印也比较友好。在调试ok后转成图模式运行，图模式在执行性能上会更友好，也可以找到一些在编写网络中的问题，比如使用了三方的算子导致梯度截断。
详情请参考 <a class="reference external" href="https://www.mindspore.cn/tutorials/zh-CN/master/advanced/error_analysis/error_scenario_analysis.html">错误分析</a>。</p>
</section>
<section id="精度调试">
<h3>精度调试<a class="headerlink" href="#精度调试" title="永久链接至标题"></a></h3>
<p>精度调试的过程基本可以分为以下过程：</p>
<section id="1检查参数">
<h4>1.检查参数<a class="headerlink" href="#1检查参数" title="永久链接至标题"></a></h4>
<p>这部分包含检查所有参数和可训练参数的数量，检查所有参数的shape。</p>
<ul>
<li><p>MindSpore获取参数方法</p>
<p>MindSpore可训练的参数和不可训练的参数都用<code class="docutils literal notranslate"><span class="pre">Parameter</span></code>。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">msNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">msNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">weight_init</span><span class="o">=</span><span class="s1">&#39;normal&#39;</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>

<span class="n">msnet</span> <span class="o">=</span> <span class="n">msNet</span><span class="p">()</span>
<span class="c1"># 获取所有参数</span>
<span class="n">all_parameter</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">msnet</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">():</span>
    <span class="n">all_parameter</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">item</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">item</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;all parameter numbers: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">all_parameter</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 获取可训练的参数</span>
<span class="n">trainable_params</span> <span class="o">=</span> <span class="n">msnet</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()</span>
<span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">trainable_params</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">item</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">item</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;trainable parameter numbers: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">trainable_params</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>fc.weight (1, 1)
fc.bias (1,)
all parameter numbers: 2
fc.weight (1, 1)
fc.bias (1,)
trainable parameter numbers: 2
</pre></div>
</div>
</li>
<li><p>PyTorch获取参数方法</p>
<p>PyTorch可训练的参数用<code class="docutils literal notranslate"><span class="pre">Parameter</span></code>，不可训练的参数<code class="docutils literal notranslate"><span class="pre">Parameter</span></code>的<code class="docutils literal notranslate"><span class="pre">requires_grad=False</span></code>或使用<code class="docutils literal notranslate"><span class="pre">buffer</span></code>。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">ptNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ptNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>

<span class="n">ptnet</span> <span class="o">=</span> <span class="n">ptNet</span><span class="p">()</span>
<span class="n">all_parameter</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">trainable_params</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c1"># 获取网络里的参数</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">ptnet</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">item</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
        <span class="n">trainable_params</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
    <span class="n">all_parameter</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">item</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">buffer</span> <span class="ow">in</span> <span class="n">ptnet</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">():</span>
    <span class="n">all_parameter</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">buffer</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">buffer</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;all parameter numbers: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">all_parameter</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;trainable parameter numbers: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">trainable_params</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>fc.weight torch.Size([1, 1])
fc.bias torch.Size([1])
all parameter numbers: 2
trainable parameter numbers: 2
</pre></div>
</div>
<p>MindSpore和PyTorch的参数除了BatchNorm区别大一点，其他都差不多。注意MindSpore里没有<code class="docutils literal notranslate"><span class="pre">num_batches_tracked</span></code>的对应，实际使用时这个参数可以用优化器里的<code class="docutils literal notranslate"><span class="pre">global_step</span></code>替代。</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>MindSpore</p></th>
<th class="head"><p>PyTorch</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>gamma</p></td>
<td><p>weight</p></td>
</tr>
<tr class="row-odd"><td><p>beta</p></td>
<td><p>bias</p></td>
</tr>
<tr class="row-even"><td><p>moving_mean</p></td>
<td><p>running_mean</p></td>
</tr>
<tr class="row-odd"><td><p>moving_variance</p></td>
<td><p>running_var</p></td>
</tr>
<tr class="row-even"><td><p>无</p></td>
<td><p>num_batches_tracked</p></td>
</tr>
</tbody>
</table>
</li>
</ul>
</section>
<section id="2模型验证">
<h4>2.模型验证<a class="headerlink" href="#2模型验证" title="永久链接至标题"></a></h4>
<p>由于模型算法的实现是和框架没有关系的，训练好的参数可以先转换成MindSpore的<a class="reference external" href="https://www.mindspore.cn/tutorials/zh-CN/master/beginner/save_load.html">checkpoint</a>文件加载到网络中进行推理验证。</p>
<p>整个模型验证的流程请参考<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/master/migration_guide/sample_code.html#%E6%A8%A1%E5%9E%8B%E9%AA%8C%E8%AF%81">resnet网络迁移</a>。</p>
</section>
<section id="3推理验证">
<h4>3.推理验证<a class="headerlink" href="#3推理验证" title="永久链接至标题"></a></h4>
<p>确认模型结构完全一致后，最好再做一次推理验证。整个推理过程除了模型外还有数据集和metrics，当推理结果不一致时，可以采用控制变量法，逐步排除问题。</p>
<p>整个推理验证的流程请参考<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/master/migration_guide/sample_code.html#%E6%8E%A8%E7%90%86%E6%B5%81%E7%A8%8B">resnet网络迁移</a>。</p>
</section>
<section id="4训练精度">
<h4>4.训练精度<a class="headerlink" href="#4训练精度" title="永久链接至标题"></a></h4>
<p>当完成了推理验证后，我们基本可以确定基础模型，数据处理和metrics计算没有问题。此时如果训练的精度还是有问题时怎么进行排查呢？</p>
<ul>
<li><p>加loss scale，在Ascend上因为Conv、Sort、TopK等算子只能是float16的，MatMul由于性能问题最好也是float16的，所以建议Loss scale操作作为网络训练的标配。</p>
<p>Ascend 上只支持float16的算子列表：</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>算子类别</p></th>
<th class="head"><p>具体算子</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>池化</p></td>
<td><p>AdaptiveMaxPool2D, AvgPool3D, AvgPool, MaxPool, MaxPoolWithArgmax, Pooling</p></td>
</tr>
<tr class="row-odd"><td><p>循环神经结构</p></td>
<td><p>LSTM, DynamicRNN, GRUV2</p></td>
</tr>
<tr class="row-even"><td><p>卷积</p></td>
<td><p>Conv2D, Conv2DTranspose, Conv3D, Conv3DTranspose, DepthwiseConv2dNative</p></td>
</tr>
<tr class="row-odd"><td><p>矩阵乘 (这类主要float32太慢, 需要转成float16的)</p></td>
<td><p>MatMul, BatchMatMul</p></td>
</tr>
<tr class="row-even"><td><p>排序</p></td>
<td><p>Sort, TopK</p></td>
</tr>
<tr class="row-odd"><td><p>其他</p></td>
<td><p>BoundingBoxEncode, ExtractImagePatches, ExtractVolumePatches, FusedDbnDw, IOU, NewIm2Col, NMSWithMask</p></td>
</tr>
</tbody>
</table>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">mindspore.train</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="c1"># Model</span>
<span class="n">loss_scale_manager</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">FixedLossScaleManager</span><span class="p">(</span><span class="n">drop_overflow_update</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="c1"># 静态loss scale</span>
<span class="c1"># loss_scale_manager = ms.amp.DynamicLossScaleManager()   # 动态loss scale</span>

<span class="c1"># 1. 一般流程</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">msnet</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">network</span><span class="o">=</span><span class="n">msnet</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">loss_scale_manager</span><span class="o">=</span><span class="n">loss_scale_manager</span><span class="p">)</span>

<span class="c1"># 2. 自已包装正向网络和loss函数</span>
<span class="n">msnet</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="n">loss</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">net_with_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">WithLossCell</span><span class="p">(</span><span class="n">msnet</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
<span class="c1"># 用Model的混合精度最好要有loss_fn，否则loss部分会使用float16计算，容易溢出</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">network</span><span class="o">=</span><span class="n">net_with_loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">)</span>

<span class="c1"># 3. 自己包装训练流程</span>
<span class="n">scale_sense</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">FixedLossScaleUpdateCell</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="c1">#(config.loss_scale) # 静态loss scale</span>
<span class="c1"># scale_sense = nn.DynamicLossScaleUpdateCell(loss_scale_value=config.loss_scale,</span>
<span class="c1">#                                             scale_factor=2, scale_window=1000) # 动态loss scale</span>
<span class="n">train_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">TrainOneStepWithLossScaleCell</span><span class="p">(</span><span class="n">net_with_loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">scale_sense</span><span class="o">=</span><span class="n">scale_sense</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">network</span><span class="o">=</span><span class="n">train_net</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>排查是否溢出，添加loss scale时，默认会加上溢出检测，可以将是否溢出的结果进行监测，如果持续溢出的话建议优先排查为什么溢出，建议使用MindSpore Insight的<a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/zh-CN/master/debugger.html">调试器</a>或者<a class="reference external" href="https://mindspore.cn/tutorials/experts/zh-CN/master/debug/dump.html">dump数据</a>。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dataset</span> <span class="k">as</span> <span class="n">ds</span>

<span class="k">def</span> <span class="nf">get_data</span><span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="n">w</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">3.0</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mf">10.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">)</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">w</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="n">noise</span>
        <span class="k">yield</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">y</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">create_dataset</span><span class="p">(</span><span class="n">num_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">repeat_size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">GeneratorDataset</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">get_data</span><span class="p">(</span><span class="n">num_data</span><span class="p">)),</span> <span class="n">column_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span><span class="p">])</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">repeat_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">input_data</span>

<span class="n">train_net</span><span class="o">.</span><span class="n">set_train</span><span class="p">()</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="mi">1600</span><span class="p">)</span>
<span class="n">iterator</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_tuple_iterator</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">iterator</span><span class="p">):</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">overflow</span><span class="p">,</span> <span class="n">scaling_sens</span> <span class="o">=</span> <span class="n">train_net</span><span class="p">(</span><span class="o">*</span><span class="n">data</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;step: </span><span class="si">{}</span><span class="s2">, loss: </span><span class="si">{}</span><span class="s2">, overflow:</span><span class="si">{}</span><span class="s2">, scale:</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">overflow</span><span class="p">,</span> <span class="n">scaling_sens</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>step: 0, loss: 138.42825, overflow:False, scale:1.0
step: 1, loss: 118.172104, overflow:False, scale:1.0
step: 2, loss: 159.14542, overflow:False, scale:1.0
step: 3, loss: 150.65671, overflow:False, scale:1.0
... ...
step: 97, loss: 69.513245, overflow:False, scale:1.0
step: 98, loss: 51.903114, overflow:False, scale:1.0
step: 99, loss: 42.250656, overflow:False, scale:1.0
</pre></div>
</div>
</li>
<li><p>排查优化器、loss和参数初始化，整个训练过程除了模型、数据集外新加的部分只有优化器、loss和参数初始化，训练有问题时需要重点排查。尤其是loss和参数初始化，出现问题的概率较大。</p></li>
<li><p>多卡确认是否加seed保证多卡初始化一致，<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/master/migration_guide/model_development/training_and_evaluation.html#%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B">自定义训练</a>确认是否进行梯度聚合。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># 会固定MindSpore的，numpy的，dataset的随机种子，API内部的需要在API属性设置</span>
</pre></div>
</div>
</li>
<li><p>排查数据处理，通过可视化等方法查看数据处理是否符合预期，重点查看数据shuffle，是否有数据不匹配的情况。</p></li>
</ul>
<p>更多精度调试策略请参考<a class="reference external" href="https://mindspore.cn/mindinsight/docs/zh-CN/master/accuracy_problem_preliminary_location.html">精度调试</a>。</p>
</section>
</section>
<section id="性能调优">
<h3>性能调优<a class="headerlink" href="#性能调优" title="永久链接至标题"></a></h3>
<p>首先需要做性能数据获取，具体的获取方式见<a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/zh-CN/master/performance_profiling_ascend.html">性能调试（Ascend）</a>、 <a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/zh-CN/master/performance_profiling_gpu.html">性能调试（GPU）</a>。</p>
<p>性能优化方向主要包含：</p>
<ol class="arabic simple">
<li><p>算子性能优化</p></li>
<li><p>框架使能性能优化</p></li>
<li><p>多机同步性能优化</p></li>
<li><p>数据处理性能优化</p></li>
</ol>
<p>可以参考<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/master/migration_guide/sample_code.html">resnet网络迁移</a>串通整个过程。</p>
<blockquote>
<div><p>有的网络很大，这种情况在图模式下编译会很慢。在性能调优过程请区分图编译和网络执行，本节主要介绍网络执行阶段的性能调优策略，如果确认是图编译慢请尝试<a class="reference external" href="https://mindspore.cn/tutorials/experts/zh-CN/master/optimize/op_compilation.html">算子增量编译</a>或者联系 <a class="reference external" href="https://gitee.com/mindspore/mindspore/issues">MindSpore社区</a> 反馈。</p>
</div></blockquote>
<section id="算子性能优化">
<h4>算子性能优化<a class="headerlink" href="#算子性能优化" title="永久链接至标题"></a></h4>
<p>单算子耗时久、对于同一种算子在不同shape或者不同 datatype 下性能差异较大的情况主要是由算子性能问题引起，通常有以下解决思路：</p>
<ol class="arabic simple">
<li><p>使用计算量更小的数据类型。例如，同一个算子在 float16 和 float32 下精度无明显差别，可使用计算量更小的 float16 格式。</p></li>
<li><p>使用算法相同的其他算子规避。</p></li>
<li><p>Ascend环境上注意16对齐。由于昇腾芯片的设计，在AICore上的计算最好是16对齐的(shape中的每一维都是16的倍数)。</p></li>
</ol>
<p>如果您发现有性能较差的算子时，建议联系 <a class="reference external" href="https://gitee.com/mindspore/mindspore/issues">MindSpore社区</a> 反馈，我们确认为性能问题后会及时优化。</p>
</section>
<section id="框架使能性能优化">
<h4>框架使能性能优化<a class="headerlink" href="#框架使能性能优化" title="永久链接至标题"></a></h4>
<ul>
<li><p>使用静态图模式</p>
<p>MindSpore一般在静态图模式下比PYNATIVE模式下快很多，最好能在静态图模式下进行训练和推理，具体原理请参考<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/master/design/dynamic_graph_and_static_graph.html">动静态图结合</a>。</p>
</li>
<li><p>on-device执行</p>
<p>MindSpore提供了一种<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/master/design/overview.html#%E9%9D%A2%E5%90%91%E6%98%87%E8%85%BE%E7%A1%AC%E4%BB%B6%E7%9A%84%E7%AB%9E%E4%BA%89%E5%8A%9B%E4%BC%98%E5%8C%96">on-device执行</a>的方法将数据处理和网络在device上的执行并行起来，只需要在<code class="docutils literal notranslate"><span class="pre">model.train</span></code>中设置<code class="docutils literal notranslate"><span class="pre">dataset_sink_mode=True</span></code>即可，注意这个配置默认是<code class="docutils literal notranslate"><span class="pre">False</span></code>，当打开这个配置时，一个epoch只会返回一个网络的结果，当进行调试时建议先将这个值改成<code class="docutils literal notranslate"><span class="pre">False</span></code>。</p>
</li>
<li><p>使用自动混合精度</p>
<p>混合精度训练方法是通过混合使用单精度和半精度数据格式来加速深度神经网络训练的过程，同时保持了单精度训练所能达到的网络精度。混合精度训练能够加速计算过程，同时减少内存使用和存取，并使得在特定的硬件上可以训练更大的模型或 batch size。</p>
<p>具体可参考 <a class="reference external" href="https://www.mindspore.cn/tutorials/zh-CN/master/advanced/mixed_precision.html">混合精度教程</a>。</p>
</li>
<li><p>使能图算融合</p>
<p>图算融合是 MindSpore 特有的网络性能优化技术。它可以通过自动分析和优化现有网络计算图逻辑，并结合目标硬件能力，对计算图进行计算化简和替代、算子拆分和融合、算子特例化编译等优化，以提升设备计算资源利用率，实现对网络性能的整体优化。相比传统优化技术，图算融合具有多算子跨边界联合优化、与算子编译跨层协同、基于Polyhedral的算子即时编译等独特优势。另外，图算融合只需要用户打开对应配置后，整个优化过程即可自动完成，不需要网络开发人员进行其它额外感知，使得用户可以聚焦网络算法实现。</p>
<p>图算融合的适用场景包括：对网络执行时间具有较高性能要求的场景；通过拼接基本算子实现自定义组合算子，并希望对这些基本算子进行自动融合，以提升自定义组合算子性能的场景。</p>
<p>具体可参考 <a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/master/design/graph_fusion_engine.html">图算融合教程</a>。</p>
</li>
<li><p>其他</p>
<p>转换算子过多（TransData、Cast类算子）且耗时明显时，如果是我们手动加入的Cast算子，可分析其必要性，如果对精度没有影响，可去掉冗余的Cast、TransData算子。</p>
<p>如果是MindSpore自动生成的转换算子过多，可能是MindSpore框架针对某些特殊情况没有充分优化，可联系 <a class="reference external" href="https://gitee.com/mindspore/mindspore/issues">MindSpore社区</a> 反馈。</p>
<p><a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/master/migration_guide/dynamic_shape.html">动态shape场景</a>目前需要不断的编图，可能会造成端到端的训练时间较长，建议优先<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/master/migration_guide/model_development/model_and_cell.html#%E5%8A%A8%E6%80%81shape%E8%A7%84%E9%81%BF%E7%AD%96%E7%95%A5">规避动态shape</a>。</p>
</li>
</ul>
</section>
<section id="多机同步性能优化">
<h4>多机同步性能优化<a class="headerlink" href="#多机同步性能优化" title="永久链接至标题"></a></h4>
<p>当进行分布式训练时，在一个Step的训练过程中，完成前向传播和梯度计算后，各个机器开始进行AllReduce梯度同步，AllReduce同步时间主要受权重数量、机器数量影响，对于越复杂、机器规模越大的网络，其 AllReduce 梯度更新时间也越久，此时我们可以进行AllReduce 切分来优化这部分耗时。</p>
<p>正常情况下，AllReduce 梯度同步会等所有反向算子执行结束，也就是对所有权重都计算出梯度后再一次性同步所有机器的梯度，而使用AllReduce切分后，我们可以在计算出一部分权重的梯度后，就立刻进行这部分权重的梯度同步，这样梯度同步和剩余算子的梯度计算可以并行执行，也就隐藏了这部分 AllReduce 梯度同步时间。切分策略通常是手动尝试，寻找一个最优的方案（支持切分大于两段）。
以 <a class="reference external" href="https://gitee.com/mindspore/models/blob/master/official/cv/ResNet/train.py">ResNet50网络</a> 为例，该网络共有 160 个 权重， [85, 160] 表示第 0 至 85个权重计算完梯度后立刻进行梯度同步，第 86 至 160 个 权重计算完后再进行梯度同步，这里共切分两段，因此需要进行两次梯度同步。代码实现如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span>

<span class="n">device_id</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;DEVICE_ID&#39;</span><span class="p">,</span> <span class="s1">&#39;0&#39;</span><span class="p">))</span>
<span class="n">rank_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;RANK_SIZE&#39;</span><span class="p">,</span> <span class="s1">&#39;1&#39;</span><span class="p">))</span>
<span class="n">rank_id</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;RANK_ID&#39;</span><span class="p">,</span> <span class="s1">&#39;0&#39;</span><span class="p">))</span>

<span class="c1"># init context</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s1">&#39;Ascend&#39;</span><span class="p">,</span> <span class="n">device_id</span><span class="o">=</span><span class="n">device_id</span><span class="p">)</span>
<span class="k">if</span> <span class="n">rank_size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">device_num</span><span class="o">=</span><span class="n">rank_size</span><span class="p">,</span> <span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">DATA_PARALLEL</span><span class="p">,</span>
                                <span class="n">gradients_mean</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">all_reduce_fusion_config</span><span class="o">=</span><span class="p">[</span><span class="mi">85</span><span class="p">,</span> <span class="mi">160</span><span class="p">])</span>
    <span class="n">init</span><span class="p">()</span>
</pre></div>
</div>
<p>更多请参考<a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/zh-CN/master/performance_profiling_of_cluster.html">集群性能调试</a>。</p>
</section>
<section id="数据处理性能优化">
<h4>数据处理性能优化<a class="headerlink" href="#数据处理性能优化" title="永久链接至标题"></a></h4>
<p>单Step性能抖动、数据队列一段时间内持续为空的情况都是由于数据预处理部分性能较差，使得数据处理速度跟不上单Step迭代速度导致，这两个现象通常成对出现。</p>
<p>当数据处理速度较慢时，队列从最开始的满队列情况逐渐消耗为空队列，训练进程会开始等待空队列填入数据，一旦有新的数据填入，网络才会继续进行单Step训练。由于数据处理没有队列作为缓冲，数据处理的性能抖动直接体现在单Step的性能上，因此还会造成单Step性能抖动。</p>
<p>关于数据的性能问题，可以参考 MindSpore Insight 组件的 <a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/zh-CN/master/performance_profiling_ascend.html#%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90">数据准备性能分析</a>，其给出了数据性能的常见问题及解决方法。</p>
<p>更多性能调试方法请参考<a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/master/optimize/execution_opt.html">性能优化</a>。</p>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="model_development/training_and_evaluation.html" class="btn btn-neutral float-left" title="训练及推理流程" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
        <a href="sample_code.html" class="btn btn-neutral float-right" title="网络迁移调试实例" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 MindSpore.</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用了 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a>开发.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>