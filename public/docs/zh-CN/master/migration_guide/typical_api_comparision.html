<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>PyTorch对比 &mdash; MindSpore master 文档</title>
      <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/training.css" type="text/css" /><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/js/training.js"></script>
        <script src="../_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="MindSpore网络搭建" href="model_development/model_development.html" />
    <link rel="prev" title="模型分析与准备" href="analysis_and_preparation.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">设计</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../design/overview.html">MindSpore设计概览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/programming_paradigm.html">函数式和对象式融合编程范式</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/all_scenarios.html">全场景统一架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/dynamic_graph_and_static_graph.html">动静态图结合</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/pluggable_device.html">三方硬件对接</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/distributed_training_design.html">原生分布式并行架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/graph_fusion_engine.html">图算融合加速引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/data_engine.html">高性能数据处理引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/glossary.html">术语</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">模型库</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/official_models.html">官方模型库</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.experimental.html">mindspore.experimental</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API映射</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/pytorch_api_mapping.html">PyTorch与MindSpore API映射表</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/tensorflow_api_mapping.html">TensorFlow与MindSpore API映射表</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">迁移指南</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">迁移指南概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="enveriment_preparation.html">环境准备与资料获取</a></li>
<li class="toctree-l1"><a class="reference internal" href="analysis_and_preparation.html">模型分析与准备</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">PyTorch对比</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_development/model_development.html">MindSpore网络搭建</a></li>
<li class="toctree-l1"><a class="reference internal" href="debug_and_tune.html">调试调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="sample_code.html">网络迁移调试实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="migrator_with_tools.html">网络迁移工具应用实践指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">常见问题</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">语法支持</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax_support.html">静态图语法支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax/operators.html">静态图语法——运算符</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax/statements.html">静态图语法——Python语句</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax/python_builtin_functions.html">静态图语法——Python内置函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/index_support.html">Tensor索引支持</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">环境变量</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/env_var_list.html">环境变量</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq/installation.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/data_processing.html">数据处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/implement_problem.html">执行问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/network_compilation.html">网络编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/operators_compile.html">算子编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/usage_migrate_3rd.html">第三方框架迁移使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/performance_tuning.html">性能调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/precision_tuning.html">精度调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/distributed_parallel.html">分布式并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/inference.html">推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/feature_advice.html">特性咨询</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>PyTorch对比</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/migration_guide/typical_api_comparision.ipynb.txt" rel="nofollow"> 查看页面源码</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars and line breaks on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
    white-space: pre;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt .copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
.jp-RenderedHTMLCommon table,
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
.jp-RenderedHTMLCommon thead,
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
.jp-RenderedHTMLCommon tr,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon td,
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
.jp-RenderedHTMLCommon th,
div.rendered_html th {
  font-weight: bold;
}
.jp-RenderedHTMLCommon tbody tr:nth-child(odd),
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.jp-RenderedHTMLCommon tbody tr:hover,
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="pytorch对比">
<h1>PyTorch对比<a class="headerlink" href="#pytorch对比" title="永久链接至标题"></a></h1>
<p><a class="reference external" href="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/master/zh_cn/migration_guide/mindspore_typical_api_comparision.ipynb"><img alt="下载Notebook" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_notebook.png" /></a> <a class="reference external" href="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/master/zh_cn/migration_guide/mindspore_typical_api_comparision.py"><img alt="下载样例代码" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_download_code.png" /></a> <a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/docs/mindspore/source_zh_cn/migration_guide/typical_api_comparision.ipynb"><img alt="查看源文件" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source.png" /></a></p>
<section id="基础逻辑">
<h2>基础逻辑<a class="headerlink" href="#基础逻辑" title="永久链接至标题"></a></h2>
<p>PyTorch和MindSpore的基础逻辑如下图所示：</p>
<p><img alt="flowchart" src="../_images/pytorch_mindspore_comparison.png" /></p>
<p>可以看到，PyTorch和MindSpore在实现流程中一般都需要网络定义、正向计算、反向计算、梯度更新等步骤。</p>
<ul class="simple">
<li><p>网络定义：在网络定义中，一般会定义出需要的前向网络，损失函数和优化器。在Net()中定义前向网络，PyTorch的网络继承nn.Module；类似地，MindSpore的网络继承nn.Cell。在MindSpore中，损失函数和优化器除了使用MindSpore中提供的外，用户还可以使用自定义的优化器。可参考<a class="reference external" href="https://mindspore.cn/tutorials/zh-CN/master/advanced/modules.html">模型模块自定义</a>。可以使用functional/nn等接口拼接需要的前向网络、损失函数和优化器，详细接口用法可参照<a class="reference external" href="#接口对比">接口对比</a>。</p></li>
<li><p>正向计算：运行实例化后的网络，可以得到logit，将logit和target作为输入计算loss。需要注意的是，如果正向计算的函数有多个输出，在反向计算时需要注意多个输出对于计算结果的影响。</p></li>
<li><p>反向计算：得到loss后，我们可以进行反向计算。在PyTorch中可使用loss.backward()计算梯度，在MindSpore中，先用mindspore.grad()定义出反向传播方程net_backward，再将输入传入net_backward中，即可计算梯度。如果正向计算的函数有多个输出，在反向计算时，可将has_aux设置为True，即可保证只有第一个输出参与求导，其它输出值将直接返回。对于反向计算中接口用法区别详见<a class="reference external" href="#自动微分对比">自动微分对比</a>。</p></li>
<li><p>梯度更新：将计算后的梯度更新到网络的Parameters中。在PyTorch中使用optim.step()；在MindSpore中，将Parameter的梯度传入定义好的optim中，即可完成梯度更新。</p></li>
</ul>
</section>
<section id="接口对比">
<h2>接口对比<a class="headerlink" href="#接口对比" title="永久链接至标题"></a></h2>
<section id="tensorparameter">
<h3>Tensor/Parameter<a class="headerlink" href="#tensorparameter" title="永久链接至标题"></a></h3>
<p>在 PyTorch 中，可以存储数据的对象总共有四种，分别时<code class="docutils literal notranslate"><span class="pre">Tensor</span></code>、<code class="docutils literal notranslate"><span class="pre">Variable</span></code>、<code class="docutils literal notranslate"><span class="pre">Parameter</span></code>、<code class="docutils literal notranslate"><span class="pre">Buffer</span></code>。这四种对象的默认行为均不相同，当我们不需要求梯度时，通常使用 <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>和 <code class="docutils literal notranslate"><span class="pre">Buffer</span></code>两类数据对象，当我们需要求梯度时，通常使用 <code class="docutils literal notranslate"><span class="pre">Variable</span></code> 和 <code class="docutils literal notranslate"><span class="pre">Parameter</span></code> 两类对象。PyTorch 在设计这四种数据对象时，功能上存在冗余（<code class="docutils literal notranslate"><span class="pre">Variable</span></code> 后续会被废弃也说明了这一点）。</p>
<p>MindSpore 优化了数据对象的设计逻辑，仅保留了两种数据对象：<code class="docutils literal notranslate"><span class="pre">Tensor</span></code> 和 <code class="docutils literal notranslate"><span class="pre">Parameter</span></code>，其中 <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> 对象仅参与运算，并不需要对其进行梯度求导和参数更新，而 <code class="docutils literal notranslate"><span class="pre">Parameter</span></code> 数据对象和 PyTorch 的 <code class="docutils literal notranslate"><span class="pre">Parameter</span></code> 意义相同，会根据其属性<code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> 来决定是否对其进行梯度求导和 参数更新。 在网络迁移时，只要是在 PyTorch 中未进行参数更新的数据对象，均可在 MindSpore 中声明为 <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>。</p>
</section>
<section id="functional接口">
<h3>functional接口<a class="headerlink" href="#functional接口" title="永久链接至标题"></a></h3>
<p>详细可参考<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/master/note/api_mapping/pytorch_api_mapping.html">PyTorch与MindSpore API映射表</a>。</p>
</section>
<section id="nn接口">
<h3>nn接口<a class="headerlink" href="#nn接口" title="永久链接至标题"></a></h3>
<section id="nn.Module">
<h4>nn.Module<a class="headerlink" href="#nn.Module" title="永久链接至标题"></a></h4>
<p>使用 PyTorch 构建网络结构时，我们会用到<code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> 类，通常将网络中的元素定义在<code class="docutils literal notranslate"><span class="pre">__init__</span></code> 函数中并对其初始化，将网络的图结构表达定义在<code class="docutils literal notranslate"><span class="pre">forward</span></code> 函数中，通过调用这些类的对象完成整个模型的构建和训练。<code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> 不仅为我们提供了构建图接口，它还为我们提供了一些常用的 <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html">API</a> ，来帮助我们执行更复杂逻辑。</p>
<p>MindSpore 中的 <code class="docutils literal notranslate"><span class="pre">nn.Cell</span></code> 类发挥着和 PyTorch 中 <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> 相同的作用，都是用来构建图结构的模块，MindSpore 也同样提供了丰富的 <a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/master/api_python/nn/mindspore.nn.Cell.html">API</a> 供开发者使用，虽然名字不能一一对应，但 <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> 中常用的功能都可以在<code class="docutils literal notranslate"><span class="pre">nn.Cell</span></code> 中找到映射。</p>
<p>以几个常用方法为例:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 29%" />
<col style="width: 29%" />
<col style="width: 42%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>常用方法</p></th>
<th class="head"><p>nn.Module</p></th>
<th class="head"><p>nn.Cell</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>获取子元素</p></td>
<td><p>named_children</p></td>
<td><p>cells_and_names</p></td>
</tr>
<tr class="row-odd"><td><p>添加子元素</p></td>
<td><p>add_module</p></td>
<td><p>insert_child_to_cell</p></td>
</tr>
<tr class="row-even"><td><p>获取元素的参数</p></td>
<td><p>parameters</p></td>
<td><p>get_parameters</p></td>
</tr>
</tbody>
</table>
</section>
<section id="nn.Dropout">
<h4>nn.Dropout<a class="headerlink" href="#nn.Dropout" title="永久链接至标题"></a></h4>
<p>Dropout 常用于防止训练过拟合，有一个重要的 <strong>概率值</strong> 参数，该参数在 MindSpore 中的意义与 PyTorch 和 TensorFlow 中的意义完全相反。</p>
<p>在 MindSpore 中，概率值对应 Dropout 算子的属性 <code class="docutils literal notranslate"><span class="pre">keep_prob</span></code>，是指输入被保留的概率，<code class="docutils literal notranslate"><span class="pre">1-keep_prob</span></code>是指输入被置 0 的概率。</p>
<p>在 PyTorch 和 TensorFlow 中，概率值分别对应 Dropout 算子的属性 <code class="docutils literal notranslate"><span class="pre">p</span></code>和 <code class="docutils literal notranslate"><span class="pre">rate</span></code>，是指输入被置 0 的概率，与 MindSpore.nn.Dropout 中的 <code class="docutils literal notranslate"><span class="pre">keep_prob</span></code> 意义相反。</p>
<p>更多信息请参考链接： <a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/master/api_python/nn/mindspore.nn.Dropout.html#mindspore.nn.Dropout">MindSpore Dropout</a> 、 <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html">PyTorch Dropout</a> 、 <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/nn/dropout">TensorFlow Dropout</a></p>
</section>
<section id="nn.BatchNorm2d">
<h4>nn.BatchNorm2d<a class="headerlink" href="#nn.BatchNorm2d" title="永久链接至标题"></a></h4>
<p>BatchNorm 是 CV 领域比较特殊的正则化方法，它在训练和推理的过程中有着不同计算流程，通常由算子属性控制。MindSpore 和 PyTorch 的 BatchNorm 在这一点上使用了两种不同的参数组。</p>
<ul class="simple">
<li><p>差异一</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">torch.nn.BatchNorm2d</span></code> 在不同参数下的状态</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 80%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>training</p></th>
<th class="head"><p>track_running_stats</p></th>
<th class="head"><p>状态</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>True</p></td>
<td><p>True</p></td>
<td><p>期望中训练的状态，running_mean 和 running_var 会跟踪整个训练过程中 batch 的统计特性，而每组输入数据用当前 batch 的 mean 和 var 统计特性做归一化，然后再更新 running_mean 和 running_var。</p></td>
</tr>
<tr class="row-odd"><td><p>True</p></td>
<td><p>False</p></td>
<td><p>每组输入数据会根据当前 batch 的统计特性做归一化，但不会有 running_mean 和 running_var 参数了。</p></td>
</tr>
<tr class="row-even"><td><p>False</p></td>
<td><p>True</p></td>
<td><p>期望中推理的状态，BN 使用 running_mean 和 running_var 做归一化，并且不会对其进行更新。</p></td>
</tr>
<tr class="row-odd"><td><p>False</p></td>
<td><p>False</p></td>
<td><p>效果同第二点，只不过处于推理状态，不会学习 weight 和 bias 两个参数。一般不采用该状态。</p></td>
</tr>
</tbody>
</table>
<p><code class="docutils literal notranslate"><span class="pre">mindspore.nn.BatchNorm2d</span></code> 在不同参数下的状态</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 11%" />
<col style="width: 89%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>use_batch_statistics</p></th>
<th class="head"><p>状态</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>True</p></td>
<td><p>期望中训练的状态，moving_mean 和 moving_var 会跟踪整个训练过程中 batch 的统计特性，而每组输入数据用当前 batch 的 mean 和 var 统计特性做归一化，然后再更新 moving_mean 和 moving_var。</p></td>
</tr>
<tr class="row-odd"><td><p>Fasle</p></td>
<td><p>期望中推理的状态，BN 使用 moving_mean 和 moving_var 做归一化，并且不会对其进行更新。</p></td>
</tr>
<tr class="row-even"><td><p>None</p></td>
<td><p>自动设置 use_batch_statistics。如果是训练，use_batch_statistics=True，如果是推理，use_batch_statistics=False。</p></td>
</tr>
</tbody>
</table>
<p>通过比较可以发现，<code class="docutils literal notranslate"><span class="pre">mindspore.nn.BatchNorm2d</span></code> 相比 <code class="docutils literal notranslate"><span class="pre">torch.nn.BatchNorm2d</span></code>，少了两种冗余的状态，仅保留了最常用的训练和推理两种状态。</p>
<ul class="simple">
<li><p>差异二</p></li>
</ul>
<p>BatchNorm系列算子 的 momentum 参数在 MindSpore 和 PyTorch 表示的意义相反，关系为：</p>
<div class="math notranslate nohighlight">
\[momentum_{pytorch} = 1 - momentum_{mindspore}\]</div>
<p>参考链接：<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/master/api_python/nn/mindspore.nn.BatchNorm2d.html">mindspore.nn.BatchNorm2d</a>，<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html">torch.nn.BatchNorm2d</a></p>
</section>
</section>
<section id="其他接口">
<h3>其他接口<a class="headerlink" href="#其他接口" title="永久链接至标题"></a></h3>
<section id="torch.device">
<h4>torch.device<a class="headerlink" href="#torch.device" title="永久链接至标题"></a></h4>
<p>PyTorch 在构建模型时，通常会利用 torch.device 指定模型和数据绑定的设备，是在 CPU 还是 GPU 上，如果支持多 GPU，还可以指定具体的 GPU 序号。绑定相应的设备后，需要将模型和数据部署到对应设备，代码如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="c1"># bind to the GPU 0 if GPU is available, otherwise bind to CPU</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span> <span class="c1"># 单 GPU 或者 CPU</span>
<span class="c1"># deploy model to specified hardware</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># deploy data to specified hardware</span>
<span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># distribute training on multiple GPUs</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># set available device</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;CUDA_VISIBLE_DEVICE&#39;</span><span class="p">]</span><span class="o">=</span><span class="s1">&#39;1&#39;</span>
    <span class="n">model</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
</pre></div>
</div>
<p>而在 MindSpore 中，我们通过 context 中 的 device_target 参数 指定模型绑定的设备，device_id 指定设备的序号。与 PyTorch 不同的是，一旦设备设置成功，输入数据和模型会默认拷贝到指定的设备中执行，不需要也无法再改变数据和模型所运行的设备类型。代码如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s1">&#39;Ascend&#39;</span><span class="p">,</span> <span class="n">device_id</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># define net</span>
<span class="n">Model</span> <span class="o">=</span> <span class="o">..</span>
<span class="c1"># define dataset</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="o">..</span>
<span class="c1"># training, automatically deploy to Ascend according to device_target</span>
<span class="n">Model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<p>此外，网络运行后返回的 <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> 默认均拷贝到 CPU 设备，可以直接对该 <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> 进行访问和修改，包括转成 <code class="docutils literal notranslate"><span class="pre">numpy</span></code> 格式，无需像 PyTorch 一样需要先执行 <code class="docutils literal notranslate"><span class="pre">tensor.cpu</span></code> 再转换成 numpy 格式。</p>
</section>
</section>
</section>
<section id="参数初始化对比">
<h2>参数初始化对比<a class="headerlink" href="#参数初始化对比" title="永久链接至标题"></a></h2>
<section id="默认权重初始化不同">
<h3>默认权重初始化不同<a class="headerlink" href="#默认权重初始化不同" title="永久链接至标题"></a></h3>
<p>我们知道权重初始化对网络的训练十分重要。每个nn接口一般会有一个隐式的声明权重，在不同的框架中，隐式的声明权重可能不同。即使功能一致，隐式声明的权重初始化方式分布如果不同，也会对训练过程产生影响，甚至无法收敛。</p>
<p>常见隐式声明权重的nn接口：Conv、Dense(Linear)、Embedding、LSTM 等，其中区别较大的是 Conv 类和 Dense 两种接口。MindSpore和PyTorch的 Conv 类和 Dense 隐式声明的权重和偏差初始化方式分布相同。</p>
<ul>
<li><p>Conv2d</p>
<ul class="simple">
<li><p>mindspore.nn.Conv2d的weight为：<span class="math notranslate nohighlight">\(\mathcal{U} (-\sqrt{k},\sqrt{k} )\)</span>，bias为：<span class="math notranslate nohighlight">\(\mathcal{U} (-\sqrt{k},\sqrt{k} )\)</span>。</p></li>
<li><p>torch.nn.Conv2d的weight为：<span class="math notranslate nohighlight">\(\mathcal{U} (-\sqrt{k},\sqrt{k} )\)</span>，bias为：<span class="math notranslate nohighlight">\(\mathcal{U} (-\sqrt{k},\sqrt{k} )\)</span>。</p></li>
<li><p>tf.keras.Layers.Conv2D的weight为：glorot_uniform，bias为：zeros。</p></li>
</ul>
<p>其中，<span class="math notranslate nohighlight">\(k=\frac{groups}{c_{in}*\prod_{i}^{}{kernel\_size[i]}}\)</span></p>
</li>
<li><p>Dense(Linear)</p>
<ul class="simple">
<li><p>mindspore.nn.Dense的weight为：<span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k},\sqrt{k})\)</span>，bias为：<span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k},\sqrt{k} )\)</span>。</p></li>
<li><p>torch.nn.Linear的weight为：<span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k},\sqrt{k})\)</span>，bias为：<span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k},\sqrt{k} )\)</span>。</p></li>
<li><p>tf.keras.Layers.Dense的weight为：glorot_uniform，bias为：zeros。</p></li>
</ul>
<p>其中，<span class="math notranslate nohighlight">\(k=\frac{groups}{in\_features}\)</span></p>
</li>
</ul>
<p>对于没有正则化的网络，如没有 BatchNorm 算子的 GAN 网络，梯度很容易爆炸或者消失，权重初始化就显得十分重要，各位开发者应注意权重初始化带来的影响。</p>
</section>
<section id="参数初始化api对比">
<h3>参数初始化API对比<a class="headerlink" href="#参数初始化api对比" title="永久链接至标题"></a></h3>
<p>每个 <code class="docutils literal notranslate"><span class="pre">torch.nn.init</span></code> 的API都可以和MindSpore一一对应，除了 <code class="docutils literal notranslate"><span class="pre">torch.nn.init.calculate_gain()</span></code> 之外。更多信息，请查看<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/master/note/api_mapping/pytorch_api_mapping.html">PyTorch与MindSpore API映射表</a>。</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">gain</span></code> 用来衡量非线性关系对于数据标准差的影响。由于非线性会影响数据的标准差，可能会导致梯度爆炸或消失。</p>
</div></blockquote>
<section id="torch.nn.init">
<h4>torch.nn.init<a class="headerlink" href="#torch.nn.init" title="永久链接至标题"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">torch.nn.init</span></code> 需要一个Tensor作为输入，将输入的Tensor原地修改为目标结果。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>运行以上代码之后，x将不在是非初始化状态，其元素将服从均匀分布。</p>
</section>
<section id="mindspore.common.initializer">
<h4>mindspore.common.initializer<a class="headerlink" href="#mindspore.common.initializer" title="永久链接至标题"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">mindspore.common.initializer</span></code> 用于在并行模式中延迟Tensor的数据的初始化。只有在调用了 <code class="docutils literal notranslate"><span class="pre">init_data()</span></code> 之后，才会使用指定的 <code class="docutils literal notranslate"><span class="pre">init</span></code> 来初始化Tensor的数据。每个Tensor只能使用一次 <code class="docutils literal notranslate"><span class="pre">init_data()</span></code> 。</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span>
<span class="kn">from</span> <span class="nn">mindspore.common.initializer</span> <span class="kn">import</span> <span class="n">initializer</span><span class="p">,</span> <span class="n">Uniform</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">initializer</span><span class="p">(</span><span class="n">Uniform</span><span class="p">(),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>在运行以上代码之后，<code class="docutils literal notranslate"><span class="pre">x</span></code> 其实尚未完成初始化。如果此时 <code class="docutils literal notranslate"><span class="pre">x</span></code> 被用来计算，将会作为0来处理。然而，在打印时，会自动调用 <code class="docutils literal notranslate"><span class="pre">init_data()</span></code> 。</p>
</section>
</section>
</section>
<section id="自动微分对比">
<h2>自动微分对比<a class="headerlink" href="#自动微分对比" title="永久链接至标题"></a></h2>
<p>MindSpore 和 PyTorch 都提供了自动微分功能，让我们在定义了正向网络后，可以通过简单的接口调用实现自动反向传播以及梯度更新。但需要注意的是，MindSpore 和 PyTorch 构建反向图的逻辑是不同的，这个差异也会带来 API 设计上的不同。</p>
<section id="pytorch的自动微分">
<h3>PyTorch的自动微分<a class="headerlink" href="#pytorch的自动微分" title="永久链接至标题"></a></h3>
<p>我们知道 PyTorch 是基于计算路径追踪的自动微分，当我们定义一个网络结构后， 并不会建立反向图，而是在执行正向图的过程中，<code class="docutils literal notranslate"><span class="pre">Variable</span></code> 或 <code class="docutils literal notranslate"><span class="pre">Parameter</span></code> 记录每一个正向计算对应的反向函数，并生成一个动态计算图，用于后续的梯度计算。当在最终的输出处调用 <code class="docutils literal notranslate"><span class="pre">backward</span></code> 时，就会从根节点到叶节点应用链式法则计算梯度。PyTorch 的动态计算图所存储的节点实际是 <code class="docutils literal notranslate"><span class="pre">Function</span></code> 函数对象，每当对 <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> 执行一步运算后，就会产生一个 <code class="docutils literal notranslate"><span class="pre">Function</span></code> 对象，它记录了反向传播中必要的信息。反向传播过程中，<code class="docutils literal notranslate"><span class="pre">autograd</span></code>
引擎会按照逆序，通过 <code class="docutils literal notranslate"><span class="pre">Function</span></code> 的 <code class="docutils literal notranslate"><span class="pre">backward</span></code> 依次计算梯度。 这一点我们可以通过 <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> 的隐藏属性查看。</p>
<p>例如，运行以下代码：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">1</span>
<span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>会自动获取<code class="docutils literal notranslate"><span class="pre">x</span></code>的定义到获取输出<code class="docutils literal notranslate"><span class="pre">y</span></code>这个过程对x的梯度结果。</p>
<p>需要注意的是PyTorch的backward是累计的，更新完之后需要清空optimizer。</p>
</section>
<section id="mindspore的自动微分">
<h3>MindSpore的自动微分<a class="headerlink" href="#mindspore的自动微分" title="永久链接至标题"></a></h3>
<p>在图模式下，MindSpore 的自动微分是基于图结构的微分，和 PyTorch 不同，它不会在正向计算过程中记录任何信息，仅仅执行正常的计算流程（在PyNative模式下和 PyTorch 类似）。那么问题来了，如果整个正向计算都结束了，MindSpore 也没有记录任何信息，那它是如何知道反向传播怎么执行的呢？</p>
<p>MindSpore 在做自动微分时，需要传入正向图结构，自动微分的过程就是通过对正向图的分析从而得到反向传播信息，自动微分的结果与正向计算中具体的数值无关，仅和正向图结构有关。通过对正向图的自动微分，我们得到了反向传播过程，而这个反向传播过程其实也是通过一个图结构来表达，也就是反向图。将反向图添加到用户定义的正向图之后，组成一个最终的计算图。不过后添加的反向图和其中的反向算子我们并不感知，也无法手动添加，只能通过 MindSpore 为我们提供的接口自动添加，这样做也避免了我们在反向构图时引入错误。</p>
<p>最终，我们看似仅执行了正向图，其实图结构里既包含了正向算子，又包含了 MindSpore 为我们添加的反向算子，也就是说，MindSpore 在我们定义的正向图后面又新加了一个看不见的 <code class="docutils literal notranslate"><span class="pre">Cell</span></code>，这个 <code class="docutils literal notranslate"><span class="pre">Cell</span></code> 里都是根据正向图推导出来的反向算子。</p>
<p>而这个帮助我们构建反向图的接口就是 <a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/master/api_python/mindspore/mindspore.grad.html">grad</a> ：</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">GradNetWrtX</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GradNetWrtX</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">net</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">gradient_function</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">gradient_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>查看文档介绍我们可以发现，grad 并不是一个算子，它的输入输出并不是 Tensor，而是我们定义的正向图和自动微分得到的反向图。为什么输入是一个图结构呢？因为构建反向图并不需要知道具体的输入数据是什么，只要知道正向图的结构就行了，有了正向图就可以推算出反向图结构，之后我们可以把正向图+反向图当成一个新的计算图来对待，这个新的计算图就像是一个函数，对于你输入的任何一组数据，它不仅能计算出正向的输出，还能计算出所有权重的梯度，由于图结构是固定的，并不保存中间变量，所以这个图结构可以被反复调用。</p>
<p>同理，之后我们再给网络加上优化器结构时，优化器也会加上优化器相关的算子，也就是再给这个计算图加点我们不感知的优化器算子，最终，计算图就构建完成。</p>
<p>在 MindSpore 中，大部分操作都会最终转换成真实的算子操作，最终加入到计算图中，因此，我们实际执行的计算图中算子的数量远多于我们最开始定义的计算图中算子的数量。</p>
<p>在MindSpore中，提供了<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/master/api_python/nn/mindspore.nn.TrainOneStepCell.html">TrainOneStepCell</a>和<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/master/api_python/nn/mindspore.nn.TrainOneStepWithLossScaleCell.html">TrainOneStepWithLossScaleCell</a>这两个接口来包装整个训练流程，如果在常规的训练流程外有其他的操作，如梯度裁剪、规约、中间变量返回等，需要自定义训练的Cell，详情请参考<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/master/migration_guide/model_development/training_and_evaluation_procession.html">训练及推理流程</a>。</p>
</section>
<section id="梯度求导">
<h3>梯度求导<a class="headerlink" href="#梯度求导" title="永久链接至标题"></a></h3>
<p>梯度求导涉及的算子和接口差异主要是由 MindSpore 和 PyTorch 自动微分原理不同引起的。</p>
</section>
<section id="torch.autograd.backward">
<h3>torch.autograd.backward<a class="headerlink" href="#torch.autograd.backward" title="永久链接至标题"></a></h3>
<p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.autograd.backward.html">torch.autograd.backward</a>对于一个标量，调用它的backward方法后会根据链式法则自动计算出叶子节点的梯度值。对于向量和矩阵，需要定义grad_tensor来计算矩阵的梯度。 通常在调用一次backward后，PyTorch会自动把计算图销毁，所以要想对某个变量重复调用backward，则需要将retain_graph参数设置为True。 若需要计算更高阶的梯度，需要将create_graph设置为True。 z.backward()和torch.autograd.backward(z)两种表达等价。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== tensor.backward ===&quot;</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="n">y</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;x.grad before backward&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y.grad before backward&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
<span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;z&quot;</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;x.grad&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y.grad&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== torch.autograd.backward ===&quot;</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="n">y</span>
<span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;z&quot;</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;x.grad&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y.grad&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
=== tensor.backward ===
x.grad before backward None
y.grad before backward None
z tensor(3., grad_fn=&lt;AddBackward0&gt;)
x.grad tensor(2.)
y.grad tensor(1.)
=== torch.autograd.backward ===
z tensor(3., grad_fn=&lt;AddBackward0&gt;)
x.grad tensor(2.)
y.grad tensor(1.)
</pre></div></div>
</div>
<p>可以看到，在调用backward函数之前，x.grad和y.grad函数为空。而backward计算过后，x.grad和y.grad分别代表导数计算后的值。</p>
<p>该接口在MindSpore中用mindspore.grad实现。上述PyTorch用例可转化为：</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== mindspore.grad ===&quot;</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">net</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="n">y</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">grad_position</span><span class="o">=</span><span class="mi">0</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;out&quot;</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
<span class="n">out1</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">grad_position</span><span class="o">=</span><span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;out1&quot;</span><span class="p">,</span> <span class="n">out1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
=== mindspore.grad ===
out 2.0
out1 1.0
</pre></div></div>
</div>
<p>若上述net有多个输出，需要注意网络多输出对于求梯度的影响。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== mindspore.grad 多个output ===&quot;</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">net</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">grad_position</span><span class="o">=</span><span class="mi">0</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;out&quot;</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
<span class="n">out1</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">grad_position</span><span class="o">=</span><span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;out1&quot;</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
=== mindspore.grad 多个output ===
out 3.0
out1 3.0
</pre></div></div>
</div>
<p>PyTorch不支持此种表达：</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== torch.autograd.backward 不支持多个output ===&quot;</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="n">y</span>
<span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;z&quot;</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;x.grad&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y.grad&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
=== torch.autograd.backward 不支持多个output ===
z tensor(3., grad_fn=&lt;AddBackward0&gt;)
x.grad tensor(2.)
y.grad tensor(1.)
</pre></div></div>
</div>
<p>因此， 若要在MindSpore只对第一个输出求梯度，在MindSpore中需要使用has_aux参数。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== mindspore.grad has_aux ===&quot;</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">net</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span>
<span class="n">grad_fcn</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">grad_position</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">grad_fcn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;out&quot;</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
<span class="n">grad_fcn1</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">grad_position</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">grad_fcn1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;out&quot;</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
=== mindspore.grad has_aux ===
out 2.0
out 1.0
</pre></div></div>
</div>
</section>
<section id="torch.autograd.grad">
<h3>torch.autograd.grad<a class="headerlink" href="#torch.autograd.grad" title="永久链接至标题"></a></h3>
<p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.autograd.grad.html">torch.autograd.grad</a>此接口与torch.autograd.backward基本一致。两者的区别为：前者是直接修改各个 Tensor 的 grad 属性，后者是返回参数的梯度值列表。因此在迁移到MindSpore时，可同样参考上述用例。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== torch.autograd.grad ===&quot;</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="n">y</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">out1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;out&quot;</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;out1&quot;</span><span class="p">,</span> <span class="n">out1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
=== torch.autograd.grad ===
out (tensor(2.),)
out1 (tensor(1.),)
</pre></div></div>
</div>
</section>
<section id="torch.no_grad">
<h3>torch.no_grad<a class="headerlink" href="#torch.no_grad" title="永久链接至标题"></a></h3>
<p>在 PyTorch 中，默认情况下，执行正向计算时会记录反向传播所需的信息，在推理阶段或无需反向传播网络中，这一操作是冗余的，会额外耗时，因此，PyTorch 提供了<code class="docutils literal notranslate"><span class="pre">torch.no_grad</span></code> 来取消该过程。</p>
<p>而 MindSpore 只有在调用<code class="docutils literal notranslate"><span class="pre">grad</span></code>才会根据正向图结构来构建反向图，正向执行时不会记录任何信息，所以 MindSpore 并不需要该接口，也可以理解为 MindSpore 的正向计算均在<code class="docutils literal notranslate"><span class="pre">torch.no_grad</span></code> 情况下进行的。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== torch.no_grad ===&quot;</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="n">y</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;z.requires_grad&quot;</span><span class="p">,</span> <span class="n">z</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="n">y</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;z.requires_grad&quot;</span><span class="p">,</span> <span class="n">z</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
=== torch.no_grad ===
z.requires_grad True
z.requires_grad False
</pre></div></div>
</div>
</section>
<section id="torch.enable_grad">
<h3>torch.enable_grad<a class="headerlink" href="#torch.enable_grad" title="永久链接至标题"></a></h3>
<p>若 PyTorch 开启了 <code class="docutils literal notranslate"><span class="pre">torch.no_grad</span></code> 禁用了梯度计算，可以使用此接口启用。</p>
<p>而 MindSpore 只有在调用<code class="docutils literal notranslate"><span class="pre">grad</span></code>才会根据正向图结构来构建反向图，正向执行时不会记录任何信息，所以 MindSpore 并不需要该接口，也可以理解为 MindSpore 的反向计算均在<code class="docutils literal notranslate"><span class="pre">torch.enable_grad</span></code> 情况下进行的。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== torch.enable_grad ===&quot;</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="n">y</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;z.requires_grad&quot;</span><span class="p">,</span> <span class="n">z</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">enable_grad</span><span class="p">():</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="n">y</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;z.requires_grad&quot;</span><span class="p">,</span> <span class="n">z</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
=== torch.enable_grad ===
z.requires_grad False
z.requires_grad True
</pre></div></div>
</div>
</section>
<section id="retain-graph">
<h3>retain_graph<a class="headerlink" href="#retain-graph" title="永久链接至标题"></a></h3>
<p>由于 PyTorch 是基于函数式的自动微分，所以默认每次执行完反向传播后都会自动清除记录的信息，从而进行下一次迭代。这就会导致当我们想再次利用这些反向图和梯度信息时，由于已被删除而获取失败。因此，PyTorch 提供了<code class="docutils literal notranslate"><span class="pre">backward(retain_graph=True)</span></code> 来主动保留这些信息。</p>
<p>而 MindSpore 则不需要这个功能，MindSpore 是基于计算图的自动微分，反向图信息在调用<code class="docutils literal notranslate"><span class="pre">grad</span></code>后便永久的记录在计算图中，只要再次调用计算图就可以获取梯度信息。</p>
</section>
<section id="高阶导数">
<h3>高阶导数<a class="headerlink" href="#高阶导数" title="永久链接至标题"></a></h3>
<p>基于计算图的自动微分还有一个好处，我们可以很方便的实现高阶求导。第一次对正向图执行<code class="docutils literal notranslate"><span class="pre">grad</span></code> 操作后，我们可以得到一阶导，此时计算图被更新为正向图+一阶导的反向图结构，但我们再次对更新后的计算图执行 <code class="docutils literal notranslate"><span class="pre">grad</span></code>后，我们就可以得到二阶导，以此类推，通过基于计算图的自动微分，我们很容易求得一个网络的高阶导数。</p>
</section>
</section>
<section id="优化器对比">
<h2>优化器对比<a class="headerlink" href="#优化器对比" title="永久链接至标题"></a></h2>
<section id="优化器支持差异">
<h3>优化器支持差异<a class="headerlink" href="#优化器支持差异" title="永久链接至标题"></a></h3>
<p>PyTorch和MindSpore同时支持的优化器异同比较详见<a class="reference external" href="https://mindspore.cn/docs/zh-CN/master/note/api_mapping/pytorch_api_mapping.html#torch-optim">API映射表</a>。MindSpore暂不支持的优化器：LBFGS，NAdam，RAdam。</p>
</section>
<section id="优化器的执行和使用差异">
<h3>优化器的执行和使用差异<a class="headerlink" href="#优化器的执行和使用差异" title="永久链接至标题"></a></h3>
<p>PyTorch单步执行优化器时，一般需要手动执行 <code class="docutils literal notranslate"><span class="pre">zero_grad()</span></code> 方法将历史梯度设置为0(或None)，然后使用 <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code> 计算当前训练step的梯度，最后调用优化器的 <code class="docutils literal notranslate"><span class="pre">step()</span></code> 方法实现网络权重的更新；</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">ExponentialLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<p>MindSpore中优化器的使用，只需要直接对梯度进行计算，然后使用 <code class="docutils literal notranslate"><span class="pre">optimizer(grads)</span></code> 执行网络权重的更新。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">grad_fn</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="n">forward_fn</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">_</span><span class="p">),</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">grad_fn</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
</section>
<section id="超参差异">
<h3>超参差异<a class="headerlink" href="#超参差异" title="永久链接至标题"></a></h3>
<section id="超参名称">
<h4>超参名称<a class="headerlink" href="#超参名称" title="永久链接至标题"></a></h4>
<p>网络权重和学习率入参名称异同：</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 21%" />
<col style="width: 18%" />
<col style="width: 34%" />
<col style="width: 26%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>参数</p></th>
<th class="head"><p>PyTorch</p></th>
<th class="head"><p>MindSpore</p></th>
<th class="head"><p>差异</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>网络权重</p></td>
<td><p>params</p></td>
<td><p>params</p></td>
<td><p>参数名相同</p></td>
</tr>
<tr class="row-odd"><td><p>学习率</p></td>
<td><p>lr</p></td>
<td><p>learning_rate</p></td>
<td><p>参数名不同</p></td>
</tr>
</tbody>
</table>
<p>MindSpore：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
</pre></div>
</div>
<p>PyTorch：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">optim</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="超参配置方式">
<h4>超参配置方式<a class="headerlink" href="#超参配置方式" title="永久链接至标题"></a></h4>
<ul>
<li><p>参数不分组：</p>
<p><code class="docutils literal notranslate"><span class="pre">params</span></code> 入参支持类型不同： PyTorch入参类型为 <code class="docutils literal notranslate"><span class="pre">iterable(Tensor)</span></code> 和 <code class="docutils literal notranslate"><span class="pre">iterable(dict)</span></code>，支持迭代器类型； MindSpore入参类型为 <code class="docutils literal notranslate"><span class="pre">list(Parameter)</span></code>，<code class="docutils literal notranslate"><span class="pre">list(dict)</span></code>，不支持迭代器。</p>
<p>其他超参配置及支持差异详见<a class="reference external" href="https://mindspore.cn/docs/zh-CN/master/note/api_mapping/pytorch_api_mapping.html#torch-optim">API映射表</a>。</p>
</li>
<li><p>参数分组：</p>
<p>PyTorch支持所有参数分组：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">([</span>
            <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">parameters</span><span class="p">()},</span>
            <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">classifier</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">}</span>
        <span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</pre></div>
</div>
<p>MindSpore仅支持特定key分组：“params”，“lr”，“weight_decay”，“grad_centralization”，“order_params”。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">conv_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()))</span>
<span class="n">no_conv_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()))</span>
<span class="n">group_params</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">conv_params</span><span class="p">,</span> <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">0.02</span><span class="p">},</span>
        <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">no_conv_params</span><span class="p">}]</span>

<span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">group_params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="运行时超参修改">
<h4>运行时超参修改<a class="headerlink" href="#运行时超参修改" title="永久链接至标题"></a></h4>
<p>PyTorch支持在训练过程中修改任意的优化器参数，并提供了 <code class="docutils literal notranslate"><span class="pre">LRScheduler</span></code> 用于动态修改学习率；</p>
<p>MindSpore当前不支持训练过程中修改优化器参数，但提供了修改学习率和权重衰减的方式，使用方式详见<a class="reference external" href="#学习率">学习率</a>和<a class="reference external" href="#权重衰减">权重衰减</a>章节。</p>
</section>
</section>
<section id="权重衰减">
<h3>权重衰减<a class="headerlink" href="#权重衰减" title="永久链接至标题"></a></h3>
<p>PyTorch中修改 <code class="docutils literal notranslate"><span class="pre">weight_decay</span></code>：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">optim</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">decay_factor</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
        <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;weight_decay&quot;</span><span class="p">]</span> <span class="o">*=</span> <span class="n">decay_factor</span>
</pre></div>
</div>
<p>MindSpore中实现动态weight decay：用户可以继承 <code class="docutils literal notranslate"><span class="pre">Cell</span></code> 自定义动态weight decay的类，传入优化器中。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ExponentialWeightDecay</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_decay</span><span class="p">,</span> <span class="n">decay_rate</span><span class="p">,</span> <span class="n">decay_steps</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ExponentialWeightDecay</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span> <span class="o">=</span> <span class="n">weight_decay</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decay_rate</span> <span class="o">=</span> <span class="n">decay_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decay_steps</span> <span class="o">=</span> <span class="n">decay_steps</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">global_step</span><span class="p">):</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">global_step</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">decay_steps</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span> <span class="o">*</span> <span class="n">ops</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">decay_rate</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>

<span class="n">weight_decay</span> <span class="o">=</span> <span class="n">ExponentialWeightDecay</span><span class="p">(</span><span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">decay_steps</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="优化器状态的保存与加载">
<h3>优化器状态的保存与加载<a class="headerlink" href="#优化器状态的保存与加载" title="永久链接至标题"></a></h3>
<p>PyTorch的优化器模块提供了 <code class="docutils literal notranslate"><span class="pre">state_dict()</span></code> 用于优化器状态的查看及保存，<code class="docutils literal notranslate"><span class="pre">load_state_dict</span></code> 用于优化器状态的加载。</p>
<ul>
<li><p>优化器保存，可以使用 <code class="docutils literal notranslate"><span class="pre">torch.save()</span></code> 把获取到的 <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> 保存到pkl文件中：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">save_path</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>优化器加载，可以使用 <code class="docutils literal notranslate"><span class="pre">torch.load()</span></code> 加载保存的 <code class="docutils literal notranslate"><span class="pre">state_dict</span></code>，然后使用 <code class="docutils literal notranslate"><span class="pre">load_state_dict</span></code> 将获取到的 <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> 加载到优化器中：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">state_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">save_path</span><span class="p">)</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
<p>MindSpore的优化器模块继承自 <code class="docutils literal notranslate"><span class="pre">Cell</span></code>，优化器的保存与加载和网络的保存与加载方式相同，通常情况下配合 <code class="docutils literal notranslate"><span class="pre">save_checkpoint</span></code> 与<code class="docutils literal notranslate"><span class="pre">load_checkpoint</span></code> 使用。</p>
<ul>
<li><p>优化器保存，可以使用 <code class="docutils literal notranslate"><span class="pre">mindspore.save_checkpoint()</span></code> 将优化器实例保存到ckpt文件中：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">state_dict</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span> <span class="n">save_path</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>优化器加载，可以使用 <code class="docutils literal notranslate"><span class="pre">mindspore.load_checkpoint()</span></code> 加载保存的ckpt文件，然后使用 <code class="docutils literal notranslate"><span class="pre">load_param_into_net</span></code> 将获取到的 <code class="docutils literal notranslate"><span class="pre">param_dict</span></code> 加载到优化器中：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">param_dict</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">(</span><span class="n">save_path</span><span class="p">)</span>
<span class="n">mindspore</span><span class="o">.</span><span class="n">load_param_into_net</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</section>
</section>
<section id="学习率策略对比">
<h2>学习率策略对比<a class="headerlink" href="#学习率策略对比" title="永久链接至标题"></a></h2>
<section id="动态学习率差异">
<h3>动态学习率差异<a class="headerlink" href="#动态学习率差异" title="永久链接至标题"></a></h3>
<p>PyTorch中定义了 <code class="docutils literal notranslate"><span class="pre">LRScheduler</span></code> 类用于对学习率进行管理。使用动态学习率时，将 <code class="docutils literal notranslate"><span class="pre">optimizer</span></code> 实例传入 <code class="docutils literal notranslate"><span class="pre">LRScheduler</span></code> 子类中，通过循环调用 <code class="docutils literal notranslate"><span class="pre">scheduler.step()</span></code> 执行学习率修改，并将修改同步至优化器中。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">ExponentialLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<p>MindSpore中的动态学习率有 <code class="docutils literal notranslate"><span class="pre">Cell</span></code> 和 <code class="docutils literal notranslate"><span class="pre">list</span></code> 两种实现方式，两种类型的动态学习率使用方式一致，都是在实例化完成之后传入优化器，前者在内部的 <code class="docutils literal notranslate"><span class="pre">construct</span></code> 中进行每一步学习率的计算，后者直接按照计算逻辑预生成学习率列表，训练过程中内部实现学习率的更新。具体请参考<a class="reference external" href="https://mindspore.cn/docs/zh-CN/master/api_python/mindspore.nn.html#%E5%8A%A8%E6%80%81%E5%AD%A6%E4%B9%A0%E7%8E%87">动态学习率</a>。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">polynomial_decay_lr</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PolynomialDecayLR</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">end_learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">decay_steps</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">power</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">polynomial_decay_lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>

<span class="n">grad_fn</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="n">forward_fn</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">_</span><span class="p">),</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">grad_fn</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
</section>
<section id="自定义学习率差异">
<h3>自定义学习率差异<a class="headerlink" href="#自定义学习率差异" title="永久链接至标题"></a></h3>
<p>PyTorch的动态学习率模块 <code class="docutils literal notranslate"><span class="pre">LRScheduler</span></code> 提供了<code class="docutils literal notranslate"><span class="pre">LambdaLR</span></code> 接口供用户自定义学习率调整规则，用户通过传入lambda表达式或自定义函数实现学习率指定。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">lbd</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">epoch</span><span class="p">:</span> <span class="n">epoch</span> <span class="o">//</span> <span class="mi">5</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">LambdaLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">lr_lambda</span><span class="o">=</span><span class="n">lbd</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">train</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">validate</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<p>MindSpore未提供类似的lambda接口，自定义学习率调整策略可以通过自定义函数或自定义 <code class="docutils literal notranslate"><span class="pre">LearningRateSchedule</span></code> 来实现。</p>
<p>方式一：定义python函数指定计算逻辑，返回学习率列表：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">dynamic_lr</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">total_step</span><span class="p">,</span> <span class="n">step_per_epoch</span><span class="p">):</span>
    <span class="n">lrs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">total_step</span><span class="p">):</span>
        <span class="n">current_epoch</span> <span class="o">=</span> <span class="n">i</span> <span class="o">//</span> <span class="n">step_per_epoch</span>
        <span class="n">factor</span> <span class="o">=</span> <span class="n">current_epoch</span> <span class="o">//</span> <span class="mi">5</span>
        <span class="n">lrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lr</span> <span class="o">*</span> <span class="n">factor</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">lrs</span>

<span class="n">decay_lr</span> <span class="o">=</span> <span class="n">dynamic_lr</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">total_step</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">step_per_epoch</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">decay_lr</span><span class="p">)</span>
</pre></div>
</div>
<p>方式二：继承 <code class="docutils literal notranslate"><span class="pre">LearningRateSchedule</span></code>，在 <code class="docutils literal notranslate"><span class="pre">construct</span></code> 方法中定义变化策略:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DynamicDecayLR</span><span class="p">(</span><span class="n">LearningRateSchedule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">step_per_epoch</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DynamicDecayLR</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step_per_epoch</span> <span class="o">=</span> <span class="n">step_per_epoch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cast</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">global_step</span><span class="p">):</span>
        <span class="n">current_epoch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">global_step</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">//</span> <span class="n">step_per_epoch</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="p">(</span><span class="n">current_epoch</span> <span class="o">//</span> <span class="mi">5</span><span class="p">)</span>

<span class="n">decay_lr</span> <span class="o">=</span> <span class="n">DynamicDecayLR</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">step_per_epoch</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">decay_lr</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="学习率获取">
<h3>学习率获取<a class="headerlink" href="#学习率获取" title="永久链接至标题"></a></h3>
<p>PyTorch：</p>
<ul class="simple">
<li><p>固定学习率情况下，通常通过 <code class="docutils literal notranslate"><span class="pre">optimizer.state_dict()</span></code> 进行学习率的查看和打印，例如参数分组时，对于第n个参数组，使用 <code class="docutils literal notranslate"><span class="pre">optimizer.state_dict()['param_groups'][n]['lr']</span></code>，参数不分组时，使用 <code class="docutils literal notranslate"><span class="pre">optimizer.state_dict()['param_groups'][0]['lr']</span></code>；</p></li>
<li><p>动态学习率情况下，可以使用 <code class="docutils literal notranslate"><span class="pre">LRScheduler</span></code> 的 <code class="docutils literal notranslate"><span class="pre">get_lr</span></code> 方法获取当前学习率，或使用 <code class="docutils literal notranslate"><span class="pre">print_lr</span></code> 方法打印学习率。</p></li>
</ul>
<p>MindSpore：</p>
<ul class="simple">
<li><p>目前未提供直接查看学习率的接口，后续版本中会针对此问题进行修复。</p></li>
</ul>
</section>
<section id="学习率更新">
<h3>学习率更新<a class="headerlink" href="#学习率更新" title="永久链接至标题"></a></h3>
<p>PyTorch：</p>
<p>PyTorch提供了<code class="docutils literal notranslate"><span class="pre">torch.optim.lr_scheduler</span></code>包用于动态修改lr，使用的时候需要显式地调用<code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code>和<code class="docutils literal notranslate"><span class="pre">scheduler.step()</span></code>来更新lr，详情请参考<a class="reference external" href="https://pytorch.org/docs/1.12/optim.html#how-to-adjust-learning-rate">如何调整学习率</a>。</p>
<p>MindSpore：</p>
<p>MindSpore的学习率是包到优化器里面的，每调用一次优化器，学习率更新的step会自动更新一次，详情请参考<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/master/migration_guide/model_development/learning_rate_and_optimizer.html">学习率与优化器</a>。</p>
</section>
</section>
<section id="随机数策略对比">
<h2>随机数策略对比<a class="headerlink" href="#随机数策略对比" title="永久链接至标题"></a></h2>
<section id="随机数api对比">
<h3>随机数API对比<a class="headerlink" href="#随机数api对比" title="永久链接至标题"></a></h3>
<p>PyTorch与MindSpore在接口名称上无差异，MindSpore由于不支持原地修改，所以缺少Tensor.random_接口。其余接口均可和PyTorch一一对应。</p>
</section>
<section id="随机种子和生成器">
<h3>随机种子和生成器<a class="headerlink" href="#随机种子和生成器" title="永久链接至标题"></a></h3>
<p>MindSpore使用seed控制随机数的生成，而PyTorch使用torch.Generator进行随机数的控制。</p>
<ol class="arabic">
<li><p>MindSpore的seed分为两个等级，graph-level和op-level。graph-level下seed作为全局变量，绝大多数情况下无需用户设置，用户只需调整op-level seed。（API中涉及的seed参数，均为op-level）如果一段程序中两次使用了同一个随机数算法，那么两次的结果是不同的（尽管设置了相同的随机种子）；如果重新运行脚本，那么第二次运行的结果应该与第一次保持一致。示例如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># If a random op is called twice within one program, the two results will be different:</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">ops</span>

<span class="n">minval</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">maxval</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">minval</span><span class="p">,</span> <span class="n">maxval</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># generates &#39;A1&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">minval</span><span class="p">,</span> <span class="n">maxval</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># generates &#39;A2&#39;</span>
<span class="c1"># If the same program runs again, it repeat the results:</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">minval</span><span class="p">,</span> <span class="n">maxval</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># generates &#39;A1&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">minval</span><span class="p">,</span> <span class="n">maxval</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># generates &#39;A2&#39;</span>
</pre></div>
</div>
</li>
<li><p>torch.Generator常在函数中作为关键字参数传入。在未指定/实例化Generator时，会使用默认Generator (torch.default_generator)。可以使用以下代码设置指定的torch.Generator的seed：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">G</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span>
<span class="n">G</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>此时和使用default_generator并将seed设置为1的结果相同。例如torch.manual_seed(1)。</p>
<p>PyTorch的Generator中的state表示的是此Generator的状态，长度为5056，dtype为uint8的Tensor。在同一个脚本中，多次使用同一个Generator，Generator的state会发生改变。在有两个/多个Generator的情况下，如g1，g2，可以设置 g2.set_state(g1.get_state()) 使得g2达到和g1相同的状态。即使用g2相当于使用当时状态的g1。如果g1和g2具有相同的seed和state，则二者生成的随机数相同。</p>
</li>
</ol>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="analysis_and_preparation.html" class="btn btn-neutral float-left" title="模型分析与准备" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
        <a href="model_development/model_development.html" class="btn btn-neutral float-right" title="MindSpore网络搭建" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 2022, MindSpore.</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用了 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a>开发.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>