

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>与PyTorch典型区别 &mdash; MindSpore master 文档</title>
  

  
  <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/training.css" type="text/css" />
  <link rel="stylesheet" href="../_static/nbsphinx-code-cells.css" type="text/css" />
  <link rel="stylesheet" href="../_static/nbsphinx-code-cells.css" type="text/css" />
  <link rel="stylesheet" href="../_static/nbsphinx-code-cells.css" type="text/css" />
  <link rel="stylesheet" href="../_static/nbsphinx-code-cells.css" type="text/css" />
  <link rel="stylesheet" href="../_static/nbsphinx-code-cells.css" type="text/css" />
  <link rel="stylesheet" href="../_static/nbsphinx-code-cells.css" type="text/css" />
  <link rel="stylesheet" href="../_static/nbsphinx-code-cells.css" type="text/css" />
  <link rel="stylesheet" href="../_static/nbsphinx-code-cells.css" type="text/css" />
  <link rel="stylesheet" href="../_static/nbsphinx-code-cells.css" type="text/css" />
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script src="../_static/js/training.js"></script>
        <script src="../_static/translations.js"></script>
        
        
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="基于自定义算子接口调用第三方算子库" href="use_third_party_op.html" />
    <link rel="prev" title="常见问题" href="faq.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">设计</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../design/overview.html">MindSpore设计概览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/programming_paradigm.html">编程范式</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/auto_gradient.html">函数式微分编程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/mindir.html">中间表示MindIR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/all_scenarios.html">全场景统一</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/dynamic_graph_and_static_graph.html">动静态图结合</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/pluggable_device.html">三方硬件对接</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/distributed_training_design.html">分布式并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/graph_fusion_engine.html">图算融合加速引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/data_engine.html">高性能数据处理引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/glossary.html">术语</a></li>
</ul>
<p class="caption"><span class="caption-text">规格</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/benchmark.html">基准性能</a></li>
<li class="toctree-l1"><a class="reference external" href="https://gitee.com/mindspore/models/blob/master/README_CN.md#目录">网络支持↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/operator_list.html">API支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/syntax_list.html">语法支持</a></li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/master/api_cpp/mindspore.html">C++ API↗</a></li>
</ul>
<p class="caption"><span class="caption-text">API映射</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/pytorch_api_mapping.html">PyTorch与MindSpore API映射表</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/tensorflow_api_mapping.html">TensorFlow与MindSpore API映射表</a></li>
</ul>
<p class="caption"><span class="caption-text">迁移指南</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">迁移指南概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="enveriment_preparation.html">环境准备与资料获取</a></li>
<li class="toctree-l1"><a class="reference internal" href="analysis_and_preparation.html">模型分析与准备</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_development/model_development.html">MindSpore网络搭建</a></li>
<li class="toctree-l1"><a class="reference internal" href="debug_and_tune.html">调试调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="sample_code.html">网络迁移调试实例</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="faq.html">常见问题</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">与PyTorch典型区别</a></li>
<li class="toctree-l2"><a class="reference internal" href="use_third_party_op.html">基于自定义算子接口调用第三方算子库</a></li>
<li class="toctree-l2"><a class="reference internal" href="tensorflow2mindspore.html">TensorFlow模型转换MindSpore模型的方法</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq/installation.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/data_processing.html">数据处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/implement_problem.html">执行问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/network_compilation.html">网络编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/operators_compile.html">算子编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/usage_migrate_3rd.html">第三方框架迁移使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/performance_tuning.html">性能调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/precision_tuning.html">精度调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/distributed_parallel.html">分布式并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/inference.html">推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/feature_advice.html">特性咨询</a></li>
</ul>
<p class="caption"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="faq.html">常见问题</a> &raquo;</li>
        
      <li>与PyTorch典型区别</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/migration_guide/typical_api_comparision.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="与pytorch典型区别">
<h1>与PyTorch典型区别<a class="headerlink" href="#与pytorch典型区别" title="永久链接至标题">¶</a></h1>
<p><a class="reference external" href="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/master/zh_cn/migration_guide/mindspore_typical_api_comparision.ipynb"><img alt="下载Notebook" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_notebook.png" /></a> <a class="reference external" href="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/master/zh_cn/migration_guide/mindspore_typical_api_comparision.py"><img alt="下载样例代码" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_download_code.png" /></a> <a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/docs/mindspore/source_zh_cn/migration_guide/typical_api_comparision.ipynb"><img alt="查看源文件" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source.png" /></a></p>
<div class="section" id="与pytorch典型接口区别">
<h2>与PyTorch典型接口区别<a class="headerlink" href="#与pytorch典型接口区别" title="永久链接至标题">¶</a></h2>
<div class="section" id="torch.device">
<h3>torch.device<a class="headerlink" href="#torch.device" title="永久链接至标题">¶</a></h3>
<p>PyTorch 在构建模型时，通常会利用 torch.device 指定模型和数据绑定的设备，是在 CPU 还是 GPU 上，如果支持多 GPU，还可以指定具体的 GPU 序号。绑定相应的设备后，需要将模型和数据部署到对应设备，代码如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="c1"># bind to the GPU 0 if GPU is available, otherwise bind to CPU</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span> <span class="c1"># 单 GPU 或者 CPU</span>
<span class="c1"># deploy model to specified hardware</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># deploy data to specified hardware</span>
<span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># distribute training on multiple GPUs</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># set available device</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;CUDA_VISIBLE_DEVICE&#39;</span><span class="p">]</span><span class="o">=</span><span class="s1">&#39;1&#39;</span>
    <span class="n">model</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
</pre></div>
</div>
<p>而在 MindSpore 中，我们通过 context 中 的 device_target 参数 指定模型绑定的设备，device_id 指定设备的序号。与 PyTorch 不同的是，一旦设备设置成功，输入数据和模型会默认拷贝到指定的设备中执行，不需要也无法再改变数据和模型所运行的设备类型。代码如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s1">&#39;Ascend&#39;</span><span class="p">,</span> <span class="n">device_id</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># define net</span>
<span class="n">Model</span> <span class="o">=</span> <span class="o">..</span>
<span class="c1"># define dataset</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="o">..</span>
<span class="c1"># training, automatically deploy to Ascend according to device_target</span>
<span class="n">Model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<p>此外，网络运行后返回的 <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> 默认均拷贝到 CPU 设备，可以直接对该 <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> 进行访问和修改，包括转成 <code class="docutils literal notranslate"><span class="pre">numpy</span></code> 格式，无需像 PyTorch 一样需要先执行 <code class="docutils literal notranslate"><span class="pre">tensor.cpu</span></code> 再转换成 numpy 格式。</p>
</div>
<div class="section" id="nn.Module">
<h3>nn.Module<a class="headerlink" href="#nn.Module" title="永久链接至标题">¶</a></h3>
<p>使用 PyTorch 构建网络结构时，我们会用到<code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> 类，通常将网络中的元素定义在<code class="docutils literal notranslate"><span class="pre">__init__</span></code> 函数中并对其初始化，将网络的图结构表达定义在<code class="docutils literal notranslate"><span class="pre">forward</span></code> 函数中，通过调用这些类的对象完成整个模型的构建和训练。<code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> 不仅为我们提供了构建图接口，它还为我们提供了一些常用的 <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html">API</a> ，来帮助我们执行更复杂逻辑。</p>
<p>MindSpore 中的 <code class="docutils literal notranslate"><span class="pre">nn.Cell</span></code> 类发挥着和 PyTorch 中 <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> 相同的作用，都是用来构建图结构的模块，MindSpore 也同样提供了丰富的 <a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/master/api_python/nn/mindspore.nn.Cell.html">API</a> 供开发者使用，虽然名字不能一一对应，但 <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> 中常用的功能都可以在<code class="docutils literal notranslate"><span class="pre">nn.Cell</span></code> 中找到映射。</p>
<p>以几个常用方法为例:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 29%" />
<col style="width: 29%" />
<col style="width: 42%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>常用方法</p></th>
<th class="head"><p>nn.Module</p></th>
<th class="head"><p>nn.Cell</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>获取子元素</p></td>
<td><p>named_children</p></td>
<td><p>cells_and_names</p></td>
</tr>
<tr class="row-odd"><td><p>添加子元素</p></td>
<td><p>add_module</p></td>
<td><p>insert_child_to_cell</p></td>
</tr>
<tr class="row-even"><td><p>获取元素的参数</p></td>
<td><p>parameters</p></td>
<td><p>get_parameters</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="数据对象">
<h3>数据对象<a class="headerlink" href="#数据对象" title="永久链接至标题">¶</a></h3>
<p>在 PyTorch 中，可以存储数据的对象总共有四种，分别时<code class="docutils literal notranslate"><span class="pre">Tensor</span></code>、<code class="docutils literal notranslate"><span class="pre">Variable</span></code>、<code class="docutils literal notranslate"><span class="pre">Parameter</span></code>、<code class="docutils literal notranslate"><span class="pre">Buffer</span></code>。这四种对象的默认行为均不相同，当我们不需要求梯度时，通常使用 <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>和 <code class="docutils literal notranslate"><span class="pre">Buffer</span></code>两类数据对象，当我们需要求梯度时，通常使用 <code class="docutils literal notranslate"><span class="pre">Variable</span></code> 和 <code class="docutils literal notranslate"><span class="pre">Parameter</span></code> 两类对象。PyTorch 在设计这四种数据对象时，功能上存在冗余（<code class="docutils literal notranslate"><span class="pre">Variable</span></code> 后续会被废弃也说明了这一点）。</p>
<p>MindSpore 优化了数据对象的设计逻辑，仅保留了两种数据对象：<code class="docutils literal notranslate"><span class="pre">Tensor</span></code> 和 <code class="docutils literal notranslate"><span class="pre">Parameter</span></code>，其中 <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> 对象仅参与运算，并不需要对其进行梯度求导和参数更新，而 <code class="docutils literal notranslate"><span class="pre">Parameter</span></code> 数据对象和 PyTorch 的 <code class="docutils literal notranslate"><span class="pre">Parameter</span></code> 意义相同，会根据其属性<code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> 来决定是否对其进行梯度求导和 参数更新。 在网络迁移时，只要是在 PyTorch 中未进行参数更新的数据对象，均可在 MindSpore 中声明为 <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>。</p>
</div>
<div class="section" id="梯度求导">
<h3>梯度求导<a class="headerlink" href="#梯度求导" title="永久链接至标题">¶</a></h3>
<p>梯度求导涉及的算子和接口差异主要是由 MindSpore 和 PyTorch 自动微分原理不同引起的。</p>
</div>
<div class="section" id="torch.no_grad">
<h3>torch.no_grad<a class="headerlink" href="#torch.no_grad" title="永久链接至标题">¶</a></h3>
<p>在 PyTorch 中，默认情况下，执行正向计算时会记录反向传播所需的信息，在推理阶段或无需反向传播网络中，这一操作是冗余的，会额外耗时，因此，PyTorch 提供了<code class="docutils literal notranslate"><span class="pre">torch.no_grad</span></code> 来取消该过程。</p>
<p>而 MindSpore 只有在调用<code class="docutils literal notranslate"><span class="pre">grad</span></code>才会根据正向图结构来构建反向图，正向执行时不会记录任何信息，所以 MindSpore 并不需要该接口，也可以理解为 MindSpore 的正向计算均在<code class="docutils literal notranslate"><span class="pre">torch.no_grad</span></code> 情况下进行的。</p>
</div>
<div class="section" id="retain-graph">
<h3>retain_graph<a class="headerlink" href="#retain-graph" title="永久链接至标题">¶</a></h3>
<p>由于 PyTorch 是基于函数式的自动微分，所以默认每次执行完反向传播后都会自动清除记录的信息，从而进行下一次迭代。这就会导致当我们想再次利用这些反向图和梯度信息时，由于已被删除而获取失败。因此，PyTorch 提供了<code class="docutils literal notranslate"><span class="pre">backward(retain_graph=True)</span></code> 来主动保留这些信息。</p>
<p>而 MindSpore 则不需要这个功能，MindSpore 是基于计算图的自动微分，反向图信息在调用<code class="docutils literal notranslate"><span class="pre">grad</span></code>后便永久的记录在计算图中，只要再次调用计算图就可以获取梯度信息。</p>
</div>
<div class="section" id="高阶导数">
<h3>高阶导数<a class="headerlink" href="#高阶导数" title="永久链接至标题">¶</a></h3>
<p>基于计算图的自动微分还有一个好处，我们可以很方便的实现高阶求导。第一次对正向图执行<code class="docutils literal notranslate"><span class="pre">grad</span></code> 操作后，我们可以得到一阶导，此时计算图被更新为正向图+一阶导的反向图结构，但我们再次对更新后的计算图执行 <code class="docutils literal notranslate"><span class="pre">grad</span></code>后，我们就可以得到二阶导，以此类推，通过基于计算图的自动微分，我们很容易求得一个网络的高阶导数。</p>
</div>
</div>
<div class="section" id="与pytorch典型算子区别">
<h2>与PyTorch典型算子区别<a class="headerlink" href="#与pytorch典型算子区别" title="永久链接至标题">¶</a></h2>
<p>MindSpore 的大部分算子 API 和 TensorFlow 相近，但也有一部分算子的默认行为与 PyTorch 或 TensorFlow 存在差异。开发者在做网络脚本迁移时，如果未注意到这些差异，直接使用默认行为，很容易出现与原迁移网络不一致的情况，影响网络训练，我们建议开发者在迁移网络时不仅对齐使用的算子，也要对齐算子的属性。这里我们总结了几个常见的差异算子。</p>
<div class="section" id="nn.Dropout">
<h3>nn.Dropout<a class="headerlink" href="#nn.Dropout" title="永久链接至标题">¶</a></h3>
<p>Dropout 常用于防止训练过拟合，有一个重要的 <strong>概率值</strong> 参数，该参数在 MindSpore 中的意义与 PyTorch 和 TensorFlow 中的意义完全相反。</p>
<p>在 MindSpore 中，概率值对应 Dropout 算子的属性 <code class="docutils literal notranslate"><span class="pre">keep_prob</span></code>，是指输入被保留的概率，<code class="docutils literal notranslate"><span class="pre">1-keep_prob</span></code>是指输入被置 0 的概率。</p>
<p>在 PyTorch 和 TensorFlow 中，概率值分别对应 Dropout 算子的属性 <code class="docutils literal notranslate"><span class="pre">p</span></code>和 <code class="docutils literal notranslate"><span class="pre">rate</span></code>，是指输入被置 0 的概率，与 MindSpore.nn.Dropout 中的 <code class="docutils literal notranslate"><span class="pre">keep_prob</span></code> 意义相反。</p>
<p>更多信息请参考链接： <a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/master/api_python/nn/mindspore.nn.Dropout.html#mindspore.nn.Dropout">MindSpore Dropout</a> 、 <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html">PyTorch Dropout</a> 、 <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/nn/dropout">TensorFlow Dropout</a></p>
</div>
<div class="section" id="nn.BatchNorm2d">
<h3>nn.BatchNorm2d<a class="headerlink" href="#nn.BatchNorm2d" title="永久链接至标题">¶</a></h3>
<p>BatchNorm 是 CV 领域比较特殊的正则化方法，它在训练和推理的过程中有着不同计算流程，通常由算子属性控制。MindSpore 和 PyTorch 的 BatchNorm 在这一点上使用了两种不同的参数组。</p>
<ul class="simple">
<li><p>差异一</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">torch.nn.BatchNorm2d</span></code> 在不同参数下的状态</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 80%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>training</p></th>
<th class="head"><p>track_running_stats</p></th>
<th class="head"><p>状态</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>True</p></td>
<td><p>True</p></td>
<td><p>期望中训练的状态，running_mean 和 running_var 会跟踪整个训练过程中 batch 的统计特性，而每组输入数据用当前 batch 的 mean 和 var 统计特性做归一化，然后再更新 running_mean 和 running_var。</p></td>
</tr>
<tr class="row-odd"><td><p>True</p></td>
<td><p>False</p></td>
<td><p>每组输入数据会根据当前 batch 的统计特性做归一化，但不会有 running_mean 和 running_var 参数了。</p></td>
</tr>
<tr class="row-even"><td><p>False</p></td>
<td><p>True</p></td>
<td><p>期望中推理的状态，BN 使用 running_mean 和 running_var 做归一化，并且不会对其进行更新。</p></td>
</tr>
<tr class="row-odd"><td><p>False</p></td>
<td><p>False</p></td>
<td><p>效果同第二点，只不过处于推理状态，不会学习 weight 和 bias 两个参数。一般不采用该状态。</p></td>
</tr>
</tbody>
</table>
<p><code class="docutils literal notranslate"><span class="pre">mindspore.nn.BatchNorm2d</span></code> 在不同参数下的状态</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 11%" />
<col style="width: 89%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>use_batch_statistics</p></th>
<th class="head"><p>状态</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>True</p></td>
<td><p>期望中训练的状态，moving_mean 和 moving_var 会跟踪整个训练过程中 batch 的统计特性，而每组输入数据用当前 batch 的 mean 和 var 统计特性做归一化，然后再更新 moving_mean 和 moving_var。</p></td>
</tr>
<tr class="row-odd"><td><p>Fasle</p></td>
<td><p>期望中推理的状态，BN 使用 moving_mean 和 moving_var 做归一化，并且不会对其进行更新。</p></td>
</tr>
<tr class="row-even"><td><p>None</p></td>
<td><p>自动设置 use_batch_statistics。如果是训练，use_batch_statistics=True，如果是推理，use_batch_statistics=False。</p></td>
</tr>
</tbody>
</table>
<p>通过比较可以发现，<code class="docutils literal notranslate"><span class="pre">mindspore.nn.BatchNorm2d</span></code> 相比 <code class="docutils literal notranslate"><span class="pre">torch.nn.BatchNorm2d</span></code>，少了两种冗余的状态，仅保留了最常用的训练和推理两种状态。</p>
<ul class="simple">
<li><p>差异二</p></li>
</ul>
<p>BatchNorm系列算子 的 momentum 参数在 MindSpore 和 PyTorch 表示的意义相反，关系为：</p>
<div class="math notranslate nohighlight">
\[momentum_{pytorch} = 1 - momentum_{mindspore}\]</div>
<p>参考链接：<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/master/api_python/nn/mindspore.nn.BatchNorm2d.html">mindspore.nn.BatchNorm2d</a>，<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html">torch.nn.BatchNorm2d</a></p>
</div>
<div class="section" id="ops.Transpose">
<h3>ops.Transpose<a class="headerlink" href="#ops.Transpose" title="永久链接至标题">¶</a></h3>
<p>在做轴变换时，PyTorch 常用到两种算子<code class="docutils literal notranslate"><span class="pre">Tensor.permute</span></code> 和 <code class="docutils literal notranslate"><span class="pre">torch.transpose</span></code>，而 MindSpore 和 TensorFlow 仅提供了 <code class="docutils literal notranslate"><span class="pre">transpose</span></code> 算子。需要注意的是，<code class="docutils literal notranslate"><span class="pre">Tensor.permute</span></code> 包含了 <code class="docutils literal notranslate"><span class="pre">torch.transpose</span></code> 的功能，<code class="docutils literal notranslate"><span class="pre">torch.transpose</span></code> 仅支持同时交换两个轴，而 <code class="docutils literal notranslate"><span class="pre">Tensor.permute</span></code> 可以同时变换多个轴。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># PyTorch 代码</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">ret1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">dim0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ret1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">ret2</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ret2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
torch.Size([4, 2, 3, 1])
torch.Size([4, 3, 2, 1])
</pre></div></div>
</div>
<p>MindSpore 和 TensorFlow 的 <code class="docutils literal notranslate"><span class="pre">transpose</span></code>算子功能相同， 虽然名字叫 <code class="docutils literal notranslate"><span class="pre">transpose</span></code>，但实际可以做多个轴的同时变换，等价于 <code class="docutils literal notranslate"><span class="pre">Tensor.permute</span></code>。因此，MindSpore 也不再单独提供类似 <code class="docutils literal notranslate"><span class="pre">torch.tranpose</span></code> 的算子。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># mindspore 代码</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">ret</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Transpose</span><span class="p">()(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ret</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
(4, 3, 2, 1)
</pre></div></div>
</div>
<p>更多信息请参考链接：<a class="reference external" href="https://www.mindspore.cn/docs/en/master/api_python/ops/mindspore.ops.Transpose.html#mindspore.ops.Transpose">MindSpore Transpose</a> 、 <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.transpose.html">PyTorch Transpose</a> 、<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.permute.html">PyTorch Permute</a> 、 <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/transpose">TensforFlow Transpose</a></p>
</div>
<div class="section" id="conv-和-pooling">
<h3>Conv 和 Pooling<a class="headerlink" href="#conv-和-pooling" title="永久链接至标题">¶</a></h3>
<p>对于类似卷积和池化的算子，我们知道算子的输出特征图大小依赖输入特征图、步长、kernel_size 和 padding 等变量。</p>
<p>如果 <code class="docutils literal notranslate"><span class="pre">pad_mode</span></code> 设置为 <code class="docutils literal notranslate"><span class="pre">valid</span></code>，则输出特征图高和宽的计算公式分别为</p>
<p><img alt="conv-formula" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/docs/mindspore/source_zh_cn/migration_guide/model_development/images/conv_formula.png" /></p>
<p>如果 pad_mode（对应 PyTorch 中的属性为 padding，与属性 pad_mode 含义并不相同） 设置为 <code class="docutils literal notranslate"><span class="pre">same</span></code> 时，有时需要对输入特征图进行自动的 padding 操作，当padding 的元素为偶数时，padding 的元素会均匀分布在特征图的上下左右，此时 MindSpore、PyTorch 和 TensorFlow 中该类算子行为一致。</p>
<p>但当 padding 的元素为奇数时，PyTorch 会优先填充在输入特征图的左侧和上侧：</p>
<p><img alt="padding1" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/docs/mindspore/source_zh_cn/migration_guide/model_development/images/padding_pattern1.png" /></p>
<p>而 MindSpore 和 TensorFlow 则优先填充在特征图的右侧和下侧：</p>
<p><img alt="padding2" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/docs/mindspore/source_zh_cn/migration_guide/model_development/images/padding_pattern2.png" /></p>
<p>举例说明：</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># mindspore example</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">9</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MaxPool</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">op</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">data</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[[[0. 1. 2.]
   [3. 4. 5.]
   [6. 7. 8.]]]]
[[[[4. 5.]
   [7. 8.]]]]
</pre></div></div>
</div>
<p>在做 MindSpore 模型迁移时，如果模型加载了 PyTorch 的预训练模型，而之后又在 MindSpore 进行 finetune，则该差异可能会导致精度下降，对于 padding 策略为 same 的卷积，开发者需要特别注意。</p>
<p>如果想和 PyTorch 行为保持一致，可以先使用 <code class="docutils literal notranslate"><span class="pre">ops.Pad</span></code> 算子手动 pad 元素，然后使用 <code class="docutils literal notranslate"><span class="pre">pad_mode=&quot;valid&quot;</span></code> 的卷积和池化操作。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">9</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="c1"># only padding on top left of feature map</span>
<span class="n">pad</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Pad</span><span class="p">(((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)))</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pad</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MaxPool</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;vaild&#39;</span><span class="p">)(</span><span class="n">data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[[[0. 2.]
   [6. 8.]]]]
</pre></div></div>
</div>
</div>
<div class="section" id="默认权重初始化不同">
<h3>默认权重初始化不同<a class="headerlink" href="#默认权重初始化不同" title="永久链接至标题">¶</a></h3>
<p>我们知道权重初始化对网络的训练十分重要。每个算子一般会有一个隐式的声明权重，在不同的框架中，隐式的声明权重可能不同。即使算子功能一致，隐式声明的权重初始化方式分布如果不同，也会对训练过程产生影响，甚至无法收敛。</p>
<p>常见隐式声明权重的算子：Conv、Dense(Linear)、Embedding、LSTM 等，其中区别较大的是 Conv 类和 Dense 两种算子。</p>
<ul>
<li><p>Conv2d</p>
<ul class="simple">
<li><p>mindspore.nn.Conv2d的weight为：<span class="math notranslate nohighlight">\(\mathcal{N}(0, 1)\)</span>，bias为：zeros。</p></li>
<li><p>torch.nn.Conv2d的weight为：<span class="math notranslate nohighlight">\(\mathcal{U} (-\sqrt{k},\sqrt{k} )\)</span>，bias为：<span class="math notranslate nohighlight">\(\mathcal{U} (-\sqrt{k},\sqrt{k} )\)</span>。</p></li>
<li><p>tf.keras.Layers.Conv2D的weight为：glorot_uniform，bias为：zeros。</p></li>
</ul>
<p>其中，<span class="math notranslate nohighlight">\(k=\frac{groups}{c_{in}*\prod_{i}^{}{kernel\_size[i]}}\)</span></p>
</li>
<li><p>Dense(Linear)</p>
<ul class="simple">
<li><p>mindspore.nn.Linear的weight为：<span class="math notranslate nohighlight">\(\mathcal{N}(0, 1)\)</span>，bias为：zeros。</p></li>
<li><p>torch.nn.Dense的weight为：<span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k},\sqrt{k})\)</span>，bias为：<span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k},\sqrt{k} )\)</span>。</p></li>
<li><p>tf.keras.Layers.Dense的weight为：glorot_uniform，bias为：zeros。</p></li>
</ul>
<p>其中，<span class="math notranslate nohighlight">\(k=\frac{groups}{in\_features}\)</span></p>
</li>
</ul>
<p>对于没有正则化的网络，如没有 BatchNorm 算子的 GAN 网络，梯度很容易爆炸或者消失，权重初始化就显得十分重要，各位开发者应注意权重初始化带来的影响。</p>
</div>
</div>
<div class="section" id="与pytorch执行流程区别">
<h2>与PyTorch执行流程区别<a class="headerlink" href="#与pytorch执行流程区别" title="永久链接至标题">¶</a></h2>
<div class="section" id="自动微分">
<h3>自动微分<a class="headerlink" href="#自动微分" title="永久链接至标题">¶</a></h3>
<p>MindSpore 和 PyTorch 都提供了自动微分功能，让我们在定义了正向网络后，可以通过简单的接口调用实现自动反向传播以及梯度更新。但需要注意的是，MindSpore 和 PyTorch 构建反向图的逻辑是不同的，这个差异也会带来 API 设计上的不同。</p>
<div class="section" id="pytorch-的自动微分">
<h4>PyTorch 的自动微分<a class="headerlink" href="#pytorch-的自动微分" title="永久链接至标题">¶</a></h4>
<p>我们知道 PyTorch 是基于计算路径追踪的自动微分，当我们定义一个网络结构后， 并不会建立反向图，而是在执行正向图的过程中，<code class="docutils literal notranslate"><span class="pre">Variable</span></code> 或 <code class="docutils literal notranslate"><span class="pre">Parameter</span></code> 记录每一个正向计算对应的反向函数，并生成一个动态计算图，用于后续的梯度计算。当在最终的输出处调用 <code class="docutils literal notranslate"><span class="pre">backward</span></code> 时，就会从根节点到叶节点应用链式法则计算梯度。PyTorch 的动态计算图所存储的节点实际是 <code class="docutils literal notranslate"><span class="pre">Function</span></code> 函数对象，每当对 <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> 执行一步运算后，就会产生一个 <code class="docutils literal notranslate"><span class="pre">Function</span></code> 对象，它记录了反向传播中必要的信息。反向传播过程中，<code class="docutils literal notranslate"><span class="pre">autograd</span></code>
引擎会按照逆序，通过 <code class="docutils literal notranslate"><span class="pre">Function</span></code> 的 <code class="docutils literal notranslate"><span class="pre">backward</span></code> 依次计算梯度。 这一点我们可以通过 <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> 的隐藏属性查看。</p>
<p>例如，运行以下代码：</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">1</span>
<span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>会自动获取<code class="docutils literal notranslate"><span class="pre">x</span></code>的定义到获取输出<code class="docutils literal notranslate"><span class="pre">y</span></code>这个过程对x的梯度结果。</p>
<p>需要注意的是PyTorch的backward是累计的，更新完之后需要清空optimizer。</p>
</div>
<div class="section" id="mindspore的自动微分">
<h4>MindSpore的自动微分<a class="headerlink" href="#mindspore的自动微分" title="永久链接至标题">¶</a></h4>
<p>在图模式下，MindSpore 的自动微分是基于图结构的微分，和 PyTorch 不同，它不会在正向计算过程中记录任何信息，仅仅执行正常的计算流程（在PyNative模式下和 PyTorch 类似）。那么问题来了，如果整个正向计算都结束了，MindSpore 也没有记录任何信息，那它是如何知道反向传播怎么执行的呢？</p>
<p>MindSpore 在做自动微分时，需要传入正向图结构，自动微分的过程就是通过对正向图的分析从而得到反向传播信息，自动微分的结果与正向计算中具体的数值无关，仅和正向图结构有关。通过对正向图的自动微分，我们得到了反向传播过程，而这个反向传播过程其实也是通过一个图结构来表达，也就是反向图。将反向图添加到用户定义的正向图之后，组成一个最终的计算图。不过后添加的反向图和其中的反向算子我们并不感知，也无法手动添加，只能通过 MindSpore 为我们提供的接口自动添加，这样做也避免了我们在反向构图时引入错误。</p>
<p>最终，我们看似仅执行了正向图，其实图结构里既包含了正向算子，又包含了 MindSpore 为我们添加的反向算子，也就是说，MindSpore 在我们定义的正向图后面又新加了一个看不见的 <code class="docutils literal notranslate"><span class="pre">Cell</span></code>，这个 <code class="docutils literal notranslate"><span class="pre">Cell</span></code> 里都是根据正向图推导出来的反向算子。</p>
<p>而这个帮助我们构建反向图的接口就是 <a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/master/api_python/mindspore/mindspore.grad.html">grad</a> ：</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">ops</span>

<span class="k">class</span> <span class="nc">GradNetWrtX</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GradNetWrtX</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">net</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">gradient_function</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">gradient_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>查看文档介绍我们可以发现，grad 并不是一个算子，它的输入输出并不是 Tensor，而是我们定义的正向图和自动微分得到的反向图。为什么输入是一个图结构呢？因为构建反向图并不需要知道具体的输入数据是什么，只要知道正向图的结构就行了，有了正向图就可以推算出反向图结构，之后我们可以把正向图+反向图当成一个新的计算图来对待，这个新的计算图就像是一个函数，对于你输入的任何一组数据，它不仅能计算出正向的输出，还能计算出所有权重的梯度，由于图结构是固定的，并不保存中间变量，所以这个图结构可以被反复调用。</p>
<p>同理，之后我们再给网络加上优化器结构时，优化器也会加上优化器相关的算子，也就是再给这个计算图加点我们不感知的优化器算子，最终，计算图就构建完成。</p>
<p>在 MindSpore 中，大部分操作都会最终转换成真实的算子操作，最终加入到计算图中，因此，我们实际执行的计算图中算子的数量远多于我们最开始定义的计算图中算子的数量。</p>
<p>在MindSpore中，提供了<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/master/api_python/nn/mindspore.nn.TrainOneStepCell.html">TrainOneStepCell</a>和<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/master/api_python/nn/mindspore.nn.TrainOneStepWithLossScaleCell.html">TrainOneStepWithLossScaleCell</a>这两个接口来包装整个训练流程，如果在常规的训练流程外有其他的操作，如梯度裁剪、规约、中间变量返回等，需要自定义训练的Cell，详情请参考<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/master/migration_guide/model_development/training_and_evaluation_procession.html">训练及推理流程</a>。</p>
</div>
</div>
<div class="section" id="学习率更新">
<h3>学习率更新<a class="headerlink" href="#学习率更新" title="永久链接至标题">¶</a></h3>
<div class="section" id="pytorch的学习率更新策略">
<h4>PyTorch的学习率更新策略<a class="headerlink" href="#pytorch的学习率更新策略" title="永久链接至标题">¶</a></h4>
<p>PyTorch提供了<code class="docutils literal notranslate"><span class="pre">torch.optim.lr_scheduler</span></code>包用于动态修改lr，使用的时候需要显式地调用<code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code>和<code class="docutils literal notranslate"><span class="pre">scheduler.step()</span></code>来更新lr，详情请参考<a class="reference external" href="https://pytorch.org/docs/1.12/optim.html#how-to-adjust-learning-rate">如何调整学习率</a>。</p>
</div>
<div class="section" id="mindspore的学习率更新策略">
<h4>MindSpore的学习率更新策略<a class="headerlink" href="#mindspore的学习率更新策略" title="永久链接至标题">¶</a></h4>
<p>MindSpore的学习率是包到优化器里面的，每调用一次优化器，学习率更新的step会自动更新一次，详情请参考<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/master/migration_guide/model_development/learning_rate_and_optimizer.html">学习率与优化器</a>。</p>
</div>
</div>
<div class="section" id="分布式场景">
<h3>分布式场景<a class="headerlink" href="#分布式场景" title="永久链接至标题">¶</a></h3>
<div class="section" id="pytorch的分布式设置">
<h4>PyTorch的分布式设置<a class="headerlink" href="#pytorch的分布式设置" title="永久链接至标题">¶</a></h4>
<p>一般分布式场景是数据并行就行，详情请参考<a class="reference external" href="https://pytorch.org/docs/1.12/notes/ddp.html">DDP</a>，下面是PyTorch的一个例子：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">import</span> <span class="nn">torch.multiprocessing</span> <span class="k">as</span> <span class="nn">mp</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">torch.nn.parallel</span> <span class="kn">import</span> <span class="n">DistributedDataParallel</span> <span class="k">as</span> <span class="n">DDP</span>


<span class="k">def</span> <span class="nf">example</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">):</span>
    <span class="c1"># create default process group</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="s2">&quot;gloo&quot;</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>
    <span class="c1"># create local model</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
    <span class="c1"># construct DDP model</span>
    <span class="n">ddp_model</span> <span class="o">=</span> <span class="n">DDP</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">rank</span><span class="p">])</span>
    <span class="c1"># define loss function and optimizer</span>
    <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">ddp_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

    <span class="c1"># forward pass</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">ddp_model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">rank</span><span class="p">))</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
    <span class="c1"># backward pass</span>
    <span class="n">loss_fn</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># update parameters</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">mp</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">example</span><span class="p">,</span>
        <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">world_size</span><span class="p">,),</span>
        <span class="n">nprocs</span><span class="o">=</span><span class="n">world_size</span><span class="p">,</span>
        <span class="n">join</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="c1"># Environment variables which need to be</span>
    <span class="c1"># set when using c10d&#39;s default &quot;env&quot;</span>
    <span class="c1"># initialization mode.</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;MASTER_ADDR&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;localhost&quot;</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;MASTER_PORT&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;29500&quot;</span>
    <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="mindspore的分布式设置">
<h4>MindSpore的分布式设置<a class="headerlink" href="#mindspore的分布式设置" title="永久链接至标题">¶</a></h4>
<p>MindSpore的分布式配置是使用运行时配置项的，详情请参考<a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/master/parallel/introduction.html">分布式并行</a>，如：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore.communication.management</span> <span class="kn">import</span> <span class="n">init</span><span class="p">,</span> <span class="n">get_rank</span><span class="p">,</span> <span class="n">get_group_size</span>

<span class="c1"># 初始化多卡环境</span>
<span class="n">init</span><span class="p">()</span>

<span class="c1"># 获取组成分布式场景的卡数和当前进程的逻辑id</span>
<span class="n">group_size</span> <span class="o">=</span> <span class="n">get_group_size</span><span class="p">()</span>
<span class="n">rank_id</span> <span class="o">=</span> <span class="n">get_rank</span><span class="p">()</span>

<span class="c1"># 配置数据并行模式运行分布式场景</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">DATA_PARALLEL</span><span class="p">,</span> <span class="n">gradients_mean</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="与pytorch优化器的区别">
<h2>与PyTorch优化器的区别<a class="headerlink" href="#与pytorch优化器的区别" title="永久链接至标题">¶</a></h2>
<div class="section" id="优化器支持差异">
<h3>优化器支持差异<a class="headerlink" href="#优化器支持差异" title="永久链接至标题">¶</a></h3>
<p>PyTorch和MindSpore同时支持的优化器异同比较详见<a class="reference external" href="https://mindspore.cn/docs/zh-CN/master/note/api_mapping/pytorch_api_mapping.html#torch-optim">API映射表</a>。MindSpore暂不支持的优化器：LBFGS，NAdam，RAdam。</p>
</div>
<div class="section" id="优化器的执行和使用差异">
<h3>优化器的执行和使用差异<a class="headerlink" href="#优化器的执行和使用差异" title="永久链接至标题">¶</a></h3>
<p>PyTorch单步执行优化器时，一般需要手动执行 <code class="docutils literal notranslate"><span class="pre">zero_grad()</span></code> 方法将历史梯度设置为0(或None)，然后使用 <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code> 计算当前训练step的梯度，最后调用优化器的 <code class="docutils literal notranslate"><span class="pre">step()</span></code> 方法实现网络权重的更新；</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">ExponentialLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<p>MindSpore中优化器的使用，只需要直接对梯度进行计算，然后使用 <code class="docutils literal notranslate"><span class="pre">optimizer(grads)</span></code> 执行网络权重的更新。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">grad_fn</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="n">forward_fn</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">_</span><span class="p">),</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">grad_fn</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
</div>
<div class="section" id="超参差异">
<h3>超参差异<a class="headerlink" href="#超参差异" title="永久链接至标题">¶</a></h3>
<div class="section" id="超参名称">
<h4>超参名称<a class="headerlink" href="#超参名称" title="永久链接至标题">¶</a></h4>
<p>网络权重和学习率入参名称异同：</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 21%" />
<col style="width: 18%" />
<col style="width: 34%" />
<col style="width: 26%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>参数</p></th>
<th class="head"><p>PyTorch</p></th>
<th class="head"><p>MindSpore</p></th>
<th class="head"><p>差异</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>网络权重</p></td>
<td><p>params</p></td>
<td><p>params</p></td>
<td><p>参数名相同</p></td>
</tr>
<tr class="row-odd"><td><p>学习率</p></td>
<td><p>lr</p></td>
<td><p>learning_rate</p></td>
<td><p>参数名不同</p></td>
</tr>
</tbody>
</table>
<p>MindSpore：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
</pre></div>
</div>
<p>PyTorch：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">optim</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="超参配置方式">
<h4>超参配置方式<a class="headerlink" href="#超参配置方式" title="永久链接至标题">¶</a></h4>
<ul class="simple">
<li><p>参数不分组：</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">params</span></code> 入参支持类型不同： PyTorch入参类型为 <code class="docutils literal notranslate"><span class="pre">iterable(Tensor)</span></code> 和 <code class="docutils literal notranslate"><span class="pre">iterable(dict)</span></code>，支持迭代器类型； MindSpore入参类型为 <code class="docutils literal notranslate"><span class="pre">list(Parameter)</span></code>，<code class="docutils literal notranslate"><span class="pre">list(dict)</span></code>，不支持迭代器。</p>
<p>其他超参配置及支持差异详见<a class="reference external" href="https://mindspore.cn/docs/zh-CN/master/note/api_mapping/pytorch_api_mapping.html#torch-optim">API映射表</a>。</p>
<ul class="simple">
<li><p>参数分组：</p></li>
</ul>
<p>PyTorch支持所有参数分组：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">([</span>
            <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">parameters</span><span class="p">()},</span>
            <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">classifier</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">}</span>
        <span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</pre></div>
</div>
<p>MindSpore仅支持特定key分组：”params”，”lr”，”weight_decay”，”grad_centralization”，”order_params”。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">conv_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()))</span>
<span class="n">no_conv_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()))</span>
<span class="n">group_params</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">conv_params</span><span class="p">,</span> <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">0.02</span><span class="p">},</span>
            <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">no_conv_params</span><span class="p">}]</span>

<span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">group_params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="运行时超参修改">
<h4>运行时超参修改<a class="headerlink" href="#运行时超参修改" title="永久链接至标题">¶</a></h4>
<p>PyTorch支持在训练过程中修改任意的优化器参数，并提供了 <code class="docutils literal notranslate"><span class="pre">LRScheduler</span></code> 用于动态修改学习率；</p>
<p>MindSpore当前不支持训练过程中修改优化器参数，但提供了修改学习率和权重衰减的方式，使用方式详见<a class="reference external" href="#学习率">学习率</a>和<a class="reference external" href="#权重衰减">权重衰减</a>章节。</p>
</div>
</div>
<div class="section" id="学习率">
<h3>学习率<a class="headerlink" href="#学习率" title="永久链接至标题">¶</a></h3>
<div class="section" id="动态学习率差异">
<h4>动态学习率差异<a class="headerlink" href="#动态学习率差异" title="永久链接至标题">¶</a></h4>
<p>PyTorch中定义了 <code class="docutils literal notranslate"><span class="pre">LRScheduler</span></code> 类用于对学习率进行管理。使用动态学习率时，将 <code class="docutils literal notranslate"><span class="pre">optimizer</span></code> 实例传入 <code class="docutils literal notranslate"><span class="pre">LRScheduler</span></code> 子类中，通过循环调用 <code class="docutils literal notranslate"><span class="pre">scheduler.step()</span></code> 执行学习率修改，并将修改同步至优化器中。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">ExponentialLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<p>MindSpore中的动态学习率有 <code class="docutils literal notranslate"><span class="pre">Cell</span></code> 和 <code class="docutils literal notranslate"><span class="pre">list</span></code> 两种实现方式，两种类型的动态学习率使用方式一致，都是在实例化完成之后传入优化器，前者在内部的 <code class="docutils literal notranslate"><span class="pre">construct</span></code> 中进行每一步学习率的计算，后者直接按照计算逻辑预生成学习率列表，训练过程中内部实现学习率的更新。具体请参考[动态学习率] (<a class="reference external" href="https://mindspore.cn/docs/zh-CN/master/api_python/mindspore.nn.html#%E5%8A%A8%E6%80%81%E5%AD%A6%E4%B9%A0%E7%8E%87">https://mindspore.cn/docs/zh-CN/master/api_python/mindspore.nn.html#%E5%8A%A8%E6%80%81%E5%AD%A6%E4%B9%A0%E7%8E%87</a>)。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">polynomial_decay_lr</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PolynomialDecayLR</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">end_learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">decay_steps</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">power</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">polynomial_decay_lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>

<span class="n">grad_fn</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="n">forward_fn</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">_</span><span class="p">),</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">grad_fn</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
</div>
<div class="section" id="自定义学习率差异">
<h4>自定义学习率差异<a class="headerlink" href="#自定义学习率差异" title="永久链接至标题">¶</a></h4>
<p>PyTorch的动态学习率模块 <code class="docutils literal notranslate"><span class="pre">LRScheduler</span></code> 提供了<code class="docutils literal notranslate"><span class="pre">LambdaLR</span></code> 接口供用户自定义学习率调整规则，用户通过传入lambda表达式或自定义函数实现学习率指定。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">lbd</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">epoch</span><span class="p">:</span> <span class="n">epoch</span> <span class="o">//</span> <span class="mi">5</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">LambdaLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">lr_lambda</span><span class="o">=</span><span class="n">lbd</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">train</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">validate</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<p>MindSpore未提供类似的lambda接口，自定义学习率调整策略可以通过自定义函数或自定义 <code class="docutils literal notranslate"><span class="pre">LearningRateSchedule</span></code> 来实现。</p>
<p>方式一：定义python函数指定计算逻辑，返回学习率列表：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">dynamic_lr</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">total_step</span><span class="p">,</span> <span class="n">step_per_epoch</span><span class="p">):</span>
    <span class="n">lrs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">total_step</span><span class="p">):</span>
        <span class="n">current_epoch</span> <span class="o">=</span> <span class="n">i</span> <span class="o">//</span> <span class="n">step_per_epoch</span>
        <span class="n">factor</span> <span class="o">=</span> <span class="n">current_epoch</span> <span class="o">//</span> <span class="mi">5</span>
        <span class="n">lrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lr</span> <span class="o">*</span> <span class="n">factor</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">lrs</span>

<span class="n">decay_lr</span> <span class="o">=</span> <span class="n">dynamic_lr</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">total_step</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">step_per_epoch</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">decay_lr</span><span class="p">)</span>
</pre></div>
</div>
<p>方式二：继承 <code class="docutils literal notranslate"><span class="pre">LearningRateSchedule</span></code>，在 <code class="docutils literal notranslate"><span class="pre">construct</span></code> 方法中定义变化策略:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DynamicDecayLR</span><span class="p">(</span><span class="n">LearningRateSchedule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">step_per_epoch</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DynamicDecayLR</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step_per_epoch</span> <span class="o">=</span> <span class="n">step_per_epoch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cast</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">global_step</span><span class="p">):</span>
        <span class="n">current_epoch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">global_step</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">//</span> <span class="n">step_per_epoch</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="p">(</span><span class="n">current_epoch</span> <span class="o">//</span> <span class="mi">5</span><span class="p">)</span>

<span class="n">decay_lr</span> <span class="o">=</span> <span class="n">DynamicDecayLR</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">step_per_epoch</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">decay_lr</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="学习率获取">
<h4>学习率获取<a class="headerlink" href="#学习率获取" title="永久链接至标题">¶</a></h4>
<p>PyTorch：</p>
<ul class="simple">
<li><p>固定学习率情况下，通常通过 <code class="docutils literal notranslate"><span class="pre">optimizer.state_dict()</span></code> 进行学习率的查看和打印，例如参数分组时，对于第n个参数组，使用 <code class="docutils literal notranslate"><span class="pre">optimizer.state_dict()['param_groups'][n]['lr']</span></code>，参数不分组时，使用 <code class="docutils literal notranslate"><span class="pre">optimizer.state_dict()['param_groups'][0]['lr']</span></code>；</p></li>
<li><p>动态学习率情况下，可以使用 <code class="docutils literal notranslate"><span class="pre">LRScheduler</span></code> 的 <code class="docutils literal notranslate"><span class="pre">get_lr</span></code> 方法获取当前学习率，或使用 <code class="docutils literal notranslate"><span class="pre">print_lr</span></code> 方法打印学习率。</p></li>
</ul>
<p>MindSpore：</p>
<ul class="simple">
<li><p>目前未提供直接查看学习率的接口，后续版本中会针对此问题进行修复。</p></li>
</ul>
</div>
</div>
<div class="section" id="权重衰减">
<h3>权重衰减<a class="headerlink" href="#权重衰减" title="永久链接至标题">¶</a></h3>
<p>PyTorch中修改 <code class="docutils literal notranslate"><span class="pre">weight_decay</span></code>：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">optim</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">decay_factor</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
        <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;weight_decay&quot;</span><span class="p">]</span> <span class="o">*=</span> <span class="n">decay_factor</span>
</pre></div>
</div>
<p>MindSpore中实现动态weight decay：用户可以继承 <code class="docutils literal notranslate"><span class="pre">Cell</span></code> 自定义动态weight decay的类，传入优化器中。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ExponentialWeightDecay</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_decay</span><span class="p">,</span> <span class="n">decay_rate</span><span class="p">,</span> <span class="n">decay_steps</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ExponentialWeightDecay</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span> <span class="o">=</span> <span class="n">weight_decay</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decay_rate</span> <span class="o">=</span> <span class="n">decay_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decay_steps</span> <span class="o">=</span> <span class="n">decay_steps</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">global_step</span><span class="p">):</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">global_step</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">decay_steps</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span> <span class="o">*</span> <span class="n">ops</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">decay_rate</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>

<span class="n">weight_decay</span> <span class="o">=</span> <span class="n">ExponentialWeightDecay</span><span class="p">(</span><span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">decay_steps</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="优化器状态的保存与加载">
<h3>优化器状态的保存与加载<a class="headerlink" href="#优化器状态的保存与加载" title="永久链接至标题">¶</a></h3>
<p>PyTorch的优化器模块提供了 <code class="docutils literal notranslate"><span class="pre">state_dict()</span></code> 用于优化器状态的查看及保存，<code class="docutils literal notranslate"><span class="pre">load_state_dict</span></code> 用于优化器状态的加载。</p>
<ul class="simple">
<li><p>优化器保存，可以使用 <code class="docutils literal notranslate"><span class="pre">torch.save()</span></code> 把获取到的 <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> 保存到pkl文件中：</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">save_path</span><span class="p">)</span>
</pre></div>
</div>
<p>优化器加载，可以使用 <code class="docutils literal notranslate"><span class="pre">torch.load()</span></code> 加载保存的 <code class="docutils literal notranslate"><span class="pre">state_dict</span></code>，然后使用 <code class="docutils literal notranslate"><span class="pre">load_state_dict</span></code> 将获取到的 <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> 加载到优化器中：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">state_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">save_path</span><span class="p">)</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
</pre></div>
</div>
<p>MindSpore的优化器模块继承自 <code class="docutils literal notranslate"><span class="pre">Cell</span></code>，优化器的保存与加载和网络的保存与加载方式相同，通常情况下配合 <code class="docutils literal notranslate"><span class="pre">save_checkpoint</span></code> 与<code class="docutils literal notranslate"><span class="pre">load_checkpoint</span></code> 使用。</p>
<p>优化器保存，可以使用 <code class="docutils literal notranslate"><span class="pre">mindspore.save_checkpoint()</span></code> 将优化器实例保存到ckpt文件中：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">state_dict</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span> <span class="n">save_path</span><span class="p">)</span>
</pre></div>
</div>
<p>优化器加载，可以使用 <code class="docutils literal notranslate"><span class="pre">mindspore.load_checkpoint()</span></code> 加载保存的ckpt文件，然后使用 <code class="docutils literal notranslate"><span class="pre">load_param_into_net</span></code> 将获取到的 <code class="docutils literal notranslate"><span class="pre">param_dict</span></code> 加载到优化器中：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">param_dict</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">(</span><span class="n">save_path</span><span class="p">)</span>
<span class="n">mindspore</span><span class="o">.</span><span class="n">load_param_into_net</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="use_third_party_op.html" class="btn btn-neutral float-right" title="基于自定义算子接口调用第三方算子库" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="faq.html" class="btn btn-neutral float-left" title="常见问题" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; 版权所有 2022, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>