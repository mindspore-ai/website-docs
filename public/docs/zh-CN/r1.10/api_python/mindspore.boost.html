<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore.boost &mdash; MindSpore master documentation</title>
      <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/training.css" type="text/css" /><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/js/training.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="PyTorch与MindSpore API映射表" href="../note/api_mapping/pytorch_api_mapping.html" />
    <link rel="prev" title="mindspore.scipy.sparse.linalg.gmres" href="scipy/mindspore.scipy.sparse.linalg.gmres.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">设计</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../design/overview.html">MindSpore设计概览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/auto_gradient.html">函数式微分编程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/mindir.html">中间表示MindIR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/all_scenarios.html">全场景统一</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/dynamic_graph_and_static_graph.html">动静态图结合</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/distributed_training_design.html">分布式并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/graph_fusion_engine.html">图算融合加速引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/data_engine.html">高性能数据处理引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/glossary.html">术语</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">规格</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/benchmark.html">基准性能</a></li>
<li class="toctree-l1"><a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.10/README_CN.md#目录">网络支持↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/operator_list.html">API支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/syntax_list.html">语法支持</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.audio.html">mindspore.dataset.audio</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.config.html">mindspore.dataset.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.text.html">mindspore.dataset.text</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.vision.html">mindspore.dataset.vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.nn.transformer.html">mindspore.nn.transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.ops.function.html">mindspore.ops.function</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.10/api_cpp/mindspore.html">C++ API↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API映射</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/pytorch_api_mapping.html">PyTorch与MindSpore API映射表</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/tensorflow_api_mapping.html">TensorFlow与MindSpore API映射表</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">迁移指南</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/overview.html">概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/enveriment_preparation.html">环境准备与资料获取</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/analysis_and_preparation.html">模型分析与准备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/model_development/model_development.html">MindSpore网络搭建</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/debug_and_tune.html">调试调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/sample_code.html">网络迁移调试实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/faq.html">常见问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/typical_api_comparision.html">与PyTorch典型区别</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/use_third_party_op.html">基于自定义算子接口调用第三方算子库</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq/installation.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/data_processing.html">数据处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/implement_problem.html">执行问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/network_compilation.html">网络编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/operators_compile.html">算子编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/usage_migrate_3rd.html">第三方框架迁移使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/performance_tuning.html">性能调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/precision_tuning.html">精度调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/distributed_configure.html">分布式配置</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/inference.html">推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/feature_advice.html">特性咨询</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>mindspore.boost</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/api_python/mindspore.boost.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="mindspore-boost">
<h1>mindspore.boost<a class="headerlink" href="#mindspore-boost" title="Permalink to this headline"></a></h1>
<p>Boost能够自动加速网络，如减少BN/梯度冻结/累积梯度等。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>此特性为测试版本，我们仍在改进其功能。</p>
</div>
<dl class="py class">
<dt class="sig sig-object py" id="mindspore.boost.AutoBoost">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.boost.</span></span><span class="sig-name descname"><span class="pre">AutoBoost</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">level</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'O0'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">boost_config_dict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/boost.html#AutoBoost"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.boost.AutoBoost" title="Permalink to this definition"></a></dt>
<dd><p>MindSpore自动优化算法库。</p>
<dl>
<dt>参数：</dt><dd><ul>
<li><p><strong>level</strong> (str) - Boost的配置级别，默认值：”O0”。</p>
<ul class="simple">
<li><p>“O0”：不变化。</p></li>
<li><p>“O1”：启用boost模式，性能将提升约20%，准确率保持不变。</p></li>
<li><p>“O2”：启用boost模式，性能将提升约30%，准确率下降小于3%。</p></li>
</ul>
</li>
<li><p><strong>boost_config_dict</strong> (dict) - 用户可配置的超参字典，建议的格式如下：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s2">&quot;boost&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;mode&quot;</span><span class="p">:</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span>
        <span class="s2">&quot;less_bn&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s2">&quot;grad_freeze&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s2">&quot;adasum&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s2">&quot;grad_accumulation&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s2">&quot;dim_reduce&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">},</span>

    <span class="s2">&quot;common&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;gradient_split_groups&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
        <span class="s2">&quot;device_number&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">},</span>

    <span class="s2">&quot;less_bn&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;fn_flag&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s2">&quot;gc_flag&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">},</span>

    <span class="s2">&quot;grad_freeze&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;param_groups&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
        <span class="s2">&quot;freeze_type&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;freeze_p&quot;</span><span class="p">:</span> <span class="mf">0.7</span><span class="p">,</span>
        <span class="s2">&quot;total_steps&quot;</span><span class="p">:</span> <span class="mi">65536</span><span class="p">},</span>

    <span class="s2">&quot;grad_accumulation&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;grad_accumulation_step&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span>

    <span class="s2">&quot;dim_reduce&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;rho&quot;</span><span class="p">:</span> <span class="mf">0.55</span><span class="p">,</span>
        <span class="s2">&quot;gamma&quot;</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>
        <span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="mf">0.001</span><span class="p">,</span>
        <span class="s2">&quot;sigma&quot;</span><span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>
        <span class="s2">&quot;n_components&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
        <span class="s2">&quot;pca_mat_path&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s2">&quot;weight_load_dir&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s2">&quot;timeout&quot;</span><span class="p">:</span> <span class="mi">1800</span><span class="p">}</span>

<span class="p">}</span>
</pre></div>
</div>
<ul>
<li><p>boost：</p>
<ul>
<li><p>mode (str)：Boost配置模式，支持 [“auto”, “manual”, “enable_all”, “disable_all”]。默认值： “auto”。</p>
<ul class="simple">
<li><p>auto：自动配置，取决于Model类中的 <cite>boost_level</cite> 参数配置。</p></li>
<li><p>manual：在 <cite>boost_config_dict</cite> 中人工配置。</p></li>
<li><p>enable_all：开启所有boost算法。</p></li>
<li><p>disable_all：关闭所有boost算法。</p></li>
</ul>
</li>
<li><p>less_bn (bool)：是否开启LessBN算法，默认：False</p></li>
<li><p>grad_freeze (bool)：是否开启梯度冻结算法，默认：False。</p></li>
<li><p>adasum (bool)：是否开启自适应求和算法，默认：False。</p></li>
<li><p>grad_accumulation (bool)：是否开启梯度累加算法，默认：False。</p></li>
<li><p>dim_reduce (bool)：是否开启降维训练算法，默认：False。</p>
<p>如果开启dim_reduce算法，其他算法会失效。
如果开启grad_freeze算法，同时关闭dim_reduce，其他算法会失效。</p>
</li>
</ul>
</li>
<li><p>common：</p>
<ul class="simple">
<li><p>gradient_split_groups (list)：网络的梯度分割点，默认：[50, 100]。</p></li>
<li><p>device_number (int)：设备数，默认：8。</p></li>
</ul>
</li>
<li><p>less_bn：</p>
<ul class="simple">
<li><p>fn_flag (bool)：是否采用fn替换fc，默认：替换。</p></li>
<li><p>gc_flag (bool)：是否启用gc，默认：启用gc。</p></li>
</ul>
</li>
<li><p>grad_freeze：</p>
<ul class="simple">
<li><p>param_groups (int)：参数分组数量，默认值：10。</p></li>
<li><p>freeze_type (int)：梯度冻结策略，参数选择[0, 1]，默认值：1。</p></li>
<li><p>freeze_p (float)：梯度冻结概率，默认值：0.7。</p></li>
<li><p>total_steps (int)：总训练步数，默认值：65536。</p></li>
</ul>
</li>
<li><p>grad_accumulation：</p>
<ul class="simple">
<li><p>grad_accumulation_step (int)：累加梯度的步数，默认值：1。</p></li>
</ul>
</li>
<li><p>dim_reduce：</p>
<p>dim_reduce主要原理：</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
grad\_k &amp;= pca\_mat \cdot grad\\
dk &amp;= - bk \cdot grad\_k\\
sk &amp;= rho ^ m \cdot dk\\
delta\_loss &amp;= sigma \cdot grad\_k.T \cdot sk
\end{align}\end{split}\]</div>
<p>其中：</p>
<ul class="simple">
<li><p>pca_mat (array)：维度(k*n)，k是 <em>n_components</em> 的大小，n是权重的大小。</p></li>
<li><p>bk (array)：维度(k*k)，bk是拟牛顿法中的对称正定矩阵。</p></li>
</ul>
<p>我们需要找到满足以下条件的m：</p>
<div class="math notranslate nohighlight">
\[new\_loss &lt; old\_loss + delta\_loss\]</div>
<p>然后使用 <em>delta_grad</em> 去更新模型的权重：</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
grad\_k\_proj &amp;= pca\_mat.T \cdot grad\_k\\
new\_grad\_momentum &amp;= gamma \cdot old\_grad\_momentum + grad - grad\_k\_proj\\
delta\_grad &amp;= alpha \cdot new\_grad\_momentum - pca\_mat.T \cdot sk
\end{align}\end{split}\]</div>
<ul class="simple">
<li><p>rho (float)：超参，一般无需调整，默认值：0.55。</p></li>
<li><p>gamma (float)：超参，一般无需调整，默认值：0.9。</p></li>
<li><p>alpha (float)：超参，一般无需调整，默认值：0.001。</p></li>
<li><p>sigma (float)：超参，一般无需调整，默认值：0.4。</p></li>
<li><p>n_components (int)：PCA后的维度，默认值：32。</p></li>
<li><p>pca_mat_path (str)：PCA矩阵的加载路径，使用绝对路径，默认值：None。</p></li>
<li><p>weight_load_dir (str)：以checkpoint形式保存的权重加载路径，用于计算PCA矩阵，默认值：None。</p></li>
<li><p>timeout (int)：加载PCA矩阵的最长等待时间，默认值：1800(s)。</p></li>
</ul>
</li>
</ul>
<p>用户可以通过加载JSON文件或者直接使用字典来配置 <em>boost_config_dict</em>。
未配置的参数会使用默认值。</p>
</li>
</ul>
</dd>
<dt>异常：</dt><dd><ul class="simple">
<li><p><strong>ValueError</strong> - Boost的模式不在[“auto”, “manual”, “enable_all”, “disable_all”]这个列表中。</p></li>
</ul>
</dd>
<dt>支持平台：</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code></p>
</dd>
</dl>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.boost</span> <span class="kn">import</span> <span class="n">AutoBoost</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#1) when configuring the dict directly:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">boost_config_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;boost&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;mode&quot;</span><span class="p">:</span> <span class="s2">&quot;auto&quot;</span><span class="p">}}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">boost</span> <span class="o">=</span> <span class="n">AutoBoost</span><span class="p">(</span><span class="s2">&quot;O1&quot;</span><span class="p">,</span> <span class="n">boost_config_dict</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#2) when loading the dict from a json file:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">json</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">boost_json</span> <span class="o">=</span> <span class="s2">&quot;/path/boost_config.json&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">boost_json</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">boost_config_dict</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">fp</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">boost</span> <span class="o">=</span> <span class="n">AutoBoost</span><span class="p">(</span><span class="s2">&quot;O1&quot;</span><span class="p">,</span> <span class="n">boost_config_dict</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.AutoBoost.network_auto_process_eval">
<span class="sig-name descname"><span class="pre">network_auto_process_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">network</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/boost.html#AutoBoost.network_auto_process_eval"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.boost.AutoBoost.network_auto_process_eval" title="Permalink to this definition"></a></dt>
<dd><p>使用Boost算法推理。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>network</strong> (Cell) - 推理网络。</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.AutoBoost.network_auto_process_train">
<span class="sig-name descname"><span class="pre">network_auto_process_train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">network</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/boost.html#AutoBoost.network_auto_process_train"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.boost.AutoBoost.network_auto_process_train" title="Permalink to this definition"></a></dt>
<dd><p>使用Boost算法训练。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>network</strong> (Cell) - 训练网络。</p></li>
<li><p><strong>optimizer</strong> (Cell) - 用于更新权重的优化器。</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.boost.OptimizerProcess">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.boost.</span></span><span class="sig-name descname"><span class="pre">OptimizerProcess</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">opt</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/base.html#OptimizerProcess"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.boost.OptimizerProcess" title="Permalink to this definition"></a></dt>
<dd><p>处理Boost的优化器。目前支持给优化器添加梯度中心化和创建新的优化器。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>opt</strong> (Cell) - 使用的优化器。</p></li>
</ul>
</dd>
</dl>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">,</span> <span class="n">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.boost</span> <span class="kn">import</span> <span class="n">OptimizerProcess</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span>
<span class="gp">... </span>                                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;weight&#39;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">output</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">size</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer_process</span> <span class="o">=</span> <span class="n">OptimizerProcess</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer_process</span><span class="o">.</span><span class="n">add_grad_centralization</span><span class="p">(</span><span class="n">network</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer_process</span><span class="o">.</span><span class="n">generate_new_optimizer</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.OptimizerProcess.add_grad_centralization">
<span class="sig-name descname"><span class="pre">add_grad_centralization</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">network</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/base.html#OptimizerProcess.add_grad_centralization"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.boost.OptimizerProcess.add_grad_centralization" title="Permalink to this definition"></a></dt>
<dd><p>添加梯度中心化。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>network</strong> (Cell) - 训练网络。</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.OptimizerProcess.build_gc_params_group">
<span class="sig-name descname"><span class="pre">build_gc_params_group</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parameters</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/base.html#OptimizerProcess.build_gc_params_group"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.boost.OptimizerProcess.build_gc_params_group" title="Permalink to this definition"></a></dt>
<dd><p>构建梯度中心化的分组权重。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>params_dict</strong> (dict) - 训练权重的字典。</p></li>
<li><p><strong>parameters</strong> (list) - 训练权重的列表。</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.OptimizerProcess.build_params_dict">
<span class="sig-name descname"><span class="pre">build_params_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">network</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/base.html#OptimizerProcess.build_params_dict"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.boost.OptimizerProcess.build_params_dict" title="Permalink to this definition"></a></dt>
<dd><p>构建网络权重的字典。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>network</strong> (Cell) - 训练网络。</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.OptimizerProcess.generate_new_optimizer">
<span class="sig-name descname"><span class="pre">generate_new_optimizer</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/base.html#OptimizerProcess.generate_new_optimizer"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.boost.OptimizerProcess.generate_new_optimizer" title="Permalink to this definition"></a></dt>
<dd><p>生成新的优化器。</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.boost.ParameterProcess">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.boost.</span></span><span class="sig-name descname"><span class="pre">ParameterProcess</span></span><a class="reference internal" href="../_modules/mindspore/boost/base.html#ParameterProcess"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.boost.ParameterProcess" title="Permalink to this definition"></a></dt>
<dd><p>处理Boost网络的权重。当前支持创建分组参数和自动设置网络梯度切分点。</p>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">,</span> <span class="n">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.boost</span> <span class="kn">import</span> <span class="n">ParameterProcess</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span>
<span class="gp">... </span>                                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;weight&#39;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">weight2</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span>
<span class="gp">... </span>                                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;weight2&#39;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">matmul2</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="gp">... </span>        <span class="n">output2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight2</span><span class="p">)</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">output</span> <span class="o">+</span> <span class="n">output2</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">size</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">new_parameter</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()[:</span><span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">group_params</span> <span class="o">=</span> <span class="n">ParameterProcess</span><span class="o">.</span><span class="n">generate_group_params</span><span class="p">(</span><span class="n">new_parameter</span><span class="p">,</span> <span class="n">network</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">())</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.ParameterProcess.assign_parameter_group">
<span class="sig-name descname"><span class="pre">assign_parameter_group</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">split_point</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/base.html#ParameterProcess.assign_parameter_group"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.boost.ParameterProcess.assign_parameter_group" title="Permalink to this definition"></a></dt>
<dd><p>设置分组权重。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>parameters</strong> (list) - 训练网络的权重。</p></li>
<li><p><strong>split_point</strong> (list) - 网络梯度切分点。默认为None。</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.ParameterProcess.generate_group_params">
<span class="sig-name descname"><span class="pre">generate_group_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">origin_params</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/base.html#ParameterProcess.generate_group_params"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.boost.ParameterProcess.generate_group_params" title="Permalink to this definition"></a></dt>
<dd><p>创建分组权重。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>parameters</strong> (list) - 训练网络的新权重。</p></li>
<li><p><strong>origin_params</strong> (list) - 训练网络的初始权重。</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.boost.BoostTrainOneStepCell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.boost.</span></span><span class="sig-name descname"><span class="pre">BoostTrainOneStepCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">network</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sens</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/boost_cell_wrapper.html#BoostTrainOneStepCell"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.boost.BoostTrainOneStepCell" title="Permalink to this definition"></a></dt>
<dd><p>Boost网络训练封装类。</p>
<p>用优化器封装网络。使用输入训练网络来获取结果。反向图在 <em>construct</em> 函数中创建，以更新参数，并且支持多种不同的并行模式。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>network</strong> (Cell) - 训练网络，当前网络只支持单个输出。</p></li>
<li><p><strong>optimizer</strong> (Union[Cell]) - 用于更新网络参数的优化器。</p></li>
<li><p><strong>sens</strong> (numbers.Number) - 作为反向传播输入要填充的缩放数，默认值为1.0。</p></li>
</ul>
</dd>
<dt>输入：</dt><dd><ul class="simple">
<li><p><strong>(*inputs)</strong> (Tuple(Tensor)) - 网络的所有输入组成的元组。</p></li>
</ul>
</dd>
<dt>输出：</dt><dd><p>Tuple，包含三个Tensor，分别为损失函数值、溢出状态和当前损失缩放系数。</p>
<ul class="simple">
<li><p>loss(Tensor)，标量Tensor。</p></li>
<li><p>overflow(Tensor)，标量Tensor，类型为bool。</p></li>
<li><p>loss scaling value(Tensor)，标量Tensor。</p></li>
</ul>
</dd>
<dt>异常：</dt><dd><ul class="simple">
<li><p><strong>TypeError</strong> - 如果 <cite>sens</cite> 不是一个数字。</p></li>
</ul>
</dd>
<dt>支持平台：</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p>
</dd>
</dl>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">boost</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#1) Using the WithLossCell existing provide</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">WithLossCell</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">train_net</span> <span class="o">=</span> <span class="n">boost</span><span class="o">.</span><span class="n">BoostTrainOneStepCell</span><span class="p">(</span><span class="n">loss_net</span><span class="p">,</span> <span class="n">optim</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#2) Using user-defined WithLossCell</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">MyWithLossCell</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">... </span>   <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">backbone</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">):</span>
<span class="gp">... </span>       <span class="nb">super</span><span class="p">(</span><span class="n">MyWithLossCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">auto_prefix</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">... </span>       <span class="bp">self</span><span class="o">.</span><span class="n">_backbone</span> <span class="o">=</span> <span class="n">backbone</span>
<span class="gp">... </span>       <span class="bp">self</span><span class="o">.</span><span class="n">_loss_fn</span> <span class="o">=</span> <span class="n">loss_fn</span>
<span class="gp">...</span>
<span class="gp">... </span>   <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
<span class="gp">... </span>       <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backbone</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">... </span>       <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_fn</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">... </span>   <span class="nd">@property</span>
<span class="gp">... </span>   <span class="k">def</span> <span class="nf">backbone_network</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">... </span>       <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backbone</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_net</span> <span class="o">=</span> <span class="n">MyWithLossCell</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">train_net</span> <span class="o">=</span> <span class="n">boost</span><span class="o">.</span><span class="n">BoostTrainOneStepCell</span><span class="p">(</span><span class="n">loss_net</span><span class="p">,</span> <span class="n">optim</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.BoostTrainOneStepCell.adasum_process">
<span class="sig-name descname"><span class="pre">adasum_process</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grads</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/boost_cell_wrapper.html#BoostTrainOneStepCell.adasum_process"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.boost.BoostTrainOneStepCell.adasum_process" title="Permalink to this definition"></a></dt>
<dd><p>使用Adasum算法训练。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>loss</strong> (Tensor) - 网络训练的loss值。</p></li>
<li><p><strong>grads</strong> (tuple(Tensor)) - 网络训练过程中的梯度。</p></li>
</ul>
</dd>
<dt>返回：</dt><dd><p>Tensor，网络训练过程中得到的loss值。</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.BoostTrainOneStepCell.check_adasum_enable">
<span class="sig-name descname"><span class="pre">check_adasum_enable</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/boost_cell_wrapper.html#BoostTrainOneStepCell.check_adasum_enable"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.boost.BoostTrainOneStepCell.check_adasum_enable" title="Permalink to this definition"></a></dt>
<dd><p>Adasum算法仅在多卡或者多机场景生效，并且要求卡数符合2的n次方，该函数用来判断adasum算法能否生效。</p>
<dl class="simple">
<dt>返回：</dt><dd><p>enable_adasum (bool)，Adasum算法是否生效。</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.BoostTrainOneStepCell.check_dim_reduce_enable">
<span class="sig-name descname"><span class="pre">check_dim_reduce_enable</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/boost_cell_wrapper.html#BoostTrainOneStepCell.check_dim_reduce_enable"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.boost.BoostTrainOneStepCell.check_dim_reduce_enable" title="Permalink to this definition"></a></dt>
<dd><p>获取当前是否使用降维二阶训练算法训练。</p>
<dl class="simple">
<dt>返回：</dt><dd><p>enable_dim_reduce (bool)，降维二阶训练算法是否生效。</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.BoostTrainOneStepCell.gradient_accumulation_process">
<span class="sig-name descname"><span class="pre">gradient_accumulation_process</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sens</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/boost_cell_wrapper.html#BoostTrainOneStepCell.gradient_accumulation_process"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.boost.BoostTrainOneStepCell.gradient_accumulation_process" title="Permalink to this definition"></a></dt>
<dd><p>使用梯度累积算法训练。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>loss</strong> (Tensor) - 网络训练的loss值。</p></li>
<li><p><strong>grads</strong> (tuple(Tensor)) - 网络训练过程中的梯度。</p></li>
<li><p><strong>sens</strong> (Tensor) - 作为反向传播输入要填充的缩放数。</p></li>
<li><p><strong>inputs</strong> (tuple(Tensor)) - 网络训练的输入。</p></li>
</ul>
</dd>
<dt>返回：</dt><dd><p>Tensor，网络训练过程中得到的loss值。</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.BoostTrainOneStepCell.gradient_freeze_process">
<span class="sig-name descname"><span class="pre">gradient_freeze_process</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/boost_cell_wrapper.html#BoostTrainOneStepCell.gradient_freeze_process"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.boost.BoostTrainOneStepCell.gradient_freeze_process" title="Permalink to this definition"></a></dt>
<dd><p>使用梯度冻结算法训练。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>inputs</strong> (tuple(Tensor)) - 网络训练的输入。</p></li>
</ul>
</dd>
<dt>返回：</dt><dd><p>Tensor，网络训练过程中得到的loss值。</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.boost.BoostTrainOneStepWithLossScaleCell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.boost.</span></span><span class="sig-name descname"><span class="pre">BoostTrainOneStepWithLossScaleCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">network</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_sense</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/boost_cell_wrapper.html#BoostTrainOneStepWithLossScaleCell"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.boost.BoostTrainOneStepWithLossScaleCell" title="Permalink to this definition"></a></dt>
<dd><p>使用混合精度功能的Boost训练网络。</p>
<p>实现了包含损失缩放（loss scale）的单次训练。它使用网络、优化器和用于更新损失缩放系数（loss scale）的Cell(或一个Tensor)作为参数。可在host侧或device侧更新损失缩放系数。BoostTrainOneStepWithLossScaleCell会被编译成图，其中inputs作为输入数据。张量类型参数 <cite>scale_sense</cite> 作为损失缩放时使用的值。
如果需要在host侧更新，使用Tensor作为 <cite>scale_sense</cite> 。如果需要在device侧更新，使用可更新损失缩放系数的Cell实例作为 <cite>scale_sense</cite> 。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>network</strong> (Cell) - 训练网络，当前网络只支持单个输出。</p></li>
<li><p><strong>optimizer</strong> (Cell) - 用于更新网络参数的优化器。</p></li>
<li><p><strong>scale_sense</strong> (Union[Tensor, Cell]) - 如果此值为Cell类型，<cite>BoostTrainOneStepWithLossScaleCell</cite> 会调用它来更新损失缩放系数。如果此值为Tensor类型，可调用 <cite>set_sense_scale</cite> 来更新损失缩放系数，shape为 <span class="math notranslate nohighlight">\(()\)</span> 或 <span class="math notranslate nohighlight">\((1,)\)</span> 。</p></li>
</ul>
</dd>
<dt>输入：</dt><dd><ul class="simple">
<li><p><strong>(*inputs)</strong> (Tuple(Tensor)) - 网络的所有输入组成的元组。</p></li>
</ul>
</dd>
<dt>输出：</dt><dd><p>Tuple，包含三个Tensor，分别为损失函数值、溢出状态和当前损失缩放系数。</p>
<ul class="simple">
<li><p><strong>loss</strong> (Tensor) - 标量Tensor。</p></li>
<li><p><strong>overflow</strong> (Tensor) - 标量Tensor，类型为bool。</p></li>
<li><p><strong>loss scaling value</strong> (Tensor) - 标量Tensor。</p></li>
</ul>
</dd>
<dt>异常：</dt><dd><ul class="simple">
<li><p><strong>TypeError</strong> - <cite>scale_sense</cite> 既不是Cell，也不是Tensor。</p></li>
<li><p><strong>ValueError</strong> - <cite>scale_sense</cite> 的shape既不是(1,)也不是()。</p></li>
</ul>
</dd>
<dt>支持平台：</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">,</span> <span class="n">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">WithLossCell</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">boost</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span>
<span class="gp">... </span>                                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;weight&#39;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">output</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">size</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#1) when the type of scale_sense is Cell:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net_with_loss</span> <span class="o">=</span> <span class="n">WithLossCell</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">manager</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">DynamicLossScaleUpdateCell</span><span class="p">(</span><span class="n">loss_scale_value</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="mi">12</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">scale_window</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">train_network</span> <span class="o">=</span> <span class="n">boost</span><span class="o">.</span><span class="n">BoostTrainOneStepWithLossScaleCell</span><span class="p">(</span><span class="n">net_with_loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scale_sense</span><span class="o">=</span><span class="n">manager</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">out_features</span><span class="p">,</span> <span class="n">in_features</span><span class="p">]),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">out_features</span><span class="p">,]),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">train_network</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#2) when the type of scale_sense is Tensor:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net_with_loss</span> <span class="o">=</span> <span class="n">WithLossCell</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">size</span><span class="p">,</span> <span class="n">in_features</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">label</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">size</span><span class="p">,</span> <span class="n">out_features</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scaling_sens</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">train_network</span> <span class="o">=</span> <span class="n">boost</span><span class="o">.</span><span class="n">BoostTrainOneStepWithLossScaleCell</span><span class="p">(</span><span class="n">net_with_loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scale_sense</span><span class="o">=</span><span class="n">scaling_sens</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">train_network</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.boost.LessBN">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.boost.</span></span><span class="sig-name descname"><span class="pre">LessBN</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">network</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fn_flag</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/less_batch_normalization.html#LessBN"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.boost.LessBN" title="Permalink to this definition"></a></dt>
<dd><p>LessBN算法，可以在不损失网络精度的前提下，自动减少网络中批归一化（Batch Normalization）的数量，来提升网络性能。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>network</strong> (Cell) - 待训练的网络模型。</p></li>
<li><p><strong>fn_flag</strong> (bool) - 是否将网络中最后一个全连接层替换为全归一化层。默认值：False。</p></li>
</ul>
</dd>
</dl>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">boost</span><span class="o">.</span><span class="n">LessBN</span><span class="p">(</span><span class="n">network</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.boost.GradientFreeze">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.boost.</span></span><span class="sig-name descname"><span class="pre">GradientFreeze</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">param_groups</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">freeze_type</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">freeze_p</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">total_steps</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/grad_freeze.html#GradientFreeze"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.boost.GradientFreeze" title="Permalink to this definition"></a></dt>
<dd><p>梯度冻结算法，根据指定策略随机冻结某些层的梯度，来提升网络训练性能。
冻结的层数和冻结的概率均可由用户配置。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>param_groups</strong> (Union[tuple, list]) - 梯度冻结训练的权重。</p></li>
<li><p><strong>freeze_type</strong> (int) - 梯度冻结训练的策略。</p></li>
<li><p><strong>freeze_p</strong> (float) - 梯度冻结训练的概率。</p></li>
<li><p><strong>total_steps</strong> (int) - 整个训练过程的总的步数。</p></li>
</ul>
</dd>
</dl>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">gradient_freeze_class</span> <span class="o">=</span> <span class="n">boost</span><span class="o">.</span><span class="n">GradientFreeze</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">2000</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="n">gradient_freeze_class</span><span class="o">.</span><span class="n">freeze_generate</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.GradientFreeze.freeze_generate">
<span class="sig-name descname"><span class="pre">freeze_generate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">network</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/grad_freeze.html#GradientFreeze.freeze_generate"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.boost.GradientFreeze.freeze_generate" title="Permalink to this definition"></a></dt>
<dd><p>生成梯度冻结的网络与优化器。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>network</strong> (Cell) - 训练网络。</p></li>
<li><p><strong>optimizer</strong> (Cell) - 用于更新权重的优化器。</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.GradientFreeze.generate_freeze_index_sequence">
<span class="sig-name descname"><span class="pre">generate_freeze_index_sequence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameter_groups_number</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">freeze_strategy</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">freeze_p</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">total_steps</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/grad_freeze.html#GradientFreeze.generate_freeze_index_sequence"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.boost.GradientFreeze.generate_freeze_index_sequence" title="Permalink to this definition"></a></dt>
<dd><p>生成梯度冻结每一步需要冻结的层数。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>parameter_groups_number</strong> (int) - 梯度冻结训练的权重个数。</p></li>
<li><p><strong>freeze_strategy</strong> (int) - 梯度冻结训练的策略。</p></li>
<li><p><strong>freeze_p</strong> (float) - 梯度冻结训练的概率。</p></li>
<li><p><strong>total_steps</strong> (int) - 整个训练过程的总的步数。</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.GradientFreeze.split_parameters_groups">
<span class="sig-name descname"><span class="pre">split_parameters_groups</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">net</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">freeze_para_groups_number</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/grad_freeze.html#GradientFreeze.split_parameters_groups"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.boost.GradientFreeze.split_parameters_groups" title="Permalink to this definition"></a></dt>
<dd><p>拆分用于梯度冻结训练的权重。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>net</strong> (Cell) - 训练网络。</p></li>
<li><p><strong>freeze_para_groups_number</strong> (int) - 梯度冻结训练的权重个数。</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.boost.FreezeOpt">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.boost.</span></span><span class="sig-name descname"><span class="pre">FreezeOpt</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">opt</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_parameter_groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_strategy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/grad_freeze.html#FreezeOpt"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.boost.FreezeOpt" title="Permalink to this definition"></a></dt>
<dd><p>支持梯度冻结训练的优化器。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>opt</strong> (Cell) - 非冻结优化器实例，如 <em>Momentum</em>，<em>SGD</em>。</p></li>
<li><p><strong>train_parameter_groups</strong> (Union[tuple, list]) - 梯度冻结训练的权重。</p></li>
<li><p><strong>train_strategy</strong> (Union[tuple(int), list(int), Tensor]) - 梯度冻结训练的策略。</p></li>
</ul>
</dd>
<dt>支持平台：</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mindspore.boost.freeze_cell">
<span class="sig-prename descclassname"><span class="pre">mindspore.boost.</span></span><span class="sig-name descname"><span class="pre">freeze_cell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">reducer_flag</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">network</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sens</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_grad_accumulation</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mean</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">degree</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_accumulation_step</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/grad_freeze.html#freeze_cell"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.boost.freeze_cell" title="Permalink to this definition"></a></dt>
<dd><p>提供带梯度冻结的网络Cell。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>reducer_flag</strong> (bool) - 是否分布式训练。</p></li>
<li><p><strong>network</strong> (Cell) - 训练网络。</p></li>
<li><p><strong>optimizer</strong> (Cell) - 优化器。</p></li>
<li><p><strong>sens</strong> (numbers.Number) - 损失缩放系数。</p></li>
<li><p><strong>grad</strong> (tuple(Tensor)) - 网络梯度。</p></li>
<li><p><strong>use_grad_accumulation</strong> (bool) - 是否使用梯度累积。</p></li>
<li><p><strong>mean</strong> (bool) - 可选参数，梯度是否求平均，仅分布式训练时生效。默认值为None。</p></li>
<li><p><strong>degree</strong> (int) - 可选参数，device卡数，仅分布式训练时生效。默认值为None。</p></li>
<li><p><strong>max_accumulation_step</strong> (int) - 可选参数，梯度累积步数。默认值为1。</p></li>
</ul>
</dd>
</dl>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">,</span> <span class="n">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.boost.grad_freeze</span> <span class="kn">import</span> <span class="n">freeze_cell</span><span class="p">,</span> <span class="n">FreezeOpt</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span>
<span class="gp">... </span>                                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;weight&#39;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">output</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">FreezeOpt</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grad</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">GradOperation</span><span class="p">(</span><span class="n">get_by_list</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sens_param</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">freeze_nets</span> <span class="o">=</span> <span class="n">freeze_cell</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.boost.GradientAccumulation">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.boost.</span></span><span class="sig-name descname"><span class="pre">GradientAccumulation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">max_accumulation_step</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/grad_accumulation.html#GradientAccumulation"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.boost.GradientAccumulation" title="Permalink to this definition"></a></dt>
<dd><p>梯度累积算法，在累积多个step的梯度之后，再用来更新网络权重，可以提高训练效率。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>max_accumulation_step</strong> (int) - 累积梯度的步数。</p></li>
<li><p><strong>optimizer</strong> (Cell) - 网络训练使用的优化器。</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.boost.AdaSum">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.boost.</span></span><span class="sig-name descname"><span class="pre">AdaSum</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rank</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_number</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group_number</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parameter_tuple</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/adasum.html#AdaSum"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.boost.AdaSum" title="Permalink to this definition"></a></dt>
<dd><p>Adaptive Summation(AdaSum)是一种优化深度学习模型并行训练的算法，它可以提升不同规模集群训练的精度，减小不同规模集群调参难度。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>rank</strong> (int) - 总的训练的卡数。</p></li>
<li><p><strong>device_number</strong> (int) - 单机的卡数。</p></li>
<li><p><strong>group_number</strong> (int) - 分组的数量。</p></li>
<li><p><strong>parameter_tuple</strong> (Tuple(Parameter)) - 网络训练权重组成的元组。</p></li>
</ul>
</dd>
<dt>输入：</dt><dd><ul class="simple">
<li><p><strong>delta_weights</strong> (Tuple(Tensor)) - 梯度tuple。</p></li>
<li><p><strong>parameters</strong> (Tuple(Parameter)) - 当前权重组成的元组。</p></li>
<li><p><strong>old_parameters</strong> (Tuple(Parameter)) - 旧的权重组成的元组。</p></li>
</ul>
</dd>
<dt>输出：</dt><dd><ul class="simple">
<li><p><strong>adasum_parameters</strong> (Tuple(Tensor)) - adasum处理后更新的权重。</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.boost.DimReduce">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.boost.</span></span><span class="sig-name descname"><span class="pre">DimReduce</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">network</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pca_mat_local</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_components</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rho</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigma</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank_size</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/dim_reduce.html#DimReduce"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.boost.DimReduce" title="Permalink to this definition"></a></dt>
<dd><p>降维训练(dimension reduce training)是一种优化深度学习模型训练的算法，它可以加速模型的收敛。</p>
<p>算法主要原理：</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
grad\_k &amp;= pca\_mat \cdot grad\\
dk &amp;= - bk \cdot grad\_k\\
sk &amp;= rho ^ m \cdot dk\\
delta\_loss &amp;= sigma \cdot grad\_k.T \cdot sk
\end{align}\end{split}\]</div>
<p>其中:</p>
<ul class="simple">
<li><p>pca_mat (array): PCA矩阵，维度(k*n)，k是 <cite>n_components</cite> 的大小，n是权重的大小。</p></li>
<li><p>bk (array): 维度(k*k)，bk是拟牛顿法中的对称正定矩阵。</p></li>
</ul>
<p>我们需要找到满足以下条件的m:</p>
<div class="math notranslate nohighlight">
\[new\_loss &lt; old\_loss + delta\_loss\]</div>
<p>然后使用delta_grad去更新模型的权重:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
grad\_k\_proj &amp;= pca\_mat.T \cdot grad\_k\\
new\_grad\_momentum &amp;= gamma \cdot old\_grad\_momentum + grad - grad\_k\_proj\\
delta\_grad &amp;= alpha \cdot new\_grad\_momentum - pca\_mat.T \cdot sk
\end{align}\end{split}\]</div>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>network</strong> (Cell) - 训练网络，只支持单输出。</p></li>
<li><p><strong>optimizer</strong> (Union[Cell]) - 更新权重的优化器。</p></li>
<li><p><strong>weight</strong> (Tuple(Parameter)) - 网络权重组成的元组。</p></li>
<li><p><strong>pca_mat_local</strong> (numpy.ndarray) - 用于PCA操作的，经过切分的PCA转换矩阵，维度为k*n，k是切分的 <cite>n_components</cite> 的大小，n是权重的大小。</p></li>
<li><p><strong>n_components</strong> (int) - PCA的主成分维度(components)。</p></li>
<li><p><strong>rho</strong> (float) - 超参。</p></li>
<li><p><strong>gamma</strong> (float) - 超参。</p></li>
<li><p><strong>alpha</strong> (float) - 超参。</p></li>
<li><p><strong>sigma</strong> (float) - 超参。</p></li>
<li><p><strong>rank</strong> (int) - Rank编号。</p></li>
<li><p><strong>rank_size</strong> (int) - Rank总数。</p></li>
</ul>
</dd>
<dt>输入：</dt><dd><ul class="simple">
<li><p><strong>loss</strong> (Tensor) - 网络loss，标量Tensor。</p></li>
<li><p><strong>old_grad</strong> (Tuple(Tensor)) - 网络权重提取组成的元组。</p></li>
<li><p><strong>weight</strong> (Tuple(Tensor)) - 网络权重组成的元组。</p></li>
<li><p><strong>weight_clone</strong> (Tuple(Tensor)) - 网络权重的副本。</p></li>
<li><p><strong>(*inputs)</strong> (Tuple(Tensor)) - 网络的所有输入组成的元组。</p></li>
</ul>
</dd>
<dt>输出：</dt><dd><ul class="simple">
<li><p><strong>loss</strong> (Tensor) - 网络loss，标量Tensor。</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.boost.GroupLossScaleManager">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.boost.</span></span><span class="sig-name descname"><span class="pre">GroupLossScaleManager</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">init_loss_scale</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_scale_groups</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/group_loss_scale_manager.html#GroupLossScaleManager"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.boost.GroupLossScaleManager" title="Permalink to this definition"></a></dt>
<dd><p>增强型混合精度算法支持不同loss scale的多层应用和损失尺度的动态更新。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>init_loss_scale</strong> (Number) - 初始化loss scale。</p></li>
<li><p><strong>loss_scale_groups</strong> (List) - 从参数列表里分离出来的loss scale组。</p></li>
</ul>
</dd>
<dt>输入：</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - 最后一个operator的输出。</p></li>
<li><p><strong>layer1</strong> (int) - 当前网络层的值。</p></li>
<li><p><strong>layer2</strong> (int) - 最后一个网络层的值。</p></li>
</ul>
</dd>
<dt>输出：</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - _DynamicLossScale operator的输出。</p></li>
</ul>
</dd>
<dt>支持平台：</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code></p>
</dd>
</dl>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">boost</span><span class="p">,</span> <span class="n">nn</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enhanced_amp</span><span class="p">,</span> <span class="n">num_class</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">num_channel</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">num_channel</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="o">*</span><span class="mi">5</span><span class="o">*</span><span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">,</span> <span class="n">weight_init</span><span class="o">=</span><span class="s1">&#39;ones&#39;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">,</span> <span class="n">weight_init</span><span class="o">=</span><span class="s1">&#39;ones&#39;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="n">num_class</span><span class="p">,</span> <span class="n">weight_init</span><span class="o">=</span><span class="s1">&#39;ones&#39;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">max_pool2d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">enhanced_amp</span> <span class="o">=</span> <span class="n">enhanced_amp</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enhanced_amp</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">... </span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
<span class="gp">... </span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
<span class="gp">... </span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">... </span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enhanced_amp</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">... </span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="gp">... </span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="gp">... </span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">... </span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enhanced_amp</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">x</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_scale_manager</span> <span class="o">=</span> <span class="n">boost</span><span class="o">.</span><span class="n">GroupLossScaleManager</span><span class="p">(</span><span class="mi">4096</span><span class="p">,</span> <span class="p">[])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="n">loss_scale_manager</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">param_group1</span> <span class="o">=</span> <span class="p">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">param_group2</span> <span class="o">=</span> <span class="p">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">in</span> <span class="n">param</span><span class="o">.</span><span class="n">name</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">param_group1</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">else</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">param_group2</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_scale_manager</span><span class="o">.</span><span class="n">loss_scale_groups</span> <span class="o">=</span> <span class="p">[</span><span class="n">param_group1</span><span class="p">,</span> <span class="n">param_group2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">boost_config_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;boost&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;mode&quot;</span><span class="p">:</span> <span class="s2">&quot;manual&quot;</span><span class="p">,</span> <span class="s2">&quot;less_bn&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;grad_freeze&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;adasum&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>         <span class="o">&gt;&gt;&gt;</span>                      <span class="s2">&quot;grad_accumulation&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;dim_reduce&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;loss_scale_group&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optim</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">loss_scale_manager</span><span class="o">=</span><span class="n">loss_scale_manager</span><span class="p">,</span>         <span class="o">&gt;&gt;&gt;</span>               <span class="n">boost_level</span><span class="o">=</span><span class="s2">&quot;O1&quot;</span><span class="p">,</span> <span class="n">boost_config_dict</span><span class="o">=</span><span class="n">boost_config_dict</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># For details about how to build the dataset, please refer to the variable `dataset_train` in tutorial</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># document on the official website:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># https://www.mindspore.cn/tutorials/zh-CN/r1.10/beginner/quick_start.html</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">create_custom_dataset</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.GroupLossScaleManager.get_loss_scale">
<span class="sig-name descname"><span class="pre">get_loss_scale</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/group_loss_scale_manager.html#GroupLossScaleManager.get_loss_scale"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.boost.GroupLossScaleManager.get_loss_scale" title="Permalink to this definition"></a></dt>
<dd><p>获取loss scale的值。</p>
<dl class="simple">
<dt>返回：</dt><dd><p>bool，<cite>loss_scale</cite> 的值。</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.GroupLossScaleManager.get_update_cell">
<span class="sig-name descname"><span class="pre">get_update_cell</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/group_loss_scale_manager.html#GroupLossScaleManager.get_update_cell"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.boost.GroupLossScaleManager.get_update_cell" title="Permalink to this definition"></a></dt>
<dd><p>返回 <a class="reference internal" href="#mindspore.boost.GroupLossScaleManager" title="mindspore.boost.GroupLossScaleManager"><code class="xref py py-class docutils literal notranslate"><span class="pre">mindspore.boost.GroupLossScaleManager</span></code></a> 实例。</p>
<dl class="simple">
<dt>返回：</dt><dd><p><a class="reference internal" href="#mindspore.boost.GroupLossScaleManager" title="mindspore.boost.GroupLossScaleManager"><code class="xref py py-class docutils literal notranslate"><span class="pre">mindspore.boost.GroupLossScaleManager</span></code></a> 实例。</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.GroupLossScaleManager.set_loss_scale_status">
<span class="sig-name descname"><span class="pre">set_loss_scale_status</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss_scale_number</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_loss_scale</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/group_loss_scale_manager.html#GroupLossScaleManager.set_loss_scale_status"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.boost.GroupLossScaleManager.set_loss_scale_status" title="Permalink to this definition"></a></dt>
<dd><p>生成动态loss scale元组，并设置溢出状态列表。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>loss_scale_number</strong> (int) - loss scale的数量。</p></li>
<li><p><strong>init_loss_scale</strong> (float) - 已初始化的loss scale。</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.GroupLossScaleManager.update_loss_scale_status">
<span class="sig-name descname"><span class="pre">update_loss_scale_status</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">update_ratio</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/group_loss_scale_manager.html#GroupLossScaleManager.update_loss_scale_status"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.boost.GroupLossScaleManager.update_loss_scale_status" title="Permalink to this definition"></a></dt>
<dd><p>更新动态loss scale。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>layer</strong> (int) - 当前层。</p></li>
<li><p><strong>update_ratio</strong> (float) - 更新loss scale的当前比例。</p></li>
</ul>
</dd>
<dt>输出：</dt><dd><p>float，新loss scale的值。</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="scipy/mindspore.scipy.sparse.linalg.gmres.html" class="btn btn-neutral float-left" title="mindspore.scipy.sparse.linalg.gmres" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../note/api_mapping/pytorch_api_mapping.html" class="btn btn-neutral float-right" title="PyTorch与MindSpore API映射表" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
        <script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>