<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore.context &mdash; MindSpore master documentation</title>
      <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/training.css" type="text/css" /><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/js/training.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="mindspore.dataset" href="mindspore.dataset.html" />
    <link rel="prev" title="mindspore.communication" href="mindspore.communication.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">设计</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../design/technical_white_paper.html">技术白皮书</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/all_scenarios_architecture.html">全场景统一</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/gradient.html">函数式微分编程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/dynamic_graph_and_static_graph.html">动静态图结合</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/heterogeneous_training.html">异构并行训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/distributed_training_design.html">分布式并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/mindir.html">中间表达MindIR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/data_engine.html">高性能数据处理引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/graph_kernel_fusion.html">图算融合加速引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/second_order_optimizer.html">二阶优化</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/zh-CN/r1.7/training_visual_design.html">可视化调试调优↗</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindarmour/docs/zh-CN/r1.7/design.html">安全可信↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/glossary.html">术语</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">参考</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/benchmark.html">基准性能</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/network_list.html">网络支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/operator_list.html">算子支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/env_var_list.html">环境变量</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/syntax_list.html">语法支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping.html">API映射</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">mindspore.context</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.audio.html">mindspore.dataset.audio</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.config.html">mindspore.dataset.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.text.html">mindspore.dataset.text</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.vision.html">mindspore.dataset.vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.nn.transformer.html">mindspore.nn.transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.ops.functional.html">mindspore.ops.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.parallel.html">mindspore.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.parallel.nn.html">mindspore.parallel.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.profiler.html">mindspore.Profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.7/api_cpp/mindspore.html">C++ API↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">迁移指南</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/overview.html">概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/preparation.html">准备工作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/script_analysis.html">网络脚本分析</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/script_development.html">网络脚本开发</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/neural_network_debug.html">网络调试</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/accuracy_optimization.html">精度调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/performance_optimization.html">性能调试</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/inference.html">推理执行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/sample_code.html">网络迁移调试实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/faq.html">常见问题</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq/installation.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/data_processing.html">数据处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/implement_problem.html">执行问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/network_compilation.html">网络编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/operators_compile.html">算子编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/usage_migrate_3rd.html">第三方框架迁移使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/performance_tuning.html">性能调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/precision_tuning.html">精度调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/distributed_configure.html">分布式配置</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/inference.html">推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/feature_advice.html">特性咨询</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>mindspore.context</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/api_python/mindspore.context.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="mindspore-context">
<h1>mindspore.context<a class="headerlink" href="#mindspore-context" title="Permalink to this headline"></a></h1>
<p>MindSpore context，用于配置当前执行环境，包括执行模式、执行后端和其他特性开关。</p>
<dl class="py function">
<dt class="sig sig-object py" id="mindspore.context.set_context">
<span class="sig-prename descclassname"><span class="pre">mindspore.context.</span></span><span class="sig-name descname"><span class="pre">set_context</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/context.html#set_context"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.context.set_context" title="Permalink to this definition"></a></dt>
<dd><p>设置运行环境的context。</p>
<p>在运行程序之前，应配置context。如果没有配置，默认情况下将根据设备目标进行自动设置。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>设置属性时，必须输入属性名称。</p>
</div>
<p>某些配置适用于特定的设备，有关详细信息，请参见下表：</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 30%" />
<col style="width: 36%" />
<col style="width: 34%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>功能分类</p></th>
<th class="head"><p>配置参数</p></th>
<th class="head"><p>硬件平台支持</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td rowspan="5"><p>系统配置</p></td>
<td><p>device_id</p></td>
<td><p>CPU/GPU/Ascend</p></td>
</tr>
<tr class="row-odd"><td><p>device_target</p></td>
<td><p>CPU/GPU/Ascend</p></td>
</tr>
<tr class="row-even"><td><p>max_device_memory</p></td>
<td><p>GPU/Ascend</p></td>
</tr>
<tr class="row-odd"><td><p>variable_memory_max_size</p></td>
<td><p>Ascend</p></td>
</tr>
<tr class="row-even"><td><p>mempool_block_size</p></td>
<td><p>GPU/Ascend</p></td>
</tr>
<tr class="row-odd"><td rowspan="11"><p>调试配置</p></td>
<td><p>save_graphs</p></td>
<td><p>CPU/GPU/Ascend</p></td>
</tr>
<tr class="row-even"><td><p>save_graphs_path</p></td>
<td><p>CPU/GPU/Ascend</p></td>
</tr>
<tr class="row-odd"><td><p>enable_dump</p></td>
<td><p>Ascend</p></td>
</tr>
<tr class="row-even"><td><p>save_dump_path</p></td>
<td><p>Ascend</p></td>
</tr>
<tr class="row-odd"><td><p>enable_profiling</p></td>
<td><p>Ascend</p></td>
</tr>
<tr class="row-even"><td><p>profiling_options</p></td>
<td><p>Ascend</p></td>
</tr>
<tr class="row-odd"><td><p>print_file_path</p></td>
<td><p>Ascend</p></td>
</tr>
<tr class="row-even"><td><p>env_config_path</p></td>
<td><p>CPU/GPU/Ascend</p></td>
</tr>
<tr class="row-odd"><td><p>precompile_only</p></td>
<td><p>CPU/GPU/Ascend</p></td>
</tr>
<tr class="row-even"><td><p>reserve_class_name_in_scope</p></td>
<td><p>CPU/GPU/Ascend</p></td>
</tr>
<tr class="row-odd"><td><p>pynative_synchronize</p></td>
<td><p>GPU/Ascend</p></td>
</tr>
<tr class="row-even"><td rowspan="12"><p>执行控制</p></td>
<td><p>mode</p></td>
<td><p>CPU/GPU/Ascend</p></td>
</tr>
<tr class="row-odd"><td><p>enable_graph_kernel</p></td>
<td><p>Ascend/GPU</p></td>
</tr>
<tr class="row-even"><td><p>graph_kernel_flags</p></td>
<td><p>Ascend/GPU</p></td>
</tr>
<tr class="row-odd"><td><p>enable_reduce_precision</p></td>
<td><p>Ascend</p></td>
</tr>
<tr class="row-even"><td><p>auto_tune_mode</p></td>
<td><p>Ascend</p></td>
</tr>
<tr class="row-odd"><td><p>check_bprop</p></td>
<td><p>CPU/GPU/Ascend</p></td>
</tr>
<tr class="row-even"><td><p>max_call_depth</p></td>
<td><p>CPU/GPU/Ascend</p></td>
</tr>
<tr class="row-odd"><td><p>enable_sparse</p></td>
<td><p>CPU/GPU/Ascend</p></td>
</tr>
<tr class="row-even"><td><p>grad_for_scalar</p></td>
<td><p>CPU/GPU/Ascend</p></td>
</tr>
<tr class="row-odd"><td><p>enable_compile_cache</p></td>
<td><p>CPU/GPU/Ascend</p></td>
</tr>
<tr class="row-even"><td><p>runtime_num_threads</p></td>
<td><p>CPU/GPU/Ascend</p></td>
</tr>
<tr class="row-odd"><td><p>compile_cache_path</p></td>
<td><p>CPU/GPU/Ascend</p></td>
</tr>
</tbody>
</table>
<p><strong>参数：</strong></p>
<ul>
<li><p><strong>device_id</strong> (int) - 表示目标设备的ID，其值必须在[0, device_num_per_host-1]范围中，且 <cite>device_num_per_host</cite> 的值不应超过4096。默认值：0。</p></li>
<li><p><strong>device_target</strong> (str) - 表示待运行的目标设备，支持’Ascend’、’GPU’和’CPU’。如果未设置此参数，则使用MindSpore包对应的后端设备。</p></li>
<li><p><strong>max_device_memory</strong> (str) - 设置设备可用的最大内存。目前，仅在GPU上支持。格式为“xxGB”。默认值：1024GB。实际使用的内存大小是设备的可用内存和 <cite>max_device_memory</cite> 值中的最小值。</p></li>
<li><p><strong>variable_memory_max_size</strong> (str) - 设置可变内存的最大值。默认值：30GB。</p></li>
<li><p><strong>mempool_block_size</strong> (str) - 设置PyNative模式下设备内存池的块大小。格式为“xxGB”。默认值：1GB。最小值是1GB。实际使用的内存池块大小是设备的可用内存和 <cite>mempool_block_size</cite> 值中的最小值。</p></li>
<li><p><strong>save_graphs</strong> (bool) - 表示是否保存计算图。默认值：False。当 <cite>save_graphs</cite> 属性设为True时， <cite>save_graphs_path</cite> 属性用于设置中间编译图的存储路径。默认情况下，计算图保存在当前目录下。</p></li>
<li><p><strong>save_graphs_path</strong> (str) - 表示保存计算图的路径。默认值：”.”。如果指定的目录不存在，系统将自动创建该目录。在分布式训练中，图形将被保存到 <cite>save_graphs_path/rank_${rank_id}/</cite> 目录下。 <cite>rank_id</cite> 为集群中当前设备的ID。</p></li>
<li><p><strong>enable_dump</strong> (bool) - 此参数已弃用，将在下一版本中删除。</p></li>
<li><p><strong>save_dump_path</strong> (str) - 此参数已弃用，将在下一版本中删除。</p></li>
<li><p><strong>enable_profiling</strong> (bool) - 此参数已弃用，将在下一版本中删除。请使用mindspore.profiler.Profiler API。</p></li>
<li><p><strong>profiling_options</strong> (str) - 此参数已弃用，将在下一版本中删除。请使用mindspore.profiler.Profiler API。</p></li>
<li><p><strong>print_file_path</strong> (str)：该路径用于保存打印数据。使用时 <code class="xref py py-class docutils literal notranslate"><span class="pre">mindspore.ops.print</span></code> 可以打印输入的张量或字符串信息，使用方法 <a class="reference internal" href="mindspore/mindspore.parse_print.html#mindspore.parse_print" title="mindspore.parse_print"><code class="xref py py-func docutils literal notranslate"><span class="pre">mindspore.parse_print()</span></code></a> 解析保存的文件。如果设置了此参数，打印数据保存到文件，未设置将显示到屏幕。如果保存的文件已经存在，则将添加时间戳后缀到文件中。将数据保存到文件解决了屏幕打印中的数据丢失问题, 如果未设置，将报告错误:”prompt to set the upper absolute path”。</p></li>
<li><p><strong>env_config_path</strong> (str) - 通过 <cite>context.set_context(env_config_path=”./mindspore_config.json”)</cite> 来设置MindSpore环境配置文件路径。</p>
<p>配置Running Data Recorder：</p>
<ul class="simple">
<li><p><strong>enable</strong>：表示在发生故障时是否启用Running Data Recorder去收集和保存训练中的关键数据。设置为True时，将打开Running Data Recorder。设置为False时，将关闭Running Data Recorder。</p></li>
<li><p><strong>mode</strong>：设置导出数据时的RDR模式。当设置为1时，RDR只在故障情况下输出数据。当设置为2时，RDR在故障情况和正常结束情况下输出数据。默认值：1。</p></li>
<li><p><strong>path</strong>：设置Running Data Recorder保存数据的路径。当前路径必须是一个绝对路径。</p></li>
</ul>
<p>内存重用：</p>
<ul class="simple">
<li><p><strong>mem_Reuse</strong>：表示内存复用功能是否打开。设置为True时，将打开内存复用功能。设置为False时，将关闭内存复用功能。
有关running data recoder和内存复用配置详细信息，请查看 <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/r1.7/debug/custom_debug.html">配置RDR和内存复用</a>。</p></li>
</ul>
</li>
<li><p><strong>precompile_only</strong> (bool) - 表示是否仅预编译网络。默认值：False。设置为True时，仅编译网络，而不执行网络。</p></li>
<li><p><strong>reserve_class_name_in_scope</strong> (bool) - 表示是否将网络类名称保存到所属ScopeName中。默认值：True。每个节点都有一个ScopeName。子节点的ScopeName是其父节点。如果 <cite>reserve_class_name_in_scope</cite> 设置为True，则类名将保存在ScopeName中的关键字“net-”之后。例如：</p>
<p>Default/net-Net1/net-Net2 (reserve_class_name_in_scope=True)</p>
<p>Default/net/net (reserve_class_name_in_scope=False)</p>
</li>
<li><p><strong>pynative_synchronize</strong> (bool) - 表示是否在PyNative模式下启动设备同步执行。默认值：False。设置为False时，将在设备上异步执行算子。当算子执行出错时，将无法定位特定错误脚本代码的位置。当设置为True时，将在设备上同步执行算子。这将降低程序的执行性能。此时，当算子执行出错时，可以根据错误的调用栈来定位错误脚本代码的位置。</p></li>
<li><p><strong>mode</strong> (int) - 表示在GRAPH_MODE(0)或PYNATIVE_MODE(1)模式中的运行。默认值：GRAPH_MODE(0)。GRAPH_MODE或PYNATIVE_MODE可以通过 <cite>mode</cite> 属性设置，两种模式都支持所有后端。默认模式为GRAPH_MODE。</p></li>
<li><p><strong>enable_graph_kernel</strong> (bool) - 表示开启图算融合去优化网络执行性能。默认值：False。如果 <cite>enable_graph_kernel</cite> 设置为True，则可以启用加速。有关图算融合的详细信息，请查看 <a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/r1.7/design/enable_graph_kernel_fusion.html">使能图算融合</a> 。</p></li>
<li><p><strong>graph_kernel_flags</strong> (str) - 图算融合的优化选项，当与enable_graph_kernel冲突时，它的优先级更高。其仅适用于有经验的用户。例如，context.set_context(graph_kernel_flags=”–opt_level=2 –dump_as_text”)。一些常用选项：</p>
<ul>
<li><p><strong>opt_level</strong>：设置优化级别。默认值：2。当opt_level的值大于0时，启动图算融合。可选值包括：</p>
<ul class="simple">
<li><p>0：关闭图算融合。</p></li>
<li><p>1：启动算子的基本融合。</p></li>
<li><p>2：包括级别1的所有优化，并打开更多的优化，如CSE优化算法、算术简化等。</p></li>
<li><p>3：包括级别2的所有优化，并打开更多的优化，如SitchingFusion、ParallelFusion等。在某些场景下，该级别的优化激进且不稳定。使用此级别时要小心。</p></li>
</ul>
</li>
<li><p><strong>dump_as_text</strong>：将关键过程的详细信息生成文本文件保存到”graph_kernel_dump”目录里。默认值：False。</p>
<p>有关更多选项，可以参考实现代码。</p>
</li>
</ul>
</li>
<li><p><strong>enable_reduce_precision</strong> (bool) - 表示是否开启降低精度计算。默认值：True。设置为True时，不支持用户指定的精度，且精度将自动更改。设置为False时，如果未指定用例的精度，则会报错并退出。</p></li>
<li><p><strong>auto_tune_mode</strong> (str) - 表示算子构建时的自动调整模式，以获得最佳的切分性能。默认值：NO_TUNE。其值必须在[‘RL’, ‘GA’, ‘RL,GA’]范围中。</p>
<ul class="simple">
<li><p>RL：强化学习调优。</p></li>
<li><p>GA：遗传算法调优。</p></li>
<li><p>RL，GA：当RL和GA优化同时打开时，工具会根据网络模型中的不同算子类型自动选择RL或GA。RL和GA的顺序没有区别。（自动选择）。</p></li>
</ul>
<p>有关启用算子调优工具设置的更多信息，请查看 <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/r1.7/debug/auto_tune.html">使能算子调优工具</a>。</p>
</li>
<li><p><strong>check_bprop</strong> (bool) - 表示是否检查反向传播节点，以确保反向传播节点输出的形状(shape)和数据类型与输入参数相同。默认值：False。</p></li>
<li><p><strong>max_call_depth</strong> (int) - 指定函数调用的最大深度。其值必须为正整数。默认值：1000。当嵌套Cell太深或子图数量太多时，需要设置 <cite>max_call_depth</cite> 参数。系统最大堆栈深度应随着 <cite>max_call_depth</cite> 的调整而设置为更大的值，否则可能会因为系统堆栈溢出而引发 “core dumped” 异常。</p></li>
<li><p><strong>enable_sparse</strong> (bool) - 表示是否启用稀疏特征。默认值：False。有关稀疏特征和稀疏张量的详细信息，请查看 <a class="reference external" href="https://www.mindspore.cn/tutorials/zh-CN/r1.7/beginner/tensor.html#稀疏张量">稀疏张量</a>。</p></li>
<li><p><strong>grad_for_scalar</strong> (bool)：  表示是否获取标量梯度。默认值：False。当 <cite>grad_for_scalar</cite> 设置为True时，则可以导出函数的标量输入。由于后端目前不支持伸缩操作，所以该接口只支持在前端可推演的简单操作。</p></li>
<li><p><strong>enable_compile_cache</strong> (bool) - 表示是否加载或者保存前端编译的图。当 <cite>enable_compile_cache</cite> 被设置为True时，在第一次执行的过程中，一个硬件无关的编译缓存会被生成并且导出为一个MINDIR文件。当该网络被再次执行时，如果 <cite>enable_compile_cache</cite> 仍然为True并且网络脚本没有被更改，那么这个编译缓存会被加载。注意目前只支持有限的Python脚本更改的自动检测，这意味着可能有正确性风险。默认值：False。这是一个实验特性，可能会被更改或者删除。</p></li>
<li><p><strong>compile_cache_path</strong> (str) - 保存前端图编译缓存的路径。默认值：”.”。如果目录不存在，系统会自动创建这个目录。缓存会被保存到如下目录： <cite>compile_cache_path/rank_${rank_id}/</cite> 。 <cite>rank_id</cite> 是集群上当前设备的ID。</p></li>
<li><p><strong>runtime_num_threads</strong> (int) - 运行时线程池的线程数控制。 默认值为30。</p></li>
</ul>
<p><strong>异常：</strong></p>
<p><strong>ValueError</strong>：输入key不是上下文中的属性。</p>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">precompile_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;Ascend&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_id</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">save_graphs</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">save_graphs_path</span><span class="o">=</span><span class="s2">&quot;./model.ms&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">enable_reduce_precision</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">enable_dump</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">save_dump_path</span><span class="o">=</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">enable_graph_kernel</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">graph_kernel_flags</span><span class="o">=</span><span class="s2">&quot;--opt_level=2 --dump_as_text&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">reserve_class_name_in_scope</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">variable_memory_max_size</span><span class="o">=</span><span class="s2">&quot;6GB&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">enable_profiling</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">... </span>                    <span class="n">profiling_options</span><span class="o">=</span><span class="s1">&#39;{&quot;output&quot;:&quot;/home/data/output&quot;,&quot;training_trace&quot;:&quot;on&quot;}&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">check_bprop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">max_device_memory</span><span class="o">=</span><span class="s2">&quot;3.5GB&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mempool_block_size</span><span class="o">=</span><span class="s2">&quot;1GB&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">print_file_path</span><span class="o">=</span><span class="s2">&quot;print.pb&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">enable_sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">max_call_depth</span><span class="o">=</span><span class="mi">80</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">env_config_path</span><span class="o">=</span><span class="s2">&quot;./env_config.json&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">auto_tune_mode</span><span class="o">=</span><span class="s2">&quot;GA,RL&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">grad_for_scalar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">enable_compile_cache</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">compile_cache_path</span><span class="o">=</span><span class="s2">&quot;./cache.ms&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">pynative_synchronize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">runtime_num_threads</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mindspore.context.get_context">
<span class="sig-prename descclassname"><span class="pre">mindspore.context.</span></span><span class="sig-name descname"><span class="pre">get_context</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">attr_key</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/context.html#get_context"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.context.get_context" title="Permalink to this definition"></a></dt>
<dd><p>根据输入key获取context中的属性值。如果该key没有设置，则会获取它们这些的默认值。</p>
<p><strong>参数：</strong></p>
<ul class="simple">
<li><p><strong>attr_key</strong> (str) - 属性的key。</p></li>
</ul>
<p><strong>返回：</strong></p>
<p>Object，表示给定属性key的值。</p>
<p><strong>异常：</strong></p>
<p><strong>ValueError</strong>：输入key不是context中的属性。</p>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_id&quot;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mindspore.context.set_auto_parallel_context">
<span class="sig-prename descclassname"><span class="pre">mindspore.context.</span></span><span class="sig-name descname"><span class="pre">set_auto_parallel_context</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/context.html#set_auto_parallel_context"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.context.set_auto_parallel_context" title="Permalink to this definition"></a></dt>
<dd><p>配置自动并行，仅在Ascend和GPU上有效。</p>
<p>应在mindspore.communication.init之前配置自动并行。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>配置时，必须输入配置的名称。如果某个程序具有不同并行模式下的任务，需要提前调用reset_auto_parallel_context()为下一个任务设置新的并行模式。若要设置或更改并行模式，必须在创建任何Initializer之前调用接口，否则，在编译网络时，可能会出现RuntimeError。</p>
</div>
<p>某些配置适用于特定的并行模式，有关详细信息，请参见下表：</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Common</p></th>
<th class="head"><p>AUTO_PARALLEL</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>device_num</p></td>
<td><p>gradient_fp32_sync</p></td>
</tr>
<tr class="row-odd"><td><p>global_rank</p></td>
<td><p>loss_repeated_mean</p></td>
</tr>
<tr class="row-even"><td><p>gradients_mean</p></td>
<td><p>auto_parallel_search_mode</p></td>
</tr>
<tr class="row-odd"><td><p>parallel_mode</p></td>
<td><p>strategy_ckpt_load_file</p></td>
</tr>
<tr class="row-even"><td><p>all_reduce_fusion_config</p></td>
<td><p>strategy_ckpt_save_file</p></td>
</tr>
<tr class="row-odd"><td><p>enable_parallel_optimizer</p></td>
<td><p>dataset_strategy</p></td>
</tr>
<tr class="row-even"><td><p>enable_alltoall</p></td>
<td><p>pipeline_stages</p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td><p>grad_accumulation_step</p></td>
</tr>
</tbody>
</table>
<p><strong>参数：</strong></p>
<ul class="simple">
<li><p><strong>device_num</strong> (int) - 表示可用设备的编号，必须在[1,4096]范围中。默认值：1。</p></li>
<li><p><strong>global_rank</strong> (int) - 表示全局RANK的ID，必须在[0,4095]范围中。默认值：0。</p></li>
<li><p><strong>gradients_mean</strong> (bool) - 表示是否在梯度的 AllReduce后执行平均算子。stand_alone不支持gradients_mean。默认值：False。</p></li>
<li><p><strong>gradient_fp32_sync</strong> (bool)：在FP32中运行梯度的 AllReduce。stand_alone、data_parallel和hybrid_parallel不支持gradient_fp32_sync。默认值：True。</p></li>
<li><p><strong>parallel_mode</strong> (str) - 有五种并行模式，分别是stand_alone、data_parallel、hybrid_parallel、semi_auto_parallel和auto_parallel。默认值：stand_alone。</p>
<ul>
<li><p>stand_alone：单卡模式。</p></li>
<li><p>data_parallel：数据并行模式。</p></li>
<li><p>hybrid_parallel：手动实现数据并行和模型并行。</p></li>
<li><p>semi_auto_parallel：半自动并行模式。</p></li>
<li><p>auto_parallel：自动并行模式。</p></li>
</ul>
</li>
<li><p><strong>search_mode</strong> (str) - 表示有三种策略搜索模式，分别是recursive_programming，dynamic_programming和sharding_propagation。默认值：dynamic_programming。</p>
<ul>
<li><p>recursive_programming：表示双递归搜索模式。</p></li>
<li><p>dynamic_programming：表示动态规划搜索模式。</p></li>
<li><p>sharding_propagation：表示从已配置算子的切分策略传播到所有算子。</p></li>
</ul>
</li>
<li><p><strong>auto_parallel_search_mode</strong> (str) - search_modes参数的兼容接口。将在后续的版本中删除。</p></li>
<li><p><strong>parameter_broadcast</strong> (bool) - 表示在训练前是否广播参数。在训练之前，为了使所有设备的网络初始化参数值相同，请将设备0上的参数广播到其他设备。不同并行模式下的参数广播不同。在data_parallel模式下，除layerwise_parallel属性为True的参数外，所有参数都会被广播。在hybrid_parallel、semi_auto_parallel和auto_parallel模式下，分段参数不参与广播。默认值：False。</p></li>
<li><p><strong>strategy_ckpt_load_file</strong> (str) - 表示用于加载并行策略checkpoint的路径。默认值：’’。</p></li>
<li><p><strong>strategy_ckpt_save_file</strong> (str) - 表示用于保存并行策略checkpoint的路径。默认值：’’。</p></li>
<li><p><strong>full_batch</strong> (bool) - 如果在auto_parallel模式下加载整个batch数据集，则此参数应设置为True。默认值：False。目前不建议使用该接口，建议使用dataset_strategy来替换它。</p></li>
<li><p><strong>dataset_strategy</strong> (Union[str, tuple]) - 表示数据集分片策略。默认值：data_parallel。dataset_strategy=”data_parallel”等于full_batch=False，dataset_strategy=”full_batch”等于full_batch=True。对于通过模型并列策略加载到网络的数据集，如ds_stra ((1, 8)、(1, 8))，需要使用set_auto_parallel_context(dataset_strategy=ds_stra)。</p></li>
<li><p><strong>enable_parallel_optimizer</strong> (bool) - 这是一个开发中的特性，它可以为数据并行训练对权重更新计算进行分片，以节省时间和内存。目前，自动和半自动并行模式支持Ascend和GPU中的所有优化器。数据并行模式仅支持Ascend中的 <cite>Lamb</cite> 和 <cite>AdamWeightDecay</cite> 。默认值：False。</p></li>
<li><p><strong>enable_alltoall</strong> (bool) - 允许在通信期间生成 <cite>AllToAll</cite> 通信算子的开关。 如果其值为 False，则将由 <cite>AllGather</cite> 、 <cite>Split</cite> 和 <cite>Concat</cite> 等通信算子的组合来代替 <cite>AllToAll</cite> 。 默认值：False。</p></li>
<li><p><strong>all_reduce_fusion_config</strong> (list) - 通过参数索引设置 AllReduce 融合策略。仅支持ReduceOp.SUM和HCCL_WORLD_GROUP/NCCL_WORLD_GROUP。没有默认值。如果不设置，则关闭算子融合。</p></li>
<li><p><strong>pipeline_stages</strong> (int) - 设置pipeline并行的阶段信息。这表明了设备如何单独分布在pipeline上。所有的设备将被划分为pipeline_stags个阶段。目前，这只能在启动semi_auto_parallel模式的情况下使用。默认值：1。</p></li>
<li><p><strong>grad_accumulation_step</strong> (int) - 在自动和半自动并行模式下设置梯度的累积step。其值应为正整数。默认值：1。</p></li>
<li><p><strong>parallel_optimizer_config</strong> (dict) - 用于开启优化器并行后的行为配置。仅在enable_parallel_optimizer=True的时候生效。目前，它支持关键字如下的关键字：</p>
<ul>
<li><p>gradient_accumulation_shard(bool)：设置累积梯度变量是否在数据并行维度上进行切分。开启后，将进一步减小模型的显存占用，但是会在反向计算梯度时引入额外的通信算子（ReduceScatter）。此配置仅在流水线并行训练和梯度累积模式下生效。默认值：True。</p></li>
<li><p>parallel_optimizer_threshold(int)：设置参数切分的阈值。占用内存小于该阈值的参数不做切分。占用内存大小 = shape[0] * … * shape[n] * size(dtype)。该阈值非负。单位： KB。默认值：64。</p></li>
</ul>
</li>
<li><p><strong>comm_fusion</strong> (dict) - 用于设置通信算子的融合配置。可以同一类型的通信算子按梯度张量的大小或者顺序分块传输。输入格式为{“通信类型”: {“mode”:str, “config”: None int 或者 list}},每种通信算子的融合配置有两个键：”mode”和”config”。支持以下通信类型的融合类型和配置：</p>
<ul>
<li><p>allreduce: 进行AllReduce算子的通信融合。”mode”包含：”auto”、”size”和”index”。在”auto”模式下，融合的是梯度变量的大小，默认值阈值为”64”MB，”config”对应的值为None。在”size”模式下，需要用户在config的字典中指定梯度大小阈值，这个值必须大于”0”MB。在”mode”为”index”时，它与”all_reduce_fusion_config”相同，用户需要给”config”传入一个列表，里面每个值表示梯度的索引。</p></li>
<li><p>allgather: 进行AllGather算子的通信融合。”mode”包含：”auto”、”size”。”auto” 和 “size”模式的配置方式与AllReduce相同。</p></li>
<li><p>reducescatter: 进行ReduceScatter算子的通信融合。”mode”包含：”auto”、”size”。”auto” 和 “size”模式的配置方式与AllReduce相同。</p></li>
</ul>
</li>
</ul>
<p><strong>异常：</strong></p>
<p><strong>ValueError</strong>：输入key不是自动并行上下文中的属性。</p>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">device_num</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">global_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">gradients_mean</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">gradient_fp32_sync</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="s2">&quot;auto_parallel&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">search_mode</span><span class="o">=</span><span class="s2">&quot;dynamic_programming&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">auto_parallel_search_mode</span><span class="o">=</span><span class="s2">&quot;dynamic_programming&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parameter_broadcast</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">strategy_ckpt_load_file</span><span class="o">=</span><span class="s2">&quot;./strategy_stage1.ckpt&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">strategy_ckpt_save_file</span><span class="o">=</span><span class="s2">&quot;./strategy_stage1.ckpt&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">dataset_strategy</span><span class="o">=</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">enable_parallel_optimizer</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">enable_alltoall</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">all_reduce_fusion_config</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">160</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">pipeline_stages</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">parallel_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;gradient_accumulation_shard&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="s2">&quot;parallel_optimizer_threshold&quot;</span><span class="p">:</span> <span class="mi">24</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_optimizer_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">,</span> <span class="n">enable_parallel_optimizer</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;allreduce&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;mode&quot;</span><span class="p">:</span> <span class="s2">&quot;size&quot;</span><span class="p">,</span> <span class="s2">&quot;config&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">},</span> <span class="s2">&quot;allgather&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;mode&quot;</span><span class="p">:</span> <span class="s2">&quot;size&quot;</span><span class="p">,</span> <span class="s2">&quot;config&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">}}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">comm_fusion</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mindspore.context.get_auto_parallel_context">
<span class="sig-prename descclassname"><span class="pre">mindspore.context.</span></span><span class="sig-name descname"><span class="pre">get_auto_parallel_context</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">attr_key</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/context.html#get_auto_parallel_context"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.context.get_auto_parallel_context" title="Permalink to this definition"></a></dt>
<dd><p>根据key获取自动并行的配置。</p>
<p><strong>参数：</strong></p>
<ul class="simple">
<li><p><strong>attr_key</strong> (str) - 配置的key。</p></li>
</ul>
<p><strong>返回：</strong></p>
<p>根据key返回配置的值。</p>
<p><strong>异常：</strong></p>
<p><strong>ValueError</strong>：输入key不在自动并行的配置列表中。</p>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">parallel_mode</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;parallel_mode&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset_strategy</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;dataset_strategy&quot;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mindspore.context.reset_auto_parallel_context">
<span class="sig-prename descclassname"><span class="pre">mindspore.context.</span></span><span class="sig-name descname"><span class="pre">reset_auto_parallel_context</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/context.html#reset_auto_parallel_context"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.context.reset_auto_parallel_context" title="Permalink to this definition"></a></dt>
<dd><p>重置自动并行的配置为默认值。</p>
<ul class="simple">
<li><p>device_num：1。</p></li>
<li><p>global_rank：0。</p></li>
<li><p>gradients_mean：False。</p></li>
<li><p>gradient_fp32_sync：True。</p></li>
<li><p>parallel_mode：’stand_alone’。</p></li>
<li><p>auto_parallel_search_mode：’dynamic_programming’。</p></li>
<li><p>parameter_broadcast：False。</p></li>
<li><p>strategy_ckpt_load_file：’’。</p></li>
<li><p>strategy_ckpt_save_file：’’。</p></li>
<li><p>full_batch：False。</p></li>
<li><p>enable_parallel_optimizer：False。</p></li>
<li><p>enable_alltoall: False。</p></li>
<li><p>pipeline_stages：1。</p></li>
</ul>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.context.ParallelMode">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.context.</span></span><span class="sig-name descname"><span class="pre">ParallelMode</span></span><a class="reference internal" href="../_modules/mindspore/context.html#ParallelMode"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.context.ParallelMode" title="Permalink to this definition"></a></dt>
<dd><p>并行模式。</p>
<p>有五种并行模式，分别是STAND_ALONE、DATA_PARALLEL、HYBRID_PARALLEL、SEMI_AUTO_PARALLEL和AUTO_PARALLEL。默认值：STAND_ALONE。</p>
<ul class="simple">
<li><p>STAND_ALONE：单卡模式。</p></li>
<li><p>DATA_PARALLEL：数据并行模式。</p></li>
<li><p>HYBRID_PARALLEL：手动实现数据并行和模型并行。</p></li>
<li><p>SEMI_AUTO_PARALLEL：半自动并行模式。</p></li>
<li><p>AUTO_PARALLEL：自动并行模式。</p></li>
</ul>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mindspore.context.set_ps_context">
<span class="sig-prename descclassname"><span class="pre">mindspore.context.</span></span><span class="sig-name descname"><span class="pre">set_ps_context</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/context.html#set_ps_context"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.context.set_ps_context" title="Permalink to this definition"></a></dt>
<dd><p>设置参数服务器训练模式的上下文。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>需要给参数服务器训练模式设置其他的环境变量。些环境变量如下所示：</p>
<ul class="simple">
<li><p>MS_SERVER_NUM：表示参数服务器数量。</p></li>
<li><p>MS_WORKER_NUM：表示工作进程数量。</p></li>
<li><p>MS_SCHED_HOST：表示调度器IP地址。</p></li>
<li><p>MS_SCHED_PORT：表示调度器开启的监听端口。</p></li>
<li><p>MS_ROLE：表示进程角色，角色列表如下：</p>
<ul>
<li><p>MS_SCHED：表示调度器。</p></li>
<li><p>MS_WORKER：表示工作进程。</p></li>
<li><p>MS_PSERVER/MS_SERVER：表示参数服务器。</p></li>
</ul>
</li>
</ul>
</div>
<p><strong>参数：</strong></p>
<ul class="simple">
<li><p><strong>enable_ps</strong> (bool) - 表示是否启用参数服务器训练模式。只有在enable_ps设置为True后，环境变量才会生效。默认值：False。</p></li>
<li><p><strong>config_file_path</strong> (string) - 配置文件路径，用于容灾恢复等, 目前参数服务器训练模式仅支持Server容灾。默认值：’’。</p></li>
<li><p><strong>scheduler_manage_port</strong> (int) - 调度器HTTP端口，对外开放用于接收和处理用户扩容/缩容等请求。默认值：11202。</p></li>
<li><p><strong>enable_ssl</strong> (bool) - 设置是否打开SSL认证。默认值：True。</p></li>
<li><p><strong>client_password</strong> (str) - 用于解密客户端证书密钥的密码。默认值：’’。</p></li>
<li><p><strong>server_password</strong> (str) - 用于解密服务端证书密钥的密码。默认值：’’。</p></li>
</ul>
<p><strong>异常：</strong></p>
<p><strong>ValueError</strong>：输入key不是参数服务器训练模式上下文中的属性。</p>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_ps_context</span><span class="p">(</span><span class="n">enable_ps</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">enable_ssl</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">client_password</span><span class="o">=</span><span class="s1">&#39;123456&#39;</span><span class="p">,</span> <span class="n">server_password</span><span class="o">=</span><span class="s1">&#39;123456&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mindspore.context.get_ps_context">
<span class="sig-prename descclassname"><span class="pre">mindspore.context.</span></span><span class="sig-name descname"><span class="pre">get_ps_context</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">attr_key</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/context.html#get_ps_context"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.context.get_ps_context" title="Permalink to this definition"></a></dt>
<dd><p>根据key获取参数服务器训练模式上下文中的属性值。</p>
<p><strong>参数：</strong></p>
<ul class="simple">
<li><p><strong>attr_key</strong> (str) - 属性的key。</p>
<ul>
<li><p>enable_ps (bool)：表示是否启用参数服务器训练模式。默认值：False。</p></li>
<li><p>config_file_path (string)：配置文件路径，用于容灾恢复等。默认值：’’。</p></li>
<li><p>scheduler_manage_port (int)：调度器HTTP端口，对外开放用于接收和处理用户扩容/缩容等请求。默认值：11202。</p></li>
<li><p>enable_ssl (bool)：设置是否打开SSL认证。默认值：False。</p></li>
<li><p>client_password (str)：用于解密客户端证书密钥的密码。默认值：’’。</p></li>
<li><p>server_password (str)：用于解密服务端证书密钥的密码。默认值：’’。</p></li>
</ul>
</li>
</ul>
<p><strong>返回：</strong></p>
<p>根据key返回属性值。</p>
<p><strong>异常：</strong></p>
<p><strong>ValueError</strong> - 输入key不是参数服务器训练模式上下文中的属性。</p>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">get_ps_context</span><span class="p">(</span><span class="s2">&quot;enable_ps&quot;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mindspore.context.reset_ps_context">
<span class="sig-prename descclassname"><span class="pre">mindspore.context.</span></span><span class="sig-name descname"><span class="pre">reset_ps_context</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/context.html#reset_ps_context"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.context.reset_ps_context" title="Permalink to this definition"></a></dt>
<dd><p>将参数服务器训练模式上下文中的属性重置为默认值。各字段的含义及其默认值见’set_ps_context’接口。</p>
</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="mindspore.communication.html" class="btn btn-neutral float-left" title="mindspore.communication" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="mindspore.dataset.html" class="btn btn-neutral float-right" title="mindspore.dataset" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>