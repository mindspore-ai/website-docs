

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>mindspore.ops.FusedSparseAdam &mdash; MindSpore master documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/training.css" type="text/css" />
   
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
        <script src="../../_static/js/training.js"></script>
        
        
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="mindspore.ops.FusedSparseFtrl" href="mindspore.ops.FusedSparseFtrl.html" />
    <link rel="prev" title="mindspore.ops.ApplyRMSProp" href="mindspore.ops.ApplyRMSProp.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">设计</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../design/technical_white_paper.html">技术白皮书</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/all_scenarios_architecture.html">全场景统一</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/gradient.html">函数式微分编程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/dynamic_graph_and_static_graph.html">动静态图结合</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/heterogeneous_training.html">异构并行训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/distributed_training_design.html">分布式并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/mindir.html">中间表达MindIR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/data_engine.html">高性能数据处理引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/graph_kernel_fusion.html">图算融合加速引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/second_order_optimizer.html">二阶优化</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/zh-CN/r1.7/training_visual_design.html">可视化调试调优↗</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindarmour/docs/zh-CN/r1.7/design.html">安全可信↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/glossary.html">术语</a></li>
</ul>
<p class="caption"><span class="caption-text">参考</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../note/benchmark.html">基准性能</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/network_list.html">网络支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/operator_list.html">算子支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/env_var_list.html">环境变量</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/syntax_list.html">语法支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/api_mapping.html">API映射</a></li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.context.html">mindspore.context</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.audio.html">mindspore.dataset.audio</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.config.html">mindspore.dataset.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.text.html">mindspore.dataset.text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.vision.html">mindspore.dataset.vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.nn.transformer.html">mindspore.nn.transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../mindspore.ops.html">mindspore.ops</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#算子原语">算子原语</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#装饰器">装饰器</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../mindspore.ops.html#神经网络层算子">神经网络层算子</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../mindspore.ops.html#神经网络">神经网络</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mindspore.ops.html#损失函数">损失函数</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mindspore.ops.html#激活函数">激活函数</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="../mindspore.ops.html#优化器">优化器</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.Adam.html">mindspore.ops.Adam</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.AdamNoUpdateParam.html">mindspore.ops.AdamNoUpdateParam</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.AdamWeightDecay.html">mindspore.ops.AdamWeightDecay</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.AdaptiveAvgPool2D.html">mindspore.ops.AdaptiveAvgPool2D</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyAdadelta.html">mindspore.ops.ApplyAdadelta</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyAdagrad.html">mindspore.ops.ApplyAdagrad</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyAdagradDA.html">mindspore.ops.ApplyAdagradDA</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyAdagradV2.html">mindspore.ops.ApplyAdagradV2</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyAdaMax.html">mindspore.ops.ApplyAdaMax</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyAddSign.html">mindspore.ops.ApplyAddSign</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyCenteredRMSProp.html">mindspore.ops.ApplyCenteredRMSProp</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyFtrl.html">mindspore.ops.ApplyFtrl</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyGradientDescent.html">mindspore.ops.ApplyGradientDescent</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyMomentum.html">mindspore.ops.ApplyMomentum</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyPowerSign.html">mindspore.ops.ApplyPowerSign</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyProximalAdagrad.html">mindspore.ops.ApplyProximalAdagrad</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyProximalGradientDescent.html">mindspore.ops.ApplyProximalGradientDescent</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyRMSProp.html">mindspore.ops.ApplyRMSProp</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">mindspore.ops.FusedSparseAdam</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.FusedSparseFtrl.html">mindspore.ops.FusedSparseFtrl</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.FusedSparseLazyAdam.html">mindspore.ops.FusedSparseLazyAdam</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.FusedSparseProximalAdagrad.html">mindspore.ops.FusedSparseProximalAdagrad</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.LARSUpdate.html">mindspore.ops.LARSUpdate</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.SparseApplyAdagrad.html">mindspore.ops.SparseApplyAdagrad</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.SparseApplyAdagradV2.html">mindspore.ops.SparseApplyAdagradV2</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.SparseApplyProximalAdagrad.html">mindspore.ops.SparseApplyProximalAdagrad</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.SGD.html">mindspore.ops.SGD</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.SparseApplyFtrl.html">mindspore.ops.SparseApplyFtrl</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.SparseApplyFtrlV2.html">mindspore.ops.SparseApplyFtrlV2</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../mindspore.ops.html#距离函数">距离函数</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mindspore.ops.html#采样算子">采样算子</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mindspore.ops.html#图像处理">图像处理</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mindspore.ops.html#文本处理">文本处理</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#数学运算算子">数学运算算子</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#tensor操作算子">Tensor操作算子</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#parameter操作算子">Parameter操作算子</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#数据操作算子">数据操作算子</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#通信算子">通信算子</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#调试算子">调试算子</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#稀疏算子">稀疏算子</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#其他算子">其他算子</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#算子信息注册">算子信息注册</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#自定义算子">自定义算子</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.ops.functional.html">mindspore.ops.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.parallel.html">mindspore.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.parallel.nn.html">mindspore.parallel.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.profiler.html">mindspore.Profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.7/api_cpp/mindspore.html">C++ API↗</a></li>
</ul>
<p class="caption"><span class="caption-text">迁移指南</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/overview.html">概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/preparation.html">准备工作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/script_analysis.html">网络脚本分析</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/script_development.html">网络脚本开发</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/neural_network_debug.html">网络调试</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/accuracy_optimization.html">精度调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/performance_optimization.html">性能调试</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/inference.html">推理执行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/sample_code.html">网络迁移调试实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/faq.html">常见问题</a></li>
</ul>
<p class="caption"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../faq/installation.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/data_processing.html">数据处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/implement_problem.html">执行问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/network_compilation.html">网络编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/operators_compile.html">算子编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/usage_migrate_3rd.html">第三方框架迁移使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/performance_tuning.html">性能调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/precision_tuning.html">精度调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/distributed_configure.html">分布式配置</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/inference.html">推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/feature_advice.html">特性咨询</a></li>
</ul>
<p class="caption"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../RELEASE.html">Release Notes</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../mindspore.ops.html">mindspore.ops</a> &raquo;</li>
        
      <li>mindspore.ops.FusedSparseAdam</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../_sources/api_python/ops/mindspore.ops.FusedSparseAdam.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="mindspore-ops-fusedsparseadam">
<h1>mindspore.ops.FusedSparseAdam<a class="headerlink" href="#mindspore-ops-fusedsparseadam" title="Permalink to this headline">¶</a></h1>
<dl class="class">
<dt id="mindspore.ops.FusedSparseAdam">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.ops.</code><code class="sig-name descname">FusedSparseAdam</code><span class="sig-paren">(</span><em class="sig-param">use_locking=False</em>, <em class="sig-param">use_nesterov=False</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/mindspore/ops/operations/nn_ops.html#FusedSparseAdam"><span class="viewcode-link">[源代码]</span></a><a class="headerlink" href="#mindspore.ops.FusedSparseAdam" title="Permalink to this definition">¶</a></dt>
<dd><p>Merges the duplicate value of the gradient and then updates parameters by the Adaptive Moment Estimation (Adam)
algorithm. This operator is used when the gradient is sparse.</p>
<p>The Adam algorithm is proposed in <a class="reference external" href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a>.</p>
<p>The updating formulas are as follows,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ll} \\
    m = \beta_1 * m + (1 - \beta_1) * g \\
    v = \beta_2 * v + (1 - \beta_2) * g * g \\
    l = \alpha * \frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t} \\
    w = w - l * \frac{m}{\sqrt{v} + \epsilon}
\end{array}\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(m\)</span> represents the 1st moment vector, <span class="math notranslate nohighlight">\(v\)</span> represents the 2nd moment vector, <span class="math notranslate nohighlight">\(g\)</span> represents
<cite>gradient</cite>, <span class="math notranslate nohighlight">\(l\)</span> represents scaling factor <cite>lr</cite>, <span class="math notranslate nohighlight">\(\beta_1, \beta_2\)</span> represent <cite>beta1</cite> and <cite>beta2</cite>,
<span class="math notranslate nohighlight">\(t\)</span> represents updating step while <span class="math notranslate nohighlight">\(\beta_1^t\)</span> and <span class="math notranslate nohighlight">\(\beta_2^t\)</span> represent <cite>beta1_power</cite> and
<cite>beta2_power</cite>, <span class="math notranslate nohighlight">\(\alpha\)</span> represents <cite>learning_rate</cite>, <span class="math notranslate nohighlight">\(w\)</span> represents <cite>var</cite>, <span class="math notranslate nohighlight">\(\epsilon\)</span> represents
<cite>epsilon</cite>.</p>
<p>All of inputs except <cite>indices</cite> comply with the implicit type conversion rules to make the data types consistent.
If they have different data types, the lower priority data type will be converted to
the relatively highest priority data type.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>use_locking</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to enable a lock to protect variable tensors from being updated.
If true, updates of the var, m, and v tensors will be protected by a lock.
If false, the result is unpredictable. Default: False.</p></li>
<li><p><strong>use_nesterov</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to use Nesterov Accelerated Gradient (NAG) algorithm to update the gradients.
If true, update the gradients using NAG.
If false, update the gradients without using NAG. Default: False.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>var</strong> (Parameter) - Parameters to be updated with float32 data type. The shape is <span class="math notranslate nohighlight">\((N, *)\)</span>
where <span class="math notranslate nohighlight">\(*\)</span> means, any number of additional dimensions.</p></li>
<li><p><strong>m</strong> (Parameter) - The 1st moment vector in the updating formula, has the same shape and data type as <cite>var</cite>.</p></li>
<li><p><strong>v</strong> (Parameter) - The 2nd moment vector in the updating formula, has the same shape and data type as <cite>var</cite>.
Mean square gradients, has the same type as <cite>var</cite> with float32 data type.</p></li>
<li><p><strong>beta1_power</strong> (Tensor) - <span class="math notranslate nohighlight">\(beta_1^t\)</span> in the updating formula with float32 data type.
The shape is <span class="math notranslate nohighlight">\((1, )\)</span>.</p></li>
<li><p><strong>beta2_power</strong> (Tensor) - <span class="math notranslate nohighlight">\(beta_2^t\)</span> in the updating formula with float32 data type.
The shape is <span class="math notranslate nohighlight">\((1, )\)</span>.</p></li>
<li><p><strong>lr</strong> (Tensor) - <span class="math notranslate nohighlight">\(l\)</span> in the updating formula. With float32 data type.
The shape is <span class="math notranslate nohighlight">\((1, )\)</span>.</p></li>
<li><p><strong>beta1</strong> (Tensor) - The exponential decay rate for the 1st moment estimations with float32 data type.
The shape is <span class="math notranslate nohighlight">\((1, )\)</span>.</p></li>
<li><p><strong>beta2</strong> (Tensor) - The exponential decay rate for the 2nd moment estimations with float32 data type.
The shape is <span class="math notranslate nohighlight">\((1, )\)</span>.</p></li>
<li><p><strong>epsilon</strong> (Tensor) - Term added to the denominator to improve numerical stability with float32 data type.
The shape is <span class="math notranslate nohighlight">\((1, )\)</span>.</p></li>
<li><p><strong>gradient</strong> (Tensor) - Gradient, has the same data type as <cite>var</cite> and
gradient.shape[1:] = var.shape[1:] if var.shape &gt; 1.</p></li>
<li><p><strong>indices</strong> (Tensor) - Gradient indices with int32 data type and indices.shape[0] = gradient.shape[0].</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tuple of 3 Tensors, this operator will update the input parameters directly, the outputs are useless.</p>
<ul class="simple">
<li><p><strong>var</strong> (Tensor) - A Tensor with shape <span class="math notranslate nohighlight">\((1, )\)</span>.</p></li>
<li><p><strong>m</strong> (Tensor) - A Tensor with shape <span class="math notranslate nohighlight">\((1, )\)</span>.</p></li>
<li><p><strong>v</strong> (Tensor) - A Tensor with shape <span class="math notranslate nohighlight">\((1, )\)</span>.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If neither <cite>use_locking</cite> nor <cite>use_neserov</cite> is a bool.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If dtype of <cite>var</cite>, <cite>m</cite>, <cite>v</cite>, <cite>beta1_power</cite>, <cite>beta2_power</cite>, <cite>lr</cite>, <cite>beta1</cite>, <cite>beta2</cite>, <cite>epsilon</cite>,
    <cite>gradient</cite> or <cite>indices</cite> is not float32.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#RuntimeError" title="(in Python v3.8)"><strong>RuntimeError</strong></a> – If the data type of all inputs except <cite>indices</cite> conversion of Parameter is not supported.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">sparse_apply_adam</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">FusedSparseAdam</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">var</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;var&quot;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;m&quot;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;v&quot;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta1_power</span><span class="p">,</span> <span class="n">beta2_power</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
<span class="gp">... </span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse_apply_adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">var</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">,</span> <span class="n">beta1_power</span><span class="p">,</span> <span class="n">beta2_power</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span>
<span class="gp">... </span>                                     <span class="n">epsilon</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">out</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beta1_power</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beta2_power</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lr</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beta1</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beta2</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gradient</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]],</span> <span class="p">[[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">beta1_power</span><span class="p">,</span> <span class="n">beta2_power</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">var</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
<span class="go">[[[0.9997121  0.9997121 ]]</span>
<span class="go"> [[0.9997121  0.9997121 ]]</span>
<span class="go"> [[0.99971527 0.99971527]]]</span>
</pre></div>
</div>
</dd></dl>

</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="mindspore.ops.FusedSparseFtrl.html" class="btn btn-neutral float-right" title="mindspore.ops.FusedSparseFtrl" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="mindspore.ops.ApplyRMSProp.html" class="btn btn-neutral float-left" title="mindspore.ops.ApplyRMSProp" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2022, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>