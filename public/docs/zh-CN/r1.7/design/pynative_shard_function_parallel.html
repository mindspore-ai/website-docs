

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>函数式算子切分 &mdash; MindSpore master documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/training.css" type="text/css" />
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script src="../_static/js/training.js"></script>
        
        
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">设计</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="technical_white_paper.html">技术白皮书</a></li>
<li class="toctree-l1"><a class="reference internal" href="all_scenarios_architecture.html">全场景统一</a></li>
<li class="toctree-l1"><a class="reference internal" href="gradient.html">函数式微分编程</a></li>
<li class="toctree-l1"><a class="reference internal" href="dynamic_graph_and_static_graph.html">动静态图结合</a></li>
<li class="toctree-l1"><a class="reference internal" href="heterogeneous_training.html">异构并行训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_training_design.html">分布式并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindir.html">中间表达MindIR</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_engine.html">高性能数据处理引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="graph_kernel_fusion.html">图算融合加速引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="second_order_optimizer.html">二阶优化</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/zh-CN/r1.7/training_visual_design.html">可视化调试调优↗</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindarmour/docs/zh-CN/r1.7/design.html">安全可信↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="glossary.html">术语</a></li>
</ul>
<p class="caption"><span class="caption-text">参考</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/benchmark.html">基准性能</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/network_list.html">网络支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/operator_list.html">算子支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/env_var_list.html">环境变量</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/syntax_list.html">语法支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping.html">API映射</a></li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.context.html">mindspore.context</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.audio.html">mindspore.dataset.audio</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.config.html">mindspore.dataset.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.text.html">mindspore.dataset.text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.vision.html">mindspore.dataset.vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.transformer.html">mindspore.nn.transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.functional.html">mindspore.ops.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.parallel.html">mindspore.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.parallel.nn.html">mindspore.parallel.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.profiler.html">mindspore.Profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.7/api_cpp/mindspore.html">C++ API↗</a></li>
</ul>
<p class="caption"><span class="caption-text">迁移指南</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/overview.html">概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/preparation.html">准备工作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/script_analysis.html">网络脚本分析</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/script_development.html">网络脚本开发</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/neural_network_debug.html">网络调试</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/accuracy_optimization.html">精度调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/performance_optimization.html">性能调试</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/inference.html">推理执行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/sample_code.html">网络迁移调试实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/faq.html">常见问题</a></li>
</ul>
<p class="caption"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq/installation.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/data_processing.html">数据处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/implement_problem.html">执行问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/network_compilation.html">网络编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/operators_compile.html">算子编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/usage_migrate_3rd.html">第三方框架迁移使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/performance_tuning.html">性能调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/precision_tuning.html">精度调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/distributed_configure.html">分布式配置</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/inference.html">推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/feature_advice.html">特性咨询</a></li>
</ul>
<p class="caption"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>函数式算子切分</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/design/pynative_shard_function_parallel.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="函数式算子切分">
<h1>函数式算子切分<a class="headerlink" href="#函数式算子切分" title="Permalink to this headline">¶</a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.7/docs/mindspore/source_zh_cn/design/pynative_shard_function_parallel.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.7/resource/_static/logo_source.png"></a></p>
<div class="section" id="概述">
<h2>概述<a class="headerlink" href="#概述" title="Permalink to this headline">¶</a></h2>
<p>动态图支持语法更丰富，使用更为灵活，但是目前MindSpore的动态图模式不支持自动并行的各种特性。借鉴Jax的pmap的设计理念，即在动态图模式下，指定某一个部分以图模式执行，并进行各种并行操作，我们设计了shard函数，支持在动态图模式下，指定某一部分以图模式执行，并且执行各种并行操作。</p>
</div>
<div class="section" id="基本原理">
<h2>基本原理<a class="headerlink" href="#基本原理" title="Permalink to this headline">¶</a></h2>
<p>MindSpore的动态图模式下，可以通过ms_function的装饰符，指定某一段以图模式编译执行，在前向执行的同时，会将执行的算子、子图记录下来，前向执行完毕后，会对得到的整图进行自动微分得到反向图，具体流程如下图所示：</p>
<p><img alt="structure image" src="../_images/pynative_ms_funtion.png" /></p>
<p><em>图1：ms_function执行示意图</em></p>
<p>Shard function沿用此模式，不同的是可以在图模式编译执行的环节进行算子级别的模型并行。</p>
</div>
<div class="section" id="操作实践">
<h2>操作实践<a class="headerlink" href="#操作实践" title="Permalink to this headline">¶</a></h2>
<div class="section" id="样例代码说明">
<h3>样例代码说明<a class="headerlink" href="#样例代码说明" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><p>你可以在这里下载完整的样例代码：</p>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/tree/r1.7/docs/sample_code/pynative_shard_function_parallel">https://gitee.com/mindspore/docs/tree/r1.7/docs/sample_code/pynative_shard_function_parallel</a>。</p>
</div></blockquote>
<p>目录结构如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>└─sample_code
    ├─shard_function_parallel
        ├── rank_table_8pcs.json
        ├── run_shard_function_example.sh
        └── shard_function_example.py
</pre></div>
</div>
<p>其中每个文件的作用如下：</p>
<ul class="simple">
<li><p>shard_function_example.py：shard function的示例代码，介绍了如何使用shard function指定部分代码并行执行。</p></li>
<li><p>rank_table_8pcs.json：RANK_TABLE_FILE的8卡配置文件。</p></li>
<li><p>run_shard_function_example.sh：shard function example的启动脚本。</p></li>
</ul>
</div>
<div class="section" id="接口介绍">
<h3>接口介绍<a class="headerlink" href="#接口介绍" title="Permalink to this headline">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">shard</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">in_strategy</span><span class="p">,</span> <span class="n">out_strategy</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;Ascend&quot;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">shard_fn</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">in_strategy</span><span class="p">,</span> <span class="n">out_strategy</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">level</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">in_strategy(tuple)</span></code>: 指定输入<code class="docutils literal notranslate"><span class="pre">Tensor</span></code>的切分策略，每个元素为元组，表示对应输入<code class="docutils literal notranslate"><span class="pre">Tensor</span></code>的切分策略，每个元组的长度要与对应<code class="docutils literal notranslate"><span class="pre">Tensor</span></code>的维度相等，表示每个维度如何切分，可以传入<code class="docutils literal notranslate"><span class="pre">None</span></code>，表示对应<code class="docutils literal notranslate"><span class="pre">Tensor</span></code>按照数据并行进行切分。</p>
<p><code class="docutils literal notranslate"><span class="pre">out_strategy(tuple)</span></code>: 指定输出<code class="docutils literal notranslate"><span class="pre">Tensor</span></code>的切分策略，用法和<code class="docutils literal notranslate"><span class="pre">in_strategy</span></code>相同。在深度学习模型中，输出策略会被覆盖为数据并行，即高维按卡数均匀切分。</p>
<p><code class="docutils literal notranslate"><span class="pre">device(string)</span></code>: 指定执行的设备，可选范围<code class="docutils literal notranslate"><span class="pre">Ascend</span></code>、<code class="docutils literal notranslate"><span class="pre">GPU</span></code>和<code class="docutils literal notranslate"><span class="pre">CPU</span></code>，默认为<code class="docutils literal notranslate"><span class="pre">Ascend</span></code>，目前尚未使能，后续会开放。</p>
<p><code class="docutils literal notranslate"><span class="pre">level(int)</span></code>: 指定全部算子搜索策略，输入输出<code class="docutils literal notranslate"><span class="pre">Tensor</span></code>的切分策略由用户指定，其余算子的切分策略会由框架搜索得到，此参数指定搜索时的目标函数，可选范围为0、1、2，分别代表最大化计算通信比、内存消耗最小、最大化运行速度，默认为0，目前尚未使能，后续会开放。</p>
</div>
<div class="section" id="执行模式">
<h3>执行模式<a class="headerlink" href="#执行模式" title="Permalink to this headline">¶</a></h3>
<p>如前所述，shard function会将动态图模式下某一部分以图模式执行算子级模型并行，因此使用shard function时需要设置模式为</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">)</span>
<span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">,</span>
                                  <span class="n">search_mode</span><span class="o">=</span><span class="s2">&quot;sharding_propagation&quot;</span><span class="p">,</span> <span class="n">device_num</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="使用方法">
<h3>使用方法<a class="headerlink" href="#使用方法" title="Permalink to this headline">¶</a></h3>
<p>shard function目前有两种使用方法，以下面的网络为例介绍shard function的使用方法。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="k">class</span> <span class="nc">BasicBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BasicBlock</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gelu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># two dimensional input x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gelu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block1</span> <span class="o">=</span> <span class="n">BasicBlock</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block2</span> <span class="o">=</span> <span class="n">BasicBlock</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block3</span> <span class="o">=</span> <span class="n">BasicBlock</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># All three blocks are executed as PyNative mode.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<ul>
<li><p>使用类方法<code class="docutils literal notranslate"><span class="pre">shard</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Net1</span><span class="p">(</span><span class="n">Net</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net1</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># slice input along the second axis and make output as data-parallel layout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block1</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">in_strategy</span><span class="o">=</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">),),</span> <span class="n">out_strategy</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,))</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># block1 is executed as GRAPH. The inputs/outputs layouts follow the user definition and the slice strategy for inner ops are obtained by auto search</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># block2 and block3 are executed as PyNative mode.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</li>
<li><p>使用函数式接口<code class="docutils literal notranslate"><span class="pre">ops.shard</span></code>，由于<code class="docutils literal notranslate"><span class="pre">shard</span></code>函数的返回值为函数，使用函数式接口的时候，不能将已经实例过的类赋值为<code class="docutils literal notranslate"><span class="pre">shard</span></code>的返回值，因为MindSpore不支持将类实例赋值为其它类型</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="k">class</span> <span class="nc">NetError</span><span class="p">(</span><span class="n">Net</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block1</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">block1</span><span class="p">,</span> <span class="n">in_strategy</span><span class="o">=</span><span class="p">((</span><span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">),),</span> <span class="n">out_strategy</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,))</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<p>如此执行会遇到报错</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>TypeError: For &#39;Cell&#39;, the type of block1 should be cell, but got function.
</pre></div>
</div>
<p>正确使用方式如下</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
<span class="k">class</span> <span class="nc">Net2</span><span class="p">(</span><span class="n">Net</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># set the return function of shard a different name with the Cell instance</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block1_graph</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">block1</span><span class="p">,</span> <span class="n">in_strategy</span><span class="o">=</span><span class="p">((</span><span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">),),</span> <span class="n">out_strategy</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block2</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">in_strategy</span><span class="o">=</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">),),</span> <span class="n">out_strategy</span><span class="o">=</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">),))</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># block1 is executed as GRAPH with input sliced along the first dimension</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block1_graph</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># block2 is executed as GRAPH as well. But the output won&#39;t follow the specified layout, it will be over-written by data-parallel layout instead.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># block3 is executed as PyNative mode.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</li>
</ul>
</div>
<div class="section" id="使用限制">
<h3>使用限制<a class="headerlink" href="#使用限制" title="Permalink to this headline">¶</a></h3>
<p>目前由于动态图模式和静态图模式HCCL通信接口不同，对于切分存在限制，即<code class="docutils literal notranslate"><span class="pre">shard</span></code>内部的模型并行产生的通信只能发生在<code class="docutils literal notranslate"><span class="pre">world</span> <span class="pre">group</span></code>内部，所以指定的切分策略目前只能支持切一个维度。</p>
<p>使用时，需设置<code class="docutils literal notranslate"><span class="pre">Ascend</span></code>后端，执行模式需设置为<code class="docutils literal notranslate"><span class="pre">PYNATIVE_MODE</span></code>，并行配置为<code class="docutils literal notranslate"><span class="pre">AUTO_PARALLEL</span></code>，<code class="docutils literal notranslate"><span class="pre">search_mode</span></code>为<code class="docutils literal notranslate"><span class="pre">sharding_propagation</span></code>。</p>
<p>在训练神经网络时，<code class="docutils literal notranslate"><span class="pre">out_strategy</span></code>会被覆盖为数据并行，来适配动态图部分的数据并行。</p>
<p>由于目前的算法实现，不支持在<code class="docutils literal notranslate"><span class="pre">__init__</span></code>中声明不会被使用的参数，定义网络时请注意。</p>
<p>会在后续迭代中解决这些问题。</p>
</div>
<div class="section" id="运行代码">
<h3>运行代码<a class="headerlink" href="#运行代码" title="Permalink to this headline">¶</a></h3>
<p>上述代码需要在配置分布式变量后才可以运行。Ascend环境需要配置RANK_TABLE_FILE、RANK_ID和DEVICE_ID。配置的过程请参考<a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/r1.7/parallel/train_ascend.html#%E9%85%8D%E7%BD%AE%E5%88%86%E5%B8%83%E5%BC%8F%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F">此处</a>。</p>
<p>Ascend分布式相关的环境变量有:</p>
<ul class="simple">
<li><p>RANK_TABLE_FILE：组网信息文件的路径。rank_table_file文件可以使用models代码仓中的hccl_tools.py生成，可以从<a class="reference external" href="https://gitee.com/mindspore/models/tree/r1.7/utils/hccl_tools">此处</a>获取。</p></li>
<li><p>DEVICE_ID：当前卡在机器上的实际序号。</p></li>
<li><p>RANK_ID：当前卡的逻辑序号。</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="nb">set</span> -e
<span class="nb">echo</span> <span class="s2">&quot;==============================================================================================================&quot;</span>
<span class="nb">echo</span> <span class="s2">&quot;Please run the script as: &quot;</span>
<span class="nb">echo</span> <span class="s2">&quot;bash run_fusion_example.sh&quot;</span>
<span class="nb">echo</span> <span class="s2">&quot;It is better to use the absolute path.&quot;</span>
<span class="nb">echo</span> <span class="s2">&quot;This example is expected to run on the Ascend environment.&quot;</span>
<span class="nb">echo</span> <span class="s2">&quot;==============================================================================================================&quot;</span>
<span class="nv">RANK_SIZE</span><span class="o">=</span><span class="m">8</span>
<span class="nv">EXEC_PATH</span><span class="o">=</span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>
<span class="nb">export</span> <span class="nv">RANK_TABLE_FILE</span><span class="o">=</span><span class="si">${</span><span class="nv">EXEC_PATH</span><span class="si">}</span>/rank_table_8pcs.json
<span class="nb">export</span> <span class="nv">RANK_SIZE</span><span class="o">=</span><span class="m">8</span>

<span class="k">for</span><span class="o">((</span><span class="nv">i</span><span class="o">=</span><span class="m">0</span><span class="p">;</span>i&lt;<span class="si">${</span><span class="nv">RANK_SIZE</span><span class="si">}</span><span class="p">;</span>i++<span class="o">))</span>
<span class="k">do</span>
    rm -rf device<span class="nv">$i</span>
    mkdir device<span class="nv">$i</span>
    cp ./shard_funtion_example.py ./device<span class="nv">$i</span>
    <span class="nb">cd</span> ./device<span class="nv">$i</span>
    <span class="nb">export</span> <span class="nv">DEVICE_ID</span><span class="o">=</span><span class="nv">$i</span>
    <span class="nb">export</span> <span class="nv">RANK_ID</span><span class="o">=</span><span class="nv">$i</span>
    <span class="nb">echo</span> <span class="s2">&quot;start training for device </span><span class="nv">$i</span><span class="s2">&quot;</span>
    env &gt; env<span class="nv">$i</span>.log
    pytest -s -v ./shard_function_example.py &gt; train.log<span class="nv">$i</span> <span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span> <span class="p">&amp;</span>
    <span class="nb">cd</span> ../
<span class="k">done</span>
<span class="nb">echo</span> <span class="s2">&quot;The program launch succeed, the log is under device0/train.log0.&quot;</span>
</pre></div>
</div>
<p>在当前目录下配置完RANK_TABLE_FILE之后，下述的命令要求用户拥有8张Ascend 910设备。运行命令如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash run_fusion_example.sh
</pre></div>
</div>
<p>执行过程中，框架会自动为<code class="docutils literal notranslate"><span class="pre">shard</span></code>的输入函数进行算子级别的模型并行，每个算子的并行策略由框架搜索得到，整个过程用户无感知。可以按如下操作存图</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">save_graphs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>在<code class="docutils literal notranslate"><span class="pre">step_parallel_end.ir</span></code>中可以看到具体每一个算子的并行策略。</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2022, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>