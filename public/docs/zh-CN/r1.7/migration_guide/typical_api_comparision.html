<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>典型算子或接口区别介绍 &mdash; MindSpore master documentation</title>
      <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/training.css" type="text/css" /><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/js/training.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="优化器迁移指南" href="optim.html" />
    <link rel="prev" title="基本执行流程横向对比" href="training_process_comparision.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">设计</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../design/technical_white_paper.html">技术白皮书</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/all_scenarios_architecture.html">全场景统一</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/gradient.html">函数式微分编程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/dynamic_graph_and_static_graph.html">动静态图结合</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/heterogeneous_training.html">异构并行训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/distributed_training_design.html">分布式并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/mindir.html">中间表达MindIR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/data_engine.html">高性能数据处理引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/graph_kernel_fusion.html">图算融合加速引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/second_order_optimizer.html">二阶优化</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/zh-CN/r1.7/training_visual_design.html">可视化调试调优↗</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/mindarmour/docs/zh-CN/r1.7/design.html">安全可信↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/glossary.html">术语</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">参考</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/benchmark.html">基准性能</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/network_list.html">网络支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/operator_list.html">算子支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/env_var_list.html">环境变量</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/syntax_list.html">语法支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping.html">API映射</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.context.html">mindspore.context</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.audio.html">mindspore.dataset.audio</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.config.html">mindspore.dataset.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.text.html">mindspore.dataset.text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.vision.html">mindspore.dataset.vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.transformer.html">mindspore.nn.transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.functional.html">mindspore.ops.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.parallel.html">mindspore.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.parallel.nn.html">mindspore.parallel.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.profiler.html">mindspore.Profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.7/api_cpp/mindspore.html">C++ API↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">迁移指南</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="preparation.html">准备工作</a></li>
<li class="toctree-l1"><a class="reference internal" href="script_analysis.html">网络脚本分析</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="script_development.html">网络脚本开发</a><ul class="current">
<li class="toctree-l2"><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/zh-CN/r1.7/migrate_3rd_scripts_mindconverter.html">使用MindConverter迁移脚本↗</a></li>
<li class="toctree-l2"><a class="reference internal" href="migration_script.html">迁移脚本</a></li>
<li class="toctree-l2"><a class="reference internal" href="training_process_comparision.html">基本执行流程横向对比</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">典型算子或接口区别介绍</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#接口类">接口类</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#torch-device">torch.device</a></li>
<li class="toctree-l4"><a class="reference internal" href="#nn-module">nn.Module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#数据对象">数据对象</a></li>
<li class="toctree-l4"><a class="reference internal" href="#梯度求导">梯度求导</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#算子类">算子类</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#参数意义不同">参数意义不同</a></li>
<li class="toctree-l4"><a class="reference internal" href="#算子行为不同">算子行为不同</a></li>
<li class="toctree-l4"><a class="reference internal" href="#默认权重初始化不同">默认权重初始化不同</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="optim.html">优化器迁移指南</a></li>
<li class="toctree-l2"><a class="reference internal" href="use_third_party_op.html">基于自定义算子接口调用第三方算子库</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="neural_network_debug.html">网络调试</a></li>
<li class="toctree-l1"><a class="reference internal" href="accuracy_optimization.html">精度调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance_optimization.html">性能调试</a></li>
<li class="toctree-l1"><a class="reference internal" href="inference.html">推理执行</a></li>
<li class="toctree-l1"><a class="reference internal" href="sample_code.html">网络迁移调试实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">常见问题</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq/installation.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/data_processing.html">数据处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/implement_problem.html">执行问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/network_compilation.html">网络编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/operators_compile.html">算子编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/usage_migrate_3rd.html">第三方框架迁移使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/performance_tuning.html">性能调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/precision_tuning.html">精度调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/distributed_configure.html">分布式配置</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/inference.html">推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/feature_advice.html">特性咨询</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="script_development.html">网络脚本开发</a> &raquo;</li>
      <li>典型算子或接口区别介绍</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/migration_guide/typical_api_comparision.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="典型算子或接口区别介绍">
<h1>典型算子或接口区别介绍<a class="headerlink" href="#典型算子或接口区别介绍" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.7/docs/mindspore/source_zh_cn/migration_guide/typical_api_comparision.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.7/resource/_static/logo_source.png"></a></p>
<section id="接口类">
<h2>接口类<a class="headerlink" href="#接口类" title="Permalink to this headline"></a></h2>
<section id="torch-device">
<h3>torch.device<a class="headerlink" href="#torch-device" title="Permalink to this headline"></a></h3>
<p>PyTorch 在构建模型时，通常会利用 torch.device 指定模型和数据绑定的设备，是在 CPU 还是 GPU 上，如果支持多 GPU，还可以指定具体的 GPU 序号。绑定相应的设备后，需要将模型和数据部署到对应设备，代码如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># bind to the GPU 0 if GPU is available, otherwise bind to CPU</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span> <span class="c1"># 单 GPU 或者 CPU</span>
<span class="c1"># deploy model to specified hardware</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># deploy data to specified hardware</span>
<span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># distribute training on multiple GPUs</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># set available device</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;CUDA_VISIBLE_DEVICE&#39;</span><span class="p">]</span><span class="o">=</span><span class="s1">&#39;1&#39;</span>
<span class="n">model</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
</pre></div>
</div>
<p>而在 MindSpore 中，我们通过  <code class="docutils literal notranslate"><span class="pre">context</span></code>  中 的  <code class="docutils literal notranslate"><span class="pre">device_target</span></code> 参数 指定模型绑定的设备，<code class="docutils literal notranslate"><span class="pre">device_id</span></code> 指定设备的序号。与 PyTorch 不同的是，一旦设备设置成功，输入数据和模型会默认拷贝到指定的设备中执行，不需要也无法再改变数据和模型所运行的设备类型。代码如下：</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s1">&#39;Ascend&#39;</span><span class="p">,</span> <span class="n">device_id</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># define net</span>
<span class="n">Model</span> <span class="o">=</span> <span class="o">..</span>
<span class="c1"># define dataset</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="o">..</span>
<span class="c1"># training, automatically deploy to Ascend according to device_target</span>
<span class="n">Model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<p>此外，网络运行后返回的 Tensor 默认均拷贝到 CPU 设备，可以直接对该 Tensor 进行访问和修改，包括转成 numpy 格式，无需像 PyTorch 一样需要要先执行  <code class="docutils literal notranslate"><span class="pre">tensor.cpu</span></code> 再转换成 numpy 格式。</p>
</section>
<section id="nn-module">
<h3>nn.Module<a class="headerlink" href="#nn-module" title="Permalink to this headline"></a></h3>
<p>使用 PyTorch 构建网络结构时，我们会用到  <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> 类，通常将网络中的元素定义在  <code class="docutils literal notranslate"><span class="pre">__init__</span></code> 函数中并对其初始化，将网络的图结构表达定义在  <code class="docutils literal notranslate"><span class="pre">forward</span></code> 函数中，通过调用这些类的对象完成整个模型的构建和训练。<code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> 不仅为我们提供了构建图接口，它还为我们提供了一些常用的 <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html">API</a> ，来帮助我们执行更复杂逻辑。</p>
<p>MindSpore 中的 <code class="docutils literal notranslate"><span class="pre">nn.Cell</span></code> 类发挥着和 PyTorch 中 <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> 相同的作用，都是用来构建图结构的模块，MindSpore 也同样提供了丰富的 <a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/r1.7/api_python/nn/mindspore.nn.Cell.html">API</a> 供开发者使用，虽然名字不能一一对应，但 <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> 中常用的功能都可以在  <code class="docutils literal notranslate"><span class="pre">nn.Cell</span></code> 中找到映射。</p>
<p>以几个常用方法为例：</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="text-left head"><p>常用方法</p></th>
<th class="text-left head"><p>nn.Module</p></th>
<th class="text-left head"><p>nn.Cell</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>获取子元素</p></td>
<td class="text-left"><p>named_children</p></td>
<td class="text-left"><p>cells_and_names</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>添加子元素</p></td>
<td class="text-left"><p>add_module</p></td>
<td class="text-left"><p>insert_child_to_cell</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>获取元素的参数</p></td>
<td class="text-left"><p>parameters</p></td>
<td class="text-left"><p>get_parameters</p></td>
</tr>
</tbody>
</table>
</section>
<section id="数据对象">
<h3>数据对象<a class="headerlink" href="#数据对象" title="Permalink to this headline"></a></h3>
<p>在 PyTorch 中，可以存储数据的对象总共有四种，分别时  <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>、<code class="docutils literal notranslate"><span class="pre">Variable</span></code>、<code class="docutils literal notranslate"><span class="pre">Parameter</span></code>、<code class="docutils literal notranslate"><span class="pre">Buffer</span></code>。这四种对象的默认行为均不相同，当我们不需要求梯度时，通常使用 <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>  和 <code class="docutils literal notranslate"><span class="pre">Buffer</span></code>  两类数据对象，当我们需要求梯度时，通常使用 <code class="docutils literal notranslate"><span class="pre">Variable</span></code> 和 <code class="docutils literal notranslate"><span class="pre">Parameter</span></code> 两类对象。PyTorch 在设计这四种数据对象时，功能上存在冗余（<code class="docutils literal notranslate"><span class="pre">Variable</span></code> 后续会被废弃也说明了这一点）。</p>
<p>MindSpore 优化了数据对象的设计逻辑，仅保留了两种数据对象：<code class="docutils literal notranslate"><span class="pre">Tensor</span></code> 和 <code class="docutils literal notranslate"><span class="pre">Parameter</span></code>，其中 <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> 对象仅参与运算，并不需要对其进行梯度求导和参数更新，而 <code class="docutils literal notranslate"><span class="pre">Parameter</span></code> 数据对象和 PyTorch 的 <code class="docutils literal notranslate"><span class="pre">Parameter</span></code> 意义相同，会根据其属性  <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> 来决定是否对其进行梯度求导和 参数更新。 在网络迁移时，只要是在 PyTorch 中未进行参数更新的数据对象，均可在 MindSpore 中声明为 <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>。</p>
</section>
<section id="梯度求导">
<h3>梯度求导<a class="headerlink" href="#梯度求导" title="Permalink to this headline"></a></h3>
<p>梯度求导涉及的算子和接口差异主要是由 MindSpore 和 PyTorch 自动微分原理不同引起的。</p>
<section id="torch-no-grad">
<h4>torch.no_grad<a class="headerlink" href="#torch-no-grad" title="Permalink to this headline"></a></h4>
<p>在 PyTorch 中，默认情况下，执行正向计算时会记录反向传播所需的信息，在推理阶段或无需反向传播网络中，这一操作是冗余的，会额外耗时，因此，PyTorch 提供了  <code class="docutils literal notranslate"><span class="pre">torch.no_grad</span></code> 来取消该过程。</p>
<p>而 MindSpore 只有在调用  <code class="docutils literal notranslate"><span class="pre">GradOperation</span></code>  才会根据正向图结构来构建反向图，正向执行时不会记录任何信息，所以 MindSpore 并不需要该接口，也可以理解为 MindSpore 的正向计算均在  <code class="docutils literal notranslate"><span class="pre">torch.no_grad</span></code> 情况下进行的。</p>
</section>
<section id="retain-graph">
<h4>retain_graph<a class="headerlink" href="#retain-graph" title="Permalink to this headline"></a></h4>
<p>由于 PyTorch 是基于函数式的自动微分，所以默认每次执行完反向传播后都会自动清除记录的信息，从而进行下一次迭代。这就会导致当我们想再次利用这些反向图和梯度信息时，由于已被删除而获取失败。因此，PyTorch 提供了  <code class="docutils literal notranslate"><span class="pre">backward(retain_graph=True)</span></code> 来主动保留这些信息。</p>
<p>而 MindSpore 则不需要这个功能，MindSpore 是基于计算图的自动微分，反向图信息在调用  <code class="docutils literal notranslate"><span class="pre">GradOperation</span></code>   后便永久的记录在计算图中，只要再次调用计算图就可以获取梯度信息。</p>
</section>
<section id="高阶导数">
<h4>高阶导数<a class="headerlink" href="#高阶导数" title="Permalink to this headline"></a></h4>
<p>基于计算图的自动微分还有一个好处，我们可以很方便的实现高阶求导。第一次对正向图执行   <code class="docutils literal notranslate"><span class="pre">GradOperation</span></code> 操作后，我们可以得到一阶导，此时计算图被更新为正向图+一阶导的反向图结构，但我们再次对更新后的计算图执行 <code class="docutils literal notranslate"><span class="pre">GradOperation</span></code>  后，我们就可以得到二阶导，以此类推，通过基于计算图的自动微分，我们很容易求得一个网络的高阶导数。</p>
</section>
</section>
</section>
<section id="算子类">
<h2>算子类<a class="headerlink" href="#算子类" title="Permalink to this headline"></a></h2>
<p>MindSpore 的大部分算子 API 和 TensorFlow 相近，但也有一部分算子的默认行为与 PyTorch 或 TensorFlow 存在差异。开发者在做网络脚本迁移时，如果未注意到这些差异，直接使用默认行为，很容易出现与原迁移网络不一致的情况，影响网络训练，我们建议开发者在迁移网络时不仅对齐使用的算子，也要对齐算子的属性。这里我们总结了几个常见的差异算子。</p>
<section id="参数意义不同">
<h3>参数意义不同<a class="headerlink" href="#参数意义不同" title="Permalink to this headline"></a></h3>
<section id="nn-dropout">
<h4>nn.Dropout<a class="headerlink" href="#nn-dropout" title="Permalink to this headline"></a></h4>
<p>Dropout 常用于防止训练过拟合，有一个重要的 <strong>概率值</strong> 参数，该参数在 MindSpore 中的意义与 PyTorch 和 TensorFlow 中的意义完全相反。</p>
<p>在 MindSpore 中，概率值对应 Dropout 算子的属性 <code class="docutils literal notranslate"><span class="pre">keep_prob</span></code>，是指输入被保留的概率，<code class="docutils literal notranslate"><span class="pre">1-keep_prob</span></code>是指输入被置 0 的概率。</p>
<p>在 PyTorch 和 TensorFlow 中，概率值分别对应 Dropout 算子的属性 <code class="docutils literal notranslate"><span class="pre">p</span></code>和 <code class="docutils literal notranslate"><span class="pre">rate</span></code>，是指输入被置 0 的概率，与 MindSpore.nn.Dropout 中的 <code class="docutils literal notranslate"><span class="pre">keep_prob</span></code> 意义相反。</p>
<p>更多信息请参考链接： <a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/r1.7/api_python/nn/mindspore.nn.Dropout.html#mindspore.nn.Dropout">MindSpore Dropout</a> 、 <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html">PyTorch Dropout</a> 、 <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/nn/dropout">TensorFlow Dropout</a></p>
</section>
<section id="nn-batchnorm2d">
<h4>nn.BatchNorm2d<a class="headerlink" href="#nn-batchnorm2d" title="Permalink to this headline"></a></h4>
<p>BatchNorm 是 CV 领域比较特殊的正则化方法，它在训练和推理的过程中有着不同计算流程，通常由算子属性控制。MindSpore 和 PyTorch 的 BatchNorm 在这一点上使用了两种不同的参数组。</p>
<ul>
<li><p>差异一</p>
<p><code class="docutils literal notranslate"><span class="pre">torch.nn.BatchNorm2d</span></code> 在不同参数下的状态</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="text-left head"><p>training</p></th>
<th class="text-left head"><p>track_running_stats</p></th>
<th class="text-left head"><p>状态</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>True</p></td>
<td class="text-left"><p>True</p></td>
<td class="text-left"><p>期望中训练的状态，running_mean 和 running_var 会跟踪整个训练过程中 batch 的统计特性，而每组输入数据用当前 batch 的 mean 和 var 统计特性做归一化，然后再更新 running_mean 和 running_var。</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>True</p></td>
<td class="text-left"><p>False</p></td>
<td class="text-left"><p>每组输入数据会根据当前 batch 的统计特性做归一化，但不会有 running_mean 和 running_var 参数了。</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>False</p></td>
<td class="text-left"><p>Ture</p></td>
<td class="text-left"><p>期望中推理的状态，BN 使用 running_mean 和 running_var 做归一化，并且不会对其进行更新。</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>False</p></td>
<td class="text-left"><p>False</p></td>
<td class="text-left"><p>效果同第二点，只不过处于推理状态，不会学习 weight 和 bias 两个参数。一般不采用该状态。</p></td>
</tr>
</tbody>
</table>
<p><code class="docutils literal notranslate"><span class="pre">mindspore.nn.BatchNorm2d</span></code> 在不同参数下的状态</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="text-left head"><p>use_batch_statistics</p></th>
<th class="text-left head"><p>状态</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>True</p></td>
<td class="text-left"><p>期望中训练的状态，moving_mean 和 moving_var 会跟踪整个训练过程中 batch 的统计特性，而每组输入数据用当前 batch 的 mean 和 var 统计特性做归一化，然后再更新 moving_mean 和 moving_var。</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Fasle</p></td>
<td class="text-left"><p>期望中推理的状态，BN 使用 moving_mean 和 moving_var 做归一化，并且不会对其进行更新。</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>None</p></td>
<td class="text-left"><p>自动设置 use_batch_statistics。如果是训练，use_batch_statistics=True，如果是推理，use_batch_statistics=False。</p></td>
</tr>
</tbody>
</table>
<p>通过比较可以发现，<code class="docutils literal notranslate"><span class="pre">mindspore.nn.BatchNorm2d</span></code>  相比 <code class="docutils literal notranslate"><span class="pre">torch.nn.BatchNorm2d</span></code>，少了两种冗余的状态，仅保留了最常用的训练和推理两种状态。</p>
</li>
<li><p>差异二</p>
<p>BatchNorm系列算子 的 momentum 参数在 MindSpore 和 PyTorch 表示的意义相反，关系为：</p>
<p><span class="math notranslate nohighlight">\(momentum_{pytorch} = 1 - momentum_{mindspore}\)</span></p>
<p>参考链接：<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/r1.7/api_python/nn/mindspore.nn.BatchNorm2d.html">mindspore.nn.BatchNorm2d</a>，<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html">torch.nn.BatchNorm2d</a></p>
</li>
</ul>
</section>
</section>
<section id="算子行为不同">
<h3>算子行为不同<a class="headerlink" href="#算子行为不同" title="Permalink to this headline"></a></h3>
<section id="ops-transpose">
<h4>ops.Transpose<a class="headerlink" href="#ops-transpose" title="Permalink to this headline"></a></h4>
<p>在做轴变换时，PyTorch 常用到两种算子 - <code class="docutils literal notranslate"><span class="pre">Tensor.permute</span></code> 和 <code class="docutils literal notranslate"><span class="pre">torch.transpose</span></code>，而 MindSpore 和 TensorFlow 仅提供了 <code class="docutils literal notranslate"><span class="pre">transpose</span></code> 算子。需要注意的是，<code class="docutils literal notranslate"><span class="pre">Tensor.permute</span></code> 包含了 <code class="docutils literal notranslate"><span class="pre">torch.transpose</span></code> 的功能，<code class="docutils literal notranslate"><span class="pre">torch.transpose</span></code> 仅支持同时交换两个轴，而 <code class="docutils literal notranslate"><span class="pre">Tensor.permute</span></code> 可以同时变换多个轴。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># pytorch 代码</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">ret1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">dim0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">ret1</span><span class="o">.</span><span class="n">shape</span> <span class="c1"># (4, 2, 3, 1)</span>
<span class="n">ret2</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="n">ret2</span><span class="o">.</span><span class="n">shape</span> <span class="c1"># (4, 3, 2, 1)</span>
</pre></div>
</div>
<p>MindSpore 和 TensorFlow 的  <code class="docutils literal notranslate"><span class="pre">transpose</span></code>算子功能相同， 虽然名字叫 <code class="docutils literal notranslate"><span class="pre">transpose</span></code>，但实际可以做多个轴的同时变换，等价于 <code class="docutils literal notranslate"><span class="pre">Tensor.permute</span></code>。因此，MindSpore 也不再单独提供类似 <code class="docutils literal notranslate"><span class="pre">torch.tranpose</span></code> 的算子。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># mindspore 代码</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">ops</span>
<span class="n">ms</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;Ascend&quot;</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">ret</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Transpose</span><span class="p">()(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="n">ret</span><span class="o">.</span><span class="n">shape</span> <span class="c1"># (4, 3, 2, 1)</span>
</pre></div>
</div>
<p>更多信息请参考链接：<a class="reference external" href="https://www.mindspore.cn/docs/en/r1.7/api_python/ops/mindspore.ops.Transpose.html#mindspore.ops.Transpose">MindSpore Transpose</a> 、 <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.transpose.html">PyTorch Transpose</a> 、<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.permute.html">PyTorch Permute</a> 、 <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/transpose">TensforFlow Transpose</a></p>
</section>
<section id="conv-和-pooling">
<h4>Conv 和 Pooling<a class="headerlink" href="#conv-和-pooling" title="Permalink to this headline"></a></h4>
<p>对于类似卷积和池化的算子，我们知道算子的输出特征图大小依赖输入特征图、步长、kernel_size 和 padding 等变量。</p>
<p>如果 <code class="docutils literal notranslate"><span class="pre">pad_mode</span></code> 设置为  <code class="docutils literal notranslate"><span class="pre">valid</span></code>，则输出特征图高和宽的计算公式分别为：</p>
<p><img alt="图片" src="../_images/conv_formula.png" /></p>
<p>如果 pad_mode（对应 PyTorch 中的属性为 padding，与属性 pad_mode 含义并不相同） 设置为  <code class="docutils literal notranslate"><span class="pre">same</span></code> 时，有时需要对输入特征图进行自动的 padding 操作，当padding 的元素为偶数时，padding 的元素会均匀分布在特征图的上下左右，此时 MindSpore、PyTorch 和 TensorFlow 中该类算子行为一致。</p>
<p>但当 padding 的元素为奇数时，PyTorch 会优先填充在输入特征图的左侧和上侧：</p>
<p><img alt="图片" src="../_images/padding_pattern1.png" /></p>
<p>而 MindSpore 和 TensorFlow 则优先填充在特征图的右侧和下侧：</p>
<p><img alt="图片" src="../_images/padding_pattern2.png" /></p>
<p>举例说明：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># mindspore example</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">ops</span>
<span class="n">ms</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;Ascend&quot;</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">9</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="c1"># data</span>
<span class="c1"># [[[[0. 1. 2.]</span>
<span class="c1">#    [3. 4. 5.]</span>
<span class="c1">#    [6. 7. 8.]]]]</span>
<span class="n">op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MaxPool</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">)</span>
<span class="n">op</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
<span class="c1"># [[[[4. 5.]</span>
<span class="c1">#    [7. 8.]]]]</span>
</pre></div>
</div>
<p>在做 MindSpore 模型迁移时，如果模型加载了 PyTorch 的预训练模型，而之后又在 MindSpore 进行 finetune，则该差异可能会导致精度下降，对于 padding 策略为 same 的卷积，开发者需要特别注意。</p>
<p>如果想和 PyTorch 行为保持一致，可以先使用   <code class="docutils literal notranslate"><span class="pre">ops.Pad</span></code> 算子手动 pad 元素，然后使用  <code class="docutils literal notranslate"><span class="pre">pad_mode=&quot;valid&quot;</span></code> 的卷积和池化操作。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">ops</span>
<span class="n">ms</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;Ascend&quot;</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">9</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="c1"># only padding on top left of feature map</span>
<span class="n">pad</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Pad</span><span class="p">(((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)))</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pad</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
<span class="n">ops</span><span class="o">.</span><span class="n">MaxPool</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">)(</span><span class="n">data</span><span class="p">)</span>
<span class="c1"># [[[[0. 2.]</span>
<span class="c1">#    [6. 8.]]]]</span>
</pre></div>
</div>
</section>
</section>
<section id="默认权重初始化不同">
<h3>默认权重初始化不同<a class="headerlink" href="#默认权重初始化不同" title="Permalink to this headline"></a></h3>
<p>我们知道权重初始化对网络的训练十分重要，通过我们会选择一个适合的分布来初始化权重。但还有一个部分算子会隐式的声明权重，在不同的框架中，即使这些算子功能一致，但如果隐式声明的权重初始化方式分布不同，也会对训练过程产生影响，甚至无法收敛。</p>
<p>常见隐式声明权重的算子：Conv、Dense(Linear)、Embedding、LSTM 等，其中区别较大的是 Conv 类和 Dense 两种算子。</p>
<ul>
<li><p>Conv2d</p>
<ul class="simple">
<li><p>mindspore.nn.Conv2d的weight为：<span class="math notranslate nohighlight">\(\mathcal{N}(0, 1)\)</span>，bias为：zeros。</p></li>
<li><p>torch.nn.Conv2d的weight为：<span class="math notranslate nohighlight">\(\mathcal{U} (-\sqrt{k},\sqrt{k} )\)</span>，bias为：<span class="math notranslate nohighlight">\(\mathcal{U} (-\sqrt{k},\sqrt{k} )\)</span>。</p></li>
<li><p>tf.keras.Layers.Conv2D的weight为：glorot_uniform，bias为：zeros。</p></li>
</ul>
<p>其中，<span class="math notranslate nohighlight">\(k=\frac{groups}{c_{in}*\prod_{i}^{}{kernel\_size[i]}}\)</span></p>
</li>
<li><p>Dense(Linear)</p>
<ul class="simple">
<li><p>mindspore.nn.Linear的weight为：<span class="math notranslate nohighlight">\(\mathcal{N}(0, 1)\)</span>，bias为：zeros。</p></li>
<li><p>torch.nn.Dense的weight为：<span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k},\sqrt{k})\)</span>，bias为：<span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k},\sqrt{k} )\)</span>。</p></li>
<li><p>tf.keras.Layers.Dense的weight为：glorot_uniform，bias为：zeros。</p></li>
</ul>
<p>其中，<span class="math notranslate nohighlight">\(k=\frac{groups}{in\_features}\)</span></p>
</li>
</ul>
<p>对于没有正则化的网络，如没有 BatchNorm 算子的 GAN 网络，梯度很容易爆炸或者消失，权重初始化就显得十分重要，各位开发者应注意权重初始化带来的影响。</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="training_process_comparision.html" class="btn btn-neutral float-left" title="基本执行流程横向对比" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="optim.html" class="btn btn-neutral float-right" title="优化器迁移指南" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>