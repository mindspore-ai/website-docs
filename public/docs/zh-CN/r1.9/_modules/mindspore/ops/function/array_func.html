<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore.ops.function.array_func &mdash; MindSpore master documentation</title>
      <link rel="stylesheet" href="../../../../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/css/training.css" type="text/css" /><link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/js/training.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">设计</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/overview.html">MindSpore设计概览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/auto_gradient.html">函数式微分编程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/mindir.html">中间表示MindIR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/all_scenarios.html">全场景统一</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/dynamic_graph_and_static_graph.html">动静态图结合</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/distributed_training_design.html">分布式并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/graph_fusion_engine.html">图算融合加速引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/data_engine.html">高性能数据处理引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/glossary.html">术语</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">规格</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/benchmark.html">基准性能</a></li>
<li class="toctree-l1"><a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.9/README_CN.md#目录">网络支持↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/operator_list.html">API支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/syntax_list.html">语法支持</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.audio.html">mindspore.dataset.audio</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.config.html">mindspore.dataset.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.text.html">mindspore.dataset.text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.vision.html">mindspore.dataset.vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.transformer.html">mindspore.nn.transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.function.html">mindspore.ops.function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.9/api_cpp/mindspore.html">C++ API↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API映射</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/api_mapping/pytorch_api_mapping.html">PyTorch与MindSpore API映射表</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/api_mapping/tensorflow_api_mapping.html">TensorFlow与MindSpore API映射表</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">迁移指南</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/overview.html">概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/enveriment_preparation.html">环境准备与资料获取</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/analysis_and_preparation.html">模型分析与准备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/model_development/model_development.html">MindSpore网络搭建</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/debug_and_tune.html">调试调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/sample_code.html">网络迁移调试实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/faq.html">常见问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/typical_api_comparision.html">与PyTorch典型区别</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/use_third_party_op.html">基于自定义算子接口调用第三方算子库</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/installation.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/data_processing.html">数据处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/implement_problem.html">执行问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/network_compilation.html">网络编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/operators_compile.html">算子编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/usage_migrate_3rd.html">第三方框架迁移使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/performance_tuning.html">性能调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/precision_tuning.html">精度调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/distributed_configure.html">分布式配置</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/inference.html">推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/feature_advice.html">特性咨询</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
      <li>mindspore.ops.function.array_func</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for mindspore.ops.function.array_func</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2022 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>

<span class="sd">&quot;&quot;&quot;Operators for function.&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span>

<span class="kn">import</span> <span class="nn">mindspore.common.dtype</span> <span class="k">as</span> <span class="nn">mstype</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.primitive</span> <span class="kn">import</span> <span class="n">constexpr</span>

<span class="kn">from</span> <span class="nn">..operations.array_ops</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">UniqueConsecutive</span><span class="p">,</span>
    <span class="n">NonZero</span><span class="p">,</span>
    <span class="n">MatrixDiagV3</span><span class="p">,</span>
    <span class="n">MatrixDiagPartV3</span><span class="p">,</span>
    <span class="n">MatrixSetDiagV3</span><span class="p">,</span>
    <span class="n">Fills</span><span class="p">,</span>
    <span class="n">Col2Im</span><span class="p">,</span>
    <span class="n">ArgMaxWithValue</span><span class="p">,</span>
    <span class="n">ScatterNdMax</span><span class="p">,</span>
    <span class="n">ScatterNdMul</span><span class="p">,</span>
    <span class="n">IndexFill</span><span class="p">,</span>
    <span class="n">AffineGrid</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">..operations.nn_ops</span> <span class="kn">import</span> <span class="n">AdaptiveMaxPool2D</span>
<span class="kn">from</span> <span class="nn">..operations.array_ops</span> <span class="kn">import</span> <span class="n">TensorScatterElements</span>
<span class="kn">from</span> <span class="nn">...common</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">.._primitive_cache</span> <span class="kn">import</span> <span class="n">_get_cache_prim</span>

<span class="n">eye_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Eye</span><span class="p">()</span>
<span class="n">fill_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Fill</span><span class="p">()</span>
<span class="n">fills_</span> <span class="o">=</span> <span class="n">Fills</span><span class="p">()</span>
<span class="n">ones_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Ones</span><span class="p">()</span>
<span class="n">ones_like_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">OnesLike</span><span class="p">()</span>
<span class="n">tile_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Tile</span><span class="p">()</span>
<span class="n">unique_with_pad_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">UniqueWithPad</span><span class="p">()</span>
<span class="n">size_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Size</span><span class="p">()</span>
<span class="n">shape_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()</span>
<span class="n">rank_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Rank</span><span class="p">()</span>
<span class="n">tensor_shape_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">()</span>
<span class="n">reshape_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()</span>
<span class="n">flatten_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>
<span class="n">tensor_slice</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Slice</span><span class="p">()</span>
<span class="n">expand_dims_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">()</span>
<span class="n">transpose_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Transpose</span><span class="p">()</span>
<span class="n">scatter_add_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScatterAdd</span><span class="p">()</span>
<span class="n">scatter_max_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScatterMax</span><span class="p">()</span>
<span class="n">scatter_min_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScatterMin</span><span class="p">()</span>
<span class="n">scatter_mul_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScatterMul</span><span class="p">()</span>
<span class="n">scatter_div_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScatterDiv</span><span class="p">()</span>
<span class="n">scatter_nd_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScatterNd</span><span class="p">()</span>
<span class="n">gather_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Gather</span><span class="p">()</span>
<span class="n">gather_d_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">GatherD</span><span class="p">()</span>
<span class="n">gather_nd_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">GatherNd</span><span class="p">()</span>
<span class="n">nonzero_</span> <span class="o">=</span> <span class="n">NonZero</span><span class="p">()</span>
<span class="n">scalar_cast_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScalarCast</span><span class="p">()</span>
<span class="n">tensor_scatter_add_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TensorScatterAdd</span><span class="p">()</span>
<span class="n">tensor_scatter_sub_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TensorScatterSub</span><span class="p">()</span>
<span class="n">tensor_scatter_mul_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TensorScatterMul</span><span class="p">()</span>
<span class="n">tensor_scatter_div_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TensorScatterDiv</span><span class="p">()</span>
<span class="n">tensor_scatter_min_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TensorScatterMin</span><span class="p">()</span>
<span class="n">tensor_scatter_max_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TensorScatterMax</span><span class="p">()</span>
<span class="n">scalar_to_array_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScalarToArray</span><span class="p">()</span>
<span class="n">scalar_to_tensor_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScalarToTensor</span><span class="p">()</span>
<span class="n">tuple_to_array_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TupleToArray</span><span class="p">()</span>
<span class="n">masked_select_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">MaskedSelect</span><span class="p">()</span>
<span class="n">matrix_band_part_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">array_ops</span><span class="o">.</span><span class="n">MatrixBandPart</span><span class="p">()</span>
<span class="n">ger_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Ger</span><span class="p">()</span>
<span class="n">diag_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Diag</span><span class="p">()</span>
<span class="n">range_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Range</span><span class="p">()</span>
<span class="n">zeros_like_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ZerosLike</span><span class="p">()</span>
<span class="n">cast_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>
<span class="n">tensor_select_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Select</span><span class="p">()</span>
<span class="n">index_fill_</span> <span class="o">=</span> <span class="n">IndexFill</span><span class="p">()</span>
<span class="n">unsorted_segment_sum_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">UnsortedSegmentSum</span><span class="p">()</span>
<span class="n">population_count_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">PopulationCount</span><span class="p">()</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">get_x_shape</span><span class="p">(</span><span class="n">x_shape</span><span class="p">):</span>
    <span class="k">if</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">in</span> <span class="n">x_shape</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)</span>
    <span class="k">if</span> <span class="o">-</span><span class="mi">2</span> <span class="ow">in</span> <span class="n">x_shape</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">x_shape</span><span class="p">:</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">s</span> <span class="o">*</span> <span class="n">i</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">s</span><span class="p">,)</span>


<span class="c1">##############################</span>
<span class="c1"># Tensor Creation Functions.</span>
<span class="c1">##############################</span>


<div class="viewcode-block" id="eye"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.eye.html#mindspore.ops.eye">[文档]</a><span class="k">def</span> <span class="nf">eye</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a tensor with ones on the diagonal and zeros in the rest.</span>

<span class="sd">    Note:</span>
<span class="sd">        Combines ReverseV2 operator to get an anti-diagonal Tensor,</span>
<span class="sd">        but ReverseV2 only supports Ascend and GPU platforms currently.</span>

<span class="sd">    Args:</span>
<span class="sd">        n (int): The number of rows of returned tensor. Constant value only.</span>
<span class="sd">        m (int): The number of columns of returned tensor. Constant value only.</span>
<span class="sd">        t (mindspore.dtype): MindSpore&#39;s dtype, the data type of the returned tensor.</span>
<span class="sd">            The data type can be Number.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, a tensor with ones on the diagonal and the rest of elements are zero. The shape of `output` depends on</span>
<span class="sd">        the user&#39;s Inputs `n` and `m`. And the data type depends on Inputs `t`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `m` or `n` is not an int.</span>
<span class="sd">        ValueError: If `m` or `n` is less than 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; output = ops.eye(2, 2, mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1 0]</span>
<span class="sd">         [0 1]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Int32</span>
<span class="sd">        &gt;&gt;&gt; output = ops.eye(1, 2, mindspore.float64)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 0.]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Float64</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">eye_</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span></div>


<div class="viewcode-block" id="matrix_band_part"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.matrix_band_part.html#mindspore.ops.matrix_band_part">[文档]</a><span class="k">def</span> <span class="nf">matrix_band_part</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Copy a tensor setting everything outside a central band in each innermost matrix to zero.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input tensor. :math:`(*, m, n)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">            The data type must be float16, float32, float64, int32 or int64.</span>
<span class="sd">        lower (Union[int, Tensor]): Number of subdiagonals to keep. The data type must be int32 or int64.</span>
<span class="sd">            If negative, keep entire lower triangle.</span>
<span class="sd">        upper (Union[int, Tensor]): Number of superdiagonals to keep. The data type must be int32 or int64.</span>
<span class="sd">            If negative, keep entire upper triangle.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same type and shape as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not one of float16, float32, float64, int32 or int64.</span>
<span class="sd">        TypeError: If `lower` is neither a number nor a Tensor.</span>
<span class="sd">        TypeError: If `upper` is neither a number nor a Tensor.</span>
<span class="sd">        TypeError: If dtype of `lower` is neither int32 nor int64.</span>
<span class="sd">        TypeError: If dtype of `upper` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If the shape of `x` is not greater than or equal to 2D.</span>
<span class="sd">        ValueError: If the shape of `lower` is not equal to 0D.</span>
<span class="sd">        ValueError: If the shape of `upper` is not equal to 0D.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([2, 4, 4]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.matrix_band_part(x, 2, 1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1. 1. 0. 0.]</span>
<span class="sd">          [1. 1. 1. 0.]</span>
<span class="sd">          [1. 1. 1. 1.]</span>
<span class="sd">          [0. 1. 1. 1.]]</span>
<span class="sd">         [[1. 1. 0. 0.]</span>
<span class="sd">          [1. 1. 1. 0.]</span>
<span class="sd">          [1. 1. 1. 1.]</span>
<span class="sd">          [0. 1. 1. 1.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">matrix_band_part_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">)</span></div>


<div class="viewcode-block" id="padding"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.padding.html#mindspore.ops.padding">[文档]</a><span class="k">def</span> <span class="nf">padding</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pad_dim_size</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extends the last dimension of the input tensor from 1 to pad_dim_size, by filling with 0.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The shape of tensor is :math:`(x_1, x_2, ..., x_R)`. The rank of `x` must be at least 2.</span>
<span class="sd">            The last dimension of `x` must be 1. The data type is Number.</span>
<span class="sd">        pad_dim_size (int): The value of the last dimension of `x` to be extended, which must be positive. Default: 8.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same type and shape as input shape value.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `pad_dim_size` is not an int.</span>
<span class="sd">        ValueError: If `pad_dim_size` is less than 1.</span>
<span class="sd">        ValueError: If last dim of `x` is not equal to 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[8], [10]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; pad_dim_size = 4</span>
<span class="sd">        &gt;&gt;&gt; output = ops.padding(x, pad_dim_size)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 8.  0.  0.  0.]</span>
<span class="sd">         [10.  0.  0.  0.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">padding_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">array_ops</span><span class="o">.</span><span class="n">Padding</span><span class="p">)(</span><span class="n">pad_dim_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">padding_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="one_hot"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.one_hot.html#mindspore.ops.one_hot">[文档]</a><span class="k">def</span> <span class="nf">one_hot</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">on_value</span><span class="p">,</span> <span class="n">off_value</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes a one-hot tensor.</span>

<span class="sd">    The locations represented by indices in `indices` take value `on_value`, while all</span>
<span class="sd">    other locations take value `off_value`.</span>

<span class="sd">    Note:</span>
<span class="sd">        If the input indices is rank `N`, the output will have rank `N+1`. The new axis is created at dimension `axis`.</span>

<span class="sd">    Args:</span>
<span class="sd">        indices(Tensor): A tensor of indices. Tensor of shape :math:`(X_0, \ldots, X_n)`.</span>
<span class="sd">            Data type must be uint8, int32 or int64.</span>
<span class="sd">        depth(int): A scalar defining the depth of the one-hot dimension.</span>
<span class="sd">        on_value(Tensor): A value to fill in output when `indices[j] = i`.</span>
<span class="sd">            Support uint8, uint16, uint32, uint64, int8, int16, int32, int64, float16, float32, float64,</span>
<span class="sd">            bool, complex64, complex128.</span>
<span class="sd">        off_value(Tensor): A value to fill in output when `indices[j] != i`.</span>
<span class="sd">            Has the same data type as `on_value`.</span>
<span class="sd">        axis(int): Position to insert the value. e.g. If shape of `self` is :math:`(N, C)`, and `axis` is -1,</span>
<span class="sd">            the output shape will be :math:`(N, C, D)`, If `axis` is 0, the output shape will be :math:`(D, N, C)`.</span>
<span class="sd">            Default: -1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, one-hot tensor. Tensor of shape :math:`(X_0, \ldots, X_{axis}, \text{depth} ,X_{axis+1}, \ldots, X_n)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` or `depth` is not an int.</span>
<span class="sd">        TypeError: If dtype of `indices` is not uint8, int32 or int64.</span>
<span class="sd">        TypeError: If `indices`, `on_value` or `off_value` is not a Tensor.</span>
<span class="sd">        ValueError: If `axis` is not in range [-1, ndim].</span>
<span class="sd">        ValueError: If `depth` is less than 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1, 2]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; depth, on_value, off_value = 3, Tensor(1.0, mindspore.float32), Tensor(0.0, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.one_hot(indices, depth, on_value, off_value, axis=-1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 0. 0.]</span>
<span class="sd">         [0. 1. 0.]</span>
<span class="sd">         [0. 0. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">onehot</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">OneHot</span><span class="p">)(</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">onehot</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">on_value</span><span class="p">,</span> <span class="n">off_value</span><span class="p">)</span></div>


<div class="viewcode-block" id="fill"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.fill.html#mindspore.ops.fill">[文档]</a><span class="k">def</span> <span class="nf">fill</span><span class="p">(</span><span class="nb">type</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create a Tensor of the specified shape and fill it with the specified value.</span>

<span class="sd">    Args:</span>
<span class="sd">        type (mindspore.dtype): The specified type of output tensor. The data type only supports</span>
<span class="sd">            `bool_ &lt;https://www.mindspore.cn/docs/en/r1.9/api_python/mindspore.html#mindspore.dtype&gt;`_ and</span>
<span class="sd">            `number &lt;https://www.mindspore.cn/docs/en/r1.9/api_python/mindspore.html#mindspore.dtype&gt;`_ .</span>
<span class="sd">        shape (tuple[int]): The specified shape of output tensor.</span>
<span class="sd">        value (Union(number.Number, bool)): Value to fill the returned tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `shape` is not a tuple.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; output = ops.fill(mindspore.float32, (2, 2), 1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 1.]</span>
<span class="sd">         [1. 1.]]</span>
<span class="sd">        &gt;&gt;&gt; output = ops.fill(mindspore.float32, (3, 3), 0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 0. 0.]</span>
<span class="sd">         [0. 0. 0.]</span>
<span class="sd">         [0. 0. 0.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">fill_</span><span class="p">(</span><span class="nb">type</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">fills</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create a tensor of the same shape and type as the input tensor and fill it with specified value.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input tensor, used to specify the shape and type of the output tensor. The data type should be</span>
<span class="sd">            int8, int16, int32, float16 or float32.</span>
<span class="sd">        value (Union[int, float, Tensor]): All elements of the output tensor will be assigned this value. The</span>
<span class="sd">            type should be int, float or 0-dimensional tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same shape and type as input tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a tensor.</span>
<span class="sd">        TypeError: If `value` has types not specified above.</span>
<span class="sd">        RuntimeError: If `value` cannot be converted to the same type as `x`.</span>
<span class="sd">        ValueError: If `value` is a tensor and the length of dimension is not 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(4).reshape((2, 2)).astype(&#39;float32&#39;))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.fills(x, 1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 1.]</span>
<span class="sd">         [1. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
        <span class="n">value_</span> <span class="o">=</span> <span class="n">value</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">value_</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">value</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;ops.fills&#39;, if the argument &#39;value&#39; is a tensor, the number of its dimension&quot;</span>
                             <span class="s2">&quot; should be 0, but got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">value</span><span class="o">.</span><span class="n">ndim</span><span class="p">))</span>
        <span class="n">value_</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For &#39;ops.fills&#39;, the type of argument &#39;value&#39; should be int, float or Tensor,&quot;</span>
                        <span class="s2">&quot; but got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">value</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">fills_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">value_</span><span class="p">)</span>


<div class="viewcode-block" id="ones"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ones.html#mindspore.ops.ones">[文档]</a><span class="k">def</span> <span class="nf">ones</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="nb">type</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a tensor filled with value ones.</span>

<span class="sd">    Creates a tensor with shape described by the first argument and</span>
<span class="sd">    fills it with value ones in type of the second argument.</span>

<span class="sd">    Args:</span>
<span class="sd">        shape (Union[tuple[int], int]): The specified shape of output tensor. Only constant positive int is allowed.</span>
<span class="sd">        type (mindspore.dtype): The specified type of output tensor. Only constant value is allowed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same type and shape as input shape value.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `shape` is neither tuple nor int.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; output = ops.ones((2, 2), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 1.]</span>
<span class="sd">         [1. 1.]]</span>
<span class="sd">        &gt;&gt;&gt; output = ops.ones((3, 3), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 1. 1.]</span>
<span class="sd">         [1. 1. 1.]</span>
<span class="sd">         [1. 1. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">ones_</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="nb">type</span><span class="p">)</span></div>


<div class="viewcode-block" id="ones_like"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ones_like.html#mindspore.ops.ones_like">[文档]</a><span class="k">def</span> <span class="nf">ones_like</span><span class="p">(</span><span class="n">input_x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a Tensor with a value of 1 and its shape and data type is the same as the input.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Tensor of any dimension.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as `input_x` but filled with ones.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[0, 1], [2, 1]]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.ones_like(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1 1]</span>
<span class="sd">         [1 1]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">ones_like_</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="tile"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.tile.html#mindspore.ops.tile">[文档]</a><span class="k">def</span> <span class="nf">tile</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">multiples</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replicates an input tensor with given multiples times.</span>

<span class="sd">    Creates a new tensor by replicating `input_x` `multiples` times. The i&#39;th dimension of</span>
<span class="sd">    output tensor has `input_x.shape[i] * multiples[i]` elements, and the values of `input_x`</span>
<span class="sd">    are replicated `multiples[i]` times along the i&#39;th dimension.</span>

<span class="sd">    Note:</span>
<span class="sd">        The length of `multiples` must be greater or equal to the length of dimension in `input_x`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): 1-D or higher dimensional Tensor. Set the shape of input tensor as</span>
<span class="sd">            :math:`(x_1, x_2, ..., x_S)` .</span>

<span class="sd">        multiples (tuple[int]): The parameter that specifies the number of replications,</span>
<span class="sd">            the parameter type is tuple, and the data type is int, i.e., :math:`(y_1, y_2, ..., y_S)`.</span>
<span class="sd">            The length of `multiples` cannot be smaller than the length of the shape of `input_x`.</span>
<span class="sd">            Only constant value is allowed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same data type as the `input_x`. Suppose the length of `multiples` is `d`,</span>
<span class="sd">        the dimension of `input_x` is `input_x.dim`, and the shape of `input_x` is :math:`(x_1, x_2, ..., x_S)`.</span>

<span class="sd">        - If `input_x.dim = d`, then the shape of their corresponding positions can be multiplied, and</span>
<span class="sd">          the shape of Outputs is :math:`(x_1*y_1, x_2*y_2, ..., x_S*y_R)`.</span>
<span class="sd">        - If `input_x.dim &lt; d`, fill in multiple 1 in the length of the shape of `input_x` until their</span>
<span class="sd">          lengths are consistent. Such as set the shape of `input_x` as :math:`(1, ..., x_1, x_2, ..., x_S)`,</span>
<span class="sd">          then the shape of their corresponding positions can be multiplied, and the shape of Outputs is</span>
<span class="sd">          :math:`(1*y_1, ..., x_S*y_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `multiples` is not a tuple or its elements are not all int.</span>
<span class="sd">        ValueError: If the elements of `multiples` are not all greater than 0.</span>
<span class="sd">        ValueError: If the length of `multiples` are smaller than the length of dimension in `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1, 2], [3, 4]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; multiples = (2, 3)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.tile(input_x, multiples)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1.  2.  1.  2.  1.  2.]</span>
<span class="sd">         [3.  4.  3.  4.  3.  4.]</span>
<span class="sd">         [1.  2.  1.  2.  1.  2.]</span>
<span class="sd">         [3.  4.  3.  4.  3.  4.]]</span>
<span class="sd">        &gt;&gt;&gt; multiples = (2, 3, 2)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.tile(input_x, multiples)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1. 2. 1. 2.]</span>
<span class="sd">          [3. 4. 3. 4.]</span>
<span class="sd">          [1. 2. 1. 2.]</span>
<span class="sd">          [3. 4. 3. 4.]</span>
<span class="sd">          [1. 2. 1. 2.]</span>
<span class="sd">          [3. 4. 3. 4.]]</span>
<span class="sd">         [[1. 2. 1. 2.]</span>
<span class="sd">          [3. 4. 3. 4.]</span>
<span class="sd">          [1. 2. 1. 2.]</span>
<span class="sd">          [3. 4. 3. 4.]</span>
<span class="sd">          [1. 2. 1. 2.]</span>
<span class="sd">          [3. 4. 3. 4.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tile_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">multiples</span><span class="p">)</span></div>


<div class="viewcode-block" id="range"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.range.html#mindspore.ops.range">[文档]</a><span class="k">def</span> <span class="nf">range</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">limit</span><span class="p">,</span> <span class="n">delta</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a sequence of numbers that begins at `start` and extends by increments of</span>
<span class="sd">    `delta` up to but not including `limit`.</span>

<span class="sd">    The types of all 3 inputs must be the same. The type of the resulting tensor is</span>
<span class="sd">    the same as the type of the inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">        start (Tensor): A scalar Tensor. The first number in the sequence. Must have</span>
<span class="sd">          type: int32 ,int64, float32 or float64.</span>
<span class="sd">        limit (Tensor): A scalar Tensor. Upper limit of the sequence, exclusive. Must</span>
<span class="sd">          have type: int32 ,int64, float32 or float64.</span>
<span class="sd">        delta (Tensor): A scalar Tensor. Number that increments `start`. Must have</span>
<span class="sd">          type: int32 ,int64, float32 or float64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A 1-D Tensor, with the same type as the inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `start`, `limit` or `delta` is not scalar Tensor.</span>
<span class="sd">        TypeError: If datatype of `start`, `limit` or `delta` is not same.</span>
<span class="sd">        TypeError: If datatype of `start`, `limit` or `delta` is not supported.</span>
<span class="sd">        ValueError: If `delta` = 0.</span>
<span class="sd">        ValueError: If `start` &gt;= `limit` when `delta` &gt; 0.</span>
<span class="sd">        ValueError: If `start` &lt;= `limit` when `delta` &lt; 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; start = Tensor(0, mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; limit = Tensor(10, mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; delta = Tensor(4, mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.range(start, limit, delta)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0 4 8]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">range_</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">limit</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span></div>


<span class="c1">##############################</span>
<span class="c1"># Tensor Operation Functions.</span>
<span class="c1">##############################</span>


<div class="viewcode-block" id="unique"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.unique.html#mindspore.ops.unique">[文档]</a><span class="k">def</span> <span class="nf">unique</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the unique elements of input tensor and also return a tensor containing the index of each value of input</span>
<span class="sd">    tensor corresponding to the output unique tensor.</span>

<span class="sd">    The output contains Tensor `y` and Tensor `idx`, the format is probably similar to (`y`, `idx`).</span>
<span class="sd">    The shape of Tensor `y` and Tensor `idx` is different in most cases, because Tensor `y` will be deduplicated,</span>
<span class="sd">    and the shape of Tensor `idx` is consistent with the input.</span>

<span class="sd">    To get the same shape between `idx` and `y`, please ref to :class:&#39;mindspore.ops.UniqueWithPad&#39; operator.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor.</span>
<span class="sd">            The shape is :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental prototype that is subject to change and/or deletion.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple, containing Tensor objects (`y`, `idx`), `y` is a tensor with the</span>
<span class="sd">        same type as `x`, and contains the unique elements in `x`.</span>
<span class="sd">        `idx` is a tensor containing indices of elements in</span>
<span class="sd">        the input corresponding to the output tensor, have the same shape with `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 5, 2]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.unique(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[3], dtype=Int32, value= [1, 2, 5]), Tensor(shape=[4], dtype=Int32, value= [0, 1, 2, 1]))</span>
<span class="sd">        &gt;&gt;&gt; y = output[0]</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [1 2 5]</span>
<span class="sd">        &gt;&gt;&gt; idx = output[1]</span>
<span class="sd">        &gt;&gt;&gt; print(idx)</span>
<span class="sd">        [0 1 2 1]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">unique_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Unique</span><span class="p">)()</span>
    <span class="n">reshape_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">)()</span>

    <span class="n">shape_x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">length_x</span> <span class="o">=</span> <span class="n">get_x_shape</span><span class="p">(</span><span class="n">shape_x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">reshape_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">length_x</span><span class="p">)</span>
    <span class="n">y</span><span class="p">,</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">unique_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">reshape_op</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">shape_x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span><span class="p">,</span> <span class="n">idx</span></div>


<div class="viewcode-block" id="unique_with_pad"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.unique_with_pad.html#mindspore.ops.unique_with_pad">[文档]</a><span class="k">def</span> <span class="nf">unique_with_pad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pad_num</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns unique elements and relative indexes in 1-D tensor, filled with padding num.</span>

<span class="sd">    The basic function is the same as the Unique operator, but the UniqueWithPad operator adds a Pad function.</span>
<span class="sd">    The returned tuple(`y`, `idx`) after the input Tensor `x` is processed by the unique operator,</span>
<span class="sd">    in which the shapes of `y` and `idx` are mostly not equal. Therefore, in order to solve the above situation,</span>
<span class="sd">    the UniqueWithPad operator will fill the `y` Tensor with the `pad_num` specified by the user</span>
<span class="sd">    to make it have the same shape as the Tensor `idx`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The tensor need to be unique. Must be 1-D vector with types: int32, int64.</span>
<span class="sd">        pad_num (int): Pad num. The data type is an int.</span>

<span class="sd">    Returns:</span>
<span class="sd">        tuple(Tensor), tuple of 2 tensors, `y` and `idx`.</span>

<span class="sd">        - y (Tensor) - The unique elements filled with pad_num, the shape and data type same as `x`.</span>
<span class="sd">        - idx (Tensor) - The index of each value of `x` in the unique output `y`, the shape and data type same as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If length of shape of `x` is not equal to 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 5, 2, 3, 5]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.unique_with_pad(x, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[6], dtype=Int32, value= [1, 2, 5, 3, 0, 0]),</span>
<span class="sd">         Tensor(shape=[6], dtype=Int32, value= [0, 1, 2, 1, 3, 2]))</span>
<span class="sd">        &gt;&gt;&gt; y = output[0]</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [1 2 5 3 0 0]</span>
<span class="sd">        &gt;&gt;&gt; idx = output[1]</span>
<span class="sd">        &gt;&gt;&gt; print(idx)</span>
<span class="sd">        [0 1 2 1 3 2]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">unique_with_pad_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pad_num</span><span class="p">)</span></div>


<div class="viewcode-block" id="unique_consecutive"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.unique_consecutive.html#mindspore.ops.unique_consecutive">[文档]</a><span class="k">def</span> <span class="nf">unique_consecutive</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">return_idx</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the elements that are unique in each consecutive group of equivalent elements in the input tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor.</span>
<span class="sd">        return_idx (bool, optional): Whether to return the indices of the end position of each element in the</span>
<span class="sd">            original input list in the returned unique list. Default: False.</span>
<span class="sd">        return_counts (bool, optional): Whether to return the counts of each unique element. Default: False.</span>
<span class="sd">        axis (int, optional): The dimension to apply unique. If None, the unique of the flattened input is</span>
<span class="sd">            returned. If specified, it must be int32 or int64. Default: None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tensor or a tuple of tensors containing tensor objects (`output`, `idx`, `counts`). `output` has the</span>
<span class="sd">        same type as `x` and is used to represent the output list of unique scalar elements. If `return_idx` is</span>
<span class="sd">        True, there will be an additional returned tensor, `idx`, which has the same shape as `x` and represents</span>
<span class="sd">        the index of where the element in the original input maps to the position in the output. If `return_counts`</span>
<span class="sd">        is True, there will be an additional returned tensor, `counts`, which represents the number of occurrences</span>
<span class="sd">        for each unique value or tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        RuntimeError: If `axis` is not in the range of :math:`[-ndim, ndim-1]`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 1, 2, 2, 3, 1, 1, 2]), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; output, idx, counts = ops.unique_consecutive(x, True, True, None)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1 2 3 1 2]</span>
<span class="sd">        &gt;&gt;&gt; print(idx)</span>
<span class="sd">        [0 0 1 1 2 3 3 4]</span>
<span class="sd">        &gt;&gt;&gt; print(counts)</span>
<span class="sd">        [2 2 1 2 1]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">unique_consecutive_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">UniqueConsecutive</span><span class="p">)(</span><span class="n">return_idx</span><span class="p">,</span> <span class="n">return_counts</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
    <span class="n">output</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">unique_consecutive_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">return_idx</span> <span class="ow">and</span> <span class="n">return_counts</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">counts</span>
    <span class="k">if</span> <span class="n">return_idx</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">idx</span>
    <span class="k">if</span> <span class="n">return_counts</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">counts</span>
    <span class="k">return</span> <span class="n">output</span></div>


<div class="viewcode-block" id="ger"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ger.html#mindspore.ops.ger">[文档]</a><span class="k">def</span> <span class="nf">ger</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Ger product of `x1` and `x2`. Calculate the outer product of two arrays. If `x1` is a 1D Tensor of</span>
<span class="sd">    shape :math:`(m,)` and `x2` is a 1D Tensor of shape :math:`(n,)`, then `output` must be a 2D Tensor of shape</span>
<span class="sd">    :math:`(m, n)`.</span>

<span class="sd">    Note:</span>
<span class="sd">        Currently Ascend does not support float64 data input.</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Tensor): input Tensor, with dtype of float16, float32 or float64.</span>
<span class="sd">        x2 (Tensor): input Tensor, with dtype of float16, float32 or float64, must have the same dtype as `x1`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, output matrix with the same dtype as inputs. With `x1` shape :math:`(m,)` and</span>
<span class="sd">        `x2` shape of :math:`(n,)`, the `output` has shape :math:`(m, n)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x1` or `x2` is not a 1-D Tensor.</span>
<span class="sd">        TypeError: If the dtype of `x1` and `x2` is not float16, float32 or float64.</span>
<span class="sd">        TypeError: If the dtype of `x1` and `x2` are not the same.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor([1., 2., 3., 4.], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor([1., 2., 3.], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.ger(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 1.  2.  3.]</span>
<span class="sd">         [ 2.  4.  6.]</span>
<span class="sd">         [ 3.  6.  9.]</span>
<span class="sd">         [ 4.  8. 12.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">ger_</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span></div>


<div class="viewcode-block" id="size"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.size.html#mindspore.ops.size">[文档]</a><span class="k">def</span> <span class="nf">size</span><span class="p">(</span><span class="n">input_x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a Scalar of type int that represents the size of the input Tensor and the total number of elements in the</span>
<span class="sd">    Tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Input parameters, the shape of tensor is :math:`(x_1, x_2, ..., x_R)`. The data type is</span>
<span class="sd">            `number &lt;https://www.mindspore.cn/docs/en/r1.9/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>

<span class="sd">    Returns:</span>
<span class="sd">        int. A scalar representing the elements&#39; size of `input_x`, tensor is the number of elements</span>
<span class="sd">        in a tensor, :math:`size=x_1*x_2*...x_R`. The data type is an int.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[2, 2], [2, 2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.size(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        4</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">size_</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="shape"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.shape.html#mindspore.ops.shape">[文档]</a><span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="n">input_x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the shape of the input tensor. And it used to be static shape.</span>

<span class="sd">    static shape: A shape that can be obtained without running the graph. It is an inherent property of tensor and</span>
<span class="sd">    may be unknown. The static shape information can be completed by artificial setting.</span>
<span class="sd">    No matter what the input of the graph is, the static shape is not affected.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        tuple[int], the output tuple is constructed by multiple integers,</span>
<span class="sd">        :math:`(x_1, x_2, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones(shape=[3, 2, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.shape(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (3, 2, 1)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">shape_</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">dyn_shape</span><span class="p">(</span><span class="n">input_x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the shape of the input tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor[int], 1-dim Tensor of type int32</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones(shape=[3, 2, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.dyn_shape(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [3 2 1]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_shape_</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>


<div class="viewcode-block" id="rank"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.rank.html#mindspore.ops.rank">[文档]</a><span class="k">def</span> <span class="nf">rank</span><span class="p">(</span><span class="n">input_x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the rank of a tensor.</span>

<span class="sd">    Returns a 0-D int32 Tensor representing the rank of input; the rank of a tensor</span>
<span class="sd">    is the number of indices required to uniquely select each element of the tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The shape of tensor is :math:`(x_1, x_2, ..., x_R)`. The data type is Number.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor. 0-D int32 Tensor representing the rank of input, i.e., :math:`R`. The data type is an int.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor(np.array([[2, 2], [2, 2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.rank(input_tensor)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        2</span>
<span class="sd">        &gt;&gt;&gt; print(type(output))</span>
<span class="sd">        &lt;class &#39;int&#39;&gt;</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">rank_</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="reshape"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.reshape.html#mindspore.ops.reshape">[文档]</a><span class="k">def</span> <span class="nf">reshape</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Rearranges the input Tensor based on the given shape.</span>

<span class="sd">    The &#39;input_shape&#39; can only have one -1 at most, in which case it’s inferred from the remaining dimensions and</span>
<span class="sd">    the number of elements in the input.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">        input_shape (Union[tuple[int], Tensor[int]]): Constructed by multiple</span>
<span class="sd">            integers, i.e., :math:`(y_1, y_2, ..., y_S)`. Only constant value is allowed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape of tensor is :math:`(y_1, y_2, ..., y_S)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: Given a shape tuple, if it has several -1; or if the product</span>
<span class="sd">            of its elements is less than or equal to 0 or cannot be divided by the product</span>
<span class="sd">            of the input tensor shape; or if it does not match the input&#39;s array size.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reshape(input_x, (3, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-0.1  0.3]</span>
<span class="sd">         [ 3.6  0.4]</span>
<span class="sd">         [ 0.5 -3.2]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">reshape_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">)</span></div>


<div class="viewcode-block" id="reverse_sequence"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.reverse_sequence.html#mindspore.ops.reverse_sequence">[文档]</a><span class="k">def</span> <span class="nf">reverse_sequence</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">seq_lengths</span><span class="p">,</span> <span class="n">seq_dim</span><span class="p">,</span> <span class="n">batch_dim</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reverses variable length slices.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input to reverse, supporting all number types including bool.</span>
<span class="sd">        seq_lengths (Tensor): Must be a 1-D vector with int32 or int64 types.</span>
<span class="sd">        seq_dim (int): The dimension where reversal is performed. Required.</span>
<span class="sd">        batch_dim (int): The input is sliced in this dimension. Default: 0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same shape and data type as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `seq_dim` or `batch_dim` is not an int.</span>
<span class="sd">        ValueError: If value of `batch_dim` is equal to or greater than length of shape of input.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; seq_lengths = Tensor(np.array([1, 2, 3]))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reverse_sequence(x, seq_lengths, seq_dim=1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 2. 3.]</span>
<span class="sd">         [5. 4. 6.]</span>
<span class="sd">         [9. 8. 7.]]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; seq_lengths = Tensor(np.array([1, 2, 3]))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reverse_sequence(x, seq_lengths, seq_dim=0, batch_dim=1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 5. 9.]</span>
<span class="sd">         [4. 2. 6.]</span>
<span class="sd">         [7. 8. 3.]]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; seq_lengths = Tensor(np.array([2, 2, 3]))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reverse_sequence(x, seq_lengths, seq_dim=1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[2. 1. 3.]</span>
<span class="sd">         [5. 4. 6.]</span>
<span class="sd">         [9. 8. 7.]]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; seq_lengths = Tensor(np.array([3, 2, 3]))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reverse_sequence(x, seq_lengths, seq_dim=1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[3. 2. 1.]</span>
<span class="sd">         [5. 4. 6.]</span>
<span class="sd">         [9. 8. 7.]]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3, 4], [5, 6, 7, 8]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; seq_lengths = Tensor(np.array([4, 4]))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reverse_sequence(x, seq_lengths, seq_dim=1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[4. 3. 2. 1.]</span>
<span class="sd">         [8. 7. 6. 5.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">P</span><span class="o">.</span><span class="n">ReverseSequence</span><span class="p">(</span><span class="n">seq_dim</span><span class="o">=</span><span class="n">seq_dim</span><span class="p">,</span> <span class="n">batch_dim</span><span class="o">=</span><span class="n">batch_dim</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">seq_lengths</span><span class="p">)</span></div>


<div class="viewcode-block" id="flatten"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.flatten.html#mindspore.ops.flatten">[文档]</a><span class="k">def</span> <span class="nf">flatten</span><span class="p">(</span><span class="n">input_x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Flattens a tensor without changing its batch size on the 0-th axis.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Tensor of shape :math:`(N, \ldots)` to be flattened, where :math:`N` is batch size.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape of the output tensor is :math:`(N, X)`, where :math:`X` is</span>
<span class="sd">        the product of the remaining dimension.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        ValueError: If length of shape of `input_x` is less than 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones(shape=[1, 2, 3, 4]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.flatten(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 24)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">flatten_</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_select_type_match</span><span class="p">(</span><span class="n">scalar</span><span class="p">,</span> <span class="n">tensor_type</span><span class="p">,</span> <span class="n">scalar_name</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scalar</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">tensor_type</span> <span class="o">!=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For functional operator[select], the input[</span><span class="si">{</span><span class="n">scalar_name</span><span class="si">}</span><span class="s2">] is int, &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;then the input[</span><span class="si">{</span><span class="n">tensor_name</span><span class="si">}</span><span class="s2">] must be a Tensor of int32.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scalar</span><span class="p">,</span> <span class="nb">float</span><span class="p">)</span> <span class="ow">and</span> <span class="n">tensor_type</span> <span class="o">!=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For functional operator[select], the input[</span><span class="si">{</span><span class="n">scalar_name</span><span class="si">}</span><span class="s2">] is float, &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;then the input[</span><span class="si">{</span><span class="n">tensor_name</span><span class="si">}</span><span class="s2">] must be a Tensor of float32.&quot;</span><span class="p">)</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_select_shape_match</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">cond_shape</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">input_shape</span> <span class="o">!=</span> <span class="n">cond_shape</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For functional operator[select], the cond shape must be same as </span><span class="si">{</span><span class="n">tensor_name</span><span class="si">}</span><span class="s2"> shape.&quot;</span><span class="p">)</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_select_type</span><span class="p">(</span><span class="n">is_cond_tensor</span><span class="p">,</span> <span class="n">is_x_scalar</span><span class="p">,</span> <span class="n">is_y_scalar</span><span class="p">,</span> <span class="n">is_x_tensor</span><span class="p">,</span> <span class="n">is_y_tensor</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_cond_tensor</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For functional operator[select], the input[cond] must be a Tensor.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_x_scalar</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_y_tensor</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For functional operator[select], the input[x] is int or float, &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;then the input[y] must be a Tensor.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_y_scalar</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_x_tensor</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For functional operator[select], the input[y] is int or float, &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;then the input[x] must be a Tensor.&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="select"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.select.html#mindspore.ops.select">[文档]</a><span class="k">def</span> <span class="nf">select</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The conditional tensor determines whether the corresponding element in the output must be</span>
<span class="sd">    selected from :math:`x` (if true) or :math:`y` (if false) based on the value of each element.</span>

<span class="sd">    It can be defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        out_i = \begin{cases}</span>
<span class="sd">        x_i, &amp; \text{if } cond_i \\</span>
<span class="sd">        y_i, &amp; \text{otherwise}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        cond (Tensor[bool]): The condition tensor, decides which element is chosen.</span>
<span class="sd">          The shape is :math:`(x_1, x_2, ..., x_N, ..., x_R)`.</span>
<span class="sd">        x (Union[Tensor, int, float]): The first Tensor or number to be selected.</span>
<span class="sd">          If x is a Tensor, the shape is :math:`(x_1, x_2, ..., x_N, ..., x_R)`. If x is an int or a float,</span>
<span class="sd">          it will be cast to the type of int32 or float32, and broadcast to the same shape as y.</span>
<span class="sd">          One of x and y must be a Tensor.</span>
<span class="sd">        y (Union[Tensor, int, float]): The second Tensor or number to be selected.</span>
<span class="sd">          If y is a Tensor, The shape is :math:`(x_1, x_2, ..., x_N, ..., x_R)`. If y is an int or a float,</span>
<span class="sd">          it will be cast to the type of int32 or float32, and broadcast to the same shape as x.</span>
<span class="sd">          One of x and y must be a Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as `cond`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` or `y` is not a Tensor, int or float.</span>
<span class="sd">        ValueError: The shapes of inputs are different.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # 1) Both inputs are Tensor</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; cond = Tensor([True, False])</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([2,3], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor([1,2], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.select(cond, x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2. 2.]</span>
<span class="sd">        &gt;&gt;&gt; # 2) y is a float</span>
<span class="sd">        &gt;&gt;&gt; cond = Tensor([True, False])</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([2,3], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = 2.0</span>
<span class="sd">        &gt;&gt;&gt; output = ops.select(cond, x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2. 2.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">is_x_scalar</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">))</span>
    <span class="n">is_y_scalar</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">))</span>
    <span class="n">is_x_tensor</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span>
    <span class="n">is_y_tensor</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span>
    <span class="n">is_cond_tensor</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span>
    <span class="n">_check_select_type</span><span class="p">(</span><span class="n">is_cond_tensor</span><span class="p">,</span> <span class="n">is_x_scalar</span><span class="p">,</span> <span class="n">is_y_scalar</span><span class="p">,</span> <span class="n">is_x_tensor</span><span class="p">,</span> <span class="n">is_y_tensor</span><span class="p">)</span>
    <span class="n">input_x</span> <span class="o">=</span> <span class="n">x</span>
    <span class="n">input_y</span> <span class="o">=</span> <span class="n">y</span>
    <span class="k">if</span> <span class="n">is_x_scalar</span><span class="p">:</span>
        <span class="n">_check_select_shape_match</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">cond</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">)</span>
        <span class="n">_check_select_type_match</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">)</span>
        <span class="n">input_x</span> <span class="o">=</span> <span class="n">zeros_like_</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="n">x</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">input_x</span> <span class="o">=</span> <span class="n">cast_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">input_x</span> <span class="o">=</span> <span class="n">cast_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">is_y_scalar</span><span class="p">:</span>
        <span class="n">_check_select_shape_match</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">cond</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;x&quot;</span><span class="p">)</span>
        <span class="n">_check_select_type_match</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="s2">&quot;x&quot;</span><span class="p">)</span>
        <span class="n">input_y</span> <span class="o">=</span> <span class="n">zeros_like_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">y</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">input_y</span> <span class="o">=</span> <span class="n">cast_</span><span class="p">(</span><span class="n">input_y</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">input_y</span> <span class="o">=</span> <span class="n">cast_</span><span class="p">(</span><span class="n">input_y</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tensor_select_</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span></div>


<div class="viewcode-block" id="slice"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.slice.html#mindspore.ops.slice">[文档]</a><span class="k">def</span> <span class="nf">slice</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">begin</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Slices a tensor in the specified shape.</span>

<span class="sd">    Slice the tensor `input_x` in shape of `size` and starting at the location specified by `begin`,</span>
<span class="sd">    The slice `begin` represents the offset in each dimension of `input_x`,</span>
<span class="sd">    The slice `size` represents the size of the output tensor.</span>

<span class="sd">    Note:</span>
<span class="sd">        `begin` is zero-based and `size` is one-based.</span>

<span class="sd">    If `size[i]` is -1, all remaining elements in dimension i are included in the slice.</span>
<span class="sd">    This is equivalent to setting :math:`size[i] = input\_x.shape(i) - begin[i]`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The target tensor.</span>
<span class="sd">            The shape is :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        begin (Union[tuple, list]): The beginning of the slice. Only constant value(&gt;=0) is allowed.</span>
<span class="sd">        size (Union[tuple, list]): The size of the slice. Only constant value is allowed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is input `size`, the data type is the same as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `begin` or `size` is neither tuple nor list.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; data = Tensor(np.array([[[1, 1, 1], [2, 2, 2]],</span>
<span class="sd">        ...                         [[3, 3, 3], [4, 4, 4]],</span>
<span class="sd">        ...                         [[5, 5, 5], [6, 6, 6]]]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.slice(data, (1, 0, 0), (1, 1, 3))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[3 3 3]]]</span>
<span class="sd">        &gt;&gt;&gt; output = ops.slice(data, (1, 0, 0), (1, 1, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[3 3]]]</span>
<span class="sd">        &gt;&gt;&gt; output = ops.slice(data, (1, 0, 0), (1, 1, 1))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[3]]]</span>
<span class="sd">        &gt;&gt;&gt; output = ops.slice(data, (1, 1, 0), (1, 1, 3))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[4 4 4]]]</span>
<span class="sd">        &gt;&gt;&gt; output = ops.slice(data, (1, 0, 1), (1, 1, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[3 3]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_slice</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">begin</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span></div>


<div class="viewcode-block" id="concat"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.concat.html#mindspore.ops.concat">[文档]</a><span class="k">def</span> <span class="nf">concat</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Connect tensor in the specified axis.</span>

<span class="sd">    Connect input tensors along with the given axis.</span>

<span class="sd">    The input data is a tuple of tensors. These tensors have the same rank :math:`R`. Set the given axis as :math:`m`,</span>
<span class="sd">    and :math:`0 \le m &lt; R`. Set the number of input tensors as :math:`N`. For the :math:`i`-th tensor :math:`t_i`,</span>
<span class="sd">    it has the shape of :math:`(x_1, x_2, ..., x_{mi}, ..., x_R)`. :math:`x_{mi}` is the :math:`m`-th dimension of the</span>
<span class="sd">    :math:`t_i`. Then, the shape of the output tensor is</span>

<span class="sd">    .. math::</span>

<span class="sd">        (x_1, x_2, ..., \sum_{i=1}^Nx_{mi}, ..., x_R)</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (tuple, list): A tuple or a list of input tensors.</span>
<span class="sd">            Suppose there are two tensors in this tuple or list, namely t1 and t2.</span>
<span class="sd">            To perform `concat` in the axis 0 direction, except for the :math:`0`-th axis,</span>
<span class="sd">            all other dimensions should be equal, that is,</span>
<span class="sd">            :math:`t1.shape[1] = t2.shape[1], t1.shape[2] = t2.shape[2], ..., t1.shape[R-1] = t2.shape[R-1]`,</span>
<span class="sd">        axis (int): The specified axis, whose value is in range :math:`[-R, R)`. Default: 0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is :math:`(x_1, x_2, ..., \sum_{i=1}^Nx_{mi}, ..., x_R)`.</span>
<span class="sd">            The data type is the same with `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>
<span class="sd">        ValueError: If `input_x` have different dimension of tensor.</span>
<span class="sd">        ValueError: If `axis` not in range :math:`[-R, R)`.</span>
<span class="sd">        RuntimeError: If tensor&#39;s shape in `input_x` except for `axis` are different.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x1 = Tensor(np.array([[0, 1], [2, 1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; input_x2 = Tensor(np.array([[0, 1], [2, 1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.concat((input_x1, input_x2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 1.]</span>
<span class="sd">         [2. 1.]</span>
<span class="sd">         [0. 1.]</span>
<span class="sd">         [2. 1.]]</span>
<span class="sd">        &gt;&gt;&gt; output = ops.concat((input_x1, input_x2), 1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 1. 0. 1.]</span>
<span class="sd">         [2. 1. 2. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_concat</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">)(</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_concat</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="stack"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.stack.html#mindspore.ops.stack">[文档]</a><span class="k">def</span> <span class="nf">stack</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Stacks a list of tensors in specified axis.</span>

<span class="sd">    Stacks the list of input tensors with the same rank `R`, output is a tensor of rank `(R+1)`.</span>

<span class="sd">    Given input tensors of shape :math:`(x_1, x_2, ..., x_R)`. Set the number of input tensors as `N`.</span>
<span class="sd">    If :math:`0 \le axis`, the shape of the output tensor is</span>
<span class="sd">    :math:`(x_1, x_2, ..., x_{axis}, N, x_{axis+1}, ..., x_R)`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Union[tuple, list]): A Tuple or list of Tensor objects with the same shape and type.</span>
<span class="sd">        axis (int): Dimension to stack. Default: 0.</span>
<span class="sd">            Negative values wrap around. The range is [-(R+1), R+1).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor. A stacked Tensor with the same type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the data types of elements in `input_x` are not the same.</span>
<span class="sd">        ValueError: If the length of `input_x` is not greater than 1;</span>
<span class="sd">                    or if axis is out of the range [-(R+1), R+1);</span>
<span class="sd">                    or if the shapes of elements in input_x are not the same.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x1 = Tensor(np.array([0, 1]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; input_x2 = Tensor(np.array([2, 3]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.stack((input_x1, input_x2), 0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 1.]</span>
<span class="sd">         [2. 3.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_stack</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Stack</span><span class="p">)(</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_stack</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="unstack"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.unstack.html#mindspore.ops.unstack">[文档]</a><span class="k">def</span> <span class="nf">unstack</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Unstacks tensor in specified axis.</span>

<span class="sd">    Unstacks a tensor of rank `R` along axis dimension, output tensors will have rank `(R-1)`.</span>

<span class="sd">    Given a tensor of shape :math:`(x_1, x_2, ..., x_R)`. If :math:`0 \le axis`,</span>
<span class="sd">    the shape of tensor in output is :math:`(x_1, x_2, ..., x_{axis}, x_{axis+2}, ..., x_R)`.</span>

<span class="sd">    This is the opposite of pack.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The shape is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">            A tensor to be unstacked and the rank of the tensor must be greater than 0.</span>
<span class="sd">        axis (int): Dimension along which to unpack. Default: 0.</span>
<span class="sd">            Negative values wrap around. The range is [-R, R).</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tuple of tensors, the shape of each objects is the same.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If axis is out of the range [-len(input_x.shape), len(input_x.shape)).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1, 1, 1, 1], [2, 2, 2, 2]]))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.unstack(input_x, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[4], dtype=Int64, value= [1, 1, 1, 1]), Tensor(shape=[4], dtype=Int64, value= [2, 2, 2, 2]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_unstack</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Unstack</span><span class="p">)(</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_unstack</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="expand_dims"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.expand_dims.html#mindspore.ops.expand_dims">[文档]</a><span class="k">def</span> <span class="nf">expand_dims</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adds an additional dimension to `input_x` at the given axis.</span>

<span class="sd">    Note:</span>
<span class="sd">        If the specified axis is a negative number, the index is counted</span>
<span class="sd">        backward from the end and starts at 1.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">        axis (int): Specifies the dimension index at which to expand</span>
<span class="sd">            the shape of `input_x`. The value of axis must be in the range</span>
<span class="sd">            `[-input_x.ndim-1, input_x.ndim]`. Only constant value is allowed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape of tensor is :math:`(1, x_1, x_2, ..., x_R)` if the</span>
<span class="sd">        value of `axis` is 0. It has the same data type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>
<span class="sd">        ValueError: If `axis` is not in the valid range :math:`[-a.ndim-1, a.ndim]`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor(np.array([[2, 2], [2, 2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.expand_dims(input_tensor, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[2. 2.]</span>
<span class="sd">          [2. 2.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">expand_dims_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="squeeze"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.squeeze.html#mindspore.ops.squeeze">[文档]</a><span class="k">def</span> <span class="nf">squeeze</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">()):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return the Tensor after deleting the dimension of size 1 in the specified `axis`.</span>

<span class="sd">    If :math:`axis=()`, it will remove all the dimensions of size 1.</span>
<span class="sd">    If `axis` is specified, it will remove the dimensions of size 1 in the given `axis`.</span>
<span class="sd">    For example, if the dimension is not specified :math:`axis=()`, input shape is (A, 1, B, C, 1, D),</span>
<span class="sd">    then the shape of the output Tensor is (A, B, C, D). If the dimension is specified, the squeeze operation</span>
<span class="sd">    is only performed in the specified dimension. If input shape is (A, 1, B), input Tensor will not be</span>
<span class="sd">    changed when :math:`axis=0` , but when :math:`axis=1` , the shape of the input Tensor will be changed to (A, B).</span>

<span class="sd">    Note:</span>
<span class="sd">        - Please note that in dynamic graph mode, the output Tensor will share data with the input Tensor,</span>
<span class="sd">          and there is no Tensor data copy process.</span>
<span class="sd">        - The dimension index starts at 0 and must be in the range `[-input.ndim, input.ndim]`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">        axis (Union[int, tuple(int)]): Specifies the dimension indexes of shape to be removed, which will remove</span>
<span class="sd">            all the dimensions of size 1 in the given axis parameter. If specified, it must be int32 or int64.</span>
<span class="sd">            Default: (), an empty tuple.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape of tensor is :math:`(x_1, x_2, ..., x_S)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a tensor.</span>
<span class="sd">        TypeError: If `axis` is neither an int nor tuple.</span>
<span class="sd">        TypeError: If `axis` is a tuple whose elements are not all int.</span>
<span class="sd">        ValueError: If the corresponding dimension of the specified axis isn&#39;t equal to 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones(shape=[3, 2, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.squeeze(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 1.]</span>
<span class="sd">         [1. 1.]</span>
<span class="sd">         [1. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">squeeze_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Squeeze</span><span class="p">)(</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">squeeze_</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="transpose"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.transpose.html#mindspore.ops.transpose">[文档]</a><span class="k">def</span> <span class="nf">transpose</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_perm</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Permutes the dimensions of the input tensor according to input permutation.</span>

<span class="sd">    For a 1-D array this has no effect, as a transposed vector is simply the same vector.</span>
<span class="sd">    To convert a 1-D array into a 2D column vector please refer the class: mindspore.ops.ExpandDims.</span>
<span class="sd">    For a 2-D array, this is a standard matrix transpose. For an n-D array, if axes are given,</span>
<span class="sd">    their order indicates how the axes are permuted (see Examples).</span>
<span class="sd">    If axes are not provided and a.shape = (i[0], i[1], ... i[n-2], i[n-1]),</span>
<span class="sd">    then a.transpose().shape = (i[n-1], i[n-2], ... i[1], i[0]).</span>

<span class="sd">    Note:</span>
<span class="sd">        On GPU and CPU, if the value of `input_perm` is negative, its actual value is `input_perm[i] + rank(input_x)`.</span>
<span class="sd">        Negative value of `input_perm` is not supported on Ascend.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">        input_perm (tuple[int]): The permutation to be converted. The elements in `input_perm` are composed of</span>
<span class="sd">            the indexes of each dimension of `input_x`. The length of `input_perm` and the shape of `input_x` must be</span>
<span class="sd">            the same. Only constant value is allowed. Must be in the range [-rank(input_x), rank(input_x)).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the type of output tensor is the same as `input_x` and the shape of output tensor is decided by the</span>
<span class="sd">        shape of `input_x` and the value of `input_perm`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_perm` is not a tuple.</span>
<span class="sd">        ValueError: If length of shape of `input_x` is not equal to length of shape of `input_perm`.</span>
<span class="sd">        ValueError: If the same element exists in `input_perm`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_perm = (0, 2, 1)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.transpose(input_x, input_perm)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[ 1.  4.]</span>
<span class="sd">          [ 2.  5.]</span>
<span class="sd">          [ 3.  6.]]</span>
<span class="sd">         [[ 7. 10.]</span>
<span class="sd">          [ 8. 11.]</span>
<span class="sd">          [ 9. 12.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">transpose_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_perm</span><span class="p">)</span></div>


<div class="viewcode-block" id="scatter_mul"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.scatter_mul.html#mindspore.ops.scatter_mul">[文档]</a><span class="k">def</span> <span class="nf">scatter_mul</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates the value of the input tensor through the multiply operation.</span>

<span class="sd">    Using given values to update tensor value through the mul operation, along with the input indices.</span>
<span class="sd">    This operation outputs the `input_x` after the update is done, which makes it convenient to use the updated value.</span>

<span class="sd">    for each `i, ..., j` in `indices.shape`:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{input_x}[\text{indices}[i, ..., j], :] \mathrel{*}= \text{updates}[i, ..., j, :]</span>

<span class="sd">    Inputs of `input_x` and `updates` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Parameter): The target tensor, with data type of Parameter.</span>
<span class="sd">            The shape is :math:`(N,*)` where :math:`*` means,any number of additional dimensions.</span>
<span class="sd">        indices (Tensor): The index to do min operation whose data type must be mindspore.int32.</span>
<span class="sd">        updates (Tensor): The tensor doing the min operation with `input_x`,</span>
<span class="sd">            the data type is same as `input_x`, the shape is `indices.shape + input_x.shape[1:]`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the updated `input_x`, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If `indices` is not an int32 or int64.</span>
<span class="sd">        ValueError: If the shape of `updates` is not equal to `indices.shape + input_x.shape[1:]`.</span>
<span class="sd">        RuntimeError: If the data type of `input_x` and `updates` conversion of Parameter</span>
<span class="sd">                      is required when data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[1.0, 1.0, 1.0], [2.0, 2.0, 2.0]]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[2.0, 2.0, 2.0], [2.0, 2.0, 2.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_mul(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[2. 2. 2.]</span>
<span class="sd">         [4. 4. 4.]]</span>
<span class="sd">        &gt;&gt;&gt; # for input_x will be updated after the operation is completed. input_x need to be re-initialized.</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[1.0, 1.0, 1.0], [2.0, 2.0, 2.0]]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # for indices = [[0, 1], [1, 1]]</span>
<span class="sd">        &gt;&gt;&gt; # step 1: [0, 1]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[0] = [1.0, 1.0, 1.0] * [1.0, 1.0, 1.0] = [1.0, 1.0, 1.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [2.0, 2.0, 2.0] * [3.0, 3.0, 3.0] = [6.0, 6.0, 6.0]</span>
<span class="sd">        &gt;&gt;&gt; # step 2: [1, 1]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [6.0, 6.0, 6.0] * [7.0, 7.0, 7.0] = [42.0, 42.0, 42.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [42.0, 42.0, 42.0] * [9.0, 9.0, 9.0] = [378.0, 378.0, 378.0]</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 1], [1, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1.0, 1.0, 1.0], [3.0, 3.0, 3.0]],</span>
<span class="sd">        ...                            [[7.0, 7.0, 7.0], [9.0, 9.0, 9.0]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_mul(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[  1.   1.   1.]</span>
<span class="sd">         [378. 378. 378.]]</span>
<span class="sd">        &gt;&gt;&gt; # for input_x will be updated after the operation is completed. input_x need to be re-initialized.</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[1.0, 1.0, 1.0], [2.0, 2.0, 2.0]]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # for indices = [[1, 0], [1, 1]]</span>
<span class="sd">        &gt;&gt;&gt; # step 1: [1, 0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[0] = [1.0, 1.0, 1.0] * [3.0, 3.0, 3.0] = [3.0, 3.0, 3.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [2.0, 2.0, 2.0] * [1.0, 1.0, 1.0] = [2.0, 2.0, 2.0]</span>
<span class="sd">        &gt;&gt;&gt; # step 2: [1, 1]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [2.0, 2.0, 2.0] * [7.0, 7.0, 7.0] = [14.0, 14.0, 14.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [14.0, 14.0, 14.0] * [9.0, 9.0, 9.0] = [126.0, 126.0, 126.0]</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[1, 0], [1, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1.0, 1.0, 1.0], [3.0, 3.0, 3.0]],</span>
<span class="sd">        ...                            [[7.0, 7.0, 7.0], [9.0, 9.0, 9.0]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_mul(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[  3.   3.   3.]</span>
<span class="sd">         [126. 126. 126.]]</span>
<span class="sd">        &gt;&gt;&gt; # for input_x will be updated after the operation is completed. input_x need to be re-initialized.</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[1.0, 1.0, 1.0], [2.0, 2.0, 2.0]]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # for indices = [[0, 1], [0, 1]]</span>
<span class="sd">        &gt;&gt;&gt; # step 1: [0, 1]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[0] = [1.0, 1.0, 1.0] * [1.0, 1.0, 1.0] = [1.0, 1.0, 1.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [2.0, 2.0, 2.0] * [3.0, 3.0, 3.0] = [6.0, 6.0, 6.0]</span>
<span class="sd">        &gt;&gt;&gt; # step 2: [0, 1]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[0] = [1.0, 1.0, 1.0] * [7.0, 7.0, 7.0] = [7.0, 7.0, 7.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [6.0, 6.0, 6.0] * [9.0, 9.0, 9.0] = [54.0, 54.0, 54.0]</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 1], [0, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1.0, 1.0, 1.0], [3.0, 3.0, 3.0]],</span>
<span class="sd">        ...                            [[7.0, 7.0, 7.0], [9.0, 9.0, 9.0]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_mul(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 7.  7.  7.]</span>
<span class="sd">         [54. 54. 54.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">scatter_mul_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>


<div class="viewcode-block" id="scatter_max"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.scatter_max.html#mindspore.ops.scatter_max">[文档]</a><span class="k">def</span> <span class="nf">scatter_max</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Using given values to update tensor value through the max operation, along with the input indices.</span>
<span class="sd">    This operation outputs the `input_x` after the update is done, which makes it convenient to use the updated value.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Parameter): The target tensor, with data type of Parameter.</span>
<span class="sd">            The shape is :math:`(N,*)` where :math:`*` means,any number of additional dimensions.</span>
<span class="sd">        indices (Tensor): The index to do max operation whose data type must be mindspore.int32.</span>
<span class="sd">        updates (Tensor): The tensor doing the max operation with `input_x`,</span>
<span class="sd">            the data type is same as `input_x`, the shape is `indices.shape + x.shape[1:]`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the updated `input_x`, the type and shape same as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `indices` is not an int32 or int64.</span>
<span class="sd">        ValueError: If the shape of `updates` is not equal to `indices.shape + input_x.shape[1:]`.</span>
<span class="sd">        RuntimeError: If the data type of `input_x` and `updates` conversion of Parameter</span>
<span class="sd">                      is required when data type conversion of Parameter is not supported.</span>
<span class="sd">        RuntimeError: On the Ascend platform, the input data dimension of `input_x` , `indices`</span>
<span class="sd">                      and `updates` is greater than 8 dimensions.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]), mindspore.float32), name=&quot;input_x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 0], [1, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.ones([2, 2, 3]) * 88, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_max(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[88. 88. 88.]</span>
<span class="sd">         [88. 88. 88.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">scatter_max_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>


<div class="viewcode-block" id="scatter_add"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.scatter_add.html#mindspore.ops.scatter_add">[文档]</a><span class="k">def</span> <span class="nf">scatter_add</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Using given values to update tensor value through the add operation, along with the input indices.</span>
<span class="sd">    This operation outputs the `input_x` after the update is done, which makes it convenient to use the updated value.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Parameter): The target tensor, with data type of Parameter.</span>
<span class="sd">            The shape is :math:`(N,*)` where :math:`*` means,any number of additional dimensions.</span>
<span class="sd">        indices (Tensor): The index to do add operation whose data type must be int32 or int64.</span>
<span class="sd">        updates (Tensor): The tensor doing the add operation with `input_x`,</span>
<span class="sd">            the data type is same as `input_x`, the shape is `indices.shape + x.shape[1:]`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the updated `input_x`, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `indices` is not an int32 or int64.</span>
<span class="sd">        ValueError: If the shape of `updates` is not equal to `indices.shape + input_x.shape[1:]`.</span>
<span class="sd">        RuntimeError: If the data type of `input_x` and `updates` conversion of Parameter</span>
<span class="sd">            is required when data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 1], [1, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1.0, 1.0, 1.0], [3.0, 3.0, 3.0]],</span>
<span class="sd">        ...                            [[7.0, 7.0, 7.0], [9.0, 9.0, 9.0]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_add(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 1.  1.  1.]</span>
<span class="sd">         [19. 19. 19.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">scatter_add_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>


<div class="viewcode-block" id="scatter_min"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.scatter_min.html#mindspore.ops.scatter_min">[文档]</a><span class="k">def</span> <span class="nf">scatter_min</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates the value of the input tensor through the minimum operation.</span>

<span class="sd">    Using given values to update tensor value through the min operation, along with the input indices.</span>
<span class="sd">    This operation outputs the `input_x` after the update is done, which makes it convenient to use the updated value.</span>

<span class="sd">    for each :math:`i, ..., j` in `indices.shape`:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{input_x}[\text{indices}[i, ..., j], :]</span>
<span class="sd">        = min(\text{input_x}[\text{indices}[i, ..., j], :], \text{updates}[i, ..., j, :])</span>

<span class="sd">    Inputs of `input_x` and `updates` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Parameter): The target tensor, with data type of Parameter.</span>
<span class="sd">            The shape is :math:`(N,*)` where :math:`*` means,any number of additional dimensions.</span>
<span class="sd">        indices (Tensor): The index to do min operation whose data type must be mindspore.int32 or mindspore.int64.</span>
<span class="sd">        updates (Tensor): The tensor doing the min operation with `input_x`,</span>
<span class="sd">            the data type is same as `input_x`, the shape is `indices.shape + input_x.shape[1:]`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the updated `input_x`, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `indices` is not an int32 or an int64.</span>
<span class="sd">        ValueError: If the shape of `updates` is not equal to `indices.shape + input_x.shape[1:]`.</span>
<span class="sd">        RuntimeError: If the data type of `input_x` and `updates` conversion of Parameter</span>
<span class="sd">                      is required when data type conversion of Parameter is not supported.</span>
<span class="sd">        RuntimeError: On the Ascend platform, the input data dimension of `input_x` , `indices`</span>
<span class="sd">                      and `updates` is greater than 8 dimensions.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.zeros((2, 3)), mindspore.float32), name=&quot;input_x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([1, 0]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; update = Tensor(np.arange(6).reshape((2, 3)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_min(input_x, indices, update)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 0. 0.]</span>
<span class="sd">         [0. 0. 0.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">scatter_min_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>


<div class="viewcode-block" id="scatter_div"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.scatter_div.html#mindspore.ops.scatter_div">[文档]</a><span class="k">def</span> <span class="nf">scatter_div</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates the value of the input tensor through the divide operation.</span>

<span class="sd">    Using given values to update tensor value through the div operation, along with the input indices.</span>
<span class="sd">    This operation outputs the `input_x` after the update is done, which makes it convenient to use the updated value.</span>

<span class="sd">    for each :math:`i, ..., j` in `indices.shape`:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{input_x}[\text{indices}[i, ..., j], :] \mathrel{/}= \text{updates}[i, ..., j, :]</span>

<span class="sd">    Inputs of `input_x` and `updates` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Parameter): The target tensor, with data type of Parameter.</span>
<span class="sd">          The shape is :math:`(N,*)` where :math:`*` means,any number of additional dimensions.</span>
<span class="sd">        indices (Tensor): The index to do divide operation whose data type must be mindspore.int32 or</span>
<span class="sd">          mindspore.int64.</span>
<span class="sd">        updates (Tensor): The tensor doing the divide operation with `input_x`,</span>
<span class="sd">          the data type is same as `input_x`, the shape is `indices.shape + input_x.shape[1:]`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the updated `input_x`, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `indices` is not an int32 or an int64.</span>
<span class="sd">        ValueError: If the shape of `updates` is not equal to `indices.shape + input_x.shape[1:]`.</span>
<span class="sd">        RuntimeError: If the data type of `input_x` and `updates` conversion of Parameter</span>
<span class="sd">                      is required when data type conversion of Parameter is not supported.</span>
<span class="sd">        RuntimeError: On the Ascend platform, the input data dimension of `input_x` , `indices`</span>
<span class="sd">                      and `updates` is greater than 8 dimensions.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[6.0, 6.0, 6.0], [2.0, 2.0, 2.0]]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[2.0, 2.0, 2.0], [2.0, 2.0, 2.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_div(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[3. 3. 3.]</span>
<span class="sd">         [1. 1. 1.]]</span>
<span class="sd">        &gt;&gt;&gt; # for input_x will be updated after the operation is completed. input_x need to be re-initialized.</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[105.0, 105.0, 105.0],</span>
<span class="sd">        ...                                      [315.0, 315.0, 315.0]]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # for indices = [[0, 1], [1, 1]]</span>
<span class="sd">        &gt;&gt;&gt; # step 1: [0, 1]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[0] = [105.0, 105.0, 105.0] / [1.0, 1.0, 1.0] = [105.0, 105.0, 105.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [315.0, 315.0, 315.0] / [3.0, 3.0, 3.0] = [105.0, 105.0, 105.0]</span>
<span class="sd">        &gt;&gt;&gt; # step 2: [1, 1]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [105.0, 105.0, 105.0] / [5.0, 5.0, 5.0] = [21.0, 21.0, 21.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [21.0, 21.0, 21.0] / [7.0, 7.0, 7.0] = [3.0, 3.0, 3.0]</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 1], [1, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1.0, 1.0, 1.0], [3.0, 3.0, 3.0]],</span>
<span class="sd">        ...                            [[5.0, 5.0, 5.0], [7.0, 7.0, 7.0]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_div(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[105. 105. 105.]</span>
<span class="sd">         [  3.   3.   3.]]</span>
<span class="sd">        &gt;&gt;&gt; # for input_x will be updated after the operation is completed. input_x need to be re-initialized.</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[105.0, 105.0, 105.0],</span>
<span class="sd">        ...                                      [315.0, 315.0, 315.0]]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # for indices = [[1, 0], [1, 1]]</span>
<span class="sd">        &gt;&gt;&gt; # step 1: [1, 0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[0] = [105.0, 105.0, 105.0] / [3.0, 3.0, 3.0] = [35.0, 35.0, 35.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [315.0, 315.0, 315.0] / [1.0, 1.0, 1.0] = [315.0, 315.0, 315.0]</span>
<span class="sd">        &gt;&gt;&gt; # step 2: [1, 1]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [315.0, 315.0, 315.0] / [5.0, 5.0, 5.0] = [63.0 63.0 63.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [63.0 63.0 63.0] / [7.0, 7.0, 7.0] = [9.0, 9.0, 9.0]</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[1, 0], [1, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1.0, 1.0, 1.0], [3.0, 3.0, 3.0]],</span>
<span class="sd">        ...                            [[5.0, 5.0, 5.0], [7.0, 7.0, 7.0]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_div(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[35. 35. 35.]</span>
<span class="sd">         [ 9.  9.  9.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">scatter_div_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>


<div class="viewcode-block" id="scatter_nd"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.scatter_nd.html#mindspore.ops.scatter_nd">[文档]</a><span class="k">def</span> <span class="nf">scatter_nd</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Scatters a tensor into a new tensor depending on the specified indices.</span>

<span class="sd">    Creates an empty tensor with the given `shape`, and set values by scattering the update tensor</span>
<span class="sd">    depending on indices. The empty tensor has rank :math:`P` and `indices` has rank :math:`Q`.</span>

<span class="sd">    The `shape` is :math:`(s_0, s_1, ..., s_{P-1})`, where :math:`P \ge 1`.</span>

<span class="sd">    `indices` has shape :math:`(i_0, i_1, ..., i_{Q-2}, N)`, where :math:`Q \ge 2` and :math:`N \le P`.</span>

<span class="sd">    The last dimension of `indices` (with length :math:`N` ) indicates slices along the :math:`N` th dimension of the</span>
<span class="sd">    empty tensor.</span>

<span class="sd">    `updates` is a tensor of rank :math:`Q-1+P-N`, and</span>
<span class="sd">    its shape is :math:`(i_0, i_1, ..., i_{Q-2}, s_N, s_{N+1}, ..., s_{P-1})`.</span>

<span class="sd">    If `indices` contains duplicates, the duplicate `updates` are summed.</span>

<span class="sd">    The following figure shows the calculation process of inserting two new value matrices into the first dimension</span>
<span class="sd">    with rank-3:</span>

<span class="sd">    .. image:: ScatterNd.png</span>

<span class="sd">    Args:</span>
<span class="sd">        indices (Tensor): Define the index of scattering in the new tensor with int32 or int64 data type.</span>
<span class="sd">            The rank of `indices` must be at least 2 and `indices.shape[-1] &lt;= len(shape)`.</span>
<span class="sd">        updates (Tensor): Define the source Tensor to be updated.</span>
<span class="sd">            It has shape `indices.shape[:-1] + shape[indices.shape[-1]:]`.</span>
<span class="sd">        shape (tuple[int]): Define the shape of the output tensor, has the same data type as indices.</span>
<span class="sd">            `shape` can not be empty, and the elements in `shape` must be greater than or equal to 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the new tensor, has the same type as `update` and the same shape as `shape`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `shape` is not a tuple.</span>
<span class="sd">        ValueError: If any element of `shape` is less than 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0], [2]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1, 1, 1, 1], [2, 2, 2, 2],</span>
<span class="sd">        ...                             [3, 3, 3, 3], [4, 4, 4, 4]],</span>
<span class="sd">        ...                            [[1, 1, 1, 1], [2, 2, 2, 2],</span>
<span class="sd">        ...                             [3, 3, 3, 3], [4, 4, 4, 4]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; shape = (4, 4, 4)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_nd(indices, updates, shape)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1. 1. 1. 1.]</span>
<span class="sd">          [2. 2. 2. 2.]</span>
<span class="sd">          [3. 3. 3. 3.]</span>
<span class="sd">          [4. 4. 4. 4.]]</span>
<span class="sd">         [[0. 0. 0. 0.]</span>
<span class="sd">          [0. 0. 0. 0.]</span>
<span class="sd">          [0. 0. 0. 0.]</span>
<span class="sd">          [0. 0. 0. 0.]]</span>
<span class="sd">         [[1. 1. 1. 1.]</span>
<span class="sd">          [2. 2. 2. 2.]</span>
<span class="sd">          [3. 3. 3. 3.]</span>
<span class="sd">          [4. 4. 4. 4.]]</span>
<span class="sd">         [[0. 0. 0. 0.]</span>
<span class="sd">          [0. 0. 0. 0.]</span>
<span class="sd">          [0. 0. 0. 0.]</span>
<span class="sd">          [0. 0. 0. 0.]]]</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 1], [1, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([3.2, 1.1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; shape = (3, 3)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_nd(indices, updates, shape)</span>
<span class="sd">        &gt;&gt;&gt; # In order to facilitate understanding, explain the operator pseudo-operation process step by step:</span>
<span class="sd">        &gt;&gt;&gt; # Step 1: Generate an empty Tensor of the specified shape according to the shape</span>
<span class="sd">        &gt;&gt;&gt; # [</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 0. 0.]</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 0. 0.]</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 0. 0.]</span>
<span class="sd">        &gt;&gt;&gt; # ]</span>
<span class="sd">        &gt;&gt;&gt; # Step 2: Modify the data at the specified location according to the indicators</span>
<span class="sd">        &gt;&gt;&gt; # 0th row of indices is [0, 1], 0th row of updates is 3.2.</span>
<span class="sd">        &gt;&gt;&gt; # means that the empty tensor in the 0th row and 1st col set to 3.2</span>
<span class="sd">        &gt;&gt;&gt; # [</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 3.2. 0.]</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 0.   0.]</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 0.   0.]</span>
<span class="sd">        &gt;&gt;&gt; # ]</span>
<span class="sd">        &gt;&gt;&gt; # 1th row of indices is [1, 1], 1th row of updates is 1.1.</span>
<span class="sd">        &gt;&gt;&gt; # means that the empty tensor in the 1th row and 1st col set to 1.1</span>
<span class="sd">        &gt;&gt;&gt; # [</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 3.2. 0.]</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 1.1  0.]</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 0.   0.]</span>
<span class="sd">        &gt;&gt;&gt; # ]</span>
<span class="sd">        &gt;&gt;&gt; # The final result is as follows:</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 3.2 0.]</span>
<span class="sd">         [0. 1.1 0.]</span>
<span class="sd">         [0. 0.  0.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">scatter_nd_</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span></div>


<div class="viewcode-block" id="scatter_update"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.scatter_update.html#mindspore.ops.scatter_update">[文档]</a><span class="k">def</span> <span class="nf">scatter_update</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates tensor values by using input indices and value.</span>

<span class="sd">    Using given values to update tensor value, along with the input indices.</span>

<span class="sd">    for each `i, ..., j` in `indices.shape`:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{input_x}[\text{indices}[i, ..., j], :] = \text{updates}[i, ..., j, :]</span>

<span class="sd">    Inputs of `input_x` and `updates` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Parameter): The target tensor, with data type of Parameter.</span>
<span class="sd">            The shape is :math:`(N,*)` where :math:`*` means,any number of additional dimensions.</span>
<span class="sd">        indices (Tensor): The index of input tensor. With int32 or int64 data type.</span>
<span class="sd">            If there are duplicates in indices, the order for updating is undefined.</span>
<span class="sd">        updates (Tensor): The tensor to update the input tensor, has the same type as input,</span>
<span class="sd">            and updates.shape = indices.shape + input_x.shape[1:].</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `indices` is not an int32 or an int64.</span>
<span class="sd">        ValueError: If the shape of `updates` is not equal to `indices.shape + input_x.shape[1:]`.</span>
<span class="sd">        RuntimeError: If the data type of `input_x` and `updates` conversion of Parameter</span>
<span class="sd">                      is required when data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; np_x = np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]])</span>
<span class="sd">        &gt;&gt;&gt; input_x = mindspore.Parameter(Tensor(np_x, mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; np_updates = np.array([[2.0, 1.2, 1.0], [3.0, 1.2, 1.0]])</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np_updates, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_update(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[2. 1.2  1.]</span>
<span class="sd">         [3. 1.2  1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">scatter_update_inner</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ScatterUpdate</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">scatter_update_inner</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>


<div class="viewcode-block" id="scatter_nd_add"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.scatter_nd_add.html#mindspore.ops.scatter_nd_add">[文档]</a><span class="k">def</span> <span class="nf">scatter_nd_add</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies sparse addition to individual values or slices in a tensor.</span>

<span class="sd">    Using given values to update tensor value through the add operation, along with the input indices.</span>
<span class="sd">    This operation outputs the `input_x` after the update is done, which makes it convenient to use the updated value.</span>

<span class="sd">    `input_x` has rank P and `indices` has rank Q where `Q &gt;= 2`.</span>

<span class="sd">    `indices` has shape :math:`(i_0, i_1, ..., i_{Q-2}, N)` where `N &lt;= P`.</span>

<span class="sd">    The last dimension of `indices` (with length `N` ) indicates slices along the `N` th dimension of `input_x`.</span>

<span class="sd">    `updates` is a tensor of rank `Q-1+P-N`. Its shape is:</span>
<span class="sd">    :math:`(i_0, i_1, ..., i_{Q-2}, x\_shape_N, ..., x\_shape_{P-1})`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Parameter): The target tensor, with data type of Parameter.</span>
<span class="sd">            The shape is :math:`(N,*)` where :math:`*` means,any number of additional dimensions.</span>
<span class="sd">        indices (Tensor): The index to do min operation whose data type must be mindspore.int32 or mindspore.int64.</span>
<span class="sd">            The rank of indices must be at least 2 and `indices.shape[-1] &lt;= len(shape)`.</span>
<span class="sd">        updates (Tensor): The tensor doing the addition operation with `input_x`,</span>
<span class="sd">            the data type is same as `input_x`, the shape is `indices.shape[:-1] + x.shape[indices.shape[-1]:]`.</span>
<span class="sd">        use_locking (bool): Whether to protect the assignment by a lock. Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the updated `input_x`, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the dtype of `use_locking` is not bool.</span>
<span class="sd">        TypeError: If the dtype of `indices` is not int32 or int64.</span>
<span class="sd">        TypeError: If dtype of `input_x` and `updates` are not the same.</span>
<span class="sd">        ValueError: If the shape of `updates` is not equal to `indices.shape[:-1] + x.shape[indices.shape[-1]:]`.</span>
<span class="sd">        RuntimeError: If the data type of `input_x` and `updates` conversion of Parameter</span>
<span class="sd">                      is required when data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([1, 2, 3, 4, 5, 6, 7, 8]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[2], [4], [1], [7]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([6, 7, 8, 9]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_nd_add(input_x, indices, updates, False)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1. 10.  9.  4. 12.  6.  7. 17.]</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.zeros((4, 4, 4)), mindspore.int32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0], [2]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]],</span>
<span class="sd">        ...                            [[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_nd_add(input_x, indices, updates, False)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1 1 1 1]</span>
<span class="sd">          [2 2 2 2]</span>
<span class="sd">          [3 3 3 3]</span>
<span class="sd">          [4 4 4 4]]</span>
<span class="sd">         [[0 0 0 0]</span>
<span class="sd">          [0 0 0 0]</span>
<span class="sd">          [0 0 0 0]</span>
<span class="sd">          [0 0 0 0]]</span>
<span class="sd">         [[5 5 5 5]</span>
<span class="sd">          [6 6 6 6]</span>
<span class="sd">          [7 7 7 7]</span>
<span class="sd">          [8 8 8 8]]</span>
<span class="sd">         [[0 0 0 0]</span>
<span class="sd">          [0 0 0 0]</span>
<span class="sd">          [0 0 0 0]</span>
<span class="sd">          [0 0 0 0]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">scatter_nd_add_inner</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ScatterNdAdd</span><span class="p">)(</span><span class="n">use_locking</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">scatter_nd_add_inner</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>


<div class="viewcode-block" id="scatter_nd_sub"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.scatter_nd_sub.html#mindspore.ops.scatter_nd_sub">[文档]</a><span class="k">def</span> <span class="nf">scatter_nd_sub</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies sparse subtraction to individual values or slices in a tensor.</span>

<span class="sd">    Using given values to update tensor value through the subtraction operation, along with the input indices.</span>
<span class="sd">    This operation outputs the `input_x` after the update is done, which makes it convenient to use the updated value.</span>

<span class="sd">    `input_x` has rank P and `indices` has rank Q where `Q &gt;= 2`.</span>

<span class="sd">    `indices` has shape :math:`(i_0, i_1, ..., i_{Q-2}, N)` where `N &lt;= P`.</span>

<span class="sd">    The last dimension of `indices` (with length `N` ) indicates slices along the `N` th dimension of `input_x`.</span>

<span class="sd">    `updates` is a tensor of rank `Q-1+P-N`. Its shape is:</span>
<span class="sd">    :math:`(i_0, i_1, ..., i_{Q-2}, x\_shape_N, ..., x\_shape_{P-1})`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Parameter): The target tensor, with data type of Parameter.</span>
<span class="sd">            The shape is :math:`(N,*)` where :math:`*` means,any number of additional dimensions.</span>
<span class="sd">        indices (Tensor): The index of input tensor, with int32 or int64 data type.</span>
<span class="sd">            The rank of indices must be at least 2 and `indices.shape[-1] &lt;= len(shape)`.</span>
<span class="sd">        updates (Tensor): The tensor doing the subtraction operation with `input_x`, has the same type as input.</span>
<span class="sd">            The shape is `indices.shape[:-1] + x.shape[indices.shape[-1]:]`.</span>
<span class="sd">        use_locking (bool): Whether to protect the assignment by a lock. Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the dtype of `use_locking` is not bool.</span>
<span class="sd">        TypeError: If the dtype of `indices` is not int32 or int64.</span>
<span class="sd">        TypeError: If dtype of `input_x` and `updates` are not the same.</span>
<span class="sd">        ValueError: If the shape of `updates` is not equal to `indices.shape[:-1] + x.shape[indices.shape[-1]:]`.</span>
<span class="sd">        RuntimeError: If the data type of `input_x` and `updates` conversion of Parameter</span>
<span class="sd">                      is required when data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([1, 2, 3, 4, 5, 6, 7, 8]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[2], [4], [1], [7]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([6, 7, 8, 9]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_nd_sub(input_x, indices, updates, False)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1. -6. -3.  4. -2.  6.  7. -1.]</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.zeros((4, 4, 4)), mindspore.int32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0], [2]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]],</span>
<span class="sd">        ...                            [[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_nd_sub(input_x, indices, updates, False)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[-1 -1 -1 -1]</span>
<span class="sd">          [-2 -2 -2 -2]</span>
<span class="sd">          [-3 -3 -3 -3]</span>
<span class="sd">          [-4 -4 -4 -4]]</span>
<span class="sd">         [[ 0  0  0  0]</span>
<span class="sd">          [ 0  0  0  0]</span>
<span class="sd">          [ 0  0  0  0]</span>
<span class="sd">          [ 0  0  0  0]]</span>
<span class="sd">         [[-5 -5 -5 -5]</span>
<span class="sd">          [-6 -6 -6 -6]</span>
<span class="sd">          [-7 -7 -7 -7]</span>
<span class="sd">          [-8 -8 -8 -8]]</span>
<span class="sd">         [[ 0  0  0  0]</span>
<span class="sd">          [ 0  0  0  0]</span>
<span class="sd">          [ 0  0  0  0]</span>
<span class="sd">          [ 0  0  0  0]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">scatter_nd_sub_inner</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ScatterNdSub</span><span class="p">)(</span><span class="n">use_locking</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">scatter_nd_sub_inner</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>


<div class="viewcode-block" id="scatter_nd_mul"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.scatter_nd_mul.html#mindspore.ops.scatter_nd_mul">[文档]</a><span class="k">def</span> <span class="nf">scatter_nd_mul</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies sparse multiplication to individual values or slices in a tensor.</span>

<span class="sd">    Using given values to update parameter value through the multiplication operation, along with the input indices.</span>
<span class="sd">    This operation outputs the `input_x` after the update is done, which makes it convenient to use the updated value.</span>

<span class="sd">    `input_x` has rank P and `indices` has rank Q, where `Q &gt;= 2`.</span>

<span class="sd">    `indices` has shape :math:`(i_0, i_1, ..., i_{Q-2}, N)` where `N &lt;= P`.</span>

<span class="sd">    The last dimension of `indices` (with length `N` ) indicates slices along the `N` th dimension of `input_x`.</span>

<span class="sd">    `updates` is a tensor of rank `Q-1+P-N`. Its shape is:</span>
<span class="sd">    :math:`(i_0, i_1, ..., i_{Q-2}, x\_shape_N, ..., x\_shape_{P-1})`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Parameter): The target tensor, with data type of Parameter.</span>
<span class="sd">            The shape is :math:`(N,*)`, where :math:`*` means any number of additional dimensions.</span>
<span class="sd">        indices (Tensor): The index to do multiplication operation whose data type must be mindspore.int32 or</span>
<span class="sd">            mindspore.int64. The rank of indices must be at least 2 and `indices.shape[-1] &lt;= len(shape)`.</span>
<span class="sd">        updates (Tensor): The tensor to do the multiplication operation with `input_x`.</span>
<span class="sd">            The data type is same as `input_x`, and the shape is `indices.shape[:-1] + x.shape[indices.shape[-1]:]`.</span>
<span class="sd">        use_locking (bool): Whether to protect the assignment by a lock. Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the updated `input_x`, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the dtype of `use_locking` is not bool.</span>
<span class="sd">        TypeError: If the dtype of `indices` is not int32 or int64.</span>
<span class="sd">        TypeError: If dtype of `input_x` and `updates` are not the same.</span>
<span class="sd">        ValueError: If the shape of `updates` is not equal to `indices.shape[:-1] + x.shape[indices.shape[-1]:]`.</span>
<span class="sd">        RuntimeError: If the data type of `input_x` and `updates` conversion of Parameter</span>
<span class="sd">                      is required when data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([1, 2, 3, 4, 5, 6, 7, 8]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[2], [4], [1], [7]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([6, 7, 8, 9]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_nd_mul(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1. 16. 18.  4. 35.  6.  7. 72.]</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.ones((4, 4, 4)), mindspore.int32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0], [2]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]],</span>
<span class="sd">        ...                            [[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_nd_mul(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1 1 1 1]</span>
<span class="sd">          [2 2 2 2]</span>
<span class="sd">          [3 3 3 3]</span>
<span class="sd">          [4 4 4 4]]</span>
<span class="sd">         [[1 1 1 1]</span>
<span class="sd">          [1 1 1 1]</span>
<span class="sd">          [1 1 1 1]</span>
<span class="sd">          [1 1 1 1]]</span>
<span class="sd">         [[5 5 5 5]</span>
<span class="sd">          [6 6 6 6]</span>
<span class="sd">          [7 7 7 7]</span>
<span class="sd">          [8 8 8 8]]</span>
<span class="sd">         [[1 1 1 1]</span>
<span class="sd">          [1 1 1 1]</span>
<span class="sd">          [1 1 1 1]</span>
<span class="sd">          [1 1 1 1]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">scatter_nd_mul_inner</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ScatterNdMul</span><span class="p">)(</span><span class="n">use_locking</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">scatter_nd_mul_inner</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>


<div class="viewcode-block" id="scatter_nd_div"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.scatter_nd_div.html#mindspore.ops.scatter_nd_div">[文档]</a><span class="k">def</span> <span class="nf">scatter_nd_div</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applying sparse division to individual values or slices in a tensor.</span>

<span class="sd">    Using given values to update tensor value through the div operation, along with the input indices.</span>
<span class="sd">    This operation outputs the `input_x` after the update is done, which makes it convenient to use the updated value.</span>

<span class="sd">    `input_x` has rank P and `indices` has rank Q, where `Q &gt;= 2`.</span>

<span class="sd">    `indices` has shape :math:`(i_0, i_1, ..., i_{Q-2}, N)` where `N &lt;= P`.</span>

<span class="sd">    The last dimension of `indices` (with length `N` ) indicates slices along the `N` th dimension of `input_x`.</span>

<span class="sd">    `updates` is a tensor of rank `Q-1+P-N`. Its shape is:</span>
<span class="sd">    :math:`(i_0, i_1, ..., i_{Q-2}, x\_shape_N, ..., x\_shape_{P-1})`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Parameter): The target tensor, with data type of Parameter.</span>
<span class="sd">            The shape is :math:`(N,*)`, where :math:`*` means any number of additional dimensions.</span>
<span class="sd">        indices (Tensor): The index to do div operation whose data type must be mindspore.int32 or mindspore.int64.</span>
<span class="sd">            The rank of indices must be at least 2 and `indices.shape[-1] &lt;= len(shape)`.</span>
<span class="sd">        updates (Tensor): The tensor to do the div operation with `input_x`.</span>
<span class="sd">            The data type is same as `input_x`, and the shape is `indices.shape[:-1] + x.shape[indices.shape[-1]:]`.</span>
<span class="sd">        use_locking (bool): Whether to protect the assignment by a lock. Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the updated `input_x`, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the dtype of `use_locking` is not bool.</span>
<span class="sd">        TypeError: If the dtype of `indices` is not int32 or int64.</span>
<span class="sd">        TypeError: If dtype of `input_x` and `updates` are not the same.</span>
<span class="sd">        ValueError: If the shape of `updates` is not equal to `indices.shape[:-1] + x.shape[indices.shape[-1]:]`.</span>
<span class="sd">        RuntimeError: If the data type of `input_x` and `updates` conversion of Parameter</span>
<span class="sd">                      is required when data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([1, 2, 3, 4, 5, 6, 7, 8]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[2], [4], [1], [7]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([6, 7, 8, 9]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_nd_div(input_x, indices, updates, False)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.         0.25       0.5        4.         0.71428573 6.</span>
<span class="sd">         7.         0.8888889 ]</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.ones((4, 4, 4)), mindspore.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0], [2]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]],</span>
<span class="sd">        ...                            [[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_nd_div(input_x, indices, updates, False)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1.         1.         1.         1.        ]</span>
<span class="sd">          [0.5        0.5        0.5        0.5       ]</span>
<span class="sd">          [0.33333334 0.33333334 0.33333334 0.33333334]</span>
<span class="sd">          [0.25       0.25       0.25       0.25      ]]</span>
<span class="sd">         [[1.         1.         1.         1.        ]</span>
<span class="sd">          [1.         1.         1.         1.        ]</span>
<span class="sd">          [1.         1.         1.         1.        ]</span>
<span class="sd">          [1.         1.         1.         1.        ]]</span>
<span class="sd">         [[0.2        0.2        0.2        0.2       ]</span>
<span class="sd">          [0.16666667 0.16666667 0.16666667 0.16666667]</span>
<span class="sd">          [0.14285715 0.14285715 0.14285715 0.14285715]</span>
<span class="sd">          [0.125      0.125      0.125      0.125     ]]</span>
<span class="sd">         [[1.         1.         1.         1.        ]</span>
<span class="sd">          [1.         1.         1.         1.        ]</span>
<span class="sd">          [1.         1.         1.         1.        ]</span>
<span class="sd">          [1.         1.         1.         1.        ]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">scatter_nd_div_inner</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ScatterNdDiv</span><span class="p">)(</span><span class="n">use_locking</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">scatter_nd_div_inner</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>


<div class="viewcode-block" id="scatter_nd_max"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.scatter_nd_max.html#mindspore.ops.scatter_nd_max">[文档]</a><span class="k">def</span> <span class="nf">scatter_nd_max</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applying sparse maximum to individual values or slices in a tensor.</span>

<span class="sd">    Using given values to update parameter value through the max operation, along with the input indices.</span>
<span class="sd">    This operation outputs the `input_x` after the update is done, which makes it convenient to use the updated value.</span>

<span class="sd">    `input_x` has rank P and `indices` has rank Q where `Q &gt;= 2`.</span>

<span class="sd">    `indices` has shape :math:`(i_0, i_1, ..., i_{Q-2}, N)` where `N &lt;= P`.</span>

<span class="sd">    The last dimension of `indices` (with length `N` ) indicates slices along the `N` th dimension of `input_x`.</span>

<span class="sd">    `updates` is a tensor of rank `Q-1+P-N`. Its shape is:</span>
<span class="sd">    :math:`(i_0, i_1, ..., i_{Q-2}, x\_shape_N, ..., x\_shape_{P-1})`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Parameter): The target tensor, with data type of Parameter.</span>
<span class="sd">            The shape is :math:`(N,*)`, where :math:`*` means any number of additional dimensions.</span>
<span class="sd">        indices (Tensor): The index to do maximum operation whose data type must be mindspore.int32 or mindspore.int64.</span>
<span class="sd">            The rank of indices must be at least 2 and `indices.shape[-1] &lt;= len(shape)`.</span>
<span class="sd">        updates (Tensor): The tensor to do the max operation with `input_x`.</span>
<span class="sd">            The data type is same as `input_x`, and the shape is `indices.shape[:-1] + x.shape[indices.shape[-1]:]`.</span>
<span class="sd">        use_locking (bool): Whether to protect the assignment by a lock. Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the updated `input_x`, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the dtype of `use_locking` is not bool.</span>
<span class="sd">        TypeError: If the dtype of `indices` is not int32 or int64.</span>
<span class="sd">        TypeError: If dtype of `input_x` and `updates` are not the same.</span>
<span class="sd">        ValueError: If the shape of `updates` is not equal to `indices.shape[:-1] + x.shape[indices.shape[-1]:]`.</span>
<span class="sd">        RuntimeError: If the data type of `input_x` and `updates` conversion of Parameter</span>
<span class="sd">                      is required when data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([1, 2, 3, 4, 5, 6, 7, 8]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[2], [4], [1], [7]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([6, 7, 8, 9]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_nd_max(input_x, indices, updates, False)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 8. 6. 4. 7. 6. 7. 9.]</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.ones((4, 4, 4)), mindspore.int32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0], [2]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]],</span>
<span class="sd">        ...                            [[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_nd_max(input_x, indices, updates, False)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1 1 1 1]</span>
<span class="sd">          [2 2 2 2]</span>
<span class="sd">          [3 3 3 3]</span>
<span class="sd">          [4 4 4 4]]</span>
<span class="sd">         [[1 1 1 1]</span>
<span class="sd">          [1 1 1 1]</span>
<span class="sd">          [1 1 1 1]</span>
<span class="sd">          [1 1 1 1]]</span>
<span class="sd">         [[5 5 5 5]</span>
<span class="sd">          [6 6 6 6]</span>
<span class="sd">          [7 7 7 7]</span>
<span class="sd">          [8 8 8 8]]</span>
<span class="sd">         [[1 1 1 1]</span>
<span class="sd">          [1 1 1 1]</span>
<span class="sd">          [1 1 1 1]</span>
<span class="sd">          [1 1 1 1]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">scatter_nd_max_inner</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ScatterNdMax</span><span class="p">)(</span><span class="n">use_locking</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">scatter_nd_max_inner</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>


<div class="viewcode-block" id="scatter_nd_min"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.scatter_nd_min.html#mindspore.ops.scatter_nd_min">[文档]</a><span class="k">def</span> <span class="nf">scatter_nd_min</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applying sparse minimum to individual values or slices in a tensor.</span>

<span class="sd">    Using given values to update tensor value through the min operation, along with the input indices.</span>
<span class="sd">    This operation outputs the `input_x` after the update is done, which makes it convenient to use the updated value.</span>

<span class="sd">    `input_x` has rank P and `indices` has rank Q where `Q &gt;= 2`.</span>

<span class="sd">    `indices` has shape :math:`(i_0, i_1, ..., i_{Q-2}, N)` where `N &lt;= P`.</span>

<span class="sd">    The last dimension of `indices` (with length `N` ) indicates slices along the `N` th dimension of `input_x`.</span>

<span class="sd">    `updates` is a tensor of rank `Q-1+P-N`. Its shape is:</span>
<span class="sd">    :math:`(i_0, i_1, ..., i_{Q-2}, x\_shape_N, ..., x\_shape_{P-1})`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Parameter): The target tensor, with data type of Parameter.</span>
<span class="sd">            The shape is :math:`(N,*)`, where :math:`*` means any number of additional dimensions.</span>
<span class="sd">        indices (Tensor): The index to do min operation whose data type must be mindspore.int32 or mindspore.int64.</span>
<span class="sd">            The rank of indices must be at least 2 and `indices.shape[-1] &lt;= len(shape)`.</span>
<span class="sd">        updates (Tensor): The tensor to do the min operation with `input_x`.</span>
<span class="sd">            The data type is same as `input_x`, and the shape is `indices.shape[:-1] + x.shape[indices.shape[-1]:]`.</span>
<span class="sd">        use_locking (bool): Whether to protect the assignment by a lock. Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the updated `input_x`, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the dtype of `use_locking` is not bool.</span>
<span class="sd">        TypeError: If the dtype of `indices` is not int32 or int64.</span>
<span class="sd">        TypeError: If dtype of `input_x` and `updates` are not the same.</span>
<span class="sd">        ValueError: If the shape of `updates` is not equal to `indices.shape[:-1] + x.shape[indices.shape[-1]:]`.</span>
<span class="sd">        RuntimeError: If the data type of `input_x` and `updates` conversion of Parameter</span>
<span class="sd">                      is required when data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.ones(8) * 10, mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[2], [4], [1], [7]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([6, 7, 8, 9]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_nd_min(input_x, indices, updates, False)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [10.  8.  6. 10.  7. 10. 10.  9.]</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.ones((4, 4, 4)) * 10, mindspore.int32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0], [2]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]],</span>
<span class="sd">        ...                            [[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_nd_min(input_x, indices, updates, False)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[ 1  1  1  1]</span>
<span class="sd">          [ 2  2  2  2]</span>
<span class="sd">          [ 3  3  3  3]</span>
<span class="sd">          [ 4  4  4  4]]</span>
<span class="sd">         [[10 10 10 10]</span>
<span class="sd">          [10 10 10 10]</span>
<span class="sd">          [10 10 10 10]</span>
<span class="sd">          [10 10 10 10]]</span>
<span class="sd">         [[ 5  5  5  5]</span>
<span class="sd">          [ 6  6  6  6]</span>
<span class="sd">          [ 7  7  7  7]</span>
<span class="sd">          [ 8  8  8  8]]</span>
<span class="sd">         [[10 10 10 10]</span>
<span class="sd">          [10 10 10 10]</span>
<span class="sd">          [10 10 10 10]</span>
<span class="sd">          [10 10 10 10]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">scatter_nd_min_inner</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ScatterNdMin</span><span class="p">)(</span><span class="n">use_locking</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">scatter_nd_min_inner</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>


<div class="viewcode-block" id="gather"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.gather.html#mindspore.ops.gather">[文档]</a><span class="k">def</span> <span class="nf">gather</span><span class="p">(</span><span class="n">input_params</span><span class="p">,</span> <span class="n">input_indices</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the slice of the input tensor corresponding to the elements of `input_indices` on the specified `axis`.</span>

<span class="sd">    The following figure shows the calculation process of Gather commonly:</span>

<span class="sd">    .. image:: Gather.png</span>

<span class="sd">    where params represents the input `input_params`, and indices represents the index to be sliced `input_indices`.</span>

<span class="sd">    .. note::</span>
<span class="sd">         1. The value of input_indices must be in the range of `[0, input_param.shape[axis])`, the result is undefined</span>
<span class="sd">            out of range.</span>

<span class="sd">         2. The data type of input_params cannot be</span>
<span class="sd">            `bool_ &lt;https://www.mindspore.cn/docs/en/r1.9/api_python/mindspore.html#mindspore.dtype&gt;`_ on Ascend</span>
<span class="sd">            platform currently.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_params (Tensor): The original Tensor. The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">        input_indices (Tensor): Index tensor to be sliced, the shape of tensor is :math:`(y_1, y_2, ..., y_S)`.</span>
<span class="sd">            Specifies the indices of elements of the original Tensor. The data type can be int32 or int64.</span>
<span class="sd">        axis (int): Specifies the dimension index to gather indices.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape of tensor is</span>
<span class="sd">        :math:`input\_params.shape[:axis] + input\_indices.shape + input\_params.shape[axis + 1:]`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>
<span class="sd">        TypeError: If `input_params` is not a tensor.</span>
<span class="sd">        TypeError: If `input_indices` is not a tensor of type int.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # case1: input_indices is a Tensor with shape (5, ).</span>
<span class="sd">        &gt;&gt;&gt; input_params = Tensor(np.array([1, 2, 3, 4, 5, 6, 7]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_indices = Tensor(np.array([0, 2, 4, 2, 6]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; axis = 0</span>
<span class="sd">        &gt;&gt;&gt; output = ops.gather(input_params, input_indices, axis)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 3. 5. 3. 7.]</span>
<span class="sd">        &gt;&gt;&gt; # case2: input_indices is a Tensor with shape (2, 2). When the input_params has one dimension,</span>
<span class="sd">        &gt;&gt;&gt; # the output shape is equal to the input_indices shape.</span>
<span class="sd">        &gt;&gt;&gt; input_indices = Tensor(np.array([[0, 2], [2, 6]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; axis = 0</span>
<span class="sd">        &gt;&gt;&gt; output = ops.gather(input_params, input_indices, axis)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 1. 3.]</span>
<span class="sd">         [ 3. 7.]]</span>
<span class="sd">        &gt;&gt;&gt; # case3: input_indices is a Tensor with shape (2, ) and</span>
<span class="sd">        &gt;&gt;&gt; # input_params is a Tensor with shape (3, 4) and axis is 0.</span>
<span class="sd">        &gt;&gt;&gt; input_params = Tensor(np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_indices = Tensor(np.array([0, 2]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; axis = 0</span>
<span class="sd">        &gt;&gt;&gt; output = ops.gather(input_params, input_indices, axis)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1.  2.  3.  4.]</span>
<span class="sd">         [9. 10. 11. 12.]]</span>
<span class="sd">        &gt;&gt;&gt; # case4: input_indices is a Tensor with shape (2, ) and</span>
<span class="sd">        &gt;&gt;&gt; # input_params is a Tensor with shape (3, 4) and axis is 1.</span>
<span class="sd">        &gt;&gt;&gt; input_params = Tensor(np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_indices = Tensor(np.array([0, 2]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; axis = 1</span>
<span class="sd">        &gt;&gt;&gt; output = ops.gather(input_params, input_indices, axis)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1.  3.]</span>
<span class="sd">         [5.  7.]</span>
<span class="sd">         [9. 11.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">gather_</span><span class="p">(</span><span class="n">input_params</span><span class="p">,</span> <span class="n">input_indices</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="gather_d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.gather_d.html#mindspore.ops.gather_d">[文档]</a><span class="k">def</span> <span class="nf">gather_d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gathers elements along an axis specified by dim.</span>

<span class="sd">    Refer to :func:`mindspore.ops.gather_elements` for more detail.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2], [3, 4]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; index = Tensor(np.array([[0, 0], [1, 0]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; dim = 1</span>
<span class="sd">        &gt;&gt;&gt; output = ops.gather_d(x, dim, index)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1 1]</span>
<span class="sd">         [4 3]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">gather_d_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span></div>


<div class="viewcode-block" id="gather_elements"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.gather_elements.html#mindspore.ops.gather_elements">[文档]</a><span class="k">def</span> <span class="nf">gather_elements</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gathers elements along an axis specified by dim.</span>

<span class="sd">    For a 3-D tensor, the output is:</span>

<span class="sd">    .. code-block::</span>

<span class="sd">        output[i][j][k] = x[index[i][j][k]][j][k]  # if dim == 0</span>

<span class="sd">        output[i][j][k] = x[i][index[i][j][k]][k]  # if dim == 1</span>

<span class="sd">        output[i][j][k] = x[i][j][index[i][j][k]]  # if dim == 2</span>

<span class="sd">    `x` and `index` have the same length of dimensions, and all dimensions except `dim` have the same size.</span>
<span class="sd">    If `dim` = i, `x` is an n-D tensor with shape :math:`(z_0, z_1, ..., z_i, ..., z_{n-1})`,</span>
<span class="sd">    the `index` must be an n-D tensor with shape :math:`(z_0, z_1, ..., y, ..., z_{n-1})`</span>
<span class="sd">    where `y`&gt;=1 and the output will have the same shape with `index`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor.</span>
<span class="sd">        dim (int): The axis along which to index. It must be int32 or int64. The value range is [-x.ndim, x.ndim).</span>
<span class="sd">        index (Tensor): The indices of elements to gather. It can be one of the following data types:</span>
<span class="sd">            int32, int64. The value range of each index element is [-x.shape(dim), x.shape(dim)).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as index tensor, the shape of tensor is :math:`(z_1, z_2, ..., z_{n-1})`,</span>
<span class="sd">        and has the same data type with `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `dim` or `index` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If length of shape of `x` is not equal to length of shape of `index`.</span>
<span class="sd">        ValueError: If the size of the dimension except `dim` is not equal between `x` and `index`.</span>
<span class="sd">        ValueError: If the value of `dim` is not in the expected range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2], [3, 4]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; index = Tensor(np.array([[0, 0], [1, 0]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; dim = 1</span>
<span class="sd">        &gt;&gt;&gt; output = mindspore.ops.gather_elements(x, dim, index)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1 1]</span>
<span class="sd">         [4 3]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">gather_d_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span></div>


<div class="viewcode-block" id="gather_nd"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.gather_nd.html#mindspore.ops.gather_nd">[文档]</a><span class="k">def</span> <span class="nf">gather_nd</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gathers slices from a tensor by indices.</span>

<span class="sd">    Using given indices to gather slices from a tensor with a specified shape.</span>

<span class="sd">    `indices` is an K-dimensional integer tensor. Supposes it as a (K-1)-dimensional tensor and each element of it</span>
<span class="sd">    defines a slice of `input_x`:</span>

<span class="sd">    .. math::</span>
<span class="sd">        output[(i_0, ..., i_{K-2})] = input\_x[indices[(i_0, ..., i_{K-2})]]</span>

<span class="sd">    The last dimension of `indices` can not more than the rank of `input_x`:</span>
<span class="sd">    :math:`indices.shape[-1] &lt;= input\_x.rank`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The target tensor to gather values.</span>
<span class="sd">            The shape is :math:`(N,*)` where :math:`*` means,any number of additional dimensions.</span>
<span class="sd">        indices (Tensor): The index tensor, with int32 or int64 data type.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same type as `input_x` and the shape is</span>
<span class="sd">        :math:`indices\_shape[:-1] + input\_x\_shape[indices\_shape[-1]:]`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If length of shape of `input_x` is less than the last dimension of `indices`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 0], [1, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.gather_nd(input_x, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-0.1  0.5]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">gather_nd_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span></div>


<div class="viewcode-block" id="tensor_scatter_add"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.tensor_scatter_add.html#mindspore.ops.tensor_scatter_add">[文档]</a><span class="k">def</span> <span class="nf">tensor_scatter_add</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a new tensor by adding the values from the positions in `input_x` indicated by</span>
<span class="sd">    `indices`, with values from `updates`. When multiple values are given for the same</span>
<span class="sd">    index, the updated result will be the sum of all values. This operation is almost</span>
<span class="sd">    equivalent to using ScatterNdAdd, except that the updates are applied on output `Tensor`</span>
<span class="sd">    instead of input `Parameter`.</span>

<span class="sd">    The last axis of `indices` is the depth of each index vectors. For each index vector,</span>
<span class="sd">    there must be a corresponding value in `updates`. The shape of `updates` should be</span>
<span class="sd">    equal to the shape of `input_x[indices]`. For more details, see use cases.</span>

<span class="sd">    Note:</span>
<span class="sd">        On GPU, if some values of the `indices` are out of bound, instead of raising an index error,</span>
<span class="sd">        the corresponding `updates` will not be updated to self tensor. On CPU, if some values of</span>
<span class="sd">        the `indices` are out of bound, raising an index error. On Ascend, out of bound checking is</span>
<span class="sd">        not supported, if some values of the `indices` are out of bound, unknown errors may be caused.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The target tensor. The dimension of input_x must be no less than indices.shape[-1].</span>
<span class="sd">        indices (Tensor): The index of input tensor whose data type is int32 or int64.</span>
<span class="sd">            The rank must be at least 2.</span>
<span class="sd">        updates (Tensor): The tensor to update the input tensor, has the same type as input,</span>
<span class="sd">            and updates. Shape should be equal to indices.shape[:-1] + input_x.shape[indices.shape[-1]:].</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `indices` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If length of shape of `input_x` is less than the last dimension of shape of `indices`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 0], [0, 0]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([1.0, 2.2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.tensor_scatter_add(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 3.1  0.3  3.6]</span>
<span class="sd">         [ 0.4  0.5 -3.2]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="n">tensor_scatter_add_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>


<div class="viewcode-block" id="tensor_scatter_sub"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.tensor_scatter_sub.html#mindspore.ops.tensor_scatter_sub">[文档]</a><span class="k">def</span> <span class="nf">tensor_scatter_sub</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a new tensor by subtracting the values from the positions in `input_x` indicated by</span>
<span class="sd">    `indices`, with values from `updates`. When multiple values are provided for the same</span>
<span class="sd">    index, the result of the update will be to subtract these values respectively. This operation is almost</span>
<span class="sd">    equivalent to using :class:`mindspore.ops.ScatterNdSub` , except that the updates are applied on output `Tensor`</span>
<span class="sd">    instead of input `Parameter`.</span>

<span class="sd">    The last axis of `indices` is the depth of each index vectors. For each index vector,</span>
<span class="sd">    there must be a corresponding value in `updates`. The shape of `updates` should be</span>
<span class="sd">    equal to the shape of `input_x[indices]`. For more details, see use cases.</span>

<span class="sd">    Note:</span>
<span class="sd">        On GPU, if some values of the `indices` are out of bound, instead of raising an index error,</span>
<span class="sd">        the corresponding `updates` will not be updated to self tensor. On CPU, if some values of</span>
<span class="sd">        the `indices` are out of bound, raising an index error. On Ascend, out of bound checking is</span>
<span class="sd">        not supported, if some values of the `indices` are out of bound, unknown errors may be caused.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The target tensor. The dimension of input_x must be no less than indices.shape[-1].</span>
<span class="sd">        indices (Tensor): The index of input tensor whose data type is int32 or int64.</span>
<span class="sd">            The rank must be at least 2.</span>
<span class="sd">        updates (Tensor): The tensor to update the input tensor, has the same type as input,</span>
<span class="sd">            and updates.shape should be equal to indices.shape[:-1] + input_x.shape[indices.shape[-1]:].</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `indices` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If length of shape of `input_x` is less than the last dimension of shape of `indices`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 0], [0, 0]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([1.0, 2.2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.tensor_scatter_sub(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-3.3000002  0.3        3.6      ]</span>
<span class="sd">         [ 0.4        0.5       -3.2      ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="n">tensor_scatter_sub_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">tensor_scatter_max</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    By comparing the value at the position indicated by `indices` in `x` with the value in the `updates`,</span>
<span class="sd">    the value at the index will eventually be equal to the largest one to create a new tensor.</span>

<span class="sd">    The last axis of the index is the depth of each index vector. For each index vector,</span>
<span class="sd">    there must be a corresponding value in `updates`. The shape of `updates` should be</span>
<span class="sd">    equal to the shape of input_x[indices].</span>
<span class="sd">    For more details, see use cases.</span>

<span class="sd">    Note:</span>
<span class="sd">        If some values of the `indices` are out of bound, instead of raising an index error,</span>
<span class="sd">        the corresponding `updates` will not be updated to `input_x`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The target tensor. The dimension of input_x must be no less than indices.shape[-1].</span>
<span class="sd">        indices (Tensor): The index of input tensor whose data type is int32 or int64.</span>
<span class="sd">            The rank must be at least 2.</span>
<span class="sd">        updates (Tensor): The tensor to update the input tensor, has the same type as input,</span>
<span class="sd">            and updates.shape should be equal to indices.shape[:-1] + input_x.shape[indices.shape[-1]:].</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as `input_x`.</span>

<span class="sd">        Tensor, has the same shape and type as `input_x`.</span>
<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `indices` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If length of shape of `input_x` is less than the last dimension of shape of `indices`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 0], [0, 0]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([1.0, 2.2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; # Next, demonstrate the approximate operation process of this operator:</span>
<span class="sd">        &gt;&gt;&gt; # 1, indices[0] = [0, 0], indices[1] = [0, 0]</span>
<span class="sd">        &gt;&gt;&gt; # 2, And input_x[0, 0] = -0.1</span>
<span class="sd">        &gt;&gt;&gt; # 3, So input_x[indices] = [-0.1, -0.1]</span>
<span class="sd">        &gt;&gt;&gt; # 4, Satisfy the above formula: input_x[indices].shape=(2) == updates.shape=(2)</span>
<span class="sd">        &gt;&gt;&gt; op = ops.TensorScatterMax()</span>
<span class="sd">        &gt;&gt;&gt; # 5, Perform the max operation for the first time:</span>
<span class="sd">        &gt;&gt;&gt; #      first_input_x = Max(input_x[0][0], updates[0]) = [[1.0, 0.3, 3.6], [0.4, 0.5, -3.2]]</span>
<span class="sd">        &gt;&gt;&gt; # 6, Perform the max operation for the second time:</span>
<span class="sd">        &gt;&gt;&gt; #      second_input_x = Max(input_x[0][0], updates[1]) = [[2.2, 0.3, 3.6], [0.4, 0.5, -3.2]]</span>
<span class="sd">        &gt;&gt;&gt; output = op(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 2.2  0.3  3.6]</span>
<span class="sd">         [ 0.4  0.5 -3.2]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_scatter_max_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">tensor_scatter_min</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    By comparing the value at the position indicated by `indices` in `input_x` with the value in the `updates`,</span>
<span class="sd">    the value at the index will eventually be equal to the smallest one to create a new tensor.</span>

<span class="sd">    The last axis of the index is the depth of each index vector. For each index vector,</span>
<span class="sd">    there must be a corresponding value in `updates`. The shape of `updates` should be</span>
<span class="sd">    equal to the shape of `input_x[indices]`. For more details, see case below.</span>

<span class="sd">    Note:</span>
<span class="sd">        If some values of the `indices` are out of range, instead of raising an index error,</span>
<span class="sd">        the corresponding `updates` will not be hw to `input_x`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The input tensor. The dimension of input_x must be no less than indices.shape[-1].</span>
<span class="sd">        indices (Tensor): The index of input tensor whose data type is int32 or int64.</span>
<span class="sd">            The rank must be at least 2.</span>
<span class="sd">        updates (Tensor): The tensor to update the input tensor, has the same type as input,</span>
<span class="sd">            and updates.shape should be equal to indices.shape[:-1] + input_x.shape[indices.shape[-1]:].</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `indices` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If length of shape of `input_x` is less than the last dimension of shape of `indices`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 0], [0, 0]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([1.0, 2.2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.tensor_scatter_min(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ -0.1  0.3  3.6]</span>
<span class="sd">        [ 0.4  0.5 -3.2]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_scatter_min_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span>


<div class="viewcode-block" id="tensor_scatter_elements"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.tensor_scatter_elements.html#mindspore.ops.tensor_scatter_elements">[文档]</a><span class="k">def</span> <span class="nf">tensor_scatter_elements</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates the value of the input tensor through the reduction operation.</span>

<span class="sd">    tensor_scatter_elements takes three inputs data, updates, and indices of the same rank r &gt;= 1,</span>
<span class="sd">    an optional attribute axis that identifies an axis of data (default is 0),  and another optional attribute reduction</span>
<span class="sd">    that identifies reduction operation. When reduction is set to &quot;none&quot;, the update value will be assigned to the</span>
<span class="sd">    output value according to the indices. When reduction is set to &quot;add&quot;, the update value will be added to the output</span>
<span class="sd">    value according to the indices.</span>

<span class="sd">    For a 3-D tensor, the output is:</span>

<span class="sd">    .. code-block::</span>

<span class="sd">        output[indices[i][j][k]][j][k] = updates[i][j][k]  # if axis == 0, reduction == &quot;none&quot;</span>

<span class="sd">        output[i][indices[i][j][k]][k] += updates[i][j][k]  # if axis == 1, reduction == &quot;add&quot;</span>

<span class="sd">        output[i][j][indices[i][j][k]] = updates[i][j][k]  # if axis == 2, reduction == &quot;none&quot;</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - The order in which updates are applied is nondeterministic, meaning that if there are multiple index vectors</span>
<span class="sd">          in `indices` that correspond to the same position, the value of that position in the output will be</span>
<span class="sd">          nondeterministic.</span>
<span class="sd">        - On Ascend, the reduction only support set to &quot;none&quot; for now.</span>
<span class="sd">        - On Ascend, the data type of `input_x` must be float16 or float32.</span>

<span class="sd">    .. note::</span>
<span class="sd">        If some values of the `indices` are out of bound, instead of raising an index error,</span>
<span class="sd">        the corresponding `updates` will not be updated to `input_x`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The target tensor.</span>
<span class="sd">          The shape is :math:`(N,*)` where :math:`*` means,any number of additional dimensions.</span>
<span class="sd">        indices (Tensor): The index to do add operation whose data type must be mindspore.int32 or</span>
<span class="sd">          mindspore.int64. Same rank as input_x. And accepted range is [-s, s) where s is the size along axis.</span>
<span class="sd">        updates (Tensor): The tensor doing the add operation with `input_x`, has the same type as input_x,</span>
<span class="sd">          and update.shape should be equal to indices.shape.</span>
<span class="sd">        axis (int): Which axis to scatter, default is 0. Accepted range is [-r, r) where r = rank(input_x).</span>
<span class="sd">        reduction (str): Which reduction operation to scatter, default is &quot;none&quot;. Other option: &quot;add&quot;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `indices` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If anyone of the rank among `input_x`, `indices` and `updates` less than 1.</span>
<span class="sd">        ValueError: If the shape of `updates` is not equal to the shape of `indices`.</span>
<span class="sd">        ValueError: If the rank of `updates` is not equal to the rank of `input_x`.</span>
<span class="sd">        RuntimeError: If the data type of `input_x` and `updates` conversion of Parameter</span>
<span class="sd">                        is required when data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[1, 2, 3, 4, 5]]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[2, 4]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[8, 8]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; axis = 1</span>
<span class="sd">        &gt;&gt;&gt; reduction = &quot;none&quot;</span>
<span class="sd">        &gt;&gt;&gt; output = ops.tensor_scatter_elements(input_x, indices, updates, axis, reduction)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 1  2  8  4  8]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_tensor_scatter_elements</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">TensorScatterElements</span><span class="p">)(</span><span class="n">axis</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_tensor_scatter_elements</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>


<div class="viewcode-block" id="space_to_batch_nd"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.space_to_batch_nd.html#mindspore.ops.space_to_batch_nd">[文档]</a><span class="k">def</span> <span class="nf">space_to_batch_nd</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">paddings</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Divides a tensor&#39;s spatial dimensions into blocks and combines the block sizes with the original batch.</span>

<span class="sd">    This operation will divide spatial dimensions into blocks with `block_size`,</span>
<span class="sd">    and after division, the output tensor&#39;s spatial dimension is the corresponding number of blocks.</span>
<span class="sd">    The output tensor&#39;s batch dimension is the product of the original batch and the product of `block_size`.</span>
<span class="sd">    Before division, the spatial dimensions of the input are zero padded according to paddings if necessary.</span>
<span class="sd">    Assume input shape is :math:`(n, c_1, ... c_k, w_1, ..., w_M)` with</span>
<span class="sd">    :math:`block\_size` and :math:`paddings`. Then the shape of the output tensor will be</span>
<span class="sd">    :math:`(n&#39;, c_1, ... c_k, w&#39;_1, ..., w&#39;_M)`, where</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            n&#39; = n*(block\_size[0] * ... * block\_size[M]) \\</span>
<span class="sd">            w&#39;_i = (w_i + paddings[i][0] + paddings[i][1])//block\_size[i]</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The input tensor. It must be a 4-D tensor on Ascend.</span>
<span class="sd">        block_size (Union[list(int), tuple(int), int]): The block size of dividing block with all value greater</span>
<span class="sd">            than 1. If `block_size` is a tuple or list, the length of `block_size` is M corresponding to the</span>
<span class="sd">            number of spatial dimensions. If `block_size` is an int, the block size of M dimensions are the same,</span>
<span class="sd">            equal to `block_size`. M must be 2 on Ascend.</span>
<span class="sd">        paddings (Union[tuple, list]): The padding values for spatial dimensions, containing M subtraction list.</span>
<span class="sd">            Each contains 2 integer values. All values must be greater than 0.</span>
<span class="sd">            `paddings[i]` specifies the paddings for the spatial dimension i,</span>
<span class="sd">            which corresponds to the input dimension i + offset.</span>
<span class="sd">            It is required that input_shape[i+offset]+paddings[i][0]+paddings[i][1] is divisible by block_size[i].</span>
<span class="sd">            M must be 2 on Ascend.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the output tensor with the same data type as input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If `block_size` is not one dimensional when `block_size` is a list or tuple.</span>
<span class="sd">        ValueError: If the length of `block_size` is not 2 on Ascend.</span>
<span class="sd">        ValueError: If the element of `block_size` is not an integer larger than 1.</span>
<span class="sd">        ValueError: If shape of `paddings` is not (M, 2), where M is the length of `block_size`.</span>
<span class="sd">        ValueError: If the element of `paddings` is not an integer larger than 0.</span>
<span class="sd">        TypeError: If `block_size` is not one of list, tuple, int.</span>
<span class="sd">        TypeError: If `paddings` is neither list nor tuple.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; block_size = [2, 2]</span>
<span class="sd">        &gt;&gt;&gt; paddings = [[0, 0], [0, 0]]</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[[1, 2], [3, 4]]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.space_to_batch_nd(input_x, block_size, paddings)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[1.]]]</span>
<span class="sd">         [[[2.]]]</span>
<span class="sd">         [[[3.]]]</span>
<span class="sd">         [[[4.]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_space_to_batch_nd</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">SpaceToBatchND</span><span class="p">)(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_space_to_batch_nd</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="batch_to_space_nd"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.batch_to_space_nd.html#mindspore.ops.batch_to_space_nd">[文档]</a><span class="k">def</span> <span class="nf">batch_to_space_nd</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">block_shape</span><span class="p">,</span> <span class="n">crops</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Divides batch dimension with blocks and interleaves these blocks back into spatial dimensions.</span>

<span class="sd">    This operation will divide batch dimension N into blocks with block_shape, the output tensor&#39;s N dimension</span>
<span class="sd">    is the corresponding number of blocks after division. The output tensor&#39;s H, W dimension is the product of</span>
<span class="sd">    original H, W dimension and block_shape with given amount to crop from dimension, respectively.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The input tensor. It must be greater or equal to 4-D tensor(equal to 4-D tensor on Ascend),</span>
<span class="sd">            batch dimension must be divisible by product of `block_shape`. The data type is float16 or float32.</span>
<span class="sd">        block_shape (Union[list(int), tuple(int), int]): The block shape of dividing block with all value greater</span>
<span class="sd">            than 1. If `block_shape` is a tuple or list, the length of `block_shape` is M corresponding to the</span>
<span class="sd">            number of spatial dimensions. If `block_shape` is an int, the block size of M dimensions are the same,</span>
<span class="sd">            equal to `block_shape`. M must be 2.</span>
<span class="sd">        crops (Union[list(int), tuple(int)]): The crop value for H and W dimension, containing 2 subtraction list,</span>
<span class="sd">            each containing 2 int value.</span>
<span class="sd">            All values must be &gt;= 0. crops[i] specifies the crop values for spatial dimension i, which corresponds to</span>
<span class="sd">            input dimension i+2. It is required that</span>

<span class="sd">            :math:`input\_shape[i+2]*block\_shape[i] &gt; crops[i][0]+crops[i][1]`</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the output tensor with the same type as input. Assume input shape is (n, c, h, w) with block_shape</span>
<span class="sd">        and crops. The output shape will be (n&#39;, c&#39;, h&#39;, w&#39;), where</span>

<span class="sd">        :math:`n&#39; = n//(block\_shape[0]*block\_shape[1])`</span>

<span class="sd">        :math:`c&#39; = c`</span>

<span class="sd">        :math:`h&#39; = h*block\_shape[0]-crops[0][0]-crops[0][1]`</span>

<span class="sd">        :math:`w&#39; = w*block\_shape[1]-crops[1][0]-crops[1][1]`</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `block_shape` is not one of list, tuple, int.</span>
<span class="sd">        TypeError: If `crops` is neither list nor tuple.</span>
<span class="sd">        ValueError: If `block_shape` is not one dimensional when `block_shape` is a list or tuple.</span>
<span class="sd">        ValueError: If length of `block_shape` or `crops` is not equal to 2.</span>
<span class="sd">        ValueError: If the element of `block_shape` is not an integer larger than 1.</span>
<span class="sd">        ValueError: If shape of `crops` is not (M, 2), where M is the length of `block_shape`.</span>
<span class="sd">        ValueError: If the element of `crops` is not an integer larger than 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; block_shape = [2, 2]</span>
<span class="sd">        &gt;&gt;&gt; crops = [[0, 0], [0, 0]]</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[[1]]], [[[2]]], [[[3]]], [[[4]]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.batch_to_space_nd(input_x, block_shape, crops)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[1.  2.]</span>
<span class="sd">           [3.  4.]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_batch_to_space_nd</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">BatchToSpaceND</span><span class="p">)(</span><span class="n">block_shape</span><span class="p">,</span> <span class="n">crops</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_batch_to_space_nd</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="nonzero"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.nonzero.html#mindspore.ops.nonzero">[文档]</a><span class="k">def</span> <span class="nf">nonzero</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return a Tensor of the positions of all non-zero values.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The shape of Tensor is :math:`(x_1, x_2, ..., x_R)`. The data type is Number or Bool.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, a 2-D Tensor whose data type is int64, containing the positions of all non-zero values of the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">       TypeError: If `x` is not Tensor.</span>
<span class="sd">       ValueError: If &#39;x&#39; dim equal to 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">       ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[1,  0], [-5, 0]]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.nonzero(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0 0 0]</span>
<span class="sd">         [0 1 0]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">nonzero_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="matrix_diag"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.matrix_diag.html#mindspore.ops.matrix_diag">[文档]</a><span class="k">def</span> <span class="nf">matrix_diag</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_rows</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_cols</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s2">&quot;RIGHT_LEFT&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a Tensor with the contents in `x` as k[0]-th to k[1]-th diagonals of a matrix, with everything else padded</span>
<span class="sd">    with `padding_value`. `num_rows` and `num_cols` specify the dimension of the innermost matrix of the output. If both</span>
<span class="sd">    are not specified, the op assumes the innermost matrix of output Tensor is square and infers its size from `k` and</span>
<span class="sd">    the innermost dimension of `x`. If the `num_rows` and `num_cols` specify only one of them, the operator will derive</span>
<span class="sd">    the smallest legal value as the dimension of output. Moreover, when only one diagonal is given</span>
<span class="sd">    (k is an integer or k[0] == k[1]), the first to the second innermost dimension of `x` is the batch size. Otherwise,</span>
<span class="sd">    the second innermost dimension is not a part of batch size.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The diagonal Tensor.</span>
<span class="sd">        k (Union[int, Tensor], optional): A Tensor of type int32. Diagonal offsets. Positive value means superdiagonal,</span>
<span class="sd">            0 refers to the main diagonal, and negative value means subdiagonals. `k` can be a single integer</span>
<span class="sd">            (for a single diagonal) or a pair of integers specifying the low and high ends of a matrix band.</span>
<span class="sd">            k[0] must not be larger than k[1]. The value must be in the range of given or derivated `num_rows`</span>
<span class="sd">            and `num_cols`, meaning value of k must be in (-num_rows, num_cols). Default: 0.</span>
<span class="sd">        num_rows (Union[int, Tensor], optional): A Tensor of type int32 with only one value. The number of rows of the</span>
<span class="sd">            output Tensor. If `num_rows` is -1, indicating that the innermost matrix of the output Tensor is a square</span>
<span class="sd">            matrix, and the real number of rows will be derivated by other inputs. That is</span>
<span class="sd">            :math:`num_rows = x.shape[-1] - min(k[1], 0)`. Otherwise, the value must be equal or greater than</span>
<span class="sd">            :math:`x.shape[-1] - min(k[1], 0)`. Default: -1.</span>
<span class="sd">        num_cols (Union[int, Tensor], optional): A Tensor of type int32 with only one value. The number of columns of</span>
<span class="sd">            the output Tensor. If `num_cols` is -1, indicating that the innermost matrix of the output</span>
<span class="sd">            Tensor is a square matrix, and the real number of columns will be derivated by other inputs.</span>
<span class="sd">            That is :math:`num_cols = x.shape[-1] + max(k[0], 0)`. Otherwise, the value must be equal or</span>
<span class="sd">            greater than :math:`x.shape[-1] - min(k[1], 0)`.  Default: -1.</span>
<span class="sd">        padding_value (Union[int, float, Tensor], optional): A Tensor with only one value. Have the same dtype as x.</span>
<span class="sd">            The number to fill the area outside the specified diagonal band.  Default: 0.</span>
<span class="sd">        align (str, optional): An optional string from: &quot;RIGHT_LEFT&quot;(default), &quot;LEFT_RIGHT&quot;, &quot;LEFT_LEFT&quot;,</span>
<span class="sd">            &quot;RIGHT_RIGHT&quot;. Align is a string specifying how superdiagonals and subdiagonals should be aligned,</span>
<span class="sd">            respectively. &quot;RIGHT_LEFT&quot; aligns superdiagonals to the right (left-pads the row) and subdiagonals</span>
<span class="sd">            to the left (right-pads the row).</span>

<span class="sd">    Returns:</span>
<span class="sd">        A Tensor. Has the same type as `x`.</span>
<span class="sd">        Suppose `x` has r dimensions with shape `(I, J, ..., M, N)`. The output Tensor has rank r + 1 with shape</span>
<span class="sd">        `(I, J, ..., M, num_rows, num_cols)` when only one diagonal is given (k is an integer or k[0] == k[1]).</span>
<span class="sd">        Otherwise, it has rank r with shape `(I, J, ..., num_rows, num_cols)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not Tensor.</span>
<span class="sd">        TypeError: If input `x` and `padding_value` are not the same dtype.</span>
<span class="sd">        TypeError: If `k`, `num_rows` or `num_cols` is not int32 dtype.</span>
<span class="sd">        ValueError: If rank of `k` is not equal to 0 or 1.</span>
<span class="sd">        ValueError: If rank of `num_rows`, `num_cols` or `padding_value` is not equal to 0.</span>
<span class="sd">        ValueError: If size of `k` is not equal to 1 or 2.</span>
<span class="sd">        ValueError: If the value of `k` is not in (-num_rows, num_cols).</span>
<span class="sd">        ValueError: If k[1] is not greater equal to k[0] when k[0] != k[1].</span>
<span class="sd">        ValueError: If rank of `x` is not greater than or is equal to 1 when k is an integer or k[0] == k[1].</span>
<span class="sd">        ValueError: If rank of `x` is not greater than or is equal to 2 when k[0] != k[1].</span>
<span class="sd">        ValueError: If x.shape[-2] is not equal to k[1] - k[0] + 1 when k[0] != k[1].</span>
<span class="sd">        ValueError: If `num_rows` and `num_cols` do not match the dimensions of `x` and the values of `k`.</span>
<span class="sd">        ValueError: If `align` is not a string or not in the valid set of values.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[8, 9, 0],</span>
<span class="sd">        ...                      [1, 2, 3],</span>
<span class="sd">        ...                      [0, 4, 5]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; k =Tensor(np.array([-1, 1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; num_rows = Tensor(np.array(3), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; num_cols = Tensor(np.array(3), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; padding_value = Tensor(np.array(11), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.matrix_diag(x, k, num_rows, num_cols, padding_value, align=&#39;LEFT_RIGHT&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 1.  8. 11.]</span>
<span class="sd">         [ 4.  2.  9.]</span>
<span class="sd">         [11.  5.  3.]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (3, 3)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">cast_</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_rows</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_rows</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="n">num_rows</span> <span class="o">=</span> <span class="n">cast_</span><span class="p">(</span><span class="n">num_rows</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_cols</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_cols</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="n">num_cols</span> <span class="o">=</span> <span class="n">cast_</span><span class="p">(</span><span class="n">num_cols</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding_value</span><span class="p">,</span> <span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">))</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding_value</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="n">padding_value</span> <span class="o">=</span> <span class="n">cast_</span><span class="p">(</span><span class="n">padding_value</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">matrix_diag_v3</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">MatrixDiagV3</span><span class="p">)(</span><span class="n">align</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">matrix_diag_v3</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">num_rows</span><span class="p">,</span> <span class="n">num_cols</span><span class="p">,</span> <span class="n">padding_value</span><span class="p">)</span></div>


<div class="viewcode-block" id="matrix_diag_part"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.matrix_diag_part.html#mindspore.ops.matrix_diag_part">[文档]</a><span class="k">def</span> <span class="nf">matrix_diag_part</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s2">&quot;RIGHT_LEFT&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the diagonal part of input tensor.</span>
<span class="sd">    Returns a tensor with the k[0]-th to k[1]-th diagonals of `x`. Some diagonals are shorter than</span>
<span class="sd">    max_diag_len and need to be padded. Input k and padding_value must be const Tensor when taking Graph mode.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input Tensor with rank r, where r &gt;= 2.</span>
<span class="sd">        k (Union[int, Tensor], optional): A Tensor of type int32. Diagonal offset(s). Positive value means</span>
<span class="sd">            superdiagonal, 0 refers to the main diagonal, and negative value means subdiagonals. k can be</span>
<span class="sd">            a single integer (for a single diagonal) or a pair of integers specifying the low and high ends</span>
<span class="sd">            of a matrix band. k[0] must not be larger than k[1]. The value of k has restructions, meaning</span>
<span class="sd">            value of k must be in (-x.shape[-2], x.shape[-1]).</span>
<span class="sd">        padding_value (Union[int, float, Tensor], optional): A Tensor with only one value. Have the same dtype as x.</span>
<span class="sd">            The number to fill the area outside the specified diagonal band. Default: 0.</span>
<span class="sd">        align (str, optional): An optional string from: &quot;RIGHT_LEFT&quot;(default), &quot;LEFT_RIGHT&quot;, &quot;LEFT_LEFT&quot;,</span>
<span class="sd">            &quot;RIGHT_RIGHT&quot;. Align is a string specifying how superdiagonals and subdiagonals should be aligned,</span>
<span class="sd">            respectively. &quot;RIGHT_LEFT&quot; aligns superdiagonals to the right (left-pads the row) and subdiagonals</span>
<span class="sd">            to the left (right-pads the row).</span>

<span class="sd">    Returns:</span>
<span class="sd">        A Tensor. Has the same type as `x`.</span>
<span class="sd">        Assume `x` has r dimensions :math:`[I, J, ..., L, M, N]`. Let `max_diag_len` be the maximum length among all</span>
<span class="sd">        diagonals to be extracted, :math:`max\_diag\_len = min(M + min(k[1], 0), N + min(-k[0], 0))`</span>
<span class="sd">        Let `num_diags` be the number of diagonals to extract, :math:`num\_diags = k[1] - k[0] + 1`.</span>
<span class="sd">        If :math:`num\_diags == 1`, the output tensor is of rank r - 1 with shape :math:`[I, J, ..., L, max\_diag\_len]`</span>
<span class="sd">        Otherwise, the output tensor has rank r with dimensions :math:`[I, J, ..., L, num\_diags, max\_diag\_len]`</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not Tensor.</span>
<span class="sd">        TypeError: If input `x` and `padding_value` are not the same dtype.</span>
<span class="sd">        TypeError: If `k` is not int32 dtype.</span>
<span class="sd">        ValueError: If `align` is not a string or not in the valid range.</span>
<span class="sd">        ValueError: If rank of `k` is not equal to 0 or 1.</span>
<span class="sd">        ValueError: If rank of `padding_value` is not equal to 0.</span>
<span class="sd">        ValueError: If rank of `x` is not greater equal to 2.</span>
<span class="sd">        ValueError: If size of `k` is not equal to 1 or 2.</span>
<span class="sd">        ValueError: If k[1] is not greater equal to k[0] in case the size of `k` is 2.</span>
<span class="sd">        ValueError: If the value of `k` is not in (-x.shape[-2], x.shape[-1]).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3, 4],</span>
<span class="sd">        ...                      [5, 6, 7, 8],</span>
<span class="sd">        ...                      [9, 8, 7, 6]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; k =Tensor(np.array([1, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; padding_value = Tensor(np.array(9), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.matrix_diag_part(x, k, padding_value, align=&#39;RIGHT_LEFT&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[9. 9. 4.]</span>
<span class="sd">         [9. 3. 8.]</span>
<span class="sd">         [2. 7. 6.]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (3, 3)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">matrix_diag_part_v3</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">MatrixDiagPartV3</span><span class="p">)(</span><span class="n">align</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">matrix_diag_part_v3</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">padding_value</span><span class="p">)</span></div>


<div class="viewcode-block" id="matrix_set_diag"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.matrix_set_diag.html#mindspore.ops.matrix_set_diag">[文档]</a><span class="k">def</span> <span class="nf">matrix_set_diag</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">diagonal</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s2">&quot;RIGHT_LEFT&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a batched matrix tensor with new batched diagonal values.</span>
<span class="sd">    Given x and diagonal, this operation returns a tensor with the same shape and values as x, except for the specified</span>
<span class="sd">    diagonals of the innermost matrices. These will be overwritten by the values in diagonal. Some diagonals are shorter</span>
<span class="sd">    than max_diag_len and need to be padded.</span>
<span class="sd">    The diagonal.shape[-2] must be equal to num_diags calculated by k[1] - k[0] + 1. The diagonal.shape[-1] must be</span>
<span class="sd">    equal to the longest diagonal value max_diag_len calculated by min(x.shape[-2] + min(k[1], 0), x.shape[-1] +</span>
<span class="sd">    min(-k[0], 0)). Let x have r + 1 dimensions [I, J, ..., L, M, N]. The diagonal tensor has rank r with shape [I, J,</span>
<span class="sd">    ..., L, max_diag_len] when k is an integer or k[0] == k[1]. Otherwise, it has rank r + 1 with shape [I, J, ..., L,</span>
<span class="sd">    num_diags, max_diag_len].</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Rank r + 1, where r &gt;= 1.</span>
<span class="sd">        diagonal (Tensor): A Tensor. Have the same dtype as x. Rank r when k is an integer or k[0] == k[1].</span>
<span class="sd">            Otherwise, it has rank r + 1.</span>
<span class="sd">        k (Union[int, Tensor], optional): A int32 Scalar or int32 Tensor. Diagonal offset(s). Positive value means</span>
<span class="sd">            superdiagonal, 0 refers to the main diagonal, and negative value means subdiagonals. k can be a</span>
<span class="sd">            single integer (for a single diagonal) or a pair of integers specifying the low and high ends of</span>
<span class="sd">            a matrix band. k[0] must not be larger than k[1].</span>
<span class="sd">            The alue of k has restructions, meaning value of k must be in (-x.shape[-2], x.shape[-1]).</span>
<span class="sd">            Input k must be const Tensor when taking Graph mode.</span>
<span class="sd">        align (str, optional): An optional string from: &quot;RIGHT_LEFT&quot;(default), &quot;LEFT_RIGHT&quot;, &quot;LEFT_LEFT&quot;,</span>
<span class="sd">            &quot;RIGHT_RIGHT&quot;. Align is a string specifying how superdiagonals and subdiagonals should be aligned,</span>
<span class="sd">            respectively. &quot;RIGHT_LEFT&quot; aligns superdiagonals to the right (left-pads the row) and subdiagonals</span>
<span class="sd">            to the left (right-pads the row).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, The same type as x. Let x has r+1 dimensions [I, J, ..., L, M, N].</span>
<span class="sd">        The output is a tensor of rank r+1 with dimensions [I, J, ..., L, M, N], the same as input x.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If input `x` or `diagonal` is not Tensor.</span>
<span class="sd">        TypeError: If input `x` and `diagonal` are not the same dtype.</span>
<span class="sd">        TypeError: If `k` is not int32 dtype.</span>
<span class="sd">        ValueError: If `align` is not a string or not in the valid range.</span>
<span class="sd">        ValueError: If rank of `k` is not equal to 0 or 1.</span>
<span class="sd">        ValueError: If rank of `x` is not greater equal to 2.</span>
<span class="sd">        ValueError: If size of `k` is not equal to 1 or 2.</span>
<span class="sd">        ValueError: If k[1] is not greater equal to k[0] in case the size of `k` is 2.</span>
<span class="sd">        ValueError: If the `diagonal` rank size don&#39;t match with input `x` rank size.</span>
<span class="sd">        ValueError: If the `diagonal` shape value don&#39;t match with input `x` shape value.</span>
<span class="sd">        ValueError: If the diagonal.shape[-2] is not equal to num_diags calculated by k[1] - k[0] + 1.</span>
<span class="sd">        ValueError: If the value of `k` is not in (-x.shape[-2], x.shape[-1]).</span>
<span class="sd">        ValueError: If the diagonal.shape[-1] is not equal to the max_diag_len calculated by min(x.shape[-2] + min(k[1],</span>
<span class="sd">            0), x.shape[-1] + min(-k[0], 0)).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[7, 7, 7, 7],</span>
<span class="sd">        ...                      [7, 7, 7, 7],</span>
<span class="sd">        ...                      [7, 7, 7, 7]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; diagonal = Tensor(np.array([[0, 9, 1],</span>
<span class="sd">        ...                             [6, 5, 8],</span>
<span class="sd">        ...                             [1, 2, 3],</span>
<span class="sd">        ...                             [4, 5, 0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; k = Tensor(np.array([-1, 2]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; align = &#39;RIGHT_LEFT&#39;</span>
<span class="sd">        &gt;&gt;&gt; output = ops.matrix_set_diag(x, diagonal, k, align)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 6. 9. 7.]</span>
<span class="sd">         [4. 2. 5. 1.]</span>
<span class="sd">         [7. 5. 3. 8.]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (3, 4)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">matrix_set_diag_v3_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">MatrixSetDiagV3</span><span class="p">)(</span><span class="n">align</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">cast_</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">matrix_set_diag_v3_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">diagonal</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span></div>


<div class="viewcode-block" id="meshgrid"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.meshgrid.html#mindspore.ops.meshgrid">[文档]</a><span class="k">def</span> <span class="nf">meshgrid</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">indexing</span><span class="o">=</span><span class="s1">&#39;xy&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates coordinate matrices from given coordinate tensors.</span>

<span class="sd">    Given N one-dimensional coordinate tensors, returns a tuple outputs of N N-D</span>
<span class="sd">    coordinate tensors for evaluating expressions on an N-D grid.</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs (Union[tuple]): A Tuple of N 1-D Tensor objects.</span>
<span class="sd">            The length of input should be greater than 1. The data type is Number.</span>
<span class="sd">        indexing (&#39;xy&#39;, &#39;ij&#39;, optional): Cartesian (&#39;xy&#39;, default) or</span>
<span class="sd">            matrix (&#39;ij&#39;) indexing of output. In the 2-D case with</span>
<span class="sd">            inputs of length `M` and `N`, the outputs are of shape `(N, M)`</span>
<span class="sd">            for &#39;xy&#39; indexing and `(M, N)` for &#39;ij&#39; indexing. In the 3-D</span>
<span class="sd">            case with inputs of length `M`, `N` and `P`, outputs are of shape</span>
<span class="sd">            `(N, M, P)` for &#39;xy&#39; indexing and `(M, N, P)` for &#39;ij&#39; indexing.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensors, a Tuple of N N-D Tensor objects. The data type is the same with the Inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `indexing` is not a str or `inputs` is not a tuple.</span>
<span class="sd">        ValueError: If `indexing` is neither &#39;xy&#39; nor &#39;ij&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3, 4]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([5, 6, 7]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; z = Tensor(np.array([8, 9, 0, 1, 2]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; inputs = (x, y, z)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.meshgrid(inputs, indexing=&#39;xy&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[3, 4, 5], dtype=Int32, value=</span>
<span class="sd">         [[[1, 1, 1, 1, 1],</span>
<span class="sd">           [2, 2, 2, 2, 2],</span>
<span class="sd">           [3, 3, 3, 3, 3],</span>
<span class="sd">           [4, 4, 4, 4, 4]],</span>
<span class="sd">          [[1, 1, 1, 1, 1],</span>
<span class="sd">           [2, 2, 2, 2, 2],</span>
<span class="sd">           [3, 3, 3, 3, 3],</span>
<span class="sd">           [4, 4, 4, 4, 4]],</span>
<span class="sd">          [[1, 1, 1, 1, 1],</span>
<span class="sd">           [2, 2, 2, 2, 2],</span>
<span class="sd">           [3, 3, 3, 3, 3],</span>
<span class="sd">           [4, 4, 4, 4, 4]]]),</span>
<span class="sd">         Tensor(shape=[3, 4, 5], dtype=Int32, value=</span>
<span class="sd">         [[[5, 5, 5, 5, 5],</span>
<span class="sd">           [5, 5, 5, 5, 5],</span>
<span class="sd">           [5, 5, 5, 5, 5],</span>
<span class="sd">           [5, 5, 5, 5, 5]],</span>
<span class="sd">          [[6, 6, 6, 6, 6],</span>
<span class="sd">           [6, 6, 6, 6, 6],</span>
<span class="sd">           [6, 6, 6, 6, 6],</span>
<span class="sd">           [6, 6, 6, 6, 6]],</span>
<span class="sd">          [[7, 7, 7, 7, 7],</span>
<span class="sd">           [7, 7, 7, 7, 7],</span>
<span class="sd">           [7, 7, 7, 7, 7],</span>
<span class="sd">           [7, 7, 7, 7, 7]]]),</span>
<span class="sd">         Tensor(shape=[3, 4, 5], dtype=Int32, value=</span>
<span class="sd">         [[[8, 9, 0, 1, 2],</span>
<span class="sd">           [8, 9, 0, 1, 2],</span>
<span class="sd">           [8, 9, 0, 1, 2],</span>
<span class="sd">           [8, 9, 0, 1, 2]],</span>
<span class="sd">          [[8, 9, 0, 1, 2],</span>
<span class="sd">           [8, 9, 0, 1, 2],</span>
<span class="sd">           [8, 9, 0, 1, 2],</span>
<span class="sd">           [8, 9, 0, 1, 2]],</span>
<span class="sd">          [[8, 9, 0, 1, 2],</span>
<span class="sd">           [8, 9, 0, 1, 2],</span>
<span class="sd">           [8, 9, 0, 1, 2],</span>
<span class="sd">           [8, 9, 0, 1, 2]]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">meshgrid_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Meshgrid</span><span class="p">)(</span><span class="n">indexing</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">meshgrid_op</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">affine_grid</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta (Tensor): The input tensor whose dtype is float16, float32.</span>
<span class="sd">            Input batch of affine matrices with shape [N, 2, 3] for 2D grid or [N, 3, 4] for 3D grid.</span>
<span class="sd">        output_size (tuple[int]): The target output image size.</span>
<span class="sd">            The value of target output with format [N, C, H, W] for 2D grid or [N, C, D, H, W] for 3D grid.</span>
<span class="sd">        align_corners (bool): If True, consider -1 and 1 to refer to the centers of the corner pixels rather</span>
<span class="sd">                              than the image corners. The default value is False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, a tensor whose data type is same as &#39;theta&#39;, and the shape is [N, H, W, 2] for 2D grid</span>
<span class="sd">        or [N, D, H, W, 3] for 3D grid.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `theta` is not a Tensor or `output_size` is not a tuple.</span>
<span class="sd">        ValueError: If the shape of `theta` is not [N, 2, 3] or [N, 3, 4].</span>
<span class="sd">        ValueError: If the size of `output_size` is not 4 or 5.</span>
<span class="sd">        ValueError: If the shape of `theta` is [N, 2, 3], the size of `output_size` is not 4;</span>
<span class="sd">                    If the shape of `theta` is [N, 3, 4], the size of `output_size` is not 5.</span>
<span class="sd">        ValueError: If the output_size[0] is not equal to the shape[0] of theta.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; theta = Tensor([[[0.8, 0.5, 0],[-0.5, 0.8, 0]]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; out_size = (1, 3, 2, 3)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.affine_grid(theta, out_size, False)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[-0.78333336 -0.06666666]</span>
<span class="sd">        [-0.25       -0.4       ]</span>
<span class="sd">        [ 0.28333336 -0.73333335]]</span>
<span class="sd">        [[-0.28333336  0.73333335]</span>
<span class="sd">        [ 0.25        0.4       ]</span>
<span class="sd">        [ 0.78333336  0.06666666]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">affine_grid_op</span> <span class="o">=</span> <span class="n">AffineGrid</span><span class="p">(</span><span class="n">align_corners</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">affine_grid_op</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>


<div class="viewcode-block" id="broadcast_to"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.broadcast_to.html#mindspore.ops.broadcast_to">[文档]</a><span class="k">def</span> <span class="nf">broadcast_to</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Broadcasts input tensor to a given shape. The dim of input shape must be smaller</span>
<span class="sd">    than or equal to that of target shape, suppose input shape :math:`(x1, x2, ..., xm)`,</span>
<span class="sd">    target shape :math:`(*, y_1, y_2, ..., y_m)`. The broadcast rules are as follows:</span>

<span class="sd">    Compare the value of `x_m` and `y_m`, `x_{m-1}` and `y_{m-1}`, ..., `x_1` and `y_1` consecutively and</span>
<span class="sd">    decide whether these shapes are broadcastable and what the broadcast result is.</span>

<span class="sd">    If the value pairs at a specific dim are equal, then that value goes right into that dim of output shape.</span>
<span class="sd">    With an input shape :math:`(2, 3)`, target shape :math:`(2, 3)` , the inferred outpyt shape is :math:`(2, 3)`.</span>

<span class="sd">    If the value pairs are unequal, there are three cases:</span>

<span class="sd">    Case 1: Value of target shape is -1, then the value of the output shape is that of the input shape&#39;s.</span>
<span class="sd">    With an input shape :math:`(3, 3)`, target shape :math:`(-1, 3)`, the output shape is :math:`(3, 3)`.</span>

<span class="sd">    Case 2: Value of target shape is not -1 but the value ot the input shape is 1, then the value of the output shape</span>
<span class="sd">    is that of the target shape&#39;s. With an input shape :math:`(1, 3)`, target</span>
<span class="sd">    shape :math:`(8, 3)`, the output shape is :math:`(8, 3)`.</span>

<span class="sd">    Case 3: All other cases mean that the two shapes are not broadcastable.</span>

<span class="sd">    So far we got the last m dims of the outshape, now focus on the first :math:`*` dims, there are</span>
<span class="sd">    two cases:</span>

<span class="sd">    If the first :math:`*` dims of output shape does not have -1 in it, then fill the input</span>
<span class="sd">    shape with ones until their length are the same, and then refer to</span>
<span class="sd">    Case 2 mentioned above to calculate the output shape. With target shape :math:` (3, 1, 4, 1, 5, 9)`,</span>
<span class="sd">    input shape :math:`(1, 5, 9)`, the filled input shape will be :math:`(1, 1, 1, 1, 5, 9)` and thus the</span>
<span class="sd">    output shape is :math:` (3, 1, 4, 1, 5, 9)`.</span>

<span class="sd">    If the first :math:`*` dims of output shape have -1 in it, it implies this -1 is conrresponding to</span>
<span class="sd">    a non-existing dim so they&#39;re not broadcastable. With target shape :math:` (3, -1, 4, 1, 5, 9)`,</span>
<span class="sd">    input shape :math:`(1, 5, 9)`, instead of operating the dim-filling process first, it raises errors directly.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor. The data type should be one of the following types:</span>
<span class="sd">                    float16, float32, int32, int8, uint8, bool.</span>
<span class="sd">                    The shape is :math:`(N,*)` where :math:`*` means,any number of additional dimensions.</span>
<span class="sd">        shape (tuple): The target shape to broadcast. Can be fully specified, or have -1 in one position</span>
<span class="sd">                       where it will be substituted by the input tensor&#39;s shape in that position, see example.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the given `shape` and the same data type as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `shape` is not a tuple.</span>
<span class="sd">        ValueError: If the target and input shapes are incompatible, or if a - 1 in the target shape is in an invalid</span>
<span class="sd">                    location.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops.function import broadcast_to</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; shape = (2, 3)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = broadcast_to(x, shape)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 2. 3.]</span>
<span class="sd">         [1. 2. 3.]]</span>
<span class="sd">        &gt;&gt;&gt; shape = (-1, 2)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1], [2]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = broadcast_to(x, shape)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 1.]</span>
<span class="sd">         [2. 2.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_broadcast_to</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">BroadcastTo</span><span class="p">)(</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_broadcast_to</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="unsorted_segment_min"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.unsorted_segment_min.html#mindspore.ops.unsorted_segment_min">[文档]</a><span class="k">def</span> <span class="nf">unsorted_segment_min</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the minimum of a tensor along segments.</span>

<span class="sd">    The following figure shows the calculation process of unsorted_segment_min:</span>

<span class="sd">    .. image:: UnsortedSegmentMin.png</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text { output }_i=\text{min}_{j \ldots} \text { data }[j \ldots]</span>

<span class="sd">    where :math:`min` over tuples :math:`j...` such that :math:`segment_ids[j...] == i`.</span>

<span class="sd">    Note:</span>
<span class="sd">        - If the segment_id i is absent in the segment_ids, then output[i] will be filled with</span>
<span class="sd">          the maximum value of the x&#39;s type.</span>
<span class="sd">        - The `segment_ids` must be non-negative tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The shape is :math:`(x_1, x_2, ..., x_R)`. With float16, float32 or int32 data type.</span>
<span class="sd">        segment_ids (Tensor): A `1-D` tensor whose shape is :math:`(x_1)`,</span>
<span class="sd">                              the value must be non-negative tensor. The data type must be int32.</span>
<span class="sd">        num_segments (int): The value specifies the number of distinct `segment_ids`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, set the number of `num_segments` as `N`, the shape is :math:`(N, x_2, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `num_segments` is not an int.</span>
<span class="sd">        ValueError: If length of shape of `segment_ids` is not equal to 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [4, 2, 1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; segment_ids = Tensor(np.array([0, 1, 1]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; num_segments = 2</span>
<span class="sd">        &gt;&gt;&gt; output = ops.unsorted_segment_min(x, segment_ids, num_segments)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 2. 3.]</span>
<span class="sd">         [4. 2. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">unsorted_segment_min_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">UnsortedSegmentMin</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">unsorted_segment_min_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">)</span></div>


<div class="viewcode-block" id="unsorted_segment_max"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.unsorted_segment_max.html#mindspore.ops.unsorted_segment_max">[文档]</a><span class="k">def</span> <span class="nf">unsorted_segment_max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the maximum along segments of a tensor.</span>

<span class="sd">    The following figure shows the calculation process of unsorted_segment_max:</span>

<span class="sd">    .. image:: UnsortedSegmentMax.png</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text { output }_i=\text{max}_{j \ldots} \text { data }[j \ldots]</span>

<span class="sd">    where :math:`max` over tuples :math:`j...` such that :math:`segment\_ids[j...] == i`.</span>

<span class="sd">    Note:</span>
<span class="sd">        - If the segment_id i is absent in the segment_ids, then output[i] will be filled with</span>
<span class="sd">          the minimum value of the x&#39;s type.</span>
<span class="sd">        - The `segment_ids` must be non-negative tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The shape is :math:`(x_1, x_2, ..., x_R)`. With float16, float32 or int32 data type.</span>
<span class="sd">        segment_ids (Tensor): A `1-D` tensor whose shape is :math:`(x_1)`,</span>
<span class="sd">                              the value must be non-negative tensor. The data type must be int32.</span>
<span class="sd">        num_segments (int): The value specifies the number of distinct `segment_ids`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, set the number of `num_segments` as `N`, the shape is :math:`(N, x_2, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `num_segments` is not an int.</span>
<span class="sd">        ValueError: If length of shape of `segment_ids` is not equal to 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [4, 2, 1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; segment_ids = Tensor(np.array([0, 1, 1]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; num_segments = 2</span>
<span class="sd">        &gt;&gt;&gt; output = ops.unsorted_segment_max(x, segment_ids, num_segments)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 2. 3.]</span>
<span class="sd">         [4. 5. 6.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">unsorted_segment_max_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">UnsortedSegmentMax</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">unsorted_segment_max_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">)</span></div>


<div class="viewcode-block" id="unsorted_segment_prod"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.unsorted_segment_prod.html#mindspore.ops.unsorted_segment_prod">[文档]</a><span class="k">def</span> <span class="nf">unsorted_segment_prod</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the product of a tensor along segments.</span>

<span class="sd">    The following figure shows the calculation process of UnsortedSegmentProd:</span>

<span class="sd">    .. image:: UnsortedSegmentProd.png</span>

<span class="sd">    Note:</span>
<span class="sd">        - If the segment_id i is absent in the segment_ids, then output[i] will be filled with 1.</span>
<span class="sd">        - The `segment_ids` must be non-negative tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The shape is :math:`(x_1, x_2, ..., x_R)`. With float16, float32 or int32 data type.</span>
<span class="sd">        segment_ids (Tensor): A `1-D` tensor whose shape is :math:`(x_1)`,</span>
<span class="sd">                              the value must be non-negative tensor. The data type must be int32.</span>
<span class="sd">        num_segments (int): The value specifies the number of distinct `segment_ids`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, set the number of `num_segments` as `N`, the shape is :math:`(N, x_2, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `num_segments` is not an int.</span>
<span class="sd">        ValueError: If length of shape of `segment_ids` is not equal to 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [4, 2, 1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; segment_ids = Tensor(np.array([0, 1, 0]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; num_segments = 2</span>
<span class="sd">        &gt;&gt;&gt; output = ops.unsorted_segment_prod(x, segment_ids, num_segments)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[4. 4. 3.]</span>
<span class="sd">         [4. 5. 6.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">unsorted_segment_prod_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">UnsortedSegmentProd</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">unsorted_segment_prod_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">)</span></div>


<div class="viewcode-block" id="adaptive_max_pool2d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.adaptive_max_pool2d.html#mindspore.ops.adaptive_max_pool2d">[文档]</a><span class="k">def</span> <span class="nf">adaptive_max_pool2d</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    adaptive_max_pool2d operation.</span>

<span class="sd">    This operator applies a 2D adaptive max pooling to an input signal composed of multiple input planes.</span>
<span class="sd">    That is, for any input size, the size of the specified output is H x W.</span>
<span class="sd">    The number of output features is equal to the number of input planes.</span>

<span class="sd">    The input and output data format can be &quot;NCHW&quot; and &quot;CHW&quot;. N is the batch size, C is the number of channels,</span>
<span class="sd">    H is the feature height, and W is the feature width.</span>

<span class="sd">    .. math::</span>

<span class="sd">        \begin{align}</span>
<span class="sd">        h_{start} &amp;= floor(i * H_{in} / H_{out})\\</span>
<span class="sd">        h_{end} &amp;= ceil((i + 1) * H_{in} / H_{out})\\</span>
<span class="sd">        w_{start} &amp;= floor(j * W_{in} / W_{out})\\</span>
<span class="sd">        w_{end} &amp;= ceil((j + 1) * W_{in} / W_{out})\\</span>
<span class="sd">        Output(i,j) &amp;= {\max Input[h_{start}:h_{end}, w_{start}:w_{end}]}</span>
<span class="sd">        \end{align}</span>

<span class="sd">    Note:</span>
<span class="sd">        Ascend platform only supports float16 type for input_x.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The input of adaptive_max_pool2d, which is a 3D or 4D tensor,</span>
<span class="sd">            with float16, float32 or float64 data type.</span>

<span class="sd">        output_size (Union[int, tuple]): The target output size is H x W.</span>
<span class="sd">            ouput_size can be a tuple, or a single H for H x H, and H and W can be int or None</span>
<span class="sd">            which means the output size is the same as the input.</span>

<span class="sd">        return_indices (bool): If `return_indices` is True, the indices of max value would be output.</span>
<span class="sd">            Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type as the `input_x`.</span>

<span class="sd">        Shape of the output is `input_x_shape[:len(input_x_shape) - len(out_shape)] + out_shape`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `output_size` is not int or tuple.</span>
<span class="sd">        TypeError: If `input_x` is not a tensor.</span>
<span class="sd">        TypeError: If `return_indices` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `input_x` is not float16, float32 or float64.</span>
<span class="sd">        ValueError: If `output_size` is a tuple and the length of `output_size` is not 2.</span>
<span class="sd">        ValueError: If the dimension of `input_x` is not NCHW or CHW.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # case 1: output_size=(None, 2)</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]],</span>
<span class="sd">        ...                             [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]],</span>
<span class="sd">        ...                             [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.adaptive_max_pool2d(input_x, (None, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[2. 3.]</span>
<span class="sd">           [5. 6.]</span>
<span class="sd">           [8. 9.]]</span>
<span class="sd">          [[2. 3.]</span>
<span class="sd">           [5. 6.]</span>
<span class="sd">           [8. 9.]]</span>
<span class="sd">          [[2. 3.]</span>
<span class="sd">           [5. 6.]</span>
<span class="sd">           [8. 9.]]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 2: output_size=2</span>
<span class="sd">        &gt;&gt;&gt; output = ops.adaptive_max_pool2d(input_x, 2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[5. 6.]</span>
<span class="sd">           [8. 9.]]</span>
<span class="sd">          [[5. 6.]</span>
<span class="sd">           [8. 9.]]</span>
<span class="sd">          [[5. 6.]</span>
<span class="sd">           [8. 9.]]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: output_size=(1, 2)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.adaptive_max_pool2d(input_x, (1, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[8. 9.]]</span>
<span class="sd">          [[8. 9.]]</span>
<span class="sd">          [[8. 9.]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_adaptive_max_pool2d</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">AdaptiveMaxPool2D</span><span class="p">)(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">return_indices</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_adaptive_max_pool2d</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="index_fill"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.index_fill.html#mindspore.ops.index_fill">[文档]</a><span class="k">def</span> <span class="nf">index_fill</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fills the elements under the `dim` dimension of the input Tensor `x` with the input `value`</span>
<span class="sd">    by selecting the indices in the order given in `index`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input Tensor.  The supported data type is Number or Bool.</span>
<span class="sd">        dim (Union[int, Tensor]): Dimension along which to fill the input Tensor. Only supports</span>
<span class="sd">            an int number or a 0-dimensional Tensor, whose data type is int32 or int64.</span>
<span class="sd">        index (Tensor): Indices of the input Tensor to fill in. The dtype must be int32.</span>
<span class="sd">        value (Union[bool, int, float, Tensor]): Value to fill the returned Tensor. If `value` is</span>
<span class="sd">            a Tensor, it must be a 0-dimensional Tensor and has the same dtype as `x`. Otherwise,</span>
<span class="sd">            the `value` will be cast to a 0-dimensional Tensor with the same data type as `x`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype and shape as input Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `dim` is neither int number nor Tensor.</span>
<span class="sd">        TypeError: When `dim` is a Tensor, its dtype is not int32 or int64.</span>
<span class="sd">        TypeError: If `index` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `index` is not int32.</span>
<span class="sd">        TypeError: If `value` is not a bool, int, float, or Tensor.</span>
<span class="sd">        TypeError: When `value` is a Tensor, the dtype of `x` and `value` are not the same.</span>
<span class="sd">        ValueError: If `dim` is a Tensor and its rank is not equal to 0.</span>
<span class="sd">        ValueError: If the rank of `index` is greater than 1D.</span>
<span class="sd">        ValueError: When `value` is a Tensor and its rank is not equal to 0.</span>
<span class="sd">        RuntimeError: If the value of `dim` is out the range of `[-x.ndim, x.ndim - 1]`.</span>
<span class="sd">        RuntimeError: If the values of `index` are out the range of `[-x.shape[dim], x.shape[dim]-1]`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; index = Tensor([0, 2], mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; value = Tensor(-2.0, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = ops.index_fill(x, 1, index, value)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[-2. 2. -2.]</span>
<span class="sd">         [-2. 5. -2.]</span>
<span class="sd">         [-2. 8. -2.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="n">cast_</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="p">(</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">)):</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">cast_</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">index_fill_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span></div>


<div class="viewcode-block" id="population_count"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.population_count.html#mindspore.ops.population_count">[文档]</a><span class="k">def</span> <span class="nf">population_count</span><span class="p">(</span><span class="n">input_x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes element-wise population count(a.k.a bitsum, bitcount).</span>
<span class="sd">    For each entry in `input_x`, calculates the number of 1 bits in the binary representation of that entry.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Tensor of any dimension. The data type must be int16 or uint16 (Ascend).</span>
<span class="sd">            The data type must be int8, int16, int32, int64, uint8, uint16, uint32, uint64 (CPU and GPU).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same shape as the input, and the data type is uint8.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input_x` is not int16, uint16 (Ascend).</span>
<span class="sd">        TypeError: If dtype of `input_x` is not int8, int16, int32, int64, uint8, uint16, uint32, uint64 (CPU and GPU).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor([0, 1, 3], mindspore.int16)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.population_count(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0 1 2]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">population_count_</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<span class="c1">##############################</span>
<span class="c1"># Type Conversion Functions.</span>
<span class="c1">##############################</span>


<div class="viewcode-block" id="scalar_cast"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.scalar_cast.html#mindspore.ops.scalar_cast">[文档]</a><span class="k">def</span> <span class="nf">scalar_cast</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Casts the input scalar to another type.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (scalar): The input scalar. Only constant value is allowed.</span>
<span class="sd">        input_y (mindspore.dtype): The type to be cast. Only constant value is allowed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Scalar. The type is the same as the python type corresponding to `input_y`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `input_x` nor `input_y` is a constant value.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scalar_cast(255.0, mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        255</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">scalar_cast_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span></div>


<div class="viewcode-block" id="tensor_scatter_mul"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.tensor_scatter_mul.html#mindspore.ops.tensor_scatter_mul">[文档]</a><span class="k">def</span> <span class="nf">tensor_scatter_mul</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a new tensor by multiplying the values from the positions in `input_x` indicated by</span>
<span class="sd">    `indices`, with values from `updates`. When divided values are provided for the same</span>
<span class="sd">    index, the result of the update will multiply these values respectively. Except that</span>
<span class="sd">    the updates are applied on output `Tensor` instead of input `Parameter`.</span>

<span class="sd">    The last axis of `indices` is the depth of each index vectors. For each index vector,</span>
<span class="sd">    there must be a corresponding value in `updates`. The shape of `updates` should be</span>
<span class="sd">    equal to the shape of `input_x[indices]`. For more details, see use cases.</span>

<span class="sd">    Note:</span>
<span class="sd">        - If some values of the `indices` are out of bound, instead of raising an index error,</span>
<span class="sd">          the corresponding `updates` will not be updated to `input_x`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The target tensor. The dimension of input_x must be no less than indices.shape[-1].</span>
<span class="sd">        indices (Tensor): The index of input tensor whose data type is int32 or int64. The rank must be at least 2.</span>
<span class="sd">        updates (Tensor): The tensor to update the input tensor, has the same type as input,</span>
<span class="sd">            and updates shape should be equal to indices.shape[:-1] + input_x.shape[indices.shape[-1]:].</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `indices` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If length of shape of `input_x` is less than the last dimension of shape of `indices`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 0], [0, 0]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([1.0, 2.2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; # Next, demonstrate the approximate operation process of this operator:</span>
<span class="sd">        &gt;&gt;&gt; # 1, indices[0] = [0, 0], indices[1] = [0, 0]</span>
<span class="sd">        &gt;&gt;&gt; # 2, And input_x[0, 0] = -0.1</span>
<span class="sd">        &gt;&gt;&gt; # 3, So input_x[indices] = [-0.1, -0.1]</span>
<span class="sd">        &gt;&gt;&gt; # 4, Satisfy the above formula: input_x[indices].shape=(2) == updates.shape=(2)</span>
<span class="sd">        &gt;&gt;&gt; # 5, Perform the multiply operation for the first time:</span>
<span class="sd">        &gt;&gt;&gt; #      first_input_x = input_x[0][0] * updates[0] = [[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]</span>
<span class="sd">        &gt;&gt;&gt; # 6, Perform the multiply operation for the second time:</span>
<span class="sd">        &gt;&gt;&gt; #      second_input_x = input_x[0][0] * updates[1] = [[-0.22, 0.3, 3.6], [0.4, 0.5, -3.2]]</span>
<span class="sd">        &gt;&gt;&gt; output = ops.tensor_scatter_mul(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-0.22  0.3   3.6  ]</span>
<span class="sd">         [ 0.4   0.5   -3.2 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_scatter_mul_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>


<div class="viewcode-block" id="tensor_scatter_div"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.tensor_scatter_div.html#mindspore.ops.tensor_scatter_div">[文档]</a><span class="k">def</span> <span class="nf">tensor_scatter_div</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a new tensor by dividing the values from the positions in `input_x` indicated by</span>
<span class="sd">    `indices`, with values from `updates`. When divided values are provided for the same</span>
<span class="sd">    index, the result of the update will be to divided these values respectively. Except that</span>
<span class="sd">    the updates are applied on output `Tensor` instead of input `Parameter`.</span>

<span class="sd">    The last axis of `indices` is the depth of each index vectors. For each index vector,</span>
<span class="sd">    there must be a corresponding value in `updates`. The shape of `updates` should be</span>
<span class="sd">    equal to the shape of `input_x[indices]`. For more details, see use cases.</span>

<span class="sd">    Note:</span>
<span class="sd">        - If some values of the `indices` are out of bound, instead of raising an index error,</span>
<span class="sd">          the corresponding `updates` will not be updated to `input_x`.</span>
<span class="sd">        - The operator can&#39;t handle division by 0 exceptions, so the user needs to make sure</span>
<span class="sd">          there is no 0 value in `updates`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The target tensor. The dimension of input_x must be no less than indices.shape[-1].</span>
<span class="sd">        indices (Tensor): The index of input tensor whose data type is int32 or int64.</span>
<span class="sd">            The rank must be at least 2.</span>
<span class="sd">        updates (Tensor): The tensor to update the input tensor, has the same type as input,</span>
<span class="sd">            and updates.shape should be equal to indices.shape[:-1] + input_x.shape[indices.shape[-1]:].</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `indices` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If length of shape of `input_x` is less than the last dimension of shape of `indices`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 0], [0, 0]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([1.0, 2.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.tensor_scatter_div(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-0.05  0.3  3.6  ]</span>
<span class="sd">         [ 0.4   0.5  -3.2 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_scatter_div_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>


<div class="viewcode-block" id="scalar_to_array"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.scalar_to_array.html#mindspore.ops.scalar_to_array">[文档]</a><span class="k">def</span> <span class="nf">scalar_to_array</span><span class="p">(</span><span class="n">input_x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts a scalar to a `Tensor`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Union[int, float]): The input is a scalar. Only constant value is allowed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor. 0-D Tensor and the content is the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is neither int nor float.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = 1.0</span>
<span class="sd">        &gt;&gt;&gt; print(type(input_x))</span>
<span class="sd">        &lt;class &#39;float&#39;&gt;</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scalar_to_array(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(type(output))</span>
<span class="sd">        &lt;class &#39;mindspore.common.tensor.Tensor&#39;&gt;</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        1.0</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">scalar_to_array_</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="scalar_to_tensor"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.scalar_to_tensor.html#mindspore.ops.scalar_to_tensor">[文档]</a><span class="k">def</span> <span class="nf">scalar_to_tensor</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts a scalar to a `Tensor`, and converts the data type to the specified type.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Union[int, float]): The input is a scalar. Only constant value is allowed.</span>
<span class="sd">        dtype (mindspore.dtype): The target data type. Default: mindspore.float32. Only</span>
<span class="sd">            constant value is allowed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor. 0-D Tensor and the content is the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is neither int nor float.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; data = 1</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scalar_to_tensor(data, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        1.0</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">scalar_to_tensor_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="tuple_to_array"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.tuple_to_array.html#mindspore.ops.tuple_to_array">[文档]</a><span class="k">def</span> <span class="nf">tuple_to_array</span><span class="p">(</span><span class="n">input_x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts a tuple to a tensor.</span>

<span class="sd">    If the type of the first number in the tuple is integer, the data type of the output tensor is int.</span>
<span class="sd">    Otherwise, the data type of the output tensor is float.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (tuple): A tuple of numbers. These numbers have the same type. Only constant value is allowed.</span>
<span class="sd">            The shape is :math:`(N,*)` where :math:`*` means,any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, if the input tuple contains `N` numbers, then the shape of the output tensor is (N,).</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a tuple.</span>
<span class="sd">        ValueError: If length of `input_x` is less than or equal to 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = (1,2,3)</span>
<span class="sd">        &gt;&gt;&gt; print(type(input_x))</span>
<span class="sd">        &lt;class &#39;tuple&#39;&gt;</span>
<span class="sd">        &gt;&gt;&gt; output = ops.tuple_to_array(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(type(output))</span>
<span class="sd">        &lt;class &#39;mindspore.common.tensor.Tensor&#39;&gt;</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1 2 3]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tuple_to_array_</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="masked_select"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.masked_select.html#mindspore.ops.masked_select">[文档]</a><span class="k">def</span> <span class="nf">masked_select</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a new 1-D Tensor which indexes the `x` tensor according to the boolean `mask`.</span>
<span class="sd">    The shapes of the `mask` tensor and the `x` tensor don&#39;t need to match, but they must be broadcastable.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">        mask (Tensor[bool]): The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A 1-D Tensor, with the same type as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` or `mask` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `mask` is not bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3, 4]), mindspore.int64)</span>
<span class="sd">        &gt;&gt;&gt; mask = Tensor(np.array([1, 0, 1, 0]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.masked_select(x, mask)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1 3]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">masked_select_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span></div>


<div class="viewcode-block" id="masked_fill"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.masked_fill.html#mindspore.ops.masked_fill">[文档]</a><span class="k">def</span> <span class="nf">masked_fill</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fills elements of Tensor with value where mask is True.</span>
<span class="sd">    The shapes of `input_x` and `mask` need to be the same or broadcastable.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The source Tensor whose data type is one of float16, float32, int8, int32.</span>
<span class="sd">        mask (Tensor[bool]): The boolean mask.</span>
<span class="sd">        value (Union[float, Tensor]): The value to fill in with, which dtype is the same as `input_x`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same type and shape as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `mask` is not bool.</span>
<span class="sd">        TypeError: If `input_x` or `mask` is not a Tensor.</span>
<span class="sd">        ValueError: If the shapes of `input_x` and `mask` could not be broadcast.</span>
<span class="sd">        TypeError: If dtype of `input_x` or `value` is not one of float16, float32, int8, int32.</span>
<span class="sd">        TypeError: If dtype of `value` is different from that of `input_x`.</span>
<span class="sd">        TypeError: If `value` is neither float number nor Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([1., 2., 3., 4.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; mask = Tensor(np.array([True, True, False, True]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.masked_fill(input, mask, 0.5)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.5 0.5 3.  0.5]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">scalar_to_tensor_</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">input_x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">masked_fill_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">MaskedFill</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">masked_fill_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span></div>


<div class="viewcode-block" id="diag"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.diag.html#mindspore.ops.diag">[文档]</a><span class="k">def</span> <span class="nf">diag</span><span class="p">(</span><span class="n">input_x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs a diagonal tensor with a given diagonal values.</span>

<span class="sd">    Assume `input_x` has dimensions :math:`[D_1,... D_k]`, the output is a tensor of</span>
<span class="sd">    rank 2k with dimensions :math:`[D_1,..., D_k, D_1,..., D_k]` where:</span>
<span class="sd">    :math:`output[i_1,..., i_k, i_1,..., i_k] = input_x[i_1,..., i_k]` and 0 everywhere else.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype as the `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        ValueError: If rank of `input_x` is less than 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor([1, 2, 3, 4]).astype(&#39;int32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.diag(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1 0 0 0]</span>
<span class="sd">         [0 2 0 0]</span>
<span class="sd">         [0 0 3 0]</span>
<span class="sd">         [0 0 0 4]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">diag_</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="col2im"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.col2im.html#mindspore.ops.col2im">[文档]</a><span class="k">def</span> <span class="nf">col2im</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">padding_value</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Combines an array of sliding local blocks into a large containing tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): 4D tensor with data type float16 or float.</span>
<span class="sd">        output_size (Tensor): 1D tensor with 2 elements of data type int.</span>
<span class="sd">        kernel_size (Union[int, tuple[int], list[int]]): The size of the kernel, should be two int</span>
<span class="sd">            for height and width. If type is int, it means that height equal with width. Must be specified.</span>
<span class="sd">        dilation (Union[int, tuple[int], list[int]]): The size of the dilation, should be two int</span>
<span class="sd">            for height and width. If type is int, it means that height equal with width. Default: 1.</span>
<span class="sd">        padding_value (Union[int, tuple[int], list[int]]): The size of the padding, should be two int</span>
<span class="sd">            for height and width. If type is int, it means that height equal with width. Default: 1.</span>
<span class="sd">        stride (Union[int, tuple[int], list[int]]): The size of the stride, should be two int</span>
<span class="sd">            for height and width. If type is int, it means that height equal with width. Default: 0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A 4D Tensor, with same type as &#39;input_x&#39;.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If :attr:`kernel_size`, `dilation`, `padding_value`, `stride` data type is not in</span>
<span class="sd">            Union[int, tuple[int], list[int]].</span>
<span class="sd">        ValueError: If :attr:`kernel_size`, `dilation`, `padding_value`, `stride` value is not</span>
<span class="sd">            greater than zero or elements number more than 2.</span>
<span class="sd">        ValueError: If :attr:`padding_value` value is less than zero or elements number more than 2.</span>
<span class="sd">        ValueError: If input_x.shape[2] != kernel_size[0] * kernel_size[1].</span>
<span class="sd">        ValueError: If input_x.shape[3] does not match the calculated number of sliding blocks.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(input_data=np.random.rand(16, 16, 4, 25), dtype=mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; output_size = Tensor(input_data=[8, 8], dtype=mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.col2im(x, output_size, [2, 2], [2, 2], [2, 2], [2, 2])</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (16, 16, 8, 8)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">c2i</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Col2Im</span><span class="p">)(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">padding_value</span><span class="p">,</span> <span class="n">stride</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">c2i</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span></div>


<div class="viewcode-block" id="split"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.split.html#mindspore.ops.split">[文档]</a><span class="k">def</span> <span class="nf">split</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">output_num</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Splits the input tensor into output_num of tensors along the given axis and output numbers.</span>

<span class="sd">    The `input_x` tensor will be split into equally sized sub-tensors.</span>
<span class="sd">    This requires that `input_x.shape(axis)` is divisible by `output_num`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">        axis (int): Index of the split position. Default: 0.</span>
<span class="sd">        output_num (int): The number of output tensors. Must be positive int. Default: 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        tuple[Tensor], the shape of each output tensor is the same, which is</span>
<span class="sd">        :math:`(y_1, y_2, ..., y_S)`. And the data type is the same with `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` or `output_num` is not an int.</span>
<span class="sd">        ValueError: If `axis` is out of the range [-len(`input_x.shape`), len(`input_x.shape`)),</span>
<span class="sd">            or if the `output_num` is less than or equal to 0.</span>
<span class="sd">        ValueError: If `input_x.shape(axis)` is not divisible by `output_num`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 1, 1, 1], [2, 2, 2, 2]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; print(x)</span>
<span class="sd">        [[1 1 1 1]</span>
<span class="sd">         [2 2 2 2]]</span>
<span class="sd">        &gt;&gt;&gt; output = ops.split(x, 1, 2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Int32, value=</span>
<span class="sd">        [[1, 1],</span>
<span class="sd">         [2, 2]]), Tensor(shape=[2, 2], dtype=Int32, value=</span>
<span class="sd">        [[1, 1],</span>
<span class="sd">         [2, 2]]))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.split(x, 1, 4)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 1], dtype=Int32, value=</span>
<span class="sd">        [[1],</span>
<span class="sd">         [2]]), Tensor(shape=[2, 1], dtype=Int32, value=</span>
<span class="sd">        [[1],</span>
<span class="sd">         [2]]), Tensor(shape=[2, 1], dtype=Int32, value=</span>
<span class="sd">        [[1],</span>
<span class="sd">         [2]]), Tensor(shape=[2, 1], dtype=Int32, value=</span>
<span class="sd">        [[1],</span>
<span class="sd">         [2]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">split_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Split</span><span class="p">)(</span><span class="n">axis</span><span class="p">,</span> <span class="n">output_num</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">split_</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="max"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.max.html#mindspore.ops.max">[文档]</a><span class="k">def</span> <span class="nf">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the maximum value with the corresponding index.</span>

<span class="sd">    Calculates the maximum value along with the given axis for the input tensor. It returns the maximum values and</span>
<span class="sd">    indices.</span>

<span class="sd">    Note:</span>
<span class="sd">        In auto_parallel and semi_auto_parallel mode, the first output index can not be used.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - If there are multiple maximum values, the index of the first maximum value is used.</span>
<span class="sd">        - The value range of &quot;axis&quot; is [-dims, dims - 1]. &quot;dims&quot; is the dimension length of &quot;x&quot;.</span>

<span class="sd">    Also see: class: `mindspore.ops.ArgMaxWithValue`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor, can be any dimension. Set the shape of input tensor as</span>
<span class="sd">          :math:`(x_1, x_2, ..., x_N)`.</span>
<span class="sd">        axis (int): The dimension to reduce. Default: 0.</span>
<span class="sd">        keep_dims (bool): Whether to reduce dimension, if true, the output will keep same dimension with the input,</span>
<span class="sd">                          the output will reduce dimension if false. Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        tuple (Tensor), tuple of 2 tensors, containing the corresponding index and the maximum value of the input</span>
<span class="sd">        tensor.</span>

<span class="sd">        - index (Tensor) - The index for the maximum value of the input tensor, with dtype int32. If `keep_dims`</span>
<span class="sd">          is true, the shape of output tensors is :math:`(x_1, x_2, ..., x_{axis-1}, 1, x_{axis+1}, ..., x_N)`.</span>
<span class="sd">          Otherwise, the shape is :math:`(x_1, x_2, ..., x_{axis-1}, x_{axis+1}, ..., x_N)` .</span>
<span class="sd">        - values (Tensor) - The maximum value of input tensor, with the same shape as index, and same dtype as x.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not Tensor.</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.0, 0.4, 0.6, 0.7, 0.1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; index, output = ops.max(x)</span>
<span class="sd">        &gt;&gt;&gt; print(index, output)</span>
<span class="sd">        3 0.7</span>
<span class="sd">        &gt;&gt;&gt; index, output = ops.max(x, keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; print(index, output)</span>
<span class="sd">        [3] [0.7]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">argmax_with_value_op</span> <span class="o">=</span> <span class="n">ArgMaxWithValue</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">argmax_with_value_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">argmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the indices of the maximum value of a tensor across the `axis`.</span>

<span class="sd">    If the shape of input tensor is :math:`(x_1, ..., x_N)`, the shape of the output tensor will be</span>
<span class="sd">    :math:`(x_1, ..., x_{axis-1}, x_{axis+1}, ..., x_N)`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input tensor. :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">          Support data type list as follows:</span>
<span class="sd">          Ascend: Float16, Float32.</span>
<span class="sd">          CPU: Float16, Float32, Float64.</span>
<span class="sd">          GPU: Float16, Float32.</span>
<span class="sd">        axis (int): Axis where the Argmax operation applies to. Default: -1.</span>
<span class="sd">        output_type (:class:`mindspore.dtype`): An optional data type of `mindspore.dtype.int32`.</span>
<span class="sd">          Default: `mindspore.dtype.int32`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, indices of the max value of input tensor across the `axis`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>
<span class="sd">        TypeError: If `output_type` is neither int32 nor int64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 20, 5], [67, 8, 9], [130, 24, 15]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.argmax(x, axis=-1, output_type=mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1 0 0]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">argmax_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Argmax</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">output_type</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">argmax_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<div class="viewcode-block" id="min"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.min.html#mindspore.ops.min">[文档]</a><span class="k">def</span> <span class="nf">min</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the minimum value with corresponding index, and returns indices and values.</span>

<span class="sd">    Calculates the minimum value along with the given axis for the input tensor. It returns the minimum values and</span>
<span class="sd">    indices.</span>

<span class="sd">    Note:</span>
<span class="sd">        In auto_parallel and semi_auto_parallel mode, the first output index can not be used.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - If there are multiple minimum values, the index of the first minimum value is used.</span>
<span class="sd">        - The value range of &quot;axis&quot; is [-dims, dims - 1]. &quot;dims&quot; is the dimension length of &quot;x&quot;.</span>

<span class="sd">    Also see: class: `mindspore.ops.ArgMinWithValue`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor, can be any dimension. Set the shape of input tensor as</span>
<span class="sd">          :math:`(x_1, x_2, ..., x_N)` . And the data type only support</span>
<span class="sd">          mindspore.uint16, mindspore.uint32, mindspore.int16, mindspore.int32, mindspore.float16, mindspore.float32.</span>
<span class="sd">        axis (int): The dimension to reduce. Default: 0.</span>
<span class="sd">        keep_dims (bool): Whether to reduce dimension, if true the output will keep the same dimension as the input,</span>
<span class="sd">                          the output will reduce dimension if false. Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        tuple (Tensor), tuple of 2 tensors, containing the corresponding index and the minimum value of the input</span>
<span class="sd">        tensor.</span>

<span class="sd">        - **index** (Tensor) - The index for the minimum value of the input tensor. If `keep_dims` is true, the shape of</span>
<span class="sd">          output tensors is :math:`(x_1, x_2, ..., x_{axis-1}, 1, x_{axis+1}, ..., x_N)`. Otherwise, the shape is</span>
<span class="sd">          :math:`(x_1, x_2, ..., x_{axis-1}, x_{axis+1}, ..., x_N)` .</span>
<span class="sd">        - **values** (Tensor) - The minimum value of input tensor, with the same shape as index.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If data type `x` is not uint16, uint32, int16, int32, float16, float32.</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.0, 0.4, 0.6, 0.7, 0.1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; index, output = ops.min(x)</span>
<span class="sd">        &gt;&gt;&gt; print(index, output)</span>
<span class="sd">        0 0.0</span>
<span class="sd">        &gt;&gt;&gt; index, output = ops.min(x, keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; print(index, output)</span>
<span class="sd">        [0] [0.0]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">argmin_with_value_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ArgMinWithValue</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">argmin_with_value_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="unsorted_segment_sum"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.unsorted_segment_sum.html#mindspore.ops.unsorted_segment_sum">[文档]</a><span class="k">def</span> <span class="nf">unsorted_segment_sum</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the sum of a tensor along segments.</span>

<span class="sd">    Calculates a tensor such that :math:`\text{output}[i] = \sum_{segment\_ids[j] == i} \text{data}[j, \ldots]`, where</span>
<span class="sd">    :math:`j` is a tuple describing the index of element in data.  `segment_ids` selects which elements in data to sum</span>
<span class="sd">    up. Segment_ids does not need to be sorted, and it does not need to cover all values in the entire valid value</span>
<span class="sd">    range.</span>

<span class="sd">    The following figure shows the calculation process of UnsortedSegmentSum:</span>

<span class="sd">    .. image:: UnsortedSegmentSum.png</span>

<span class="sd">    Note:</span>
<span class="sd">        - If the segment_id i is absent in the segment_ids, then output[i] will be filled with 0.</span>
<span class="sd">        - On Ascend, if the value of segment_id is less than 0 or greater than the length of the input data shape, an</span>
<span class="sd">          execution error will occur.</span>

<span class="sd">    If the sum of the given segment_ids :math:`i` is empty, then :math:`\text{output}[i] = 0`. If the given segment_ids</span>
<span class="sd">    is negative, the value will be ignored. &#39;num_segments&#39; must be equal to the number of different segment_ids.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The shape is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">        segment_ids (Tensor): Set the shape as :math:`(x_1, x_2, ..., x_N)`, where 0 &lt; N &lt;= R.</span>
<span class="sd">        num_segments (Union[int, Tensor], optional): Set :math:`z` as num_segments.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is :math:`(z, x_{N+1}, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `num_segments` is not an int.</span>
<span class="sd">        ValueError: If length of shape of `segment_ids` is less than 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor([1, 2, 3, 4], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; segment_ids = Tensor([0, 0, 1, 2], mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; num_segments = 4</span>
<span class="sd">        &gt;&gt;&gt; output = ops.unsorted_segment_sum(input_x, segment_ids, num_segments)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [3. 3. 4. 0.]</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor([1, 2, 3, 4, 2, 5], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; segment_ids = Tensor([0, 0, 1, 2, 3, 4], mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; num_segments = 6</span>
<span class="sd">        &gt;&gt;&gt; output = ops.unsorted_segment_sum(input_x, segment_ids, num_segments)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [3. 3. 4. 2. 5. 0.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">unsorted_segment_sum_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">)</span></div>


<div class="viewcode-block" id="top_k"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.top_k.html#mindspore.ops.top_k">[文档]</a><span class="k">def</span> <span class="nf">top_k</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="nb">sorted</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Finds values and indices of the `k` largest entries along the last dimension.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - If sorted is set to &#39;False&#39;, it will use the aicpu operator, the performance may be reduced.</span>

<span class="sd">    If the `input_x` is a one-dimensional Tensor, finds the `k` largest entries in the Tensor,</span>
<span class="sd">    and outputs its value and index as a Tensor. Therefore, values[`k`] is the `k` largest item in `input_x`,</span>
<span class="sd">    and its index is indices [`k`].</span>

<span class="sd">    For a multi-dimensional matrix,</span>
<span class="sd">    calculates the first `k` entries in each row (corresponding vector along the last dimension), therefore:</span>

<span class="sd">    .. math::</span>

<span class="sd">        values.shape = indices.shape = input.shape[:-1] + [k].</span>

<span class="sd">    If the two compared elements are the same, the one with the smaller index value is returned first.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Input to be computed, data type must be float16, float32 or int32.</span>
<span class="sd">        k (int): The number of top elements to be computed along the last dimension, constant input is needed.</span>
<span class="sd">        sorted (bool, optional): If true, the obtained elements will be sorted by the values in descending order.</span>
<span class="sd">            Default: True.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple of 2 tensors, the values and the indices.</span>

<span class="sd">        - values (Tensor): The `k` largest elements in each slice of the last dimension.</span>
<span class="sd">        - indices (Tensor): The indices of values within the last dimension of input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `sorted` is not a bool.</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        TypeError: If `k` is not an int.</span>
<span class="sd">        TypeError: If dtype of `input_x` is not one of the following: float16, float32 or int32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor([1, 2, 3, 4, 5], mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; k = 3</span>
<span class="sd">        &gt;&gt;&gt; values, indices = ops.top_k(input_x, k, sorted=True)</span>
<span class="sd">        &gt;&gt;&gt; print((values, indices))</span>
<span class="sd">        (Tensor(shape=[3], dtype=Float16, value= [ 5.0000e+00,  4.0000e+00,  3.0000e+00]), Tensor(shape=[3],</span>
<span class="sd">          dtype=Int32, value= [4, 3, 2]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">top_k_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">TopK</span><span class="p">)(</span><span class="nb">sorted</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">top_k_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span></div>


<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;unique&#39;</span><span class="p">,</span>
    <span class="s1">&#39;unique_with_pad&#39;</span><span class="p">,</span>
    <span class="s1">&#39;unique_consecutive&#39;</span><span class="p">,</span>
    <span class="s1">&#39;eye&#39;</span><span class="p">,</span>
    <span class="s1">&#39;matrix_band_part&#39;</span><span class="p">,</span>
    <span class="s1">&#39;padding&#39;</span><span class="p">,</span>
    <span class="s1">&#39;fill&#39;</span><span class="p">,</span>
    <span class="s1">&#39;fill_&#39;</span><span class="p">,</span>
    <span class="s1">&#39;fills&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tile&#39;</span><span class="p">,</span>
    <span class="s1">&#39;size&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ger&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ones&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ones_like&#39;</span><span class="p">,</span>
    <span class="s1">&#39;shape&#39;</span><span class="p">,</span>
    <span class="s1">&#39;shape_&#39;</span><span class="p">,</span>
    <span class="s1">&#39;reverse_sequence&#39;</span><span class="p">,</span>
    <span class="s1">&#39;dyn_shape&#39;</span><span class="p">,</span>
    <span class="s1">&#39;rank&#39;</span><span class="p">,</span>
    <span class="s1">&#39;range&#39;</span><span class="p">,</span>
    <span class="s1">&#39;reshape&#39;</span><span class="p">,</span>
    <span class="s1">&#39;reshape_&#39;</span><span class="p">,</span>
    <span class="s1">&#39;flatten&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_slice&#39;</span><span class="p">,</span>
    <span class="s1">&#39;slice&#39;</span><span class="p">,</span>
    <span class="s1">&#39;concat&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stack&#39;</span><span class="p">,</span>
    <span class="s1">&#39;unstack&#39;</span><span class="p">,</span>
    <span class="s1">&#39;scalar_cast&#39;</span><span class="p">,</span>
    <span class="s1">&#39;scalar_to_array&#39;</span><span class="p">,</span>
    <span class="s1">&#39;scalar_to_tensor&#39;</span><span class="p">,</span>
    <span class="s1">&#39;space_to_batch_nd&#39;</span><span class="p">,</span>
    <span class="s1">&#39;batch_to_space_nd&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tuple_to_array&#39;</span><span class="p">,</span>
    <span class="s1">&#39;expand_dims&#39;</span><span class="p">,</span>
    <span class="s1">&#39;squeeze&#39;</span><span class="p">,</span>
    <span class="s1">&#39;transpose&#39;</span><span class="p">,</span>
    <span class="s1">&#39;scatter_nd&#39;</span><span class="p">,</span>
    <span class="s1">&#39;scatter_nd_add&#39;</span><span class="p">,</span>
    <span class="s1">&#39;scatter_nd_sub&#39;</span><span class="p">,</span>
    <span class="s1">&#39;scatter_nd_mul&#39;</span><span class="p">,</span>
    <span class="s1">&#39;scatter_nd_div&#39;</span><span class="p">,</span>
    <span class="s1">&#39;scatter_nd_max&#39;</span><span class="p">,</span>
    <span class="s1">&#39;scatter_nd_min&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_scatter_add&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_scatter_sub&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_scatter_mul&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_scatter_div&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_scatter_max&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_scatter_min&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_scatter_elements&#39;</span><span class="p">,</span>
    <span class="s1">&#39;unsorted_segment_min&#39;</span><span class="p">,</span>
    <span class="s1">&#39;unsorted_segment_max&#39;</span><span class="p">,</span>
    <span class="s1">&#39;unsorted_segment_prod&#39;</span><span class="p">,</span>
    <span class="s1">&#39;gather&#39;</span><span class="p">,</span>
    <span class="s1">&#39;gather_d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;gather_elements&#39;</span><span class="p">,</span>
    <span class="s1">&#39;gather_nd&#39;</span><span class="p">,</span>
    <span class="s1">&#39;one_hot&#39;</span><span class="p">,</span>
    <span class="s1">&#39;masked_fill&#39;</span><span class="p">,</span>
    <span class="s1">&#39;masked_select&#39;</span><span class="p">,</span>
    <span class="s1">&#39;scatter_add&#39;</span><span class="p">,</span>
    <span class="s1">&#39;scatter_mul&#39;</span><span class="p">,</span>
    <span class="s1">&#39;scatter_max&#39;</span><span class="p">,</span>
    <span class="s1">&#39;scatter_min&#39;</span><span class="p">,</span>
    <span class="s1">&#39;scatter_div&#39;</span><span class="p">,</span>
    <span class="s1">&#39;scatter_update&#39;</span><span class="p">,</span>
    <span class="s1">&#39;select&#39;</span><span class="p">,</span>
    <span class="s1">&#39;nonzero&#39;</span><span class="p">,</span>
    <span class="s1">&#39;matrix_diag&#39;</span><span class="p">,</span>
    <span class="s1">&#39;matrix_diag_part&#39;</span><span class="p">,</span>
    <span class="s1">&#39;matrix_set_diag&#39;</span><span class="p">,</span>
    <span class="s1">&#39;diag&#39;</span><span class="p">,</span>
    <span class="s1">&#39;meshgrid&#39;</span><span class="p">,</span>
    <span class="s1">&#39;affine_grid&#39;</span><span class="p">,</span>
    <span class="s1">&#39;adaptive_max_pool2d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;meshgrid&#39;</span><span class="p">,</span>
    <span class="s1">&#39;broadcast_to&#39;</span><span class="p">,</span>
    <span class="s1">&#39;col2im&#39;</span><span class="p">,</span>
    <span class="s1">&#39;split&#39;</span><span class="p">,</span>
    <span class="s2">&quot;index_fill&quot;</span><span class="p">,</span>
    <span class="s1">&#39;max&#39;</span><span class="p">,</span>
    <span class="s1">&#39;argmax&#39;</span><span class="p">,</span>
    <span class="s1">&#39;min&#39;</span><span class="p">,</span>
    <span class="s1">&#39;unsorted_segment_sum&#39;</span><span class="p">,</span>
    <span class="s1">&#39;population_count&#39;</span><span class="p">,</span>
    <span class="s1">&#39;top_k&#39;</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">__all__</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>