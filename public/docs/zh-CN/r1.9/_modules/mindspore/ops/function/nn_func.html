

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>mindspore.ops.function.nn_func &mdash; MindSpore master documentation</title>
  

  
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/training.css" type="text/css" />
   
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/js/training.js"></script>
        
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">设计</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/overview.html">MindSpore设计概览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/auto_gradient.html">函数式微分编程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/mindir.html">中间表示MindIR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/all_scenarios.html">全场景统一</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/dynamic_graph_and_static_graph.html">动静态图结合</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/distributed_training_design.html">分布式并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/graph_fusion_engine.html">图算融合加速引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/data_engine.html">高性能数据处理引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/glossary.html">术语</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">规格</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/benchmark.html">基准性能</a></li>
<li class="toctree-l1"><a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.9/README_CN.md#目录">网络支持↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/operator_list.html">API支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/syntax_list.html">语法支持</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.audio.html">mindspore.dataset.audio</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.config.html">mindspore.dataset.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.text.html">mindspore.dataset.text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.vision.html">mindspore.dataset.vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.transformer.html">mindspore.nn.transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.function.html">mindspore.ops.function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.9/api_cpp/mindspore.html">C++ API↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API映射</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/api_mapping/pytorch_api_mapping.html">PyTorch与MindSpore API映射表</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/api_mapping/tensorflow_api_mapping.html">TensorFlow与MindSpore API映射表</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">迁移指南</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/overview.html">概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/enveriment_preparation.html">环境准备与资料获取</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/analysis_and_preparation.html">模型分析与准备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/model_development/model_development.html">MindSpore网络搭建</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/debug_and_tune.html">调试调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/sample_code.html">网络迁移调试实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/faq.html">常见问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/typical_api_comparision.html">与PyTorch典型区别</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/use_third_party_op.html">基于自定义算子接口调用第三方算子库</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/installation.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/data_processing.html">数据处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/implement_problem.html">执行问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/network_compilation.html">网络编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/operators_compile.html">算子编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/usage_migrate_3rd.html">第三方框架迁移使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/performance_tuning.html">性能调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/precision_tuning.html">精度调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/distributed_configure.html">分布式配置</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/inference.html">推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/feature_advice.html">特性咨询</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../RELEASE.html">Release Notes</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
      <li>mindspore.ops.function.nn_func</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for mindspore.ops.function.nn_func</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2022 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>

<span class="sd">&quot;&quot;&quot;Defines nn operators with functional form.&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span>

<span class="kn">from</span> <span class="nn">mindspore.ops.primitive</span> <span class="kn">import</span> <span class="n">constexpr</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations</span> <span class="kn">import</span> <span class="n">nn_ops</span> <span class="k">as</span> <span class="n">NN_OPS</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations</span> <span class="kn">import</span> <span class="n">image_ops</span> <span class="k">as</span> <span class="n">IMG</span>
<span class="kn">from</span> <span class="nn">mindspore.ops._utils</span> <span class="kn">import</span> <span class="n">is_shape_unknown</span>
<span class="kn">import</span> <span class="nn">mindspore.common.dtype</span> <span class="k">as</span> <span class="nn">mstype</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.function.math_func</span> <span class="kn">import</span> <span class="n">logsumexp</span>
<span class="kn">from</span> <span class="nn">mindspore.common.tensor</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">Tensor</span> <span class="k">as</span> <span class="n">Tensor_</span>
<span class="kn">from</span> <span class="nn">mindspore.ops._primitive_cache</span> <span class="kn">import</span> <span class="n">_get_cache_prim</span>
<span class="kn">from</span> <span class="nn">mindspore._checkparam</span> <span class="kn">import</span> <span class="n">Rel</span>
<span class="kn">from</span> <span class="nn">mindspore._checkparam</span> <span class="kn">import</span> <span class="n">Validator</span> <span class="k">as</span> <span class="n">validator</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.composite.multitype_ops._constexpr_utils</span> <span class="kn">import</span> <span class="n">raise_value_error</span>

<span class="n">slice_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Slice</span><span class="p">()</span>
<span class="n">fast_gelu_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">FastGeLU</span><span class="p">()</span>
<span class="n">softsign_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Softsign</span><span class="p">()</span>
<span class="n">hardswish_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">HSwish</span><span class="p">()</span>
<span class="n">mish_</span> <span class="o">=</span> <span class="n">NN_OPS</span><span class="o">.</span><span class="n">Mish</span><span class="p">()</span>
<span class="n">selu_</span> <span class="o">=</span> <span class="n">NN_OPS</span><span class="o">.</span><span class="n">SeLU</span><span class="p">()</span>
<span class="n">sigmoid_</span> <span class="o">=</span> <span class="n">NN_OPS</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>


<div class="viewcode-block" id="adaptive_avg_pool2d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.adaptive_avg_pool2d.html#mindspore.ops.adaptive_avg_pool2d">[文档]</a><span class="k">def</span> <span class="nf">adaptive_avg_pool2d</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    2D adaptive average pooling for temporal data.</span>

<span class="sd">    This operator applies a 2D adaptive average pooling to an input signal composed of multiple input planes.</span>
<span class="sd">    That is, for any input size, the size of the specified output is H x W.</span>
<span class="sd">    The number of output features is equal to the number of input features.</span>

<span class="sd">    The input and output data format can be &quot;NCHW&quot; and &quot;CHW&quot;. N is the batch size, C is the number of channels,</span>
<span class="sd">    H is the feature height, and W is the feature width.</span>

<span class="sd">    For adaptive average pooling for 2D:</span>

<span class="sd">    ..  math::</span>
<span class="sd">        \begin{align}</span>
<span class="sd">        h_{start} &amp;= floor(i * H_{in} / H_{out})\\</span>
<span class="sd">        h_{end} &amp;= ceil((i + 1) * H_{in} / H_{out})\\</span>
<span class="sd">        w_{start} &amp;= floor(j * W_{in} / W_{out})\\</span>
<span class="sd">        w_{end} &amp;= ceil((j + 1) * W_{in} / W_{out})\\</span>
<span class="sd">        Output(i,j) &amp;= \frac{\sum Input[h_{start}:h_{end}, w_{start}:w_{end}]}{(h_{end}- h_{start})</span>
<span class="sd">        * (w_{end}- w_{start})}</span>
<span class="sd">        \end{align}</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The input of adaptive_avg_pool2d, which is a 3D or 4D tensor,</span>
<span class="sd">          with float16, float32 or float64 data type.</span>
<span class="sd">        output_size (Union[int, tuple]): The target output size is H x W.</span>
<span class="sd">            `ouput_size` can be a tuple consisted of int type H and W, or a single H for H x H, or None.</span>
<span class="sd">            If it is None, it means the output size is the same as the input size.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type as the `input_x`.</span>

<span class="sd">        Shape of the output is `input_x_shape[:len(input_x_shape) - len(out_shape)] + out_shape`.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out\_shape = \begin{cases}</span>
<span class="sd">        input\_x\_shape[-2] + output\_size[1], &amp; \text{if output_size is (None, w);}\\</span>
<span class="sd">        output\_size[0] + input\_x\_shape[-1], &amp; \text{if output_size is (h, None);}\\</span>
<span class="sd">        input\_x\_shape[-2:], &amp; \text{if output_size is (None, None);}\\</span>
<span class="sd">        (h, h), &amp; \text{if output_size is h;}\\</span>
<span class="sd">        (h, w), &amp; \text{if output_size is (h, w)}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If `output_size` is a tuple and the length of `output_size` is not 2.</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input_x` is not float16, float32 or float64.</span>
<span class="sd">        ValueError: If the dimension of `input_x` is less than or equal to the dimension of `output_size`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # case 1: output_size=(None, 2)</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]],</span>
<span class="sd">        ...                            [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]],</span>
<span class="sd">        ...                            [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.adaptive_avg_pool2d(input_x, (None, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1.5 2.5]</span>
<span class="sd">          [4.5 5.5]</span>
<span class="sd">          [7.5 8.5]]</span>
<span class="sd">         [[1.5 2.5]</span>
<span class="sd">          [4.5 5.5]</span>
<span class="sd">          [7.5 8.5]]</span>
<span class="sd">         [[1.5 2.5]</span>
<span class="sd">          [4.5 5.5]</span>
<span class="sd">          [7.5 8.5]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 2: output_size=2</span>
<span class="sd">        &gt;&gt;&gt; output = ops.adaptive_avg_pool2d(input_x, 2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[3. 4.]</span>
<span class="sd">          [6. 7.]]</span>
<span class="sd">         [[3. 4.]</span>
<span class="sd">          [6. 7.]]</span>
<span class="sd">         [[3. 4.]</span>
<span class="sd">          [6. 7.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: output_size=(1, 2)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.adaptive_avg_pool2d(input_x, (1, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[4.5 5.5]]</span>
<span class="sd">         [[4.5 5.5]]</span>
<span class="sd">         [[4.5 5.5]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">adaptive_avgpool2d_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">AdaptiveAvgPool2D</span><span class="p">)(</span><span class="n">output_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">adaptive_avgpool2d_</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="adaptive_avg_pool3d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.adaptive_avg_pool3d.html#mindspore.ops.adaptive_avg_pool3d">[文档]</a><span class="k">def</span> <span class="nf">adaptive_avg_pool3d</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    3D adaptive average pooling for temporal data.</span>

<span class="sd">    This operator applies a 3D adaptive average pooling to an input signal composed of multiple input planes.</span>
<span class="sd">    That is, for any input size, the size of the specified output is :math:`(D, H, W)`.</span>
<span class="sd">    The number of output features is equal to the number of input planes.</span>

<span class="sd">    Suppose the last 3 dimension size of x is :math:`(inD, inH, inW)`, the last 3 dimension size of output is</span>
<span class="sd">    :math:`(outD, outH, outW)`.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            \forall \quad od \in [0,outD-1], oh \in [0,outH-1], ow \in [0,outW-1]\\</span>
<span class="sd">            output[od,oh,ow] = \\</span>
<span class="sd">            \qquad mean(x[istartD:iendD+1,istartH:iendH+1,istartW:iendW+1])\\</span>
<span class="sd">            where,\\</span>
<span class="sd">            \qquad istartD= \left\lceil \frac{od * inD}{outD} \right\rceil \\</span>
<span class="sd">            \qquad iendD=\left\lfloor \frac{(od+1)* inD}{outD} \right\rfloor \\</span>
<span class="sd">            \qquad istartH=\left\lceil \frac{oh * inH}{outH} \right\rceil \\</span>
<span class="sd">            \qquad iendH=\left\lfloor \frac{(oh+1) * inH}{outH} \right\rfloor \\</span>
<span class="sd">            \qquad istartW=\left\lceil \frac{ow * inW}{outW} \right\rceil \\</span>
<span class="sd">            \qquad iendW=\left\lfloor \frac{(ow+1) * inW}{outW} \right\rfloor</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The input of adaptive_avg_pool3d, which is a 5D or 4D Tensor.</span>
<span class="sd">        output_size (Union[int, tuple]): The target output size. `ouput_size` can be a tuple :math:`(D, H, W)`,</span>
<span class="sd">            or an int D for :math:`(D, D, D)`. :math:`(D)`, :math:`(H)` and :math:`(W)` can be int or None</span>
<span class="sd">            which means the output size is the same as that of the input.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type as the `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input_x` is not float16, float32 or float64.</span>
<span class="sd">        ValueError: If the dimension of `input_x` is not 4D or 5D.</span>
<span class="sd">        ValueError: If `output_size` value is not positive.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # case 1: output_size=(3, 3, 4)</span>
<span class="sd">        &gt;&gt;&gt; output_size=(3, 3, 4)</span>
<span class="sd">        &gt;&gt;&gt; input_x_val = np.random.randn(4, 3, 5, 6, 7)</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(input_x_val, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.adaptive_avg_pool3d(input_x, output_size)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (4, 3, 3, 3, 4)</span>
<span class="sd">        &gt;&gt;&gt; # case 2: output_size=4</span>
<span class="sd">        &gt;&gt;&gt; output_size=5</span>
<span class="sd">        &gt;&gt;&gt; input_x_val = np.random.randn(2, 3, 8, 6, 12)</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(input_x_val, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.adaptive_avg_pool3d(input_x, output_size)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 3, 5, 5, 5)</span>
<span class="sd">        &gt;&gt;&gt; # case 3: output_size=(None, 4, 5)</span>
<span class="sd">        &gt;&gt;&gt; output_size=(None, 4, 5)</span>
<span class="sd">        &gt;&gt;&gt; input_x_val = np.random.randn(4, 1, 9, 10, 8)</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(input_x_val, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.adaptive_avg_pool3d(input_x, output_size)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (4, 1, 9, 4, 5)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">adaptive_avg_pool3d_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">AdaptiveAvgPool3D</span><span class="p">)(</span><span class="n">output_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">adaptive_avg_pool3d_</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="avg_pool2d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.avg_pool2d.html#mindspore.ops.avg_pool2d">[文档]</a><span class="k">def</span> <span class="nf">avg_pool2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;NCHW&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Average pooling operation.</span>

<span class="sd">    Applies a 2D average pooling over an input Tensor which can be regarded as a composition of 2D input planes.</span>
<span class="sd">    Typically the input is of shape :math:`(N_{in}, C_{in}, H_{in}, W_{in})`, outputs regional average in the</span>
<span class="sd">    :math:`(H_{in}, W_{in})`-dimension. Given kernel size :math:`(k_{h}, k_{w})` and `strides` , the operation</span>
<span class="sd">    is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(N_i, C_j, h, w) = \frac{1}{k_{h} * k_{w}} \sum_{m=0}^{k_{h}-1} \sum_{n=0}^{k_{w}-1}</span>
<span class="sd">        \text{input}(N_i, C_j, strides[0] \times h + m, strides[1] \times w + n)</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - Global pooling is supported.</span>
<span class="sd">        - For Ascend, the height of `kernel_size` and the weight of `kernel_size` are positive integers</span>
<span class="sd">          within the range [1, 255]. ksize_h * ksize_w &lt; 256.</span>
<span class="sd">        - For Ascend, due to instruction restrictions, the values of &#39;strides_h&#39; and &#39;strides_w&#39; are</span>
<span class="sd">          positive integers within the range [1, 63].</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of kernel used to take the average value.</span>
<span class="sd">            It is an int number that represents height and width of the kernel, or a tuple</span>
<span class="sd">            of two int numbers that represent height and width respectively. Default: 1.</span>
<span class="sd">        strides (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the height and width of movement are both strides, or a tuple of two int numbers that</span>
<span class="sd">            represent height and width of movement respectively. Default: 1.</span>
<span class="sd">        pad_mode (str): The optional value for pad mode, is &#39;same&#39; or &#39;valid&#39;.</span>
<span class="sd">            Default: &#39;valid&#39;.</span>

<span class="sd">            - same: The height and width of the output are the same as the input divided by &#39;strides&#39;</span>
<span class="sd">              and rounded up.</span>

<span class="sd">            - valid: Returns the output of the valid calculation without filling. Redundant pixels that</span>
<span class="sd">              do not satisfy the calculation will be discarded.</span>
<span class="sd">        data_format (str): The format of input and output data. It should be &#39;NHWC&#39; or &#39;NCHW&#39;.</span>
<span class="sd">            Default: &#39;NCHW&#39;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with shape :math:`(N, C_{out}, H_{out}, W_{out})`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `kernel_size` or `strides` is neither int nor tuple.</span>
<span class="sd">        ValueError: If `kernel_size` or `strides` is less than 1.</span>
<span class="sd">        ValueError: If `pad_mode` is neither &#39;valid&#39; nor &#39;same&#39; with not case sensitive.</span>
<span class="sd">        ValueError: If `data_format` is neither &#39;NCHW&#39; nor &#39;NHWC&#39;.</span>
<span class="sd">        ValueError: If length of shape of `x` is not equal to 4.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(1 * 3 * 3 * 4).reshape(1, 3, 3, 4), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.avg_pool2d(x, kernel_size=2, strides=1, pad_mode=&#39;VALID&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[ 2.5   3.5   4.5]</span>
<span class="sd">           [ 6.5   7.5   8.5]]</span>
<span class="sd">          [[14.5  15.5  16.5]</span>
<span class="sd">           [18.5  19.5  20.5]]</span>
<span class="sd">          [[26.5  27.5  28.5]</span>
<span class="sd">           [30.5  31.5  32.5]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_avg_pool</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">AvgPool</span><span class="p">)(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="n">data_format</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_avg_pool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">adaptive_max_pool3d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a 3D adaptive max pooling over an input signal composed of several input planes.</span>

<span class="sd">    The output is of size :math:`(D, H, W)`, for any input size.</span>
<span class="sd">    The number of output features is equal to the number of input planes.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Tensor, with shape :math:`(C, D, H, W)` or :math:`(N, C, D, H, W)`, which support int8, int16,</span>
<span class="sd">            int32, int64, uint8, uint16, uint32, uint64, float16, float32 or float64 data type.</span>
<span class="sd">        output_size (Union[int, tuple]): The target output size. `ouput_size` can be a tuple :math:`(D, H, W)`,</span>
<span class="sd">            or an int D for :math:`(D, D, D)`. :math:`(D)`, :math:`(H)` and :math:`(W)` can be int or None</span>
<span class="sd">            which means the output size is the same as that of the input.</span>
<span class="sd">        return_indices (bool): If `return_indices` is True, the indices of max value would be output,</span>
<span class="sd">            else would not be output. Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        - **y** (Tensor) - Tensor, with the same number of dims and data type as the `x`.</span>
<span class="sd">        - **argmax** (Tensor) - Tensor, the indices of max value, which has the same shape as the</span>
<span class="sd">          `y` and it&#39;s data type is int32. It will output only when `return_indices` is True.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        ValueError: If the dimensions number of `x` is not 4 or 5.</span>
<span class="sd">        TypeError: If dtype of `x` is not int8, int16, int32, int64, uint8, uint16, uint32, uint64,</span>
<span class="sd">                   float16, float32 or float64.</span>
<span class="sd">        ValueError: If `output_size` is neither an int nor a tuple with shape (3,).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(0,36).reshape((1, 3, 3, 4)).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output_size = (1, 1, 2)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.adaptive_max_pool3d(x, output_size, True)</span>
<span class="sd">        &gt;&gt;&gt; print(output[0].asnumpy())</span>
<span class="sd">        [[[[33. 35.]]]]</span>
<span class="sd">        &gt;&gt;&gt; print(output[1].asnumpy())</span>
<span class="sd">        [[[[33 35]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">adaptive_max_pool3d_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">AdaptiveMaxPool3D</span><span class="p">)()</span>
    <span class="n">output_size_</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">adaptive_max_pool3d_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">output_size_</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">out</span> <span class="k">if</span> <span class="n">return_indices</span> <span class="k">else</span> <span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">output</span>


<div class="viewcode-block" id="binary_cross_entropy_with_logits"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.binary_cross_entropy_with_logits.html#mindspore.ops.binary_cross_entropy_with_logits">[文档]</a><span class="k">def</span> <span class="nf">binary_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">pos_weight</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adds sigmoid activation function to input `logits`, and uses the given logits to compute binary cross entropy</span>
<span class="sd">    between the logits and the label.</span>

<span class="sd">    Sets input logits as :math:`X`, input label as :math:`Y`, input weight as :math:`W`, output as :math:`L`. Then,</span>

<span class="sd">    .. math::</span>

<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            p_{ij} = sigmoid(X_{ij}) = \frac{1}{1 + e^{-X_{ij}}} \\</span>
<span class="sd">            L_{ij} = -[Y_{ij} * log(p_{ij}) + (1 - Y_{ij})log(1 - p_{ij})]</span>
<span class="sd">        \end{array}</span>

<span class="sd">    :math:`i` indicates the :math:`i^{th}` sample, :math:`j` indicates the category. Then,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = \begin{cases}</span>
<span class="sd">        L, &amp; \text{if reduction} = \text{&#39;none&#39;;}\\</span>
<span class="sd">        \operatorname{mean}(L), &amp; \text{if reduction} = \text{&#39;mean&#39;;}\\</span>
<span class="sd">        \operatorname{sum}(L),  &amp; \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    :math:`\ell` indicates the method of calculating the loss. There are three methods:</span>
<span class="sd">    the first method is to provide the loss value directly,</span>
<span class="sd">    the second method is to calculate the average value of all losses,</span>
<span class="sd">    and the third method is to calculate the sum of all losses.</span>

<span class="sd">    This operator will multiply the output by the corresponding weight.</span>
<span class="sd">    The tensor weight assigns different weights to each piece of data in the batch,</span>
<span class="sd">    and the tensor pos_weight adds corresponding weights to the positive examples of each category.</span>

<span class="sd">    In addition, it can trade off recall and precision by adding weights to positive examples.</span>
<span class="sd">    In the case of multi-label classification the loss can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            p_{ij,c} = sigmoid(X_{ij,c}) = \frac{1}{1 + e^{-X_{ij,c}}} \\</span>
<span class="sd">            L_{ij,c} = -[P_{c}Y_{ij,c} * log(p_{ij,c}) + (1 - Y_{ij,c})log(1 - p_{ij,c})]</span>
<span class="sd">        \end{array}</span>

<span class="sd">    where c is the class number (c&gt;1 for multi-label binary classification, c=1 for single-label binary classification),</span>
<span class="sd">    n is the number of the sample in the batch and :math:`p_c` is the weight of the positive answer for the class c.</span>
<span class="sd">    :math:`p_c&gt;1` increases the recall, :math:`p_c&lt;1` increases the precision.</span>

<span class="sd">    Args:</span>
<span class="sd">        logits (Tensor): Input logits. Data type must be float16 or float32.</span>
<span class="sd">          Tensor of shape :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        label (Tensor): Ground truth label, has the same shape as `logits`.</span>
<span class="sd">          Data type must be float16 or float32.</span>
<span class="sd">        weight (Tensor): A rescaling weight applied to the loss of each batch element. It can be</span>
<span class="sd">          broadcast to a tensor with shape of `logits`. Data type must be float16 or float32.</span>
<span class="sd">        pos_weight (Tensor): A weight of positive examples. Must be a vector with length equal to the</span>
<span class="sd">          number of classes. It can be broadcast to a tensor with shape of `logits`.</span>
<span class="sd">          Data type must be float16 or float32.</span>
<span class="sd">        reduction (str): Type of reduction to be applied to loss. The optional values are &#39;mean&#39;, &#39;sum&#39;, and &#39;none&#39;,</span>
<span class="sd">             not case sensitive. If &#39;none&#39;, do not perform reduction. Default: &#39;mean&#39;.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Tensor or Scalar, if `reduction` is &#39;none&#39;, it&#39;s a tensor with the same shape and type as input `logits`.</span>
<span class="sd">        Otherwise, the output is a scalar.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If input `logits`, `label`, `weight`, `pos_weight` is not Tensor.</span>
<span class="sd">        TypeError: If data type of input `logits`, `label`, `weight`, `pos_weight` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If data type of input `reduction` is not string.</span>
<span class="sd">        ValueError: If `weight` or `pos_weight` can not be broadcast to a tensor with shape of `logits`.</span>
<span class="sd">        ValueError: If `reduction` is not one of &#39;none&#39;, &#39;mean&#39; or &#39;sum&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([[-0.8, 1.2, 0.7], [-0.1, -0.4, 0.7]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; label = Tensor(np.array([[0.3, 0.8, 1.2], [-0.6, 0.1, 2.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.array([1.0, 1.0, 1.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; pos_weight = Tensor(np.array([1.0, 1.0, 1.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.binary_cross_entropy_with_logits(logits, label, weight, pos_weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.3463612</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">bce_with_logits_loss_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">)(</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">bce_with_logits_loss_op</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">pos_weight</span><span class="p">)</span></div>


<div class="viewcode-block" id="dropout"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.dropout.html#mindspore.ops.dropout">[文档]</a><span class="k">def</span> <span class="nf">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">seed0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">seed1</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    During training, randomly zeroes some of the elements of the input tensor</span>
<span class="sd">    with probability `p` from a Bernoulli distribution.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input of Dropout, a Tensor of any shape with data type of float16 or float32.</span>
<span class="sd">        p (float): The dropping rate, between 0 and 1, e.g. p = 0.1,</span>
<span class="sd">            means dropping out 10% of input units. Default: 0.5.</span>
<span class="sd">        seed0 (int): seed0 value for random generating. Default: 0.</span>
<span class="sd">        seed1 (int): seed1 value for random generating. Default: 0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        - **output** (Tensor) - With the same shape and data type as `x`.</span>
<span class="sd">        - **mask** (Tensor) - With the same shape as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `p` is not a float.</span>
<span class="sd">        TypeError: If `seed0` or `seed1` is not an int.</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import dropout</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(((20, 16), (50, 50)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output, mask = dropout(x, p=0.5)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 2)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">keep_prob</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">p</span>
    <span class="n">dropout_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">keep_prob</span><span class="o">=</span><span class="n">keep_prob</span><span class="p">,</span> <span class="n">Seed0</span><span class="o">=</span><span class="n">seed0</span><span class="p">,</span> <span class="n">Seed1</span><span class="o">=</span><span class="n">seed1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dropout_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">celu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes celu (Continuously differentiable exponential linear units) of input tensors element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{CeLU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1))</span>

<span class="sd">    It returns :math:`\max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1))` element-wise.</span>

<span class="sd">    The picture about celu looks like this `celu &lt;https://arxiv.org/abs/1704.07483&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input of celu with data type of float16 or float32.</span>
<span class="sd">        alpha (float): The :math:`\alpha` value for the Celu formulation. Default: 1.0</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same data type and shape as the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `alpha` is not a float.</span>
<span class="sd">        ValueError: If `alpha` has the value of 0.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-2.0, -1.0, 1.0, 2.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.celu(x, alpha=1.0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-0.86466473 -0.63212055  1.          2.        ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">celu_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">CeLU</span><span class="p">)(</span><span class="n">alpha</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">celu_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<div class="viewcode-block" id="dropout2d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.dropout2d.html#mindspore.ops.dropout2d">[文档]</a><span class="k">def</span> <span class="nf">dropout2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    During training, randomly zeroes some channels of the input tensor with probability `p`</span>
<span class="sd">    from a Bernoulli distribution(For a 4-dimensional tensor with a shape of :math:`NCHW`,</span>
<span class="sd">    the channel feature map refers to a 2-dimensional feature map with the shape of :math:`HW`).</span>

<span class="sd">    For example, the :math:`j\_th` channel of the :math:`i\_th` sample in the batched input is a to-be-processed</span>
<span class="sd">    `2D` tensor input[i,j].</span>
<span class="sd">    Each channel will be zeroed out independently on every forward call which based on Bernoulli distribution</span>
<span class="sd">    probability `p`.</span>
<span class="sd">    The parper `Dropout: A Simple Way to Prevent Neural Networks from Overfitting</span>
<span class="sd">    &lt;http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf&gt;`_ mentioned this technology，And it is proved that</span>
<span class="sd">    it can effectively reduce over fitting and prevent neuronal coadaptation.</span>
<span class="sd">    For more details, refer to `Improving neural networks by preventing co-adaptation of feature detectors</span>
<span class="sd">    &lt;https://arxiv.org/pdf/1207.0580.pdf&gt;`_ .</span>

<span class="sd">    `dropout2d` can improve the independence between channel feature maps.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): A `4D` tensor with shape :math:`(N, C, H, W)`, where `N` is the batch size, `C` is the number</span>
<span class="sd">            of channels, `H` is the feature height, and `W` is the feature width. The data type must be int8,</span>
<span class="sd">            int16, int32, int64, float16, float32 or float64.</span>
<span class="sd">        p (float): The dropping probability of a channel, between 0 and 1, e.g. `p` = 0.8,</span>
<span class="sd">            which means dropping out 80% of channels. Default: 0.5.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, output, with the same shape and data type as `x`.</span>

<span class="sd">        Tensor, mask, with the same shape as `x` and the data type is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not int8, int16, int32, int64, float16, float32 or float64.</span>
<span class="sd">        TypeError: If the data type of `p` is not float.</span>
<span class="sd">        ValueError: If `p` is out of the range `[0.0, 1.0]`.</span>
<span class="sd">        ValueError: If `x` shape is not `4D`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones([2, 1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output, mask = dropout2d(input_x, 0.5)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 1, 2, 3)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dropout_2d_op</span> <span class="o">=</span> <span class="n">NN_OPS</span><span class="o">.</span><span class="n">Dropout2D</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dropout_2d_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="dropout3d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.dropout3d.html#mindspore.ops.dropout3d">[文档]</a><span class="k">def</span> <span class="nf">dropout3d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    During training, randomly zeroes some channels of the input tensor</span>
<span class="sd">    with probability `p` from a Bernoulli distribution(For a 5-dimensional tensor</span>
<span class="sd">    with a shape of :math:`NCDHW`, the channel feature map refers to a 3-dimensional</span>
<span class="sd">    feature map with a shape of :math:`DHW`).</span>

<span class="sd">    For example, the :math:`j\_th` channel of the :math:`i\_th` sample in the batched input is a to-be-processed</span>
<span class="sd">    `3D` tensor input[i,j].</span>
<span class="sd">    Each channel will be zeroed out independently on every forward call which based on Bernoulli distribution</span>
<span class="sd">    probability `p`.</span>

<span class="sd">    `dropout3d` can improve the independence between channel feature maps.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): A `5D` tensor with shape :math:`(N, C, D, H, W)`, where `N` is the batch size, `C` is the number</span>
<span class="sd">            of channels, `D` is the feature depth, `H` is the feature height, and `W` is the feature width.</span>
<span class="sd">            The data type must be int8, int16, int32, int64, float16, float32 or float64.</span>
<span class="sd">        p (float): The dropping probability of a channel, between 0 and 1, e.g. `p` = 0.8,</span>
<span class="sd">            which means dropping out 80% of channels. Default: 0.5.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, output, with the same shape and data type as `x`.</span>

<span class="sd">        Tensor, mask, with the same shape as `x` and the data type is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not int8, int16, int32, int64, float16, float32 or float64.</span>
<span class="sd">        TypeError: If the data type of `p` is not float.</span>
<span class="sd">        ValueError: If `p` is out of the range `[0.0, 1.0]`.</span>
<span class="sd">        ValueError: If `x` shape is not 5D.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones([2, 1, 2, 1, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output, mask = dropout3d(input_x, 0.5)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 1, 2, 1, 2)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dropout_3d_op</span> <span class="o">=</span> <span class="n">NN_OPS</span><span class="o">.</span><span class="n">Dropout3D</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dropout_3d_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="fast_gelu"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.fast_gelu.html#mindspore.ops.fast_gelu">[文档]</a><span class="k">def</span> <span class="nf">fast_gelu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fast Gaussian Error Linear Units activation function.</span>

<span class="sd">    FastGeLU is defined as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output} = \frac {x} {1 + \exp(-1.702 * \left| x \right|)} * \exp(0.851 * (x - \left| x \right|)),</span>

<span class="sd">    where :math:`x` is the element of the input.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input to compute the FastGeLU with data type of float16 or float32.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.fast_gelu(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-1.5418735e-01  3.9921875e+00 -9.7473649e-06]</span>
<span class="sd">         [ 1.9375000e+00 -1.0052517e-03  8.9824219e+00]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">fast_gelu_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="kl_div"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.kl_div.html#mindspore.ops.kl_div">[文档]</a><span class="k">def</span> <span class="nf">kl_div</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Kullback-Leibler divergence between the logits and the labels.</span>

<span class="sd">    The updating formulas of KLDivLoss algorithm are as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">        l_n = target_n \cdot (\log target_n - x_n)</span>

<span class="sd">    Then,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, target) = \begin{cases}</span>
<span class="sd">        L, &amp; \text{if reduction} = \text{&#39;none&#39;;}\\</span>
<span class="sd">        \operatorname{mean}(L), &amp; \text{if reduction} = \text{&#39;mean&#39;;}\\</span>
<span class="sd">        \operatorname{batchmean}(L), &amp; \text{if reduction} = \text{&#39;batchmean&#39;;}\\</span>
<span class="sd">        \operatorname{sum}(L),  &amp; \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    where :math:`x` represents `logits`.</span>
<span class="sd">    :math:`target` represents `labels`.</span>
<span class="sd">    :math:`\ell(x, target)` represents `output`.</span>

<span class="sd">    Args:</span>
<span class="sd">        logits (Tensor): The input Tensor. The data type must be float16, float32 or float64.</span>
<span class="sd">        labels (Tensor): The label Tensor which has the same shape and data type as `logits`.</span>
<span class="sd">        reduction (str): Specifies the reduction to be applied to the output.</span>
<span class="sd">            Its value must be one of &#39;none&#39;, &#39;mean&#39;, &#39;batchmean&#39; or &#39;sum&#39;. Default: &#39;mean&#39;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or Scalar, if `reduction` is &#39;none&#39;, then output is a tensor and has the same shape as `logits`.</span>
<span class="sd">        Otherwise, it is a scalar.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `reduction` is not a str.</span>
<span class="sd">        TypeError: If neither `logits` nor `labels` is a Tensor.</span>
<span class="sd">        TypeError: If dtype of `logits` or `labels` is not float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Note:</span>
<span class="sd">        Currently it does not support float64 input on `Ascend`.</span>
<span class="sd">        It behaves the same as the mathematical definition only when `reduction` is set to `batchmean`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([0.2, 0.7, 0.1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([0., 1., 0.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = mindspore.ops.kl_div(logits, labels, &#39;mean&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        -0.23333333</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">reduction</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;kl_div&#39;, the &#39;reduction&#39; must be str and must be in &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;&#39;[&#39;none&#39;, &#39;mean&#39;, &#39;batchmean&#39;, &#39;sum&#39;]&#39;, but got &#39;</span><span class="si">{</span><span class="n">reduction</span><span class="si">}</span><span class="s2">&#39;.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;batchmean&#39;</span><span class="p">:</span>
        <span class="n">kl_div_sum</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">KLDivLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">)(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">()(</span><span class="n">logits</span><span class="p">)</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">kl_div_sum</span> <span class="o">/</span> <span class="n">batch_size</span>

    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
        <span class="n">kl_div_sum</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">KLDivLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">)(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">()(</span><span class="n">logits</span><span class="p">)</span>
        <span class="n">total_size</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="n">shape</span><span class="p">:</span>
            <span class="n">total_size</span> <span class="o">=</span> <span class="n">total_size</span> <span class="o">*</span> <span class="n">dim</span>
        <span class="k">return</span> <span class="n">kl_div_sum</span> <span class="o">/</span> <span class="n">total_size</span>

    <span class="k">return</span> <span class="n">P</span><span class="o">.</span><span class="n">KLDivLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span></div>


<div class="viewcode-block" id="hardshrink"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.hardshrink.html#mindspore.ops.hardshrink">[文档]</a><span class="k">def</span> <span class="nf">hardshrink</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hard Shrink activation function. Calculates the output according to the input elements.</span>

<span class="sd">    The formula is defined as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{HardShrink}(x) =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">        x, &amp; \text{ if } x &gt; \lambda \\</span>
<span class="sd">        x, &amp; \text{ if } x &lt; -\lambda \\</span>
<span class="sd">        0, &amp; \text{ otherwise }</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input of Hard Shrink with data type of float16 or float32.</span>
<span class="sd">        lambd (float): The threshold :math:`\lambda` defined by the Hard Shrink formula. Default: 0.5.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same data type and shape as the input `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `lambd` is not a float.</span>
<span class="sd">        TypeError: If `x` is not a tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[ 0.5,  1,  2.0], [0.0533,0.0776,-2.1233]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.hardshrink(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 0.      1.      2.    ]</span>
<span class="sd">        [ 0.      0.     -2.1233]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">hshrink_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">HShrink</span><span class="p">)(</span><span class="n">lambd</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">hshrink_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="hardswish"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.hardswish.html#mindspore.ops.hardswish">[文档]</a><span class="k">def</span> <span class="nf">hardswish</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hard swish activation function.</span>

<span class="sd">    Applies hswish-type activation element-wise. The input is a Tensor with any valid shape.</span>

<span class="sd">    Hard swish is defined as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{hswish}(x_{i}) = x_{i} * \frac{ReLU6(x_{i} + 3)}{6},</span>

<span class="sd">    where :math:`x_i` is an element of the input Tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input to compute the Hard Swish with data type of float16 or float32.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same data type and shape as the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-1, -2, 0, 2, 1]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.hardswish(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-0.3333  -0.3333  0  1.666  0.6665]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">hardswish_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_interpolate_inputs</span><span class="p">(</span><span class="n">input_dims</span><span class="p">,</span> <span class="n">roi</span><span class="p">,</span> <span class="n">scales</span><span class="p">,</span> <span class="n">sizes</span><span class="p">,</span> <span class="n">coordinate_transformation_mode</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span>
                              <span class="n">prim_name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check input&quot;&quot;&quot;</span>
    <span class="n">msg_prefix</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="n">prim_name</span><span class="si">}</span><span class="s2">&#39;, the&quot;</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;coordinate_transformation_mode&quot;</span><span class="p">,</span> <span class="n">coordinate_transformation_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">prim_name</span><span class="p">)</span>
    <span class="n">support_coordinate_mode_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;align_corners&quot;</span><span class="p">,</span> <span class="s2">&quot;half_pixel&quot;</span><span class="p">,</span> <span class="s2">&quot;asymmetric&quot;</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">coordinate_transformation_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">support_coordinate_mode_list</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">msg_prefix</span><span class="si">}</span><span class="s2"> coordinate_transformation_mode must be in </span><span class="si">{</span><span class="n">support_coordinate_mode_list</span><span class="si">}</span><span class="s2">,&quot;</span>
                        <span class="s2">&quot; but got </span><span class="si">{coordinate_transformation_mode}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;mode&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">prim_name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;linear&quot;</span><span class="p">:</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">input_dims</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;input dims&quot;</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;bilinear&quot;</span><span class="p">:</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">input_dims</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;input dims&quot;</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">msg_prefix</span><span class="si">}</span><span class="s2"> mode must be &#39;linear&#39; or &#39;bilinear&#39;, but got </span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">sizes</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">scales</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">msg_prefix</span><span class="si">}</span><span class="s2"> &#39;sizes&#39; and &#39;scale&#39; both none.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">sizes</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">scales</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">msg_prefix</span><span class="si">}</span><span class="s2"> &#39;sizes&#39; and &#39;scale&#39; both not none.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">sizes</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sizes</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">msg_prefix</span><span class="si">}</span><span class="s2"> &#39;sizes&#39; must be tuple or None, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">sizes</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">sizes</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s1">&#39;sizes item&#39;</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;sizes item&quot;</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sizes</span><span class="p">),</span> <span class="n">input_dims</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;sizes&quot;</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>
        <span class="k">return</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scales</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">msg_prefix</span><span class="si">}</span><span class="s2"> &#39;scales&#39; must be tuple or None, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">scales</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">scales</span><span class="p">:</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_float</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s1">&#39;scales item&#39;</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;scales item&quot;</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>
    <span class="n">scales_dims</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">scales</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">scales_dims</span><span class="p">,</span> <span class="n">input_dims</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;scales dims&quot;</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_float</span><span class="p">(</span><span class="n">scales</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;scales[0]&quot;</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_float</span><span class="p">(</span><span class="n">scales</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;scales[1]&quot;</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_interpolate_output_shape</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">scales</span><span class="p">,</span> <span class="n">sizes</span><span class="p">,</span> <span class="n">mode</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;calculate output shape&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">sizes</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;bilinear&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">sizes</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">sizes</span><span class="p">)</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)):</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">ret</span> <span class="o">+</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">scales</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">]),)</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;bilinear&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">ret</span>
    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span>


<div class="viewcode-block" id="interpolate"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.interpolate.html#mindspore.ops.interpolate">[文档]</a><span class="k">def</span> <span class="nf">interpolate</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">roi</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scales</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sizes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">coordinate_transformation_mode</span><span class="o">=</span><span class="s2">&quot;align_corners&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Using the interpolate method specified by `mode` resize the input tensor `x`.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - This is an experimental prototype that is subject to change.</span>
<span class="sd">        - The `roi` is reserved interface for &#39;crop_and_resize&#39; coordinate transformation mode,</span>
<span class="sd">          which is not support now.</span>
<span class="sd">        - The Ascend platforms is currently not supported when `mode` is &quot;linear&quot;.</span>
<span class="sd">        - The &#39;half_pixel&#39; coordinate_transformation_mode is currently not supported on CPU device</span>
<span class="sd">          when mode is &quot;bilinear&quot;.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): a tensor which to resize. `x` is a 3-D tensor when `mode` is &quot;linear&quot;. `x` is a 4-D tensor when</span>
<span class="sd">            `mode` is &quot;bilinear&quot;.</span>
<span class="sd">        roi (tuple[float], optional): a tuple of float. Only takes effect when attr coordinate_transformation_mode is</span>
<span class="sd">            &#39;crop_and_resize&#39;.</span>
<span class="sd">        scales (tuple[float], optional): a tuple of float. Describe the scale along each dimension.</span>
<span class="sd">            Its length is the same as that of shape of `x`. The numbers in `scales` must all be positive. Only one of</span>
<span class="sd">            `scales` and `sizes` can be specified.</span>
<span class="sd">        sizes (tuple[int], optional): a tuple of int, describes the shape of the output tensor. The numbers in `sizes`</span>
<span class="sd">            must all be positive. Only one of `scales` and `sizes` can be specified.  If `sizes` is specified, then set</span>
<span class="sd">            `scales` to &#39;None&#39; in this operator&#39;s input list. It is 1 int elements :math:`(new\_width,)` when `mode`</span>
<span class="sd">            is &quot;linear&quot;. It is 2 int elements :math:`(new\_height, new\_width)` when `mode` is &quot;bilinear&quot;.</span>
<span class="sd">        coordinate_transformation_mode (str): Default is &#39;align_corners&#39;. Describes how to transform the coordinate</span>
<span class="sd">            in the resized tensor to the coordinate in the original tensor. Other optional: &#39;half_pixel&#39;, &#39;asymmetric&#39;.</span>
<span class="sd">            For example, we want to resize the original tensor along axis x. Let&#39;s denote `new_i` as the i-th coordinate</span>
<span class="sd">            of the resized tensor along axis x, `old_i` as the coordinate of the original tensor along axis x,</span>
<span class="sd">            `new_length` as the length of the resized tensor along axis x, `old_length` as the length of the original</span>
<span class="sd">            tensor along axis x. We compute the `old_i` via the following formula:</span>

<span class="sd">            .. code-block::</span>

<span class="sd">                old_i = new_length != 1 ? new_i * (old_length - 1) / (new_length - 1) : 0  # if set to &#39;align_corners&#39;</span>

<span class="sd">                old_i = new_length &gt; 1 ? (new_x + 0.5) * old_length / new_length - 0.5 : 0  # if set to &#39;half_pixel&#39;</span>

<span class="sd">                old_i = new_length != 0 ? new_i * old_length / new_length : 0  # if set to &#39;asymmetric&#39;</span>

<span class="sd">        mode (str): The method used to interpolate: &#39;linear&#39; | &#39;bilinear&#39;. Default is &#39;linear&#39;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Resized tensor, with the same data type as input `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If the data type of `x` is not supported.</span>
<span class="sd">        TypeError: If `scales` is not a float tuple.</span>
<span class="sd">        ValueError: If not all numbers in `scales` are positive.</span>
<span class="sd">        TypeError: If `sizes` is not an int tuple.</span>
<span class="sd">        ValueError: If not all numbers in `sizes` are positive.</span>
<span class="sd">        TypeError: If `coordinate_transformation_mode` is not a string.</span>
<span class="sd">        ValueError: If `coordinate_transformation_mode` is not in the support list.</span>
<span class="sd">        TypeError: If `mode` is not a string.</span>
<span class="sd">        ValueError: If `mode` is not in the support list.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # case 1: linear mode</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[[1, 2, 3], [4, 5, 6]]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.interpolate(x, None, None, (6,), &quot;align_corners&quot;)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1. 1.4 1.8 2.2 2.6 3.]</span>
<span class="sd">          [4. 4.4 4.8 5.2 5.6 6.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 2: bilinear mode</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[[[1, 2, 3, 4, 5], [1, 2, 3, 4, 5]]]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.interpolate(x, None, None, (5, 5), &quot;asymmetric&quot;, &quot;bilinear&quot;)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[1. 2. 3. 4. 5.]</span>
<span class="sd">           [1. 2. 3. 4. 5.]</span>
<span class="sd">           [1. 2. 3. 4. 5.]</span>
<span class="sd">           [1. 2. 3. 4. 5.]</span>
<span class="sd">           [1. 2. 3. 4. 5.]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For interpolate, the input x must be tensor&quot;</span><span class="p">)</span>
    <span class="n">input_shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">input_dims</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
    <span class="n">_check_interpolate_inputs</span><span class="p">(</span><span class="n">input_dims</span><span class="p">,</span> <span class="n">roi</span><span class="p">,</span> <span class="n">scales</span><span class="p">,</span> <span class="n">sizes</span><span class="p">,</span> <span class="n">coordinate_transformation_mode</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span>
                              <span class="s2">&quot;interpolate&quot;</span><span class="p">)</span>
    <span class="n">output_size</span> <span class="o">=</span> <span class="n">_interpolate_output_shape</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">scales</span><span class="p">,</span> <span class="n">sizes</span><span class="p">,</span> <span class="n">mode</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;linear&quot;</span><span class="p">:</span>
        <span class="n">resize_linear_inner</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">IMG</span><span class="o">.</span><span class="n">ResizeLinear1D</span><span class="p">)(</span>
            <span class="n">coordinate_transformation_mode</span><span class="o">=</span><span class="n">coordinate_transformation_mode</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">resize_linear_inner</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;bilinear&quot;</span><span class="p">:</span>
        <span class="n">align_corners</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">half_pixel_centers</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">coordinate_transformation_mode</span> <span class="o">==</span> <span class="s2">&quot;align_corners&quot;</span><span class="p">:</span>
            <span class="n">align_corners</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">elif</span> <span class="n">coordinate_transformation_mode</span> <span class="o">==</span> <span class="s2">&quot;half_pixel&quot;</span><span class="p">:</span>
            <span class="n">half_pixel_centers</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">resize_bilinear_inner</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">IMG</span><span class="o">.</span><span class="n">ResizeBilinearV2</span><span class="p">)(</span><span class="n">align_corners</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">resize_bilinear_inner</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>

    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
        <span class="s2">&quot;Input Error: For interpolate,  </span><span class="si">{}</span><span class="s2"> mode is not support now&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mode</span><span class="p">))</span></div>


<div class="viewcode-block" id="softsign"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.softsign.html#mindspore.ops.softsign">[文档]</a><span class="k">def</span> <span class="nf">softsign</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softsign activation function.</span>

<span class="sd">    The function is shown as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{SoftSign}(x) = \frac{x}{1 + |x|}</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Tensor of shape :math:`(N, *)`, where :math:`*` means, any number of</span>
<span class="sd">            additional dimensions, with float16 or float32 data type.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0, -1, 2, 30, -30]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.softsign(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0.        -0.5         0.6666667  0.9677419 -0.9677419]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">softsign_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="softmax"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.softmax.html#mindspore.ops.softmax">[文档]</a><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax operation.</span>

<span class="sd">    Applies the Softmax operation to the input tensor on the specified axis.</span>
<span class="sd">    Suppose a slice in the given axis :math:`x`, then for each element :math:`x_i`,</span>
<span class="sd">    the Softmax function is shown as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(x_i) = \frac{exp(x_i)}{\sum_{j = 0}^{N-1}\exp(x_j)},</span>

<span class="sd">    where :math:`N` is the length of the tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (Int): The axis to perform the Softmax operation. Default: -1.</span>
<span class="sd">        x (Tensor): Tensor of shape :math:`(N, *)`, where :math:`*` means, any number of</span>
<span class="sd">          additional dimensions, with float16 or float32 data type.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as the logits.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is nnot an int.</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 nor float32.</span>
<span class="sd">        ValueError: If `axis` is a tuple whose length is less than 1.</span>
<span class="sd">        ValueError: If `axis` is a tuple whose elements are not all in range [-len(logits.shape), len(logits.shape))</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3, 4, 5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.softmax(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.01165623 0.03168492 0.08612854 0.23412167 0.6364086 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">type_axis</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot; the type of &#39;axis&#39; must be &#39;int&#39;, but got &#39;</span><span class="si">{</span><span class="n">axis</span><span class="si">}</span><span class="s2">&#39; with type &#39;</span><span class="si">{</span><span class="n">type_axis</span><span class="si">}</span><span class="s2">&#39;.&quot;</span><span class="p">)</span>
    <span class="n">softmax_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Softmax</span><span class="p">)(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">softmax_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="soft_shrink"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.soft_shrink.html#mindspore.ops.soft_shrink">[文档]</a><span class="k">def</span> <span class="nf">soft_shrink</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies the SoftShrink function element-wise.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{SoftShrink}(x) =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">        x - \lambda, &amp; \text{ if } x &gt; \lambda \\</span>
<span class="sd">        x + \lambda, &amp; \text{ if } x &lt; -\lambda \\</span>
<span class="sd">        0, &amp; \text{ otherwise }</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input of soft shrink with data type of float16 or float32.</span>
<span class="sd">        lambd(float): The :math:`\lambda` must be no less than zero. Default: 0.5.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and data type as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If lambd is not a float.</span>
<span class="sd">        TypeError: If input_x is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of input_x is neither float16 nor float32.</span>
<span class="sd">        ValueError: If lambd is less than 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[ 0.5297,  0.7871,  1.1754], [ 0.7836,  0.6218, -1.1542]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.soft_shrink(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 0.02979  0.287    0.676  ]</span>
<span class="sd">         [ 0.2837   0.1216  -0.6543 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">soft_shrink_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">SoftShrink</span><span class="p">)(</span><span class="n">lambd</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">soft_shrink_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="selu"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.selu.html#mindspore.ops.selu">[文档]</a><span class="k">def</span> <span class="nf">selu</span><span class="p">(</span><span class="n">input_x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Activation function SeLU (Scaled exponential Linear Unit).</span>

<span class="sd">    The activation function is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        E_{i} =</span>
<span class="sd">        scale *</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">        x_{i}, &amp;\text{if } x_{i} \geq 0; \cr</span>
<span class="sd">        \text{alpha} * (\exp(x_i) - 1), &amp;\text{otherwise.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    where :math:`alpha` and :math:`scale` are pre-defined constants(:math:`alpha=1.67326324`</span>
<span class="sd">    and :math:`scale=1.05070098`).</span>

<span class="sd">    See more details in `Self-Normalizing Neural Networks &lt;https://arxiv.org/abs/1706.02515&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Tensor of any dimension, the data type is float16 or float32.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as the `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `input_x` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.selu(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-1.1113307 4.202804 -1.7575096]</span>
<span class="sd">        [ 2.101402 -1.7462534 9.456309 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">selu_</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">input_x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sigmoid activation function.</span>

<span class="sd">    Computes Sigmoid of input element-wise. The Sigmoid function is defined as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{sigmoid}(x_i) = \frac{1}{1 + \exp(-x_i)}</span>

<span class="sd">    where :math:`x_i` is an element of the input_x.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Tensor of any dimension, the data type is float16, float32, float64, complex64 or complex128.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as the input_x.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `input_x` is not float16, float32, float64, complex64 or complex128.</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3, 4, 5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.sigmoid(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.7310586  0.880797   0.95257413 0.98201376 0.9933072 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">sigmoid_</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>


<div class="viewcode-block" id="deformable_conv2d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.deformable_conv2d.html#mindspore.ops.deformable_conv2d">[文档]</a><span class="k">def</span> <span class="nf">deformable_conv2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilations</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                      <span class="n">deformable_groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">modulated</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Given 4D tensor inputs `x`, `weight` and `offsets`, compute a 2D deformable convolution. The deformable convolution</span>
<span class="sd">    operation can be expressed as follow:</span>

<span class="sd">    Deformable Convolution v1:</span>

<span class="sd">    .. math::</span>
<span class="sd">        y(p)=\sum_{k=1}^{K}w_{k}\cdot x(p+p_{k}+\Delta{p_{k}})</span>

<span class="sd">    Deformable Convolution v2:</span>

<span class="sd">    .. math::</span>
<span class="sd">        y(p)=\sum_{k=1}^{K}w_{k}\cdot x(p+p_{k}+\Delta{p_{k}})\cdot \Delta{m_{k}}</span>

<span class="sd">    Where :math:`\Delta{p_{k}}` and :math:`\Delta{m_{k}}` are the learnable offset and modulation scalar for the k-th</span>
<span class="sd">    location. For details, please refer to `Deformable ConvNets v2: More Deformable, Better Results</span>
<span class="sd">    &lt;https://arxiv.org/abs/1811.11168&gt;`_ and `Deformable Convolutional Networks &lt;https://arxiv.org/abs/1703.06211&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): A 4D tensor of input image. With the format &quot;NCHW&quot;,</span>
<span class="sd">            the shape is :math:`(N, C_{in}, H_{in}, W_{in})`. Dtype: float16 or float32.</span>
<span class="sd">        weight (Tensor): A 4D tensor of learnable filters. Must have the same type as `x`.</span>
<span class="sd">            The shape is :math:`(C_{out}, C_{in} / groups, H_{f}, W_{f})`.</span>
<span class="sd">        offsets (Tensor): A 4D tensor of x-y coordinates offset and mask. With the format &quot;NCHW&quot;,</span>
<span class="sd">            the shape is :math:`(batch, 3 * deformable\_groups * H_{f} * W_{f}, H_{out}, W_{out})`. Note the C dimension</span>
<span class="sd">            is stored in the order of (offset_x, offset_y, mask). Must have the same type as `x`.</span>
<span class="sd">        kernel_size (tuple[int]): A tuple of 2 integers. The size of kernel.</span>
<span class="sd">        strides (tuple[int]): A tuple of 4 integers. The stride of the sliding window for each dimension of</span>
<span class="sd">            input. The dimension order is interpreted according to the data format of `x`. The N and C dimensions must</span>
<span class="sd">            be set to 1.</span>
<span class="sd">        padding (tuple[int]): A tuple of 4 integers. The number of pixels to add to each (top, bottom, left,</span>
<span class="sd">            right) side of the input.</span>
<span class="sd">        bias (Tensor, optional): An 1D tensor of additive biases to the filter outputs.</span>
<span class="sd">            The shape is :math:`(C_{out})`. Defaults to None.</span>
<span class="sd">        dilations (tuple[int], optional): A tuple of 4 integers. The dilation factor for each dimension of input. The</span>
<span class="sd">            dimension order is interpreted according to the data format of `x`. The N and C dimensions must be set</span>
<span class="sd">            to 1. Defaults to (1, 1, 1, 1).</span>
<span class="sd">        groups (int, optional): An integer of type int32. The number of blocked connections from input channels</span>
<span class="sd">            to output channels. In_channels and out_channels must both be divisible by `groups`. Defaults to 1.</span>
<span class="sd">        deformable_groups (int, optional): An integer of type int32. The number of deformable group partitions.</span>
<span class="sd">            In_channels must be divisible by `deformable_groups`. Defaults to 1.</span>
<span class="sd">        modulated (bool, optional): Specifies version of DeformableConv2D, True means v2, False means v1, currently</span>
<span class="sd">            only supports v2. Defaults to True.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, A 4D Tensor of output feature map. With the same type as `x`. With the format &quot;NCHW&quot;,</span>
<span class="sd">        the shape is :math:`(N, C_{out}, H_{out}, W_{out})`.</span>

<span class="sd">        .. math::</span>
<span class="sd">            \begin{array}{ll} \\</span>
<span class="sd">                H_{out} = \left \lfloor{\frac{H_{in} + padding[0] + padding[1] - (H_{f} - 1) \times</span>
<span class="sd">                \text{dilations[2]} - 1 }{\text{stride[0]}} + 1} \right \rfloor \\</span>
<span class="sd">                W_{out} = \left \lfloor{\frac{W_{in} + padding[2] + padding[3] - (W_{f} - 1) \times</span>
<span class="sd">                \text{dilations[3]} - 1 }{\text{stride[1]}} + 1} \right \rfloor \\</span>
<span class="sd">            \end{array}</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `strides`, `padding`, `kernel_size` or `dilations` is not a tuple with integer elements.</span>
<span class="sd">        TypeError: If `modulated` is not a bool.</span>
<span class="sd">        ValueError: If the tuple size of `strides`, `padding`, `kernel_size` or `dilations` is not expected.</span>
<span class="sd">        ValueError: The N or C dimensions of &#39;strides&#39; or `dilations` is not set to 1.</span>
<span class="sd">        ValueError: If `modulated` is not set to True.</span>

<span class="sd">    Note:</span>
<span class="sd">        - This is an experimental interface that is subject to change or deletion.</span>
<span class="sd">        - For Ascend platform, only AI-CORE kernel is implemented, which has the following limitations:</span>

<span class="sd">          - :math:`C_{in}` cannot be divisible by 8 is not supported, e.g. `x` is :math:`(N, 2, H_{in}, W_{in})`.</span>
<span class="sd">          - `deformable_groups` must equal to 1.</span>
<span class="sd">          - `offsets` value is float which does not contain a decimal part is not supported, e.g. `offsets` is assigned</span>
<span class="sd">            with &quot;numpy.ones()&quot;.</span>
<span class="sd">          - `kernel_size` should meet the requirement::math:`3 * kernel\_size[0] * kernel\_size[1] &gt; 8`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones((4, 3, 10, 10)), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; kh, kw = 3, 3</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones((5, 3, kh, kw)), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; offsets = Tensor(np.ones((4, 3 * kh * kw, 8, 8)), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.deformable_conv2d(x, weight, offsets, (kh, kw), (1, 1, 1, 1), (0, 0, 0, 0))</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (4, 5, 8, 8)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">deformable_offsets</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">DeformableOffsets</span><span class="p">)(</span><span class="n">strides</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">dilations</span><span class="p">,</span> <span class="s2">&quot;NCHW&quot;</span><span class="p">,</span>
                                                                   <span class="n">deformable_groups</span><span class="p">,</span>
                                                                   <span class="n">modulated</span><span class="p">)</span>
    <span class="n">fm_offset</span> <span class="o">=</span> <span class="n">deformable_offsets</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">offsets</span><span class="p">)</span>

    <span class="n">weight_shape</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">out_channel</span> <span class="o">=</span> <span class="n">weight_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">strides_conv</span> <span class="o">=</span> <span class="p">(</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">conv</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">)(</span><span class="n">out_channel</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">strides_conv</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="p">)</span>
    <span class="n">bias_add_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">BiasAdd</span><span class="p">)()</span>

    <span class="n">output</span> <span class="o">=</span> <span class="n">conv</span><span class="p">(</span><span class="n">fm_offset</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">bias_add_</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span></div>


<span class="k">def</span> <span class="nf">pdist</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">2.0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the p-norm distance between each pair of row vectors in the input. If `x` is a 2D Tensor of</span>
<span class="sd">    shape :math:`(N, M)`, then `output` must be a 1D Tensor of shape :math:`(N * (N - 1) / 2,)`. If `x` is a</span>
<span class="sd">    Tensor of shape :math:`(*B, N, M)`, then `output` must be a Tensor of shape :math:`(*B, N * (N - 1) / 2)`.</span>

<span class="sd">    .. math::</span>
<span class="sd">        y[n] = \sqrt[p]{{\mid x_{i} - x_{j} \mid}^p}</span>

<span class="sd">    where :math:`x_{i}, x_{j}` are two different row vectors in the input.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input tensor of shape :math:`(*B, N, M)`. :math:`*B` is batch size, one-dim or multi-dim.</span>
<span class="sd">            dtype: float16, float32 or float64.</span>
<span class="sd">        p (float): p value for the p-norm distance to calculate between each vector pair. :math:`p∈[0,∞]`. Default: 2.0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64.</span>
<span class="sd">        TypeError: If `p` is not a float.</span>
<span class="sd">        ValueError: If `p` is a negative float.</span>
<span class="sd">        ValueError: If dimension of `x` is less than 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1.0, 1.0], [2.0, 2.0], [3.0, 3.0]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; y = ops.pdist(x, p=2.0)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [1.4142135 2.828427 1.4142135]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">pdist_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">Pdist</span><span class="p">)(</span><span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pdist_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_pad_inputs</span><span class="p">(</span><span class="n">x_shape</span><span class="p">,</span> <span class="n">paddings</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;check the input of pad&quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">pd</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">paddings</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pd</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">pd</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pd</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> \
                <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pd</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;pad&#39;, each element in &#39;paddings&#39; must be a list or tuple of 2 int, but got </span><span class="si">{</span><span class="n">pd</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">x_shape_unknown</span> <span class="o">=</span> <span class="n">is_shape_unknown</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">x_shape_unknown</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">paddings</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;pad&#39;, the size of paddings must be 2 * </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span><span class="si">}</span><span class="s2">, but got </span><span class="si">{</span><span class="mi">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nb">len</span><span class="p">(</span><span class="n">paddings</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">pad_all_non_negative</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">pd</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">paddings</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">pd</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">pd</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">pad_all_non_negative</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">x_shape_unknown</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">pad_all_non_negative</span><span class="p">:</span>
        <span class="c1"># in this case, we can not infer the slice size</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;pad&#39;, if &#39;input_x&#39; is dynamic shape, &#39;paddings&#39; must be non-negative value, but got &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">paddings</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="pad"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.pad.html#mindspore.ops.pad">[文档]</a><span class="k">def</span> <span class="nf">pad</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">paddings</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Pads the input tensor according to the paddings.</span>

<span class="sd">    The formula to calculate the shape of the output tensor is as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{aligned}</span>
<span class="sd">            &amp;\text{ input_x_shape} = (N_{1},N_{2},...,N_{n}) \\</span>
<span class="sd">            &amp;\begin{aligned}</span>
<span class="sd">                \text{output_shape = }(&amp;N_{1}+paddings[0,0]+paddings[0,1], \\</span>
<span class="sd">                                 &amp; N_{2}+paddings[1,0]+paddings[1,1], \\</span>
<span class="sd">                                 &amp;... , \\</span>
<span class="sd">                                 &amp; N_{n}+paddings[n-1,0]+paddings[n-1,1])</span>
<span class="sd">            \end{aligned}</span>
<span class="sd">        \end{aligned}</span>

<span class="sd">    Note:</span>
<span class="sd">        Negative `paddings` value is only supported when `input_x` is not dynamic shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Tensor of shape :math:`(N, *)`, where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        paddings (tuple): The shape of parameter `paddings` is (N, 2). N is the rank of input data. All elements of</span>
<span class="sd">            paddings are int type. For the input in `D` th dimension, paddings[D, 0] indicates how many sizes to be</span>
<span class="sd">            extended(if this value &gt; 0) or clipped(if this value &lt; 0) ahead of the input tensor in the `D` th</span>
<span class="sd">            dimension, and paddings[D, 1] indicates how many sizes to be extended(if this value &gt; 0) or</span>
<span class="sd">            clipped(if this value &lt; 0) behind the input tensor in the `D` th dimension.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the tensor after padding.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `paddings` is not a tuple.</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        ValueError: If shape of `paddings` is not :math:`(N, 2)`.</span>
<span class="sd">        ValueError: If paddings.size is not equal to 2 * len(input_x).</span>
<span class="sd">        ValueError: If the calculated output shape contains zero or negative dimension.</span>
<span class="sd">        ValueError: If `paddings` contains negative value and `input_x` is dynamic shape.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; paddings = ((1, 2), (2, 1))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.pad(input_x, paddings)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 0.   0.   0.   0.   0.   0. ]</span>
<span class="sd">         [ 0.   0.  -0.1  0.3  3.6  0. ]</span>
<span class="sd">         [ 0.   0.   0.4  0.5 -3.2  0. ]</span>
<span class="sd">         [ 0.   0.   0.   0.   0.   0. ]</span>
<span class="sd">         [ 0.   0.   0.   0.   0.   0. ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;pad&#39;, the type of &#39;input_x&#39; must be Tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;pad&#39;, the type of &#39;paddings&#39; must be tuple, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">paddings</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">x_shape</span> <span class="o">=</span> <span class="n">input_x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">_check_pad_inputs</span><span class="p">(</span><span class="n">x_shape</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>
    <span class="n">x_shape_unknown</span> <span class="o">=</span> <span class="n">is_shape_unknown</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span>
    <span class="c1"># input_x is dynamic shape</span>
    <span class="k">if</span> <span class="n">x_shape_unknown</span><span class="p">:</span>
        <span class="n">_pad</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Pad</span><span class="p">)(</span><span class="n">paddings</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">_pad</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
    <span class="c1"># input_x is static shape</span>
    <span class="n">pad_all_non_negative</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">pad_all_non_positive</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">slice_begin</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">slice_size</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">non_negative_padding</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">pd</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">paddings</span><span class="p">):</span>
        <span class="n">sz</span> <span class="o">=</span> <span class="n">x_shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">pd</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">sz</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;pad&#39;, input_x_shape[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">] + paddings[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">, 0] is </span><span class="si">{</span><span class="n">sz</span><span class="si">}</span><span class="s2">, which is &lt;= 0 and causes &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;the output shape invalid.&quot;</span><span class="p">)</span>
        <span class="n">sz</span> <span class="o">=</span> <span class="n">sz</span> <span class="o">+</span> <span class="n">pd</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">sz</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;pad&#39;, input_x_shape[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">] + paddings[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">, 0] + paddings[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">, 1] is </span><span class="si">{</span><span class="n">sz</span><span class="si">}</span><span class="s2">, which is &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;&lt;= 0 and causes the output shape invalid.&quot;</span><span class="p">)</span>
        <span class="n">slice_size</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sz</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">pd</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">slice_begin</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">pd</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">slice_begin</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">pd</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">pd</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">pad_all_non_negative</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">pd</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">pd</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">pad_all_non_positive</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">non_negative_padding</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">pd</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">pd</span><span class="p">[</span><span class="mi">1</span><span class="p">])))</span>
    <span class="k">if</span> <span class="n">pad_all_non_negative</span><span class="p">:</span>
        <span class="n">_pad</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Pad</span><span class="p">)(</span><span class="n">paddings</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">_pad</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">pad_all_non_positive</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">slice_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">slice_begin</span><span class="p">,</span> <span class="n">slice_size</span><span class="p">)</span>
    <span class="n">_pad</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Pad</span><span class="p">)(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">non_negative_padding</span><span class="p">))</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">_pad</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">slice_</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">slice_begin</span><span class="p">,</span> <span class="n">slice_size</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes ReLU (Rectified Linear Unit activation function) of input tensors element-wise.</span>

<span class="sd">    It returns max(x, 0) element-wise. Specially, the neurons with the negative output</span>
<span class="sd">    will be suppressed and the active neurons will stay the same.</span>

<span class="sd">    .. math::</span>

<span class="sd">        ReLU(x) = (x)^+ = max(0, x)</span>

<span class="sd">    Note:</span>
<span class="sd">        In general, this operator is more commonly used. The difference from `ReLuV2` is that the `ReLuV2` will</span>
<span class="sd">        output one more Mask.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Tensor of shape :math:`(N, *)`, where :math:`*` means, any number of</span>
<span class="sd">          additional dimensions, data type is</span>
<span class="sd">          `number &lt;https://www.mindspore.cn/docs/en/r1.9/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor of shape :math:`(N, *)`, with the same dtype and shape as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x` is not a number.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.relu(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 4. 0.]</span>
<span class="sd">         [2. 0. 9.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">relu_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">ReLU</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">relu_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">relu6</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes ReLU (Rectified Linear Unit) upper bounded by 6 of input tensors element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{ReLU6}(x) = \min(\max(0,x), 6)</span>

<span class="sd">    It returns :math:`\min(\max(0,x), 6)` element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Tensor of shape :math:`(N, *)`, where :math:`*` means, any number of</span>
<span class="sd">            additional dimensions, with float16 or float32 data type.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same dtype and shape as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; result = ops.relu6(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [[0. 4. 0.]</span>
<span class="sd">         [2. 0. 6.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">relu6_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">ReLU6</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">relu6_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">prelu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Parametric Rectified Linear Unit activation function.</span>

<span class="sd">    PReLU is described in the paper `Delving Deep into Rectifiers: Surpassing Human-Level Performance on</span>
<span class="sd">    ImageNet Classification &lt;https://arxiv.org/abs/1502.01852&gt;`_. Defined as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        prelu(x_i)= \max(0, x_i) + \min(0, w * x_i),</span>

<span class="sd">    where :math:`x_i` is an element of a channel of the input, `w` is the weight of the channel.</span>

<span class="sd">    Note:</span>
<span class="sd">        Scalar or 1-D input x is not supported on Ascend.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input Tensor of the activation function. The data type is float16 or float32.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        weight (Tensor):  Weight Tensor. The data type is float16 or float32.</span>
<span class="sd">          The weight can only be a vector, and the length is the same as the number of channels C of the `input_x`.</span>
<span class="sd">          On GPU devices, when the input is a scalar, the shape is 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same shape and dtype as `x`.</span>

<span class="sd">    For detailed information, please refer to :class:`mindspore.nn.PReLU`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x` or `weight` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If the `x` or the `weight` is not a Tensor.</span>
<span class="sd">        ValueError: If the `x` is a 0-D or 1-D Tensor on Ascend.</span>
<span class="sd">        ValueError: If the `weight` is not a 1-D Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(-6, 6).reshape((2, 3, 2)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.array([0.1, 0.6, -0.3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.prelu(x, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[-0.60 -0.50]</span>
<span class="sd">          [-2.40 -1.80]</span>
<span class="sd">          [ 0.60  0.30]]</span>
<span class="sd">         [[ 0.00  1.00]</span>
<span class="sd">          [ 2.00  3.00]</span>
<span class="sd">          [ 4.0   5.00]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">prelu_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">PReLU</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">prelu_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">mirror_pad</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">mode</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Pads the input tensor according to the paddings and mode.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Tensor of shape :math:`(N, *)`, where :math:`*` means, any number of</span>
<span class="sd">          additional dimensions.</span>
<span class="sd">        paddings (Tensor): Paddings requires constant tensor. The value of `paddings` is a</span>
<span class="sd">          matrix(list), and its shape is (N, 2). N is the rank of input data. All elements of paddings</span>
<span class="sd">          are int type. For the input in the `D` th dimension, paddings[D, 0] indicates how many sizes</span>
<span class="sd">          to be extended ahead of the input tensor in the `D` th dimension, and paddings[D, 1]</span>
<span class="sd">          indicates how many sizes to be extended behind the input tensor in the `D` th dimension. Both</span>
<span class="sd">          paddings[D, 0] and paddings[D, 1] must be no greater than input_x.dim_size(D)</span>
<span class="sd">          (or input_x.dim_size(D) - 1) if mode is SYMMETRIC (if REFLECT, respectively).</span>
<span class="sd">        mode (str): Specifies the padding mode. The optional values are &quot;REFLECT&quot; and &quot;SYMMETRIC&quot;.</span>
<span class="sd">            Default: &quot;REFLECT&quot;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the tensor after padding.</span>

<span class="sd">        - If `mode` is &quot;REFLECT&quot;, it uses a way of symmetrical copying through the axis of symmetry to fill in.</span>
<span class="sd">          If the `input_x` is [[1,2,3], [4,5,6], [7,8,9]] and `paddings` is [[1,1], [2,2]], then the</span>
<span class="sd">          `Outputs` is [[6,5,4,5,6,5,4], [3,2,1,2,3,2,1], [6,5,4,5,6,5,4], [9,8,7,8,9,8,7], [6,5,4,5,6,5,4]].</span>
<span class="sd">          For a more intuitive understanding, please see the example below.</span>
<span class="sd">        - If `mode` is &quot;SYMMETRIC&quot;, the filling method is similar to the &quot;REFLECT&quot;. It is also copied</span>
<span class="sd">          according to the symmetry axis, except that it includes the symmetry axis. If the `input_x`</span>
<span class="sd">          is [[1,2,3], [4,5,6], [7,8,9]] and `paddings` is [[1,1], [2,2]], then the `Outputs` is</span>
<span class="sd">          [[2,1,1,2,3,3,2], [2,1,1,2,3,3,2], [5,4,4,5,6,6,5], [8,7,7,8,9,9,8], [8,7,7,8,9,9,8]].</span>
<span class="sd">          For a more intuitive understanding, please see the example below.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` or `paddings` is not a Tensor.</span>
<span class="sd">        TypeError: If `mode` is not a str.</span>
<span class="sd">        ValueError: If paddings.size is not equal to 2 * rank of input_x.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor([[1,2,3], [4,5,6], [7,8,9]])</span>
<span class="sd">        &gt;&gt;&gt; mode = &quot;REFLECT&quot;</span>
<span class="sd">        &gt;&gt;&gt; paddings = Tensor([[1, 1], [2, 2]])</span>
<span class="sd">        &gt;&gt;&gt; output = ops.mirror_pad(input_x, paddings, mode)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[6 5 4 5 6 5 4]</span>
<span class="sd">         [3 2 1 2 3 2 1]</span>
<span class="sd">         [6 5 4 5 6 5 4]</span>
<span class="sd">         [9 8 7 8 9 8 7]</span>
<span class="sd">         [6 5 4 5 6 5 4]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_mirror_pad</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">MirrorPad</span><span class="p">)(</span><span class="n">mode</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_mirror_pad</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_innner_log_softmax</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;inner implementation of log_softmax, since the LogSoftmaxGrad op do not support inputs &gt; 2d&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">inputs</span> <span class="o">-</span> <span class="n">logsumexp</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>


<div class="viewcode-block" id="cross_entropy"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.cross_entropy.html#mindspore.ops.cross_entropy">[文档]</a><span class="k">def</span> <span class="nf">cross_entropy</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The cross entropy loss between input and target.</span>

<span class="sd">    The cross entropy support two kind of targets:</span>

<span class="sd">    - Class indices (int) in the range :math:`[0, C)` where :math:`C` is the number of classes,</span>
<span class="sd">      the loss with reduction=none can be described as:</span>

<span class="sd">      .. math::</span>

<span class="sd">          \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">          l_n = - w_{y_n} \log \frac{\exp(x_{n,y_n})}{\sum_{c=1}^C \exp(x_{n,c})}</span>
<span class="sd">          \cdot \mathbb{1}\{y_n \not= \text{ignore_index}\}</span>

<span class="sd">      where :math:`x` is the inputs, :math:`t` is the target, :math:`w` is the weight,</span>
<span class="sd">      N is the batch size, :math:`c` belonging to [0, C-1] is class index, where :math:`C` is the number of classes.</span>

<span class="sd">      If reduction is not &#39;none&#39; (default &#39;mean&#39;), then</span>

<span class="sd">      .. math::</span>

<span class="sd">          \ell(x, y) = \begin{cases}</span>
<span class="sd">              \sum_{n=1}^N \frac{1}{\sum_{n=1}^N w_{y_n} \cdot \mathbb{1}\{y_n \not= \text{ignore_index}\}} l_n, &amp;</span>
<span class="sd">              \text{if reduction} = \text{&#39;mean&#39;,}\\</span>
<span class="sd">              \sum_{n=1}^N l_n,  &amp;</span>
<span class="sd">              \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">              \end{cases}</span>

<span class="sd">    - Probabilities (float) for each class, useful when labels beyond a single class per minibatch item</span>
<span class="sd">      are required, the loss with reduction=none can be described as:</span>

<span class="sd">      .. math::</span>

<span class="sd">          \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">          l_n = - \sum_{c=1}^C w_c \log \frac{\exp(x_{n,c})}{\sum_{i=1}^C \exp(x_{n,i})} y_{n,c}</span>

<span class="sd">      where :math:`x` is the inputs, :math:`t` is the target, :math:`w` is the weight,</span>
<span class="sd">      N is the batch size, :math:`c` belonging to [0, C-1] is class index, where :math:`C` is the number of classes.</span>

<span class="sd">      If reduction is not &#39;none&#39; (default &#39;mean&#39;), then</span>

<span class="sd">      .. math::</span>

<span class="sd">          \ell(x, y) = \begin{cases}</span>
<span class="sd">              \frac{\sum_{n=1}^N l_n}{N}, &amp;</span>
<span class="sd">              \text{if reduction} = \text{&#39;mean&#39;,}\\</span>
<span class="sd">              \sum_{n=1}^N l_n,  &amp;</span>
<span class="sd">              \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">              \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs (Tensor): :math:`(N, C)` where `C = number of classes` or :math:`(N, C, H, W)`</span>
<span class="sd">            in case of 2D Loss, or :math:`(N, C, d_1, d_2, ..., d_K)`.</span>
<span class="sd">            `inputs` is expected to be log-probabilities, data type must be float16 or float32.</span>
<span class="sd">        target (Tensor): :math:`(N)` or :math:`(N, d_1, d_2, ..., d_K)` for</span>
<span class="sd">            high-dimensional loss.</span>
<span class="sd">        weight (Tensor): A rescaling weight applied to the loss of each batch element.</span>
<span class="sd">            If not None, the shape is :math:`(C,)`,</span>
<span class="sd">            data type must be float16 or float32. Default: None.</span>
<span class="sd">        ignore_index (int): Specifies a target value that is ignored</span>
<span class="sd">            and does not contribute to the input gradient. Default: -100</span>
<span class="sd">        reduction (str):  Apply specific reduction method to the output: &#39;none&#39;, &#39;mean&#39;, or &#39;sum&#39;.</span>
<span class="sd">            Default: &#39;mean&#39;.</span>
<span class="sd">        label_smoothing (float): Label smoothing values, a regularization tool used to prevent the model</span>
<span class="sd">            from overfitting when calculating Loss. The value range is [0.0, 1.0]. Default value: 0.0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the computed loss value.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # Case 1: Indices labels</span>
<span class="sd">        &gt;&gt;&gt; inputs = mindspore.Tensor(np.random.randn(3, 5), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; target = mindspore.Tensor(np.array([1, 0, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.cross_entropy(inputs, target)</span>
<span class="sd">        &gt;&gt;&gt; # Case 2: Probability labels</span>
<span class="sd">        &gt;&gt;&gt; inputs = mindspore.Tensor(np.random.randn(3, 5), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; target = mindspore.Tensor(np.random.randn(3, 5), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.cross_entropy(inputs, target)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">class_dim</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">inputs</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">inputs</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_cross_entropy</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">class_dim</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">nll_loss</span><span class="p">(</span><span class="n">_innner_log_softmax</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">class_dim</span><span class="p">),</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_cross_entropy</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">target_dim</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;cross entropy inner function&quot;&quot;&quot;</span>
    <span class="n">_ones_like</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">OnesLike</span><span class="p">)()</span>

    <span class="n">class_dim</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">inputs</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">1</span>
    <span class="n">n_classes</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">class_dim</span><span class="p">]</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">_innner_log_softmax</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">class_dim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">label_smoothing</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">:</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">label_smoothing</span><span class="p">)</span> <span class="o">+</span> <span class="n">label_smoothing</span> <span class="o">/</span> <span class="n">n_classes</span>

    <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">_ones_like</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">broadcast_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">ndim</span><span class="p">)]</span>
        <span class="n">broadcast_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">broadcast_shape</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="n">inputs</span> <span class="o">*</span> <span class="n">target</span> <span class="o">*</span> <span class="n">weight</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">size</span> <span class="o">/</span> <span class="n">n_classes</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;sum&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="n">inputs</span> <span class="o">*</span> <span class="n">target</span> <span class="o">*</span> <span class="n">weight</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="n">inputs</span> <span class="o">*</span> <span class="n">target</span> <span class="o">*</span> <span class="n">weight</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">class_dim</span><span class="p">)</span>


<div class="viewcode-block" id="nll_loss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.nll_loss.html#mindspore.ops.nll_loss">[文档]</a><span class="k">def</span> <span class="nf">nll_loss</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets the negative log likelihood loss between inputs and target.</span>

<span class="sd">    The nll loss with reduction=none can be described as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \ell(x, t)=L=\left\{l_{1}, \ldots, l_{N}\right\}^{\top},</span>
<span class="sd">        \quad l_{n}=-w_{t_{n}} x_{n, t_{n}},</span>
<span class="sd">        \quad w_{c}=\text { weight }[c] \cdot \mathbb{1}</span>
<span class="sd">        \{c \not= \text{ignore_index}\},</span>

<span class="sd">    where :math:`x` is the inputs, :math:`t` is the target, :math:`w` is the weight,</span>
<span class="sd">    N is the batch size, :math:`c` belonging to [0, C-1] is class index, where :math:`C` is the number of classes.</span>

<span class="sd">    If reduction is not &#39;none&#39; (default &#39;mean&#39;), then</span>

<span class="sd">    .. math::</span>

<span class="sd">        \ell(x, t)=\left\{\begin{array}{ll}</span>
<span class="sd">        \sum_{n=1}^{N} \frac{1}{\sum_{n=1}^{N} w_{t n}} l_{n}, &amp; \text { if reduction }=\text { &#39;mean&#39;, } \\</span>
<span class="sd">        \sum_{n=1}^{N} l_{n}, &amp; \text { if reduction }=\text { &#39;sum&#39; }</span>
<span class="sd">        \end{array}\right.</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs (Tensor): :math:`(N, C)` where `C = number of classes` or :math:`(N, C, H, W)`</span>
<span class="sd">            in case of 2D Loss, or :math:`(N, C, d_1, d_2, ..., d_K)`.</span>
<span class="sd">            `inputs` is expected to be log-probabilities, data type must be float16 or float32.</span>
<span class="sd">        target (Tensor): :math:`(N)` or :math:`(N, d_1, d_2, ..., d_K)` for</span>
<span class="sd">            high-dimensional loss, data type must be int32.</span>
<span class="sd">        weight (Tensor): A rescaling weight applied to the loss of each batch element.</span>
<span class="sd">            If not None, the shape is :math:`(C,)`.</span>
<span class="sd">            The data type must be float16 or float32. Default: None.</span>
<span class="sd">        ignore_index (int): Specifies a target value that is ignored</span>
<span class="sd">            and does not contribute to the input gradient. Default: -100</span>
<span class="sd">        reduction (str):  Apply specific reduction method to the output: &#39;none&#39;, &#39;mean&#39;, or &#39;sum&#39;.</span>
<span class="sd">            Default: &#39;mean&#39;.</span>
<span class="sd">        label_smoothing (float): Label smoothing values, a regularization tool used to prevent the model</span>
<span class="sd">            from overfitting when calculating Loss. The value range is [0.0, 1.0]. Default value: 0.0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the computed loss value.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>

<span class="sd">        &gt;&gt;&gt; inputs = mindspore.Tensor(np.random.randn(3, 5), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; target = mindspore.Tensor(np.array([1, 0, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.nll_loss(inputs, target)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">ndim</span>
    <span class="k">if</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">_nll_loss</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">_nll_loss</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">_nll_loss</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">n</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">out_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">n</span><span class="p">,)</span> <span class="o">+</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">reduction</span> <span class="o">!=</span> <span class="s1">&#39;none&#39;</span><span class="p">:</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">_nll_loss</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">_nll_loss</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="o">=</span><span class="n">label_smoothing</span><span class="p">)</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">ret</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">out_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span></div>


<span class="k">def</span> <span class="nf">_nll_loss</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">target_dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;nll loss inner function&quot;&quot;&quot;</span>
    <span class="n">_neg</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Neg</span><span class="p">)()</span>
    <span class="n">_gather_d</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">GatherD</span><span class="p">)()</span>
    <span class="n">_gather</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Gather</span><span class="p">)()</span>
    <span class="n">_ones_like</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">OnesLike</span><span class="p">)()</span>
    <span class="n">_equal</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Equal</span><span class="p">)()</span>

    <span class="k">if</span> <span class="n">target</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="n">inputs</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">target_dim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ignore_index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">non_pad_mask</span> <span class="o">=</span> <span class="n">_equal</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">)</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">non_pad_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">non_pad_mask</span> <span class="o">=</span> <span class="n">target</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">_neg</span><span class="p">(</span><span class="n">_gather_d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target_dim</span><span class="p">,</span> <span class="n">target</span><span class="p">))</span>
    <span class="n">smooth_loss</span> <span class="o">=</span> <span class="n">_neg</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">target_dim</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">loss_weights</span> <span class="o">=</span> <span class="n">_gather</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">*</span> <span class="n">loss_weights</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">loss_weights</span> <span class="o">=</span> <span class="n">_ones_like</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ignore_index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">non_pad_mask</span><span class="p">,</span> <span class="mf">0.</span><span class="p">)</span>
        <span class="n">loss_weights</span> <span class="o">=</span> <span class="n">loss_weights</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">non_pad_mask</span><span class="p">,</span> <span class="mf">0.</span><span class="p">)</span>
        <span class="n">smooth_loss</span> <span class="o">=</span> <span class="n">smooth_loss</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">non_pad_mask</span><span class="p">,</span> <span class="mf">0.</span><span class="p">)</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">target_dim</span><span class="p">)</span>
    <span class="n">smooth_loss</span> <span class="o">=</span> <span class="n">smooth_loss</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">target_dim</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;sum&#39;</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">smooth_loss</span> <span class="o">=</span> <span class="n">smooth_loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">loss_weights</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">smooth_loss</span> <span class="o">=</span> <span class="n">smooth_loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="n">eps_i</span> <span class="o">=</span> <span class="n">label_smoothing</span> <span class="o">/</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">target_dim</span><span class="p">]</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">label_smoothing</span><span class="p">)</span> <span class="o">*</span> <span class="n">loss</span> <span class="o">+</span> <span class="n">eps_i</span> <span class="o">*</span> <span class="n">smooth_loss</span>

    <span class="k">return</span> <span class="n">loss</span>


<div class="viewcode-block" id="smooth_l1_loss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.smooth_l1_loss.html#mindspore.ops.smooth_l1_loss">[文档]</a><span class="k">def</span> <span class="nf">smooth_l1_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes smooth L1 loss, a robust L1 loss.</span>

<span class="sd">    SmoothL1Loss is a Loss similar to MSELoss but less sensitive to outliers as described in the</span>
<span class="sd">    `Fast R-CNN &lt;https://arxiv.org/abs/1504.08083&gt;`_ by Ross Girshick.</span>

<span class="sd">    Given two input :math:`x,\  y` of length :math:`N`, the unreduced SmoothL1Loss can be described</span>
<span class="sd">    as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        L_{i} =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">        \frac{0.5 (x_i - y_i)^{2}}{\text{beta}}, &amp; \text{if } |x_i - y_i| &lt; \text{beta} \\</span>
<span class="sd">        |x_i - y_i| - 0.5 \text{beta}, &amp; \text{otherwise. }</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    If `reduction` is not `none`, then:</span>

<span class="sd">    .. math::</span>
<span class="sd">        L =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">            \operatorname{mean}(L_{i}), &amp;  \text{if reduction} = \text{&#39;mean&#39;;}\\</span>
<span class="sd">            \operatorname{sum}(L_{i}),  &amp;  \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Here :math:`\text{beta}` controls the point where the loss function changes from quadratic to linear.</span>
<span class="sd">    Its default value is 1.0. :math:`N` is the batch size.</span>

<span class="sd">    Note:</span>
<span class="sd">        For Ascend platform, the float64 data type of `logits` is not support now.</span>

<span class="sd">    Args:</span>
<span class="sd">        logits (Tensor): Tensor of shape :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        labels (Tensor): Ground truth data, tensor of shape :math:`(N, *)`, same shape and dtype as the `logits`.</span>
<span class="sd">        beta (float): A parameter used to control the point where the function will change from</span>
<span class="sd">            quadratic to linear. Default: 1.0.</span>
<span class="sd">        reduction (str): Apply specific reduction method to the output: &#39;none&#39;, &#39;mean&#39; or &#39;sum&#39;. Default: &#39;none&#39;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, if `reduction` is &#39;none&#39;, then output is a tensor with the same shape as `logits`.</span>
<span class="sd">        Otherwise the shape of output tensor is `(1,)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `beta` is not a float.</span>
<span class="sd">        ValueError: If `reduction` is not one of &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;.</span>
<span class="sd">        TypeError: If dtype of `logits` or `labels` is neither float16 nor float32.</span>
<span class="sd">        ValueError: If `beta` is less than or equal to 0.</span>
<span class="sd">        ValueError: If shape of `logits` is not the same as `labels`.</span>
<span class="sd">        TypeError: The float64 data type of `logits` is support on Ascend platform.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([1, 2, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.smooth_l1_loss(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.  0.  0.5]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_smooth_l1_loss</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">SmoothL1Loss</span><span class="p">)(</span><span class="n">beta</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_smooth_l1_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span></div>


<div class="viewcode-block" id="intopk"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.intopk.html#mindspore.ops.intopk">[文档]</a><span class="k">def</span> <span class="nf">intopk</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Determines whether the targets are in the top `k` predictions.</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Tensor): A 2D Tensor defines the predictions of a batch of samples with float16 or float32</span>
<span class="sd">          data type.</span>
<span class="sd">        x2 (Tensor): A 1D Tensor defines the labels of a batch of samples with int32 data type. The size of `x2`</span>
<span class="sd">          must be equal to the first dimension of `x1`. The values of `x2` can not be negative and</span>
<span class="sd">          must be equal to or less than index of x1&#39;s second dimension.</span>
<span class="sd">        k (int): Specifies the number of top elements to be used for computing precision along the last dimension.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor has 1 dimension of type bool and the same shape with `x2`. For labeling sample `i` in `x2`,</span>
<span class="sd">        if the label in the first `k` predictions for sample `i` is in `x1`, then the value is True, otherwise False.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `k` is not an int.</span>
<span class="sd">        TypeError: If `x1` or `x2` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x1` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([[1, 8, 5, 2, 7], [4, 9, 1, 3, 5]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([1, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.intopk(x1, x2, 3)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True  False]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_in_topk</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">InTopK</span><span class="p">)(</span><span class="n">k</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_in_topk</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span></div>


<div class="viewcode-block" id="log_softmax"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.log_softmax.html#mindspore.ops.log_softmax">[文档]</a><span class="k">def</span> <span class="nf">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Log Softmax activation function.</span>

<span class="sd">    Applies the Log Softmax function to the input tensor on the specified axis.</span>
<span class="sd">    Supposes a slice in the given axis, :math:`x` for each element :math:`x_i`,</span>
<span class="sd">    the Log Softmax function is shown as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(x_i) = \log \left(\frac{\exp(x_i)} {\sum_{j = 0}^{N-1}\exp(x_j)}\right),</span>

<span class="sd">    where :math:`N` is the length of the Tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        logits (Tensor): Tensor of shape :math:`(N, *)`, where :math:`*` means, any number of</span>
<span class="sd">          additional dimensions, with float16 or float32 data type.</span>
<span class="sd">        axis (int): The axis to perform the Log softmax operation. Default: -1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as the logits.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>
<span class="sd">        TypeError: If dtype of `logits` is neither float16 nor float32.</span>
<span class="sd">        ValueError: If `axis` is not in range [-len(logits.shape), len(logits.shape)).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([1, 2, 3, 4, 5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.log_softmax(logits)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-4.4519143 -3.4519143 -2.4519143 -1.4519144 -0.4519144]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_log_softmax</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">)(</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span></div>


<div class="viewcode-block" id="lrn"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.lrn.html#mindspore.ops.lrn">[文档]</a><span class="k">def</span> <span class="nf">lrn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">depth_radius</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">norm_region</span><span class="o">=</span><span class="s2">&quot;ACROSS_CHANNELS&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Local Response Normalization.</span>

<span class="sd">    .. math::</span>

<span class="sd">        b_{c} = a_{c}\left(k + \frac{\alpha}{n}</span>
<span class="sd">        \sum_{c&#39;=\max(0, c-n/2)}^{\min(N-1,c+n/2)}a_{c&#39;}^2\right)^{-\beta}</span>

<span class="sd">    where the :math:`a_{c}` indicates the specific value of the pixel corresponding to c in feature map;</span>
<span class="sd">    where the :math:`n/2` indicates the `depth_radius`; where the :math:`k` indicates the `bias`;</span>
<span class="sd">    where the :math:`\alpha` indicates the `alpha`; where the :math:`\beta` indicates the `beta`.</span>

<span class="sd">    Args:</span>
<span class="sd">        depth_radius (int): Half-width of the 1-D normalization window with the shape of 0-D. Default: 5.</span>
<span class="sd">        bias (float): An offset (usually positive to avoid dividing by 0). Default: 1.0.</span>
<span class="sd">        alpha (float): A scale factor, usually positive. Default: 1.0.</span>
<span class="sd">        beta (float): An exponent. Default: 0.5.</span>
<span class="sd">        norm_region (str): Specifies normalization region. Options: &quot;ACROSS_CHANNELS&quot;. Default: &quot;ACROSS_CHANNELS&quot;.</span>
<span class="sd">        x (Tensor): A 4-D Tensor with float16 or float32 data type.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same shape and data type as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `depth_radius` is not an int.</span>
<span class="sd">        TypeError: If `bias`, `alpha` or `beta` is not a float.</span>
<span class="sd">        TypeError: If `norm_region` is not a str.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[[0.1], [0.2]],</span>
<span class="sd">        ...                       [[0.3], [0.4]]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.lrn(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[0.09534626]</span>
<span class="sd">           [0.1825742 ]]</span>
<span class="sd">          [[0.2860388 ]</span>
<span class="sd">           [0.3651484 ]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">lrn_op</span> <span class="o">=</span> <span class="n">NN_OPS</span><span class="o">.</span><span class="n">LRN</span><span class="p">(</span><span class="n">depth_radius</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">norm_region</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">lrn_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="mish"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.mish.html#mindspore.ops.mish">[文档]</a><span class="k">def</span> <span class="nf">mish</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes MISH(A Self Regularized Non-Monotonic Neural Activation Function) of input tensors element-wise.</span>

<span class="sd">    The function is shown as follows:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{output} = x * \tanh(\log(1 + \exp(\text{x})))</span>

<span class="sd">    See more details in `A Self Regularized Non-Monotonic Neural Activation Function</span>
<span class="sd">    &lt;https://arxiv.org/abs/1908.08681&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Tensor of shape :math:`(N, *)`, where :math:`*` means, any number of</span>
<span class="sd">            additional dimensions, with float16 or float32 data type.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.mish(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-3.0340147e-01  3.9974129e+00 -2.68311895e-03]</span>
<span class="sd">         [ 1.9439590e+00  -3.3576239e-02 8.99999990e+00]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">mish_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="max_pool3d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.max_pool3d.html#mindspore.ops.max_pool3d">[文档]</a><span class="k">def</span> <span class="nf">max_pool3d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs a 3D max pooling on the input Tensor.</span>

<span class="sd">    Typically the input is a Tensor with shape :math:`(N_{in}, C_{in}, D_{in}, H_{in}, W_{in})`, outputs</span>
<span class="sd">    regional maximum in the :math:`(D_{in}, H_{in}, W_{in})`-dimension. Given `kernel_size`</span>
<span class="sd">    :math:`ks = (d_{ker}, h_{ker}, w_{ker})` and `stride` :math:`s = (s_0, s_1, s_2)`, the operation is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(N_i, C_j, d, h, w) =</span>
<span class="sd">        \max_{l=0, \ldots, d_{ker}-1} \max_{m=0, \ldots, h_{ker}-1} \max_{n=0, \ldots, w_{ker}-1}</span>
<span class="sd">        \text{input}(N_i, C_j, s_0 \times d + l, s_1 \times h + m, s_2 \times w + n)</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Tensor of shape :math:`(N_{in}, C_{in}, D_{in}, H_{in}, W_{in})` with data type of int8,</span>
<span class="sd">            int16, int32, int64, uint8, uint16, uint32, uint64, float16, float32 or float64.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of kernel used to take the maximum value and arg</span>
<span class="sd">            value, is an int number that represents depth, height and width of the kernel, or a tuple of</span>
<span class="sd">            three int numbers that represent depth, height and width respectively.</span>
<span class="sd">        stride (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the depth, height and width of movement are both stride, or a tuple of three int numbers that</span>
<span class="sd">            represent depth, height and width of movement respectively. Default: `kernel_size`.</span>
<span class="sd">        padding (Union[int, tuple[int]]): An int number that represents the depth, height and width of movement are both</span>
<span class="sd">            strides, or a tuple of three int numbers that represent depth, height and width of movement respectively.</span>
<span class="sd">            Default: 0.</span>
<span class="sd">        dilation (Union[int, tuple[int]]): Control the stride of elements in the kernel. Default: 1.</span>
<span class="sd">        ceil_mode (bool): Whether to use ceil instead of floor to calculate output shape. Default: False.</span>
<span class="sd">        return_indices (bool): Whether to output the indices of max value. Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        If `return_indices` is False, return a Tensor `output`, else return a tuple (`output`, `argmax`).</span>

<span class="sd">        - **output** (Tensor) - Maxpooling result, with shape :math:`(N_{out}, C_{out}, D_{out}, H_{out}, W_{out})`.</span>
<span class="sd">          It has the same data type as `x`.</span>
<span class="sd">        - **argmax** (Tensor) - Index corresponding to the maximum value. Data type is int64. It will be return</span>
<span class="sd">          only when `return_indices` is True.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        ValueError: If length of shape of `x` is not equal to 5.</span>
<span class="sd">        TypeError: If `kernel_size` , `stride` , `padding` or `dilation` is not int or tuple.</span>
<span class="sd">        ValueError: If `kernel_size` or `stride` is less than 1.</span>
<span class="sd">        ValueError: If `padding` is less than 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(2 * 1 * 2 * 2 * 2).reshape((2, 1, 2, 2, 2)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output_tensor, argmax = ops.max_pool3d(x, kernel_size=2, stride=1, padding=1, return_indices=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output_tensor.shape)</span>
<span class="sd">        (2, 1, 3, 3, 3)</span>
<span class="sd">        &gt;&gt;&gt; print(argmax.shape)</span>
<span class="sd">        (2, 1, 3, 3, 3)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">strides</span> <span class="o">=</span> <span class="n">stride</span> <span class="k">if</span> <span class="p">(</span><span class="n">stride</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="k">else</span> <span class="n">kernel_size</span>
    <span class="n">max_pool3d_with_argmax_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">MaxPool3DWithArgmax</span><span class="p">)(</span>
        <span class="n">kernel_size</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">)</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">max_pool3d_with_argmax_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">return_indices</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">indices</span>
    <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="grid_sample"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.grid_sample.html#mindspore.ops.grid_sample">[文档]</a><span class="k">def</span> <span class="nf">grid_sample</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="n">interpolation_mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Given an `input_x` and a flow-field `grid`, computes the `output` using `input_x` values and pixel locations from</span>
<span class="sd">    `grid`. Only spatial (4-D) and volumetric (5-D) `input_x` is supported.</span>

<span class="sd">    In the spatial (4-D) case, for `input_x` with shape :math:`(N, C, H_{in}, W_{in})` and `grid` with shape</span>
<span class="sd">    :math:`(N, H_{out}, W_{out}, 2)`, the `output` will have shape :math:`(N, C, H_{out}, W_{out})`.</span>

<span class="sd">    For each output location `output[n, :, h, w]`, the size-2 vector `grid[n, h, w]` specifies `input_x` pixel</span>
<span class="sd">    locations `x` and `y`, which are used to interpolate the output value `output[n, :, h, w]`. In the case of 5D</span>
<span class="sd">    inputs, `grid[n, d, h, w]`, specifies the `x`, `y`, `z` pixel locations for interpolating</span>
<span class="sd">    `output[n, :, d, h, w]`. And `interpolation_mode` argument specifies &quot;nearest&quot; or &quot;bilinear&quot; or &quot;bicubic&quot;</span>
<span class="sd">    (supported in 4D case only) interpolation method to sample the input pixels.</span>

<span class="sd">    `grid` specifies the sampling pixel locations normalized by the `input_x` spatial dimensions. Therefore, it should</span>
<span class="sd">    have most values in the range of :math:`[-1, 1]`.</span>

<span class="sd">    If `grid` has values outside the range of :math:`[-1, 1]`, the corresponding outputs are handled as defined by</span>
<span class="sd">    `padding_mode`. If `padding_mode` is set to be &quot;zeros&quot;, use :math:`0` for out-of-bound grid locations. If</span>
<span class="sd">    `padding_mode` is set to be &quot;border&quot;, use border values for out-of-bound grid locations. If `padding_mode` is set</span>
<span class="sd">    to be &quot;reflection&quot;, use values at locations reflected by the border for out-of-bound grid locations. For location</span>
<span class="sd">    far away from the border, it will keep being reflected until becoming in bound.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): input with shape of :math:`(N, C, H_{in}, W_{in})` (4-D case) or :math:`(N, C, D_{in},</span>
<span class="sd">            H_{in}, W_{in})` (5-D case) and dtype of float32 or float64.</span>
<span class="sd">        grid (Tensor): flow-field with shape of :math:`(N, H_{out}, W_{out}, 2)` (4-D case) or :math:`(N, D_{out},</span>
<span class="sd">            H_{out}, W_{out}, 3)` (5-D case) and same dtype as `input_x`.</span>
<span class="sd">        interpolation_mode (str): An optional string specifying the interpolation method. The optional values are</span>
<span class="sd">            &quot;bilinear&quot;, &quot;nearest&quot; or &quot;bicubic&quot;. Default: &quot;bilinear&quot;. Note: `bicubic` supports only 4-D input. When</span>
<span class="sd">            `interpolation_mode=&quot;bilinear&quot;` and the input is 5-D, the interpolation mode used internally will actually</span>
<span class="sd">            be trilinear. However, when the input is 4-D, the interpolation mode will legistimately be bilinear.</span>
<span class="sd">        padding_mode (str): An optional string specifying the pad method. The optional values are &quot;zeros&quot;, &quot;border&quot; or</span>
<span class="sd">            &quot;reflection&quot;. Default: &quot;zeros&quot;.</span>
<span class="sd">        align_corners (bool): An optional bool. If set to `True`, the extrema (-1 and 1) are considered as referring to</span>
<span class="sd">            the center points of the input’s corner pixels. If set to `False`, they are instead considered as referring</span>
<span class="sd">            to the corner points of the input’s corner pixels, making the sampling more resolution agnostic. Default:</span>
<span class="sd">            `False`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, dtype is the same as `input_x` and whose shape is :math:`(N, C, H_{out}, W_{out})` (4-D) and</span>
<span class="sd">        :math:`(N, C, D_{out}, H_{out}, W_{out})` (5-D).</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` or `grid` is not a Tensor.</span>
<span class="sd">        TypeError: If the dtypes of `input_x` and `grid` are inconsistent.</span>
<span class="sd">        TypeError: If the dtype of `input_x` or `grid` is not a valid type.</span>
<span class="sd">        TypeError: If `align_corners` is not a boolean value.</span>
<span class="sd">        ValueError: If the rank of `input_x` or `grid` is not equal to 4(4-D case) or 5(5-D case).</span>
<span class="sd">        ValueError: If the first dimension of `input_x` is not equal to that of `grid`.</span>
<span class="sd">        ValueError: If the last dimension of `grid` is not equal to 2(4-D case) or 3(5-D case).</span>
<span class="sd">        ValueError: If `interpolation_mode` is not &quot;bilinear&quot;, &quot;nearest&quot;, &quot;bicubic&quot; or a string value.</span>
<span class="sd">        ValueError: If `padding_mode` is not &quot;zeros&quot;, &quot;border&quot;, &quot;reflection&quot; or a string value.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.arange(16).reshape((2, 2, 2, 2)).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; grid = Tensor(np.arange(0.2, 1, 0.1).reshape((2, 2, 1, 2)).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.grid_sample(input_x, grid, interpolation_mode=&#39;bilinear&#39;, padding_mode=&#39;zeros&#39;,</span>
<span class="sd">        ...                          align_corners=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[ 1.9      ]</span>
<span class="sd">           [ 2.1999998]]</span>
<span class="sd">          [[ 5.9      ]</span>
<span class="sd">           [ 6.2      ]]]</span>
<span class="sd">         [[[10.5      ]</span>
<span class="sd">           [10.8      ]]</span>
<span class="sd">          [[14.5      ]</span>
<span class="sd">           [14.8      ]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">input_x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="n">_grid_sampler_2d</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">GridSampler2D</span><span class="p">)(</span><span class="n">interpolation_mode</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">_grid_sampler_2d</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span>
    <span class="n">_grid_sampler_3d</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">GridSampler3D</span><span class="p">)(</span><span class="n">interpolation_mode</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_grid_sampler_3d</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">ctc_loss</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">target_lengths</span><span class="p">,</span> <span class="n">blank</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="n">zero_infinity</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the CTC (Connectionist Temporal Classification) loss and the gradient.</span>

<span class="sd">    The CTC algorithm is proposed in `Connectionist Temporal Classification: Labeling Unsegmented Sequence Data with</span>
<span class="sd">    Recurrent Neural Networks &lt;http://www.cs.toronto.edu/~graves/icml_2006.pdf&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        log_probs (Tensor): A tensor of shape (T, N, C), where T is input length, N is batch size and C is</span>
<span class="sd">            number of classes (including blank).</span>
<span class="sd">        targets (Tensor): A tensor of shape (N, S), where S is max target length, means the target sequences.</span>
<span class="sd">        input_lengths (Union(Tuple, Tensor)): A tuple or Tensor of shape(N). It means the lengths of the input.</span>
<span class="sd">        target_lengths (Union(Tuple, Tensor)): A tuple or Tensor of shape(N). It means the lengths of the target.</span>
<span class="sd">        blank (int): The blank label. Default: 0.</span>
<span class="sd">        reduction (string): Apply specific reduction method to the output: &#39;none&#39;, &#39;mean&#39;, or &#39;sum&#39;. Default: &#39;mean&#39;.</span>
<span class="sd">        zero_infinity (bool): Whether to set infinite loss and correlation gradient to zero. Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        neg_log_likelihood (Tensor), A loss value which is differentiable with respect to each input node.</span>

<span class="sd">        log_alpha (Tensor), The probability of possible trace of input to target.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `zero_infinity` is not a bool, reduction is not string.</span>
<span class="sd">        TypeError: If the dtype of `log_probs` or `grad_out` is not float or double.</span>
<span class="sd">        TypeError: If the dtype of `targets`, `input_lengths` or `target_lengths` is not int32 or int64.</span>
<span class="sd">        RuntimeError: If the rank of `log_probs` is not 3.</span>
<span class="sd">        RuntimeError: If the rank of `targets` is not 2.</span>
<span class="sd">        RuntimeError: If the shape of `input_lengths` does not match {batch_size|N}.</span>
<span class="sd">        RuntimeError: If the shape of `target_lengths` does not match {batch_size|N}.</span>
<span class="sd">        RuntimeError: If the types of `targets`, `input_lengths`, `grad_out` or `target_lengths` are different.</span>
<span class="sd">        RuntimeError: If the value of `blank` is not in range [0, num_labels|C).</span>
<span class="sd">        RuntimeError: If any value of `input_lengths` is larger than (time_series|T).</span>
<span class="sd">        RuntimeError: If any target_lengths[i] is not in range [0, input_length[i]].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; log_probs = Tensor(np.array([[[0.3, 0.6, 0.6]],</span>
<span class="sd">        ...                              [[0.9, 0.4, 0.2]]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; targets = Tensor(np.array([[0, 1]]), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; input_lengths = Tensor(np.array([2]), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; target_lengths = Tensor(np.array([1]), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; loss, log_alpha = ops.ctc_loss(log_probs, targets, input_lengths,</span>
<span class="sd">        ...                                target_lengths, 0, &#39;mean&#39;, True)</span>
<span class="sd">        &gt;&gt;&gt; print(loss)</span>
<span class="sd">        -2.2986124</span>
<span class="sd">        &gt;&gt;&gt; print(log_alpha)</span>
<span class="sd">        [[[0.3       0.3            -inf      -inf      -inf]</span>
<span class="sd">          [1.2       1.8931472 1.2            -inf      -inf]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ctc_loss_op</span> <span class="o">=</span> <span class="n">NN_OPS</span><span class="o">.</span><span class="n">CTCLossV2</span><span class="p">(</span><span class="n">blank</span><span class="o">=</span><span class="n">blank</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span> <span class="n">zero_infinity</span><span class="o">=</span><span class="n">zero_infinity</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">log_alpha</span> <span class="o">=</span> <span class="n">ctc_loss_op</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">target_lengths</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;sum&#39;</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
        <span class="n">input_type</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">target_length_t</span> <span class="o">=</span> <span class="n">target_lengths</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">target_length_t</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">input_type</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">log_alpha</span><span class="p">)</span>


<div class="viewcode-block" id="ctc_greedy_decoder"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ctc_greedy_decoder.html#mindspore.ops.ctc_greedy_decoder">[文档]</a><span class="k">def</span> <span class="nf">ctc_greedy_decoder</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">merge_repeated</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs greedy decoding on the logits given in inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs (Tensor): The input Tensor must be a 3-D tensor whose shape is</span>
<span class="sd">            :math:`(max\_time, batch\_size, num\_classes)`. `num_classes` must be `num_labels + 1` classes,</span>
<span class="sd">            `num_labels` indicates the number of actual labels. Blank labels are reserved.</span>
<span class="sd">            Default blank label is `num_classes - 1`. Data type must be float32 or float64.</span>
<span class="sd">        sequence_length (Tensor): A tensor containing sequence lengths with the shape of :math:`(batch\_size, )`.</span>
<span class="sd">            The type must be int32. Each value in the tensor must be equal to or less than `max_time`.</span>
<span class="sd">        merge_repeated (bool): If true, merge repeated classes in output. Default: True.</span>

<span class="sd">    Returns:</span>
<span class="sd">        decoded_indices (Tensor), A tensor with shape of :math:`(total\_decoded\_outputs, 2)`.</span>
<span class="sd">        Data type is int64.</span>

<span class="sd">        decoded_values (Tensor), A tensor with shape of :math:`(total\_decoded\_outputs, )`,</span>
<span class="sd">        it stores the decoded classes. Data type is int64.</span>

<span class="sd">        decoded_shape (Tensor), A tensor with shape of :math:`(batch\_size, max\_decoded\_length)`.</span>
<span class="sd">        Data type is int64.</span>

<span class="sd">        log_probability (Tensor), A tensor with shape of :math:`(batch\_size, 1)`,</span>
<span class="sd">        containing sequence log-probability, has the same type as `inputs`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `merge_repeated` is not a bool.</span>
<span class="sd">        ValueError: If length of shape of `inputs` is not equal to 3.</span>
<span class="sd">        ValueError: If length of shape of `sequence_length` is not equal to 1.</span>
<span class="sd">        ValueError: If value in the `sequence_length` is larger than `max_time`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; inputs = Tensor(np.array([[[0.6, 0.4, 0.2], [0.8, 0.6, 0.3]],</span>
<span class="sd">        ...                           [[0.0, 0.6, 0.0], [0.5, 0.4, 0.5]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; sequence_length = Tensor(np.array([2, 2]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; decoded_indices, decoded_values, decoded_shape, log_probability = ops.ctc_greedy_decoder(inputs,</span>
<span class="sd">        ...                                                                                          sequence_length)</span>
<span class="sd">        &gt;&gt;&gt; print(decoded_indices)</span>
<span class="sd">        [[0 0]</span>
<span class="sd">         [0 1]</span>
<span class="sd">         [1 0]]</span>
<span class="sd">        &gt;&gt;&gt; print(decoded_values)</span>
<span class="sd">        [0 1 0]</span>
<span class="sd">        &gt;&gt;&gt; print(decoded_shape)</span>
<span class="sd">        [2 2]</span>
<span class="sd">        &gt;&gt;&gt; print(log_probability)</span>
<span class="sd">        [[-1.2]</span>
<span class="sd">         [-1.3]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_ctc_greedy_decoder</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">CTCGreedyDecoder</span><span class="p">)(</span><span class="n">merge_repeated</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_ctc_greedy_decoder</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">conv3d_transpose</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                     <span class="n">output_padding</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes a 3D transposed convolution, which is also known as a deconvolution</span>
<span class="sd">    (although it is not an actual deconvolution).</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs (Tensor): The gradients with respect to the output of the convolution.</span>
<span class="sd">           The shape conforms to the default.</span>
<span class="sd">           data_format :math:`(N, C_{in}, D_{out}, H_{out}, W_{out})`. Currently dout data type only supports float16</span>
<span class="sd">           and float32.</span>
<span class="sd">        weight (Tensor): Set size of kernel is :math:`(K_d, K_h, K_w)`, then the shape is</span>
<span class="sd">           :math:`(C_{in}, C_{out}//group, K_d, K_h, K_w)`. Where :math:`group` is the Args parameter,</span>
<span class="sd">           :math:`//` is the symbol for integer division.</span>
<span class="sd">           Currently weight data type only supports float16 and float32.</span>
<span class="sd">        pad_mode (str): Specifies padding mode. The optional values are</span>
<span class="sd">            &quot;same&quot;, &quot;valid&quot;, &quot;pad&quot;. Default: &quot;valid&quot;.</span>

<span class="sd">            - same: Adopts the way of completion. The depth, height and width of the output will be equal to</span>
<span class="sd">              the input `x` divided by stride. The padding will be evenly calculated in head and tail, top and bottom,</span>
<span class="sd">              left and right directions possibility.</span>
<span class="sd">              Otherwise, the last extra padding will be calculated from the tail, bottom and the right side.</span>
<span class="sd">              If this mode is set, `pad` must be 0.</span>

<span class="sd">            - valid: Adopts the way of discarding. The possible largest depth, height and width of output</span>
<span class="sd">              will be returned without padding. Extra pixels will be discarded. If this mode is set, `pad`</span>
<span class="sd">              and `output_padding` must be 0.</span>

<span class="sd">            - pad: Implicit paddings on both sides of the input in depth, height and width. The number of `pad` will</span>
<span class="sd">              be padded to the input Tensor borders. `pad` must be greater than or equal to 0.</span>

<span class="sd">        padding (Union(int, tuple[int])): The padding value to be filled. Default: 0. If `padding` is an integer, the</span>
<span class="sd">            paddings of head, tail, top, bottom, left and right are the same, equal to pad. If `padding` is a tuple of</span>
<span class="sd">            six integers, the padding of head, tail, top, bottom, left and right equal to padding[0], padding[1],</span>
<span class="sd">            padding[2], padding[3], padding[4] and padding[5] correspondingly.</span>
<span class="sd">        stride (Union(int, tuple[int])): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the depth, height and width of movement are both strides, or a tuple of three int numbers that</span>
<span class="sd">            represent depth, height and width of movement respectively. Default: 1.</span>
<span class="sd">        dilation (Union(int, tuple[int])): Specifies the space to use between kernel elements. Default: 1.</span>
<span class="sd">        group (int): Splits input into groups. Default: 1. Only 1 is currently supported.</span>
<span class="sd">        output_padding (Union(int, tuple[int])): Add extra size to each dimension of the output. Default: 0.</span>


<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the gradients with respect to the input of convolution 3D.</span>
<span class="sd">        Tensor of shape :math:`(N, C_{out}//group, D_{out}, H_{out}, W_{out})`,</span>
<span class="sd">        where :math:`group` is the Args parameter.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `group` is not an int.</span>
<span class="sd">        TypeError: If `stride`, `padding` , `dilation` or `output_padding` is neither an int not a tuple.</span>
<span class="sd">        ValueError: If the rank of `inputs`, `weight` is not equal to 5.</span>
<span class="sd">        ValueError: If `stride` or `dilation` is less than 1.</span>
<span class="sd">        ValueError: if inputs[1], weight[1] and weight[2:5] i.e. `in_channel`, `out_channel` and `kernel_size` is less</span>
<span class="sd">                    than 1.</span>
<span class="sd">        ValueError: If `padding` is less than 0.</span>
<span class="sd">        ValueError: If `pad_mode` is not one of &#39;same&#39;, &#39;valid&#39; nor &#39;pad&#39;.</span>
<span class="sd">        ValueError: If `padding` is a tuple whose length is not equal to 6.</span>
<span class="sd">        ValueError: If `pad_mode` is not equal to &#39;padding&#39; and `padding` is not equal to (0, 0, 0, 0, 0, 0).</span>
<span class="sd">        ValueError: If `data_format` is not &#39;NCDHW&#39;.</span>
<span class="sd">        TypeError: If data type of dout and weight is not float16.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; dout = Tensor(np.ones([32, 16, 10, 32, 32]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([16, 3, 4, 6, 2]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; output = conv3d_transpose(dout, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (32, 3, 13, 37, 33)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">5</span><span class="p">:</span>
        <span class="n">raise_value_error</span><span class="p">(</span><span class="s2">&quot;the rank of inputs tensor should be 5.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">5</span><span class="p">:</span>
        <span class="n">raise_value_error</span><span class="p">(</span><span class="s2">&quot;the rank of weight tensor should be 5.&quot;</span><span class="p">)</span>
    <span class="n">in_channel</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">out_channel</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">5</span><span class="p">]</span>
    <span class="n">_conv_3d_transpose</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NN_OPS</span><span class="o">.</span><span class="n">Conv3DTranspose</span><span class="p">)(</span><span class="n">in_channel</span><span class="p">,</span> <span class="n">out_channel</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span>
                                                                 <span class="n">padding</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="n">output_padding</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_conv_3d_transpose</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>


<div class="viewcode-block" id="conv2d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.conv2d.html#mindspore.ops.conv2d">[文档]</a><span class="k">def</span> <span class="nf">conv2d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    2D convolution layer.</span>

<span class="sd">    Applies a 2D convolution over an input tensor which is typically of shape :math:`(N, C_{in}, H_{in}, W_{in})`,</span>
<span class="sd">    where :math:`N` is batch size, :math:`C` is channel number, :math:`H` is height, :math:`W` is width, :math:`X_i` is</span>
<span class="sd">    the :math:`i^{th}` input value and :math:`b_i` indicates the deviation value of the :math:`i^{th}` input value.</span>
<span class="sd">    For each batch of shape :math:`(C_{in}, H_{in}, W_{in})`, the formula is defined as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_j = \sum_{i=0}^{C_{in} - 1} ccor(W_{ij}, X_i) + b_j,</span>

<span class="sd">    where :math:`ccor` is the cross correlation operator, :math:`C_{in}` is the input channel number, :math:`j` ranges</span>
<span class="sd">    from :math:`0` to :math:`C_{out} - 1`, :math:`W_{ij}` corresponds to the :math:`i`-th channel of the :math:`j`-th</span>
<span class="sd">    filter and :math:`out_{j}` corresponds to the :math:`j`-th channel of the output. :math:`W_{ij}` is a slice</span>
<span class="sd">    of kernel and it has shape :math:`(\text{kernel_size[0]}, \text{kernel_size[1]})`,where :math:`\text{</span>
<span class="sd">    kernel_size[0]}` and :math:`\text{kernel_size[1]}` are the height and width of the convolution kernel.</span>
<span class="sd">    The full kernel has shape :math:`(C_{out}, C_{in} / \text{group}, \text{kernel_size[0]}, \text{kernel_size[1]})`,</span>
<span class="sd">    where group is the group number to split the input in the channel dimension.</span>

<span class="sd">    If the &#39;pad_mode&#39; is set to be &quot;valid&quot;, the output height and width will be</span>
<span class="sd">    :math:`\left \lfloor{</span>
<span class="sd">    1 + \frac{H_{in} + \text{padding[0]} + \text{padding[1]} - \text{kernel_size[0]} -</span>
<span class="sd">    (\text{kernel_size[0]} - 1) \times(\text{dilation[0]} - 1)} {\text { stride[0] }}} \right \rfloor` and</span>

<span class="sd">    :math:`\left \lfloor{</span>
<span class="sd">    1 + \frac{W_{in} + \text{padding[2]} + \text{padding[3]} - \text{kernel_size[1]} -</span>
<span class="sd">    (\text{kernel_size[1]} - 1) \times(\text{dilation[1]} - 1)} {\text { stride[1] }}} \right \rfloor` respectively.</span>

<span class="sd">    Where :math:`dilation` is Spacing between kernel elements, :math:`stride` is The step length of each step,</span>
<span class="sd">    :math:`padding` is zero-padding added to both sides of the input.</span>

<span class="sd">    The first introduction can be found in paper `Gradient Based Learning Applied to Document Recognition</span>
<span class="sd">    &lt;http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf&gt;`_. More detailed introduction can be found here:</span>
<span class="sd">    http://cs231n.github.io/convolutional-networks/.</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs (Tensor): Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.</span>
<span class="sd">        weight (Tensor): Set size of kernel is :math:`(\text{kernel_size[0]}, \text{kernel_size[1]})`,</span>
<span class="sd">            then the shape is :math:`(C_{out}, C_{in}, \text{kernel_size[0]}, \text{kernel_size[1]})`.</span>
<span class="sd">        pad_mode (str): Specifies padding mode. The optional values are</span>
<span class="sd">            &quot;same&quot;, &quot;valid&quot; and &quot;pad&quot;. Default: &quot;valid&quot;.</span>

<span class="sd">            - same: Adopts the way of completion. The height and width of the output will be equal to</span>
<span class="sd">              the input `x` divided by stride. The padding will be evenly calculated in top and bottom,</span>
<span class="sd">              left and right possibility.</span>
<span class="sd">              Otherwise, the last extra padding will be calculated from the bottom and the right side.</span>
<span class="sd">              If this mode is set, `padding` must be 0.</span>

<span class="sd">            - valid: Adopts the way of discarding. The possible largest height and width of output will be returned</span>
<span class="sd">              without padding. Extra pixels will be discarded. If this mode is set, `padding` must be 0.</span>

<span class="sd">            - pad: Implicit paddings on both sides of the input `x`. The number of `padding` will be padded to the input</span>
<span class="sd">              Tensor borders. `padding` must be greater than or equal to 0.</span>
<span class="sd">        padding (Union(int, tuple[int])): Implicit paddings on both sides of the input `x`. If `padding` is one integer,</span>
<span class="sd">                    the paddings of top, bottom, left and right are the same, equal to padding. If `padding` is a tuple</span>
<span class="sd">                    with four integers, the paddings of top, bottom, left and right will be equal to padding[0],</span>
<span class="sd">                    padding[1], padding[2], and padding[3] accordingly. Default: 0.</span>
<span class="sd">        stride (Union(int, tuple[int])): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the height and width of movement are both strides, or a tuple of two int numbers that</span>
<span class="sd">            represent height and width of movement respectively. Default: 1.</span>
<span class="sd">        dilation (Union(int, tuple[int])): The data type is int or a tuple of 2 integers. Specifies the dilation rate</span>
<span class="sd">                                      to use for dilated convolution. If set to be :math:`k &gt; 1`, there will</span>
<span class="sd">                                      be :math:`k - 1` pixels skipped for each sampling location. Its value must</span>
<span class="sd">                                      be greater than or equal to 1 and bounded by the height and width of the</span>
<span class="sd">                                      input `x`. Default: 1.</span>
<span class="sd">        group (int): Splits inputs into groups. Default: 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the value that applied 2D convolution. The shape is :math:`(N, C_{out}, H_{out}, W_{out})`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `stride`, `padding` or `dilation` is neither an int nor a tuple.</span>
<span class="sd">        TypeError: If `out_channel` or `group` is not an int.</span>
<span class="sd">        ValueError: If `stride` or `dilation` is less than 1.</span>
<span class="sd">        ValueError: If `pad_mode` is not one of &#39;same&#39;, &#39;valid&#39; or &#39;pad&#39;.</span>
<span class="sd">        ValueError: If `padding` is a tuple whose length is not equal to 4.</span>
<span class="sd">        ValueError: If `pad_mode` it not equal to &#39;pad&#39; and `padding` is not equal to (0, 0, 0, 0).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([10, 32, 32, 32]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([32, 32, 3, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.conv2d(x, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (10, 32, 30, 30)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">weight_shape</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">out_channel</span> <span class="o">=</span> <span class="n">weight_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">weight_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">4</span><span class="p">]</span>
    <span class="n">conv</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">)(</span><span class="n">out_channel</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="s2">&quot;NCHW&quot;</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">conv</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span></div>


<span class="k">def</span> <span class="nf">hardsigmoid</span><span class="p">(</span><span class="n">input_x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hard sigmoid activation function.</span>

<span class="sd">    Applies hard sigmoid activation element-wise. The input is a Tensor with any valid shape.</span>

<span class="sd">    Hard sigmoid is defined as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{hsigmoid}(x_{i}) = max(0, min(1, \frac{x_{i} + 3}{6})),</span>

<span class="sd">    where :math:`x_i` is an element of the input Tensor.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor of shape :math:`(*)`, where :math:`*` means any number of</span>
<span class="sd">          dimensions, with float16, float32 or float64 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        A Tensor whose dtype and shape are the same as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input_x` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([ -3.5,  0,  4.3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = F.hardsigmoid(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.  0.5 1. ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">hardsigmoid_</span> <span class="o">=</span> <span class="n">NN_OPS</span><span class="o">.</span><span class="n">HSigmoid</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">hardsigmoid_</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">adaptive_avg_pool1d</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    1D adaptive average pooling for temporal data.</span>

<span class="sd">    Applies a 1D adaptive average pooling over an input Tensor which can be regarded as a composition of 1D input</span>
<span class="sd">    planes.</span>

<span class="sd">    Typically, the input is of shape :math:`(N_{in}, C_{in}, L_{in})`, adaptive_avg_pool1d outputs regional average</span>
<span class="sd">    in the :math:`L_{in}`-dimension. The output is of shape :math:`(N_{in}, C_{in}, L_{out})`, where :math:`L_{out}`</span>
<span class="sd">    is defined by `output_size`.</span>

<span class="sd">    Note:</span>
<span class="sd">        :math:`L_{in}` must be divisible by `output_size`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Tensor of shape :math:`(N, C_{in}, L_{in})`, with float16 or float32 data type.</span>
<span class="sd">        output_size (int): the target output size :math:`L_{out}`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor of shape :math:`(N, C_{in}, L_{out})`, has the same type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `output_size` is not an int.</span>
<span class="sd">        TypeError: If `input_x` is neither float16 nor float32.</span>
<span class="sd">        ValueError: If `output_size` is less than 1.</span>
<span class="sd">        ValueError: If length of shape of `input_x` is not equal to 3.</span>
<span class="sd">        ValueError: If the last dimension of `input_x` is smaller than `output_size`.</span>
<span class="sd">        ValueError: If the last dimension of `input_x` is not divisible by `output_size`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.random.randint(0, 10, [1, 3, 6]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.adaptive_avg_pool1d(input_x, output_size=2)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 3, 2)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For adaptive_avg_pool1d, the input input_x must be tensor&quot;</span><span class="p">)</span>

    <span class="n">x_in_shape</span> <span class="o">=</span> <span class="n">input_x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">x_dtype</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">)()(</span><span class="n">input_x</span><span class="p">)</span>

    <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s2">&quot;output_size&quot;</span><span class="p">,</span> <span class="s1">&#39;adaptive_avg_pool1d&#39;</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;output_size&#39;</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="s1">&#39;adaptive_avg_pool1d&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_in_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For adaptive_avg_pool1d input must have 3 dim, but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_in_shape</span><span class="p">)))</span>
    <span class="k">if</span> <span class="n">x_in_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">output_size</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For adaptive_avg_pool1d input&#39;s last dimension must be greater or equal to &quot;</span>
                         <span class="s2">&quot;output size </span><span class="si">{}</span><span class="s2">, but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">x_in_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
    <span class="k">if</span> <span class="n">x_in_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">%</span> <span class="n">output_size</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For adaptive_avg_pool1d input&#39;s last dimension must be divisible by &quot;</span>
                         <span class="s2">&quot;output size </span><span class="si">{}</span><span class="s2">, but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">x_in_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
    <span class="k">if</span> <span class="n">x_dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For adaptive_avg_pool1d, the input_x dtype must be float16 or float32, &quot;</span>
                        <span class="s2">&quot;but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x_dtype</span><span class="p">))</span>

    <span class="n">expand_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">)()</span>
    <span class="n">squeeze_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Squeeze</span><span class="p">)(</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">width</span> <span class="o">=</span> <span class="n">x_in_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">width</span> <span class="o">//</span> <span class="n">output_size</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">width</span> <span class="o">-</span> <span class="p">(</span><span class="n">output_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">width</span> <span class="o">//</span> <span class="n">output_size</span><span class="p">)</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">)</span>

    <span class="n">avg_pool_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">AvgPool</span><span class="p">)(</span><span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="n">stride</span><span class="p">)</span>

    <span class="n">input_x</span> <span class="o">=</span> <span class="n">expand_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">input_x</span> <span class="o">=</span> <span class="n">avg_pool_</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
    <span class="n">input_x</span> <span class="o">=</span> <span class="n">squeeze_</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">input_x</span>


<span class="k">def</span> <span class="nf">adaptive_max_pool1d</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    1D adaptive maximum pooling for temporal data.</span>

<span class="sd">    Applies a 1D adaptive maximum pooling over an input Tensor which can be regarded as</span>
<span class="sd">    a composition of 1D input planes.</span>

<span class="sd">    Typically, the input is of shape :math:`(N_{in}, C_{in}, L_{in})`,</span>
<span class="sd">    adaptive_max_pool1d outputs regional maximum in the :math:`L_{in}`-dimension. The output is of</span>
<span class="sd">    shape :math:`(N_{in}, C_{in}, L_{out})`, where :math:`L_{out}` is defined by `output_size`.</span>

<span class="sd">    Note:</span>
<span class="sd">        :math:`L_{in}` must be divisible by `output_size`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Tensor of shape :math:`(N, C_{in}, L_{in})`, with float16 or float32 data type.</span>
<span class="sd">        output_size (int): the target output size :math:`L_{out}`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor of shape :math:`(N, C_{in}, L_{out})`, has the same type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `output_size` is not an int.</span>
<span class="sd">        ValueError: If `output_size` is less than 1.</span>
<span class="sd">        ValueError: If the last dimension of `input_x` is smaller than `output_size`.</span>
<span class="sd">        ValueError: If the last dimension of `input_x` is not divisible by `output_size`.</span>
<span class="sd">        ValueError: If length of shape of `input_x` is not equal to 3.</span>


<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.random.randint(0, 10, [1, 3, 6]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.adaptive_max_pool1d(input_x, output_size=2)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 3, 2)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For adaptive_max_pool1d, the input input_x must be tensor&quot;</span><span class="p">)</span>

    <span class="n">x_in_shape</span> <span class="o">=</span> <span class="n">input_x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">x_dtype</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">)()(</span><span class="n">input_x</span><span class="p">)</span>

    <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s2">&quot;output_size&quot;</span><span class="p">,</span> <span class="s1">&#39;adaptive_max_pool1d&#39;</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;output_size&#39;</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="s1">&#39;adaptive_max_pool1d&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_in_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For adaptive_max_pool1d input must have 3 dim, but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_in_shape</span><span class="p">)))</span>
    <span class="k">if</span> <span class="n">x_in_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">output_size</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For adaptive_max_pool1d input&#39;s last dimension must be greater or equal to &quot;</span>
                         <span class="s2">&quot;output size </span><span class="si">{}</span><span class="s2">, but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">x_in_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
    <span class="k">if</span> <span class="n">x_in_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">%</span> <span class="n">output_size</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For adaptive_max_pool1d input&#39;s last dimension must be divisible by &quot;</span>
                         <span class="s2">&quot;output size </span><span class="si">{}</span><span class="s2">, but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">x_in_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
    <span class="k">if</span> <span class="n">x_dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For adaptive_max_pool1d, the input_x dtype must be float16 or float32, &quot;</span>
                        <span class="s2">&quot;but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x_dtype</span><span class="p">))</span>

    <span class="n">expand_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">)()</span>
    <span class="n">squeeze_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Squeeze</span><span class="p">)(</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">width</span> <span class="o">=</span> <span class="n">x_in_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">width</span> <span class="o">//</span> <span class="n">output_size</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">width</span> <span class="o">-</span> <span class="p">(</span><span class="n">output_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride</span>

    <span class="n">stride</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">width</span> <span class="o">//</span> <span class="n">output_size</span><span class="p">)</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">)</span>

    <span class="n">max_pool_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">MaxPool</span><span class="p">)(</span><span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="n">stride</span><span class="p">)</span>
    <span class="n">input_x</span> <span class="o">=</span> <span class="n">expand_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">input_x</span> <span class="o">=</span> <span class="n">max_pool_</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
    <span class="n">input_x</span> <span class="o">=</span> <span class="n">squeeze_</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">input_x</span>


<span class="k">def</span> <span class="nf">batch_norm</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">running_mean</span><span class="p">,</span> <span class="n">running_var</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Batch Normalization for input data and updated parameters.</span>

<span class="sd">    Batch Normalization is widely used in convolutional neural networks. This operation</span>
<span class="sd">    applies Batch Normalization over inputs to avoid internal covariate shift as described</span>
<span class="sd">    in the paper `Batch Normalization: Accelerating Deep Network Training by Reducing Internal</span>
<span class="sd">    Covariate Shift &lt;https://arxiv.org/abs/1502.03167&gt;`_. It rescales and recenters the</span>
<span class="sd">    features using a mini-batch of data and the learned parameters can be described</span>
<span class="sd">    in the following formula,</span>

<span class="sd">    .. math::</span>

<span class="sd">        y = \frac{x - mean}{\sqrt{variance + \epsilon}} * \gamma + \beta</span>

<span class="sd">    where :math:`\gamma` is `weight`, :math:`\beta` is `bias`, :math:`\epsilon` is `eps`, :math:`mean` is the</span>
<span class="sd">    mean of `input_x`, :math:`variance` is the variance of `input_x`.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - For Ascend 310, the result accuracy fails to reach 1‰ due to the square root instruction.</span>

<span class="sd">    .. note::</span>
<span class="sd">        - If `training` is `False`, `weight`, `bias`, `running_mean` and `running_var` are Tensors.</span>
<span class="sd">        - If `training` is `True`, `weight`, `bias`, `running_mean` and `running_var` are Parameters.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Tensor of shape :math:`(N, C)`, with float16 or float32 data type.</span>
<span class="sd">        running_mean (Union[Tensor, Parameter]): The shape :math:`(C,)`, has the same data type with `weight`.</span>
<span class="sd">        running_var (Union[Tensor, Parameter]): The shape :math:`(C,)`, has the same data type with `weight`.</span>
<span class="sd">        weight (Union[Tensor, Parameter]): The shape :math:`(C,)`, with float16 or float32 data type.</span>
<span class="sd">        bias (Union[Tensor, Parameter]): The shape :math:`(C,)`, has the same data type with `weight`.</span>
<span class="sd">        training (bool): If `training` is `True`, `mean` and `variance` are computed during training.</span>
<span class="sd">            If `training` is `False`, they&#39;re loaded from checkpoint during inference. Default: False.</span>
<span class="sd">        momentum (float): The hyper parameter to compute moving average for `running_mean` and `running_var`</span>
<span class="sd">            (e.g. :math:`new\_running\_mean = (1 - momentum) * running\_mean + momentum * current\_mean`).</span>
<span class="sd">            Momentum value must be `[0, 1]`. Default: 0.1.</span>
<span class="sd">        eps (float): A small value added for numerical stability. Default: 1e-5.</span>

<span class="sd">    Returns:</span>
<span class="sd">        output_x (Tensor) - The same type and shape as the `input_x`. The shape is :math:`(N, C)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `training` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `eps` or `momentum` is not float.</span>
<span class="sd">        TypeError: If `input_x`, `weight`, `bias`, `running_mean` or `running_var` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input_x`, `weight` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones([2, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; running_mean = Tensor(np.ones([2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; running_var = Tensor(np.ones([2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; bias = Tensor(np.ones([2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.batch_norm(input_x, running_mean, running_var, weight, bias)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 1.]</span>
<span class="sd">         [1. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch_norm_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">)(</span><span class="n">is_training</span><span class="o">=</span><span class="n">training</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">batch_norm_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">running_mean</span><span class="p">,</span> <span class="n">running_var</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>


<div class="viewcode-block" id="bias_add"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bias_add.html#mindspore.ops.bias_add">[文档]</a><span class="k">def</span> <span class="nf">bias_add</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the sum of the `input_x` and the `bias` Tensor. Before adding, the `bias` Tensor will be broadcasted to be</span>
<span class="sd">    consistent with the shape of the `input_x` Tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The input tensor. The shape can be 2-5 dimensions.</span>
<span class="sd">            The data type should be float16 or float32.</span>
<span class="sd">        bias (Tensor): The bias tensor, with shape :math:`(C)`. C must be the same as channel dimension C of</span>
<span class="sd">            `input_x`. The data type should be float16 or float32.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same shape and data type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` or `bias` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input_x` or `bias` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `input_x` or `bias` is inconsistent.</span>
<span class="sd">        TypeError: If dimension of `input_x` is not in the range [2, 5].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.arange(6).reshape((2, 3)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; bias = Tensor(np.random.random(3).reshape((3)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bias_add(input_x, bias)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 3)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">bias_add_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">BiasAdd</span><span class="p">)(</span><span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">bias_add_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">binary_cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the binary cross entropy between the logits and the labels.</span>

<span class="sd">    Sets `logits` as :math:`x`, `labels` as :math:`y`, output as :math:`\ell(x, y)`.</span>
<span class="sd">    Let,</span>

<span class="sd">    .. math::</span>
<span class="sd">        L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">        l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right]</span>

<span class="sd">    In which, :math:`L` indicates the loss of all `batch_size`, :math:`l` indicates the loss of one `batch_size`,</span>
<span class="sd">    and :math:`n` indicates one `batch_size` in the :math:`1-N` range. Then,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = \begin{cases}</span>
<span class="sd">        L, &amp; \text{if reduction} = \text{&#39;none&#39;;}\\</span>
<span class="sd">        \operatorname{mean}(L), &amp; \text{if reduction} = \text{&#39;mean&#39;;}\\</span>
<span class="sd">        \operatorname{sum}(L),  &amp; \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - The value of `logits` must range from `0` to `l`.</span>
<span class="sd">        - The value of `labels` must be `0` or `l`.</span>

<span class="sd">    Args:</span>
<span class="sd">        logits (Tensor): The input Tensor. The data type must be float16 or float32,</span>
<span class="sd">            The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        labels (Tensor): The label Tensor which has the same shape and data type as `logits`.</span>
<span class="sd">        weight (Tensor, optional): A rescaling weight applied to the loss of each batch element.</span>
<span class="sd">            And it must have the same shape and data type as `logits`. Default: None.</span>
<span class="sd">        reduction (str): Specifies the reduction to be applied to the output.</span>
<span class="sd">            Its value must be one of &#39;none&#39;, &#39;mean&#39; or &#39;sum&#39;. Default: &#39;mean&#39;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype as `logits`. if `reduction` is &#39;none&#39;, then it has the same shape as `logits`.</span>
<span class="sd">        Otherwise, it is a scalar Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `logits`, `labels` or `weight` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `logits`, `labels` or `weight` (if given) is neither float16 nor float32.</span>
<span class="sd">        ValueError: If `reduction` is not one of &#39;none&#39;, &#39;mean&#39; or &#39;sum&#39;.</span>
<span class="sd">        ValueError: If shape of `labels` is not the same as `logits` or `weight` (if given).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([0.2, 0.7, 0.1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([0., 1., 0.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.array([1, 2, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.binary_cross_entropy(logits, labels, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.38240486</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">binary_cross_entropy_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">BinaryCrossEntropy</span><span class="p">)(</span><span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">binary_cross_entropy_op</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>


<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;adaptive_avg_pool1d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;adaptive_avg_pool2d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;adaptive_avg_pool3d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;adaptive_max_pool1d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;adaptive_max_pool3d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;avg_pool2d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;batch_norm&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bias_add&#39;</span><span class="p">,</span>
    <span class="s1">&#39;binary_cross_entropy&#39;</span><span class="p">,</span>
    <span class="s1">&#39;binary_cross_entropy_with_logits&#39;</span><span class="p">,</span>
    <span class="s1">&#39;max_pool3d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;kl_div&#39;</span><span class="p">,</span>
    <span class="s1">&#39;celu&#39;</span><span class="p">,</span>
    <span class="s1">&#39;deformable_conv2d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;dropout2d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;dropout3d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;fast_gelu&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hardshrink&#39;</span><span class="p">,</span>
    <span class="s1">&#39;soft_shrink&#39;</span><span class="p">,</span>
    <span class="s1">&#39;intopk&#39;</span><span class="p">,</span>
    <span class="s1">&#39;interpolate&#39;</span><span class="p">,</span>
    <span class="s1">&#39;log_softmax&#39;</span><span class="p">,</span>
    <span class="s1">&#39;mish&#39;</span><span class="p">,</span>
    <span class="s1">&#39;lrn&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hardswish&#39;</span><span class="p">,</span>
    <span class="s1">&#39;softsign&#39;</span><span class="p">,</span>
    <span class="s1">&#39;selu&#39;</span><span class="p">,</span>
    <span class="s1">&#39;softmax&#39;</span><span class="p">,</span>
    <span class="s1">&#39;pdist&#39;</span><span class="p">,</span>
    <span class="s1">&#39;pad&#39;</span><span class="p">,</span>
    <span class="s1">&#39;prelu&#39;</span><span class="p">,</span>
    <span class="s1">&#39;mirror_pad&#39;</span><span class="p">,</span>
    <span class="s1">&#39;cross_entropy&#39;</span><span class="p">,</span>
    <span class="s1">&#39;grid_sample&#39;</span><span class="p">,</span>
    <span class="s1">&#39;smooth_l1_loss&#39;</span><span class="p">,</span>
    <span class="s1">&#39;nll_loss&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ctc_loss&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ctc_greedy_decoder&#39;</span><span class="p">,</span>
    <span class="s1">&#39;dropout&#39;</span><span class="p">,</span>
    <span class="s1">&#39;conv3d_transpose&#39;</span><span class="p">,</span>
    <span class="s1">&#39;conv2d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;sigmoid&#39;</span><span class="p">,</span>
    <span class="s1">&#39;relu&#39;</span><span class="p">,</span>
    <span class="s1">&#39;relu6&#39;</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">__all__</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>
</pre></div>

           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
</body>
</html>